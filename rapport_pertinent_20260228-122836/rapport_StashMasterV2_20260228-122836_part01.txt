===== RAPPORT PERTINENT : F:\StashMasterV2 =====
Fichiers inclus (extensions): .bat, .cfg, .conf, .csv, .env, .ini, .json, .jsonc, .md, .ps1, .py, .rst, .toml, .txt, .yaml, .yml
Toujours inclus (noms): .editorconfig, .gitignore, Dockerfile, LICENSE, Makefile, Pipfile, Pipfile.lock, README, README.md, pyproject.toml, requirements-dev.txt, requirements.txt, setup.cfg, setup.py
Dossiers exclus: .cache, .coverage, .env, .git, .idea, .mypy_cache, .pytest_cache, .ruff_cache, .venv, .vs, .vscode, __pycache__, backup, backups, build, dist, env, log, logs, node_modules, sauvegarde, sauvegardes, site-packages, temp, tmp, venv
Extensions exclues: .7z, .avi, .bmp, .bz2, .db, .dll, .dylib, .exe, .gif, .gz, .ico, .jpeg, .jpg, .log, .mdb, .mkv, .mov, .mp3, .mp4, .otf, .png, .pyd, .rar, .so, .sqlite, .svg, .ttf, .wav, .webp, .woff, .woff2, .xz, .zip

===== ARBORESCENCE FILTR√âE =====
F:\StashMasterV2
+--- config
+--- data
|   +--- performers
|   \--- README.md
+--- gui
|   +--- dvd_frame.py
|   +--- performer_frame.py
|   +--- scene_frame.py
|   \--- url_verification_dialog.py
+--- Legacy
|   +--- config
|   |   +--- __init__.py
|   |   \--- settings.yaml
|   +--- files
|   |   \--- stashmaster-v2
|   |       \--- stashmaster-v2
|   |           +--- data
|   |           |   +--- performers
|   |           |   \--- README.md
|   |           +--- .gitignore
|   |           +--- CHANGELOG.md
|   |           +--- config.json
|   |           +--- CONTRIBUTING.md
|   |           +--- EXAMPLES.md
|   |           +--- README.md
|   |           +--- requirements.txt
|   |           +--- scrapers.py
|   |           +--- stashmaster_unified.py
|   |           \--- test_stashmaster.py
|   +--- gui
|   |   +--- files
|   |   +--- __init__.py
|   |   +--- app.py
|   |   +--- bio_studio_window.py
|   |   +--- bio_wizard.py
|   |   +--- data_review_window.py
|   |   +--- group_frame.py
|   |   +--- group_phase1.py
|   |   +--- group_phase2.py
|   |   +--- launcher.py
|   |   +--- performer_base.py
|   |   +--- performer_frame.py
|   |   +--- performer_phase1.py
|   |   +--- performer_phase2.py
|   |   +--- phase1_conflict_dialog.py
|   |   +--- phase2_field_wizard.py
|   |   +--- phase2_merge_dialog.py
|   |   \--- validation_window.py
|   +--- services
|   |   +--- extractors
|   |   |   +--- dvd
|   |   |   |   +--- __init__.py
|   |   |   |   +--- adultempire_dvd.py
|   |   |   |   +--- base_dvd.py
|   |   |   |   +--- data18_dvd.py
|   |   |   |   +--- iafd_dvd.py
|   |   |   |   \--- jeedoo_dvd.py
|   |   |   +--- __init__.py
|   |   |   +--- babepedia.py
|   |   |   +--- base.py
|   |   |   +--- freeones.py
|   |   |   +--- iafd.py
|   |   |   \--- thenude.py
|   |   +--- __init__.py
|   |   +--- bio_generator.py
|   |   +--- db.py
|   |   +--- group_phase1_merger.py
|   |   +--- group_phase1_scraper.py
|   |   +--- group_phase2_merger.py
|   |   +--- group_phase2_scraper.py
|   |   +--- phase1_merger.py
|   |   +--- phase2_merger.py
|   |   +--- phase2_scraper.py
|   |   \--- scrape_cache.py
|   +--- tests
|   |   +--- __init__.py
|   |   +--- test_db.py
|   |   \--- test_performer_fields.py
|   +--- urlscraping
|   |   +--- bridgette b - iafd.com_files
|   |   +--- Bridgette B bio _ Read about her profile at FreeOnes_files
|   |   \--- Bridgette B nude from Scoreland and Twistys at theNude.com_files
|   +--- utils
|   |   +--- __init__.py
|   |   +--- audit_markers.csv
|   |   +--- body_art_parser.py
|   |   +--- cleanup_all.py
|   |   +--- cleanup_specific.py
|   |   +--- customfield_utils.py
|   |   +--- duration.py
|   |   +--- list_short_markers.py
|   |   +--- marker.py
|   |   +--- meta_tag_utils.py
|   |   +--- short_markers.csv
|   |   \--- url_cleaner.py
|   +--- .gitignore
|   +--- check_db.py
|   +--- main.py
|   +--- README.md
|   +--- requirements.txt
|   \--- start.bat
+--- rapport_pertinent_20260228-122836
|   \--- rapport_StashMasterV2_20260228-122836_part01.txt
+--- services
|   +--- __init__.py
|   +--- bio_generator.py
|   +--- config_manager.py
|   +--- database.py
|   +--- scrapers.py
|   +--- source_finder.py
|   +--- url_manager.py
|   \--- url_validator.py
+--- utils
|   +--- awards_cleaner.py
|   +--- normalizer.py
|   +--- tag_engine.py
|   \--- url_utils.py
+--- .gitignore
+--- CHANGELOG.md
+--- check_db_145.py
+--- check_performer_145.py
+--- config.json
+--- CONTRIBUTING.md
+--- EXAMPLES.md
+--- inspect_custom_fields.py
+--- inspect_db.py
+--- main.py
+--- PROJET_COMPLET.md
+--- rapport_Ollama_20260228-103346.txt
+--- rapport_Ollama_20260228-105113.txt
+--- rapport_Ollama_20260228-114402.txt
+--- rapport_Ollama_20260228-115224.txt
+--- rapport_Ollama_20260228-115726.txt
+--- rapport_Ollama_20260228-115933.txt
+--- rapport_Ollama_20260228-120405.txt
+--- rapport_Ollama_20260228-121645.txt
+--- rapport_V1.txt
+--- read_docx.py
+--- README.md
+--- requirements.txt
+--- start.bat
+--- stashmaster_unified.py
+--- structure_bdd.md
+--- test_gui_load.py
+--- test_id145.py
+--- test_import.py
+--- test_stashmaster.py
+--- V2.md
\--- verify_stashmaster.py

===== CONTENU DES FICHIERS PERTINENTS (SECRETS MASQU√âS) =====
Total fichiers : 124
------------------------------------------------------------


============================================================
[1/124] .gitignore
------------------------------------------------------------
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
ENV/
env/
.venv

# IDEs
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store

# Data
data/
*.sqlite
*.db

# Logs
*.log

# Test
.coverage
.pytest_cache/
htmlcov/

# Configuration locale
config.local.json
*.local.*

# Temporary files
*.tmp
*.temp
.cache/


============================================================
[2/124] CHANGELOG.md
------------------------------------------------------------
# Changelog

Toutes les modifications notables du projet sont document√©es dans ce fichier.

## [2.0.0] - 2026-02-25

### Ajout√©
- ‚ú® **Interface unifi√©e** : Fusion compl√®te des Phase 1 et Phase 2 en une seule GUI
- üè∑Ô∏è **Syst√®me de tags intelligent** : G√©n√©ration automatique bas√©e sur des r√®gles m√©tadonn√©es
  - Tags bas√©s sur l'ethnicit√© (Caucasian, Latina, Asian, Ebony)
  - Tags bas√©s sur la couleur de cheveux (Blonde, Brunette, Redhead, Black Hair)
  - Tags bas√©s sur les mesures (Big Boobs, Small Boobs)
  - Tags pour piercings et tattoos
  - Tag MILF bas√© sur l'√¢ge de carri√®re
- üìù **Champs multilignes** pour Piercings, Tattoos et URLs
- ü™ü **Fen√™tre Trivia & Awards d√©di√©e** avec :
  - Scraping cibl√© depuis IAFD
  - Affichage s√©par√© des requ√™tes et r√©sultats
  - Nettoyage automatique des awards (1 par ligne)
- üìÑ **G√©n√©ration de bio automatique** avec 2 modes :
  - Bio Google : Template de 3000 caract√®res professionnel
  - Bio Ollama : IA locale avec prompt personnalis√©
- üîÑ **ScraperOrchestrator** : Scraping multi-sources avec fusion intelligente
- ‚úÖ **DataMerger** : D√©tection automatique des donn√©es confirm√©es et conflits
- üßπ **AwardsCleaner** : Formatage intelligent des awards
- üìä **Onglets organis√©s** : M√©tadonn√©es, Champs Avanc√©s, Bio

### Modifi√©
- üîß **Tags** : Ne sont plus scrap√©s, uniquement g√©n√©r√©s par r√®gles
- üìã **Interface** : Onglets au lieu de fen√™tres s√©par√©es
- üéØ **Workflow** : Simplifi√© et plus intuitif
- üíæ **Architecture** : Code modulaire avec s√©paration des responsabilit√©s

### Supprim√©
- ‚ùå Scraping de tags depuis les sources (remplac√© par g√©n√©ration automatique)
- ‚ùå Fen√™tres multiples (remplac√© par onglets)

### Technique
- üêç Python 3.8+ requis
- üì¶ D√©pendances : requests, beautifulsoup4, lxml
- ü§ñ Support optionnel d'Ollama pour g√©n√©ration IA
- üèóÔ∏è Architecture MVC am√©lior√©e

### Documentation
- üìñ README complet avec guide d'utilisation
- üéì Documentation des r√®gles de tags
- üí° Exemples et FAQ
- üõ†Ô∏è Guide de configuration avanc√©e

---

## [1.0.0] - Version Pr√©c√©dente

### Fonctionnalit√©s
- Interface Phase 1 : M√©tadonn√©es usuelles avec scraping
- Interface Phase 2 : Champs avanc√©s s√©par√©s
- Scraping basique depuis IAFD et autres sources
- Tags scrap√©s depuis les sources
- Bio manuelle

### Limitations
- Deux fen√™tres s√©par√©es
- Tags scrap√©s pas toujours coh√©rents
- Pas de g√©n√©ration automatique de bio
- Awards bruts non format√©s
- Workflow moins fluide

---

## √Ä venir

### [2.1.0] - Planifi√©
- [ ] Base de donn√©es SQLite int√©gr√©e
- [ ] Export vers Stash
- [ ] Import depuis fichiers JSON
- [ ] Historique des modifications
- [ ] Undo/Redo
- [ ] Raccourcis clavier
- [ ] Th√®mes dark/light
- [ ] Support multi-langues

### [2.2.0] - En r√©flexion
- [ ] Scraping d'images
- [ ] D√©tection automatique de doublons
- [ ] Suggestions intelligentes
- [ ] API REST pour int√©grations
- [ ] Plugin system
- [ ] Scraping de sc√®nes/films
- [ ] Statistiques et graphiques

---

**Format du Changelog** : [Keep a Changelog](https://keepachangelog.com/)  
**Versioning** : [Semantic Versioning](https://semver.org/)


============================================================
[3/124] check_db_145.py
------------------------------------------------------------
import sqlite3
import os
import sys

# Try multiple paths
paths = [
    r"H:\Stash\stash-go.sqlite",
    r"F:\Nouveau dossier\data\database.sqlite",
    "stash-go.sqlite"
]

DB_PATH = None
for p in paths:
    if os.path.exists(p):
        DB_PATH = p
        break

if not DB_PATH:
    print(f"Database not found in: {paths}")
    # Force creation of result file to signal failure
    with open("performer_145_data.txt", "w") as f:
        f.write("DB NOT FOUND")
    sys.exit(1)

print(f"Using DB: {DB_PATH}")

try:
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    cur = conn.cursor()
    
    cur.execute("SELECT * FROM performers WHERE id = ?", (145,))
    row = cur.fetchone()
    
    with open("performer_145_data.txt", "w", encoding="utf-8") as f:
        if row:
            f.write(f"--- Performer 145 Found ---\n")
            f.write(f"Name: {row['name']}\n")
            f.write(f"URLs: {row['urls']}\n")
            # Dump all fields just in case
            f.write("Full Data:\n")
            for key in row.keys():
                f.write(f"{key}: {row[key]}\n")
            print(f"Data written to performer_145_data.txt")
        else:
            f.write("Performer 145 NOT FOUND\n")
            print("Performer 145 NOT FOUND")
        
    conn.close()
except Exception as e:
    print(f"Error: {e}")
    with open("performer_145_data.txt", "w") as f:
        f.write(f"Error: {e}")



============================================================
[4/124] check_performer_145.py
------------------------------------------------------------
import sqlite3
import json

DB_PATH = "H:/Stash/stash-go.sqlite" # Assuming this is the path based on inspect_db.py
# If not found, try config
import os
if not os.path.exists(DB_PATH):
    # Try to load config
    try:
        from services.config_manager import ConfigManager
        config = ConfigManager()
        DB_PATH = config.get("database_path")
    except:
        DB_PATH = "data/database.sqlite"

print(f"Using DB: {DB_PATH}")

def get_performer(id):
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    cur = conn.cursor()
    
    # Check if table exists
    try:
        cur.execute("SELECT * FROM performers WHERE id = ?", (id,))
        row = cur.fetchone()
        if row:
            print(f"--- Performer {id} ---")
            for key in row.keys():
                print(f"{key}: {row[key]}")
        else:
            print(f"Performer {id} not found.")
    except Exception as e:
        print(f"Error: {e}")
    finally:
        conn.close()

if __name__ == "__main__":
    get_performer(145)


============================================================
[5/124] config.json
------------------------------------------------------------
{
  "scrapers": {
    "timeout": 10,
    "retry_count": 3,
    "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
  },
  "bio_generation": {
    "google_template_length": 3000,
    "ollama_url": "http://localhost:11434/api/generate",
    "ollama_path": "E:\\Ollama",
    "ollama_model": "dolphin-mistral:7b",
    "ollama_timeout": 120
  },
  "tag_rules": {
    "ethnicity_tags": {
      "caucasian": "Caucasian",
      "latin": "Latina",
      "cuban": "Latina",
      "asian": "Asian",
      "ebony": "Ebony",
      "african": "Ebony"
    },
    "hair_color_tags": {
      "blonde": "Blonde",
      "blond": "Blonde",
      "brown": "Brunette",
      "brunette": "Brunette",
      "red": "Redhead",
      "auburn": "Redhead",
      "black": "Black Hair"
    },
    "measurements_thresholds": {
      "big_boobs_min": 36,
      "small_boobs_max": 32
    },
    "career_length_thresholds": {
      "milf_years": 10
    }
  },
  "sources": {
    "iafd": {
      "enabled": true,
      "priority": 1
    },
    "freeones": {
      "enabled": true,
      "priority": 2
    },
    "babepedia": {
      "enabled": true,
      "priority": 3
    },
    "thenude": {
      "enabled": true,
      "priority": 4
    }
  },
  "ui": {
    "window_width": 1200,
    "window_height": 900,
    "theme": "default"
  },
  "data": {
    "performers_dir": "data/performers",
    "database_path": "data/database.sqlite"
  }
}

============================================================
[6/124] CONTRIBUTING.md
------------------------------------------------------------
# Guide de Contribution

Merci de votre int√©r√™t pour contribuer √† StashMaster V2 ! üéâ

## üìã Table des Mati√®res

- [Code de Conduite](#code-de-conduite)
- [Comment Contribuer](#comment-contribuer)
- [D√©veloppement](#d√©veloppement)
- [Standards de Code](#standards-de-code)
- [Tests](#tests)
- [Documentation](#documentation)

## ü§ù Code de Conduite

- Soyez respectueux envers tous les contributeurs
- Fournissez des critiques constructives
- Concentrez-vous sur ce qui est le mieux pour le projet
- Acceptez les feedbacks avec gr√¢ce

## üí° Comment Contribuer

### Rapporter des Bugs

Avant de cr√©er une issue :
1. V√©rifiez si le bug n'a pas d√©j√† √©t√© rapport√©
2. Utilisez la derni√®re version du code
3. Testez avec une installation propre

Pour rapporter un bug, incluez :
- **Description claire** du probl√®me
- **√âtapes pour reproduire** le bug
- **Comportement attendu** vs. comportement observ√©
- **Screenshots** si applicable
- **Environnement** : OS, version Python, d√©pendances
- **Logs d'erreur** si disponibles

### Sugg√©rer des Am√©liorations

Pour sugg√©rer une nouvelle fonctionnalit√© :
1. V√©rifiez si elle n'est pas d√©j√† planifi√©e (voir CHANGELOG)
2. Cr√©ez une issue avec le label "enhancement"
3. D√©crivez clairement :
   - Le probl√®me que √ßa r√©sout
   - Comment √ßa devrait fonctionner
   - Des exemples d'utilisation
   - Des alternatives consid√©r√©es

### Soumettre des Pull Requests

1. **Fork** le projet
2. **Cr√©ez une branche** pour votre fonctionnalit√©
   ```bash
   git checkout -b feature/ma-super-feature
   ```
3. **Committez** vos changements
   ```bash
   git commit -m "feat: ajout de ma super feature"
   ```
4. **Pushez** vers la branche
   ```bash
   git push origin feature/ma-super-feature
   ```
5. **Ouvrez une Pull Request**

## üõ†Ô∏è D√©veloppement

### Configuration de l'Environnement

```bash
# Cloner le repository
git clone https://github.com/votre-username/stashmaster-v2.git
cd stashmaster-v2

# Cr√©er un environnement virtuel
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer les d√©pendances
pip install -r requirements.txt

# Installer les d√©pendances de d√©veloppement (si disponibles)
pip install -r requirements-dev.txt
```

### Structure du Projet

```
stashmaster-v2/
‚îÇ
‚îú‚îÄ‚îÄ stashmaster_unified.py    # Application principale
‚îÇ   ‚îú‚îÄ‚îÄ MainWindow            # GUI principale
‚îÇ   ‚îú‚îÄ‚îÄ TriviaAwardsWindow    # Fen√™tre Trivia/Awards
‚îÇ   ‚îú‚îÄ‚îÄ BioGenerationWindow   # Fen√™tre g√©n√©ration de bio
‚îÇ   ‚îú‚îÄ‚îÄ TagRulesEngine        # Moteur de tags
‚îÇ   ‚îú‚îÄ‚îÄ AwardsCleaner         # Nettoyeur d'awards
‚îÇ   ‚îî‚îÄ‚îÄ BioGenerator          # G√©n√©rateur de bio
‚îÇ
‚îú‚îÄ‚îÄ scrapers.py               # Modules de scraping
‚îÇ   ‚îú‚îÄ‚îÄ ScraperBase           # Classe de base
‚îÇ   ‚îú‚îÄ‚îÄ IAFDScraper           # Scraper IAFD
‚îÇ   ‚îú‚îÄ‚îÄ FreeonesScraper       # Scraper Freeones
‚îÇ   ‚îú‚îÄ‚îÄ BabepaediaScraper     # Scraper Babepedia
‚îÇ   ‚îú‚îÄ‚îÄ TheNudeScraper        # Scraper TheNude
‚îÇ   ‚îú‚îÄ‚îÄ DataMerger            # Fusionneur de donn√©es
‚îÇ   ‚îî‚îÄ‚îÄ ScraperOrchestrator   # Orchestrateur
‚îÇ
‚îú‚îÄ‚îÄ test_stashmaster.py       # Tests unitaires
‚îú‚îÄ‚îÄ config.json               # Configuration
‚îú‚îÄ‚îÄ requirements.txt          # D√©pendances
‚îú‚îÄ‚îÄ README.md                 # Documentation
‚îú‚îÄ‚îÄ CHANGELOG.md              # Historique des versions
‚îî‚îÄ‚îÄ CONTRIBUTING.md           # Ce fichier
```

### Lancer l'Application en Mode D√©veloppement

```bash
# Mode normal
python3 stashmaster_unified.py

# Avec logs de debug (√† impl√©menter)
python3 stashmaster_unified.py --debug

# Avec un performer sp√©cifique (√† impl√©menter)
python3 stashmaster_unified.py --performer "Bridgette B"
```

## üìù Standards de Code

### Style Python

Suivez [PEP 8](https://www.python.org/dev/peps/pep-0008/) :

```python
# Bonnes pratiques
class MyClass:
    """Docstring pour la classe"""
    
    def my_method(self, param1: str, param2: int) -> bool:
        """Docstring pour la m√©thode
        
        Args:
            param1: Description du param√®tre 1
            param2: Description du param√®tre 2
            
        Returns:
            Description du retour
        """
        # Code ici
        return True

# Imports group√©s
import sys
import os
from typing import Dict, List

import requests
from bs4 import BeautifulSoup

from scrapers import IAFDScraper
```

### Nommage

- **Classes** : PascalCase (`TagRulesEngine`)
- **Fonctions/M√©thodes** : snake_case (`generate_tags`)
- **Constantes** : UPPER_CASE (`MAX_RETRIES`)
- **Variables priv√©es** : pr√©fixe `_` (`_internal_method`)

### Docstrings

Utilisez le format Google :

```python
def scrape_performer(self, url: str) -> Dict:
    """Scrape les donn√©es d'un performer.
    
    Args:
        url: L'URL de la page du performer
        
    Returns:
        Dictionnaire contenant les m√©tadonn√©es du performer
        
    Raises:
        ValueError: Si l'URL est invalide
        RequestException: Si le scraping √©choue
        
    Examples:
        >>> scraper.scrape_performer("https://example.com/performer")
        {'name': 'John Doe', 'birthdate': '1990-01-01'}
    """
    pass
```

### Type Hints

Utilisez les type hints pour am√©liorer la lisibilit√© :

```python
from typing import Dict, List, Optional, Tuple

def merge_data(self, sources: List[Dict]) -> Tuple[Dict, Dict]:
    """Fusionne les donn√©es de plusieurs sources"""
    pass

def get_performer(self, id: int) -> Optional[Dict]:
    """R√©cup√®re un performer par ID"""
    pass
```

## üß™ Tests

### Lancer les Tests

```bash
# Tous les tests
python3 test_stashmaster.py

# Tests sp√©cifiques
python3 -m unittest test_stashmaster.TestTagRulesEngine

# Avec couverture (si coverage install√©)
coverage run test_stashmaster.py
coverage report
```

### √âcrire des Tests

```python
import unittest

class TestMyFeature(unittest.TestCase):
    def setUp(self):
        """Pr√©paration avant chaque test"""
        self.engine = TagRulesEngine()
    
    def tearDown(self):
        """Nettoyage apr√®s chaque test"""
        pass
    
    def test_my_feature(self):
        """Test de ma fonctionnalit√©"""
        result = self.engine.generate_tags({})
        self.assertIsInstance(result, list)
        self.assertEqual(len(result), 0)
```

### Couverture de Tests

Visez au minimum :
- 80% de couverture pour le code principal
- 60% pour les scrapers (d√©pendent de sources externes)
- 100% pour les utilitaires critiques (TagRulesEngine, DataMerger)

## üìö Documentation

### Documenter le Code

- **Classes** : Docstring avec description, attributs
- **M√©thodes** : Docstring avec Args, Returns, Raises
- **Modules** : Docstring d'en-t√™te avec description g√©n√©rale

### Mettre √† Jour la Documentation

Lors de l'ajout de fonctionnalit√©s :
1. **README.md** : Ajouter dans la section correspondante
2. **CHANGELOG.md** : Documenter le changement
3. **Docstrings** : Commenter le code
4. **config.json** : Ajouter les nouvelles options

## üîÄ Workflow Git

### Branches

- `main` : Code stable, production
- `develop` : D√©veloppement en cours
- `feature/*` : Nouvelles fonctionnalit√©s
- `bugfix/*` : Corrections de bugs
- `hotfix/*` : Corrections urgentes

### Messages de Commit

Utilisez [Conventional Commits](https://www.conventionalcommits.org/) :

```bash
# Format
<type>(<scope>): <description>

[corps optionnel]

[footer(s) optionnel(s)]

# Exemples
feat(tags): ajout de la r√®gle MILF bas√©e sur l'√¢ge
fix(scraper): correction du parsing IAFD
docs(readme): mise √† jour des instructions d'installation
test(tags): ajout de tests pour les tags d'ethnicit√©
refactor(bio): am√©lioration de la g√©n√©ration Google
style(ui): correction de l'alignement des boutons
```

Types :
- `feat` : Nouvelle fonctionnalit√©
- `fix` : Correction de bug
- `docs` : Documentation
- `style` : Formatage (pas de changement de code)
- `refactor` : Refactoring
- `test` : Ajout de tests
- `chore` : Maintenance

## üéØ Priorit√©s de D√©veloppement

Consultez le [CHANGELOG.md](CHANGELOG.md) pour voir les fonctionnalit√©s planifi√©es.

### Court Terme (v2.1)
- Base de donn√©es SQLite
- Export vers Stash
- Import JSON
- Historique des modifications

### Moyen Terme (v2.2)
- Scraping d'images
- D√©tection de doublons
- API REST
- Plugin system

### Long Terme (v3.0)
- Interface web
- Multi-utilisateurs
- Synchronisation cloud
- Mobile app

## ‚ùì Questions ?

N'h√©sitez pas √† :
- Ouvrir une issue pour discuter
- Rejoindre les discussions
- Contacter les mainteneurs

## üôè Remerciements

Merci √† tous les contributeurs qui aident √† am√©liorer StashMaster V2 !

---

**Happy Coding!** üöÄ


============================================================
[7/124] data\README.md
------------------------------------------------------------
# Dossier Data

Ce dossier contient toutes les donn√©es sauvegard√©es par StashMaster V2.

## Structure

```
data/
‚îú‚îÄ‚îÄ performers/          # JSON files des performers
‚îÇ   ‚îú‚îÄ‚îÄ performer_1.json
‚îÇ   ‚îú‚îÄ‚îÄ performer_2.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ database.sqlite     # Base de donn√©es (futur)
```

## Format JSON des Performers

```json
{
  "name": "Performer Name",
  "aliases": ["Alias 1", "Alias 2"],
  "birthdate": "January 1, 1990",
  "birthplace": "City, Country",
  "ethnicity": "Caucasian",
  "hair_color": "Blonde",
  "eye_color": "Blue",
  "height": "170 cm",
  "weight": "55 kg",
  "measurements": "34DD-25-36",
  "tattoos": "Description of tattoos",
  "piercings": "Description of piercings",
  "career_length": "2010-",
  "tags": ["Tag1", "Tag2", "Tag3"],
  "urls": [
    "https://source1.com/...",
    "https://source2.com/..."
  ],
  "trivia": "Interesting facts...",
  "awards": "Awards and nominations...",
  "bio": "Full biography (3000 characters)..."
}
```

## Sauvegarde et Restauration

### Sauvegarde manuelle
Copiez simplement le dossier `data/` pour cr√©er une sauvegarde.

### Restauration
Remplacez le dossier `data/` par votre sauvegarde.

## Notes

- Les fichiers JSON sont encod√©s en UTF-8
- Les noms de fichiers sont en minuscules avec underscores
- La base de donn√©es SQLite sera ajout√©e dans une version future


============================================================
[8/124] EXAMPLES.md
------------------------------------------------------------
# Exemples d'Utilisation

Ce document contient des exemples pratiques pour utiliser StashMaster V2.

## üìã Exemples Basiques

### Exemple 1 : Scraping Simple

```python
# Dans un script Python
from scrapers import ScraperOrchestrator

# Cr√©er l'orchestrateur
orchestrator = ScraperOrchestrator()

# URLs √† scraper
urls = [
    "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
    "https://www.freeones.xxx/bridgette-b"
]

# Scraper et fusionner
confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)

# Afficher les r√©sultats
print(f"Donn√©es de {num_sources} source(s)")
print("\nConfirm√©es:")
for field, info in confirmed.items():
    print(f"  {field}: {info['value']}")

print("\nConflits:")
for field, values in conflicts.items():
    print(f"  {field}:")
    for v in values:
        print(f"    - {v['value']} ({', '.join(v['sources'])})")
```

### Exemple 2 : G√©n√©ration de Tags

```python
from stashmaster_unified import TagRulesEngine

# Cr√©er le moteur de r√®gles
engine = TagRulesEngine()

# M√©tadonn√©es d'exemple
metadata = {
    'ethnicity': 'Latina',
    'hair_color': 'Blonde',
    'measurements': '36DD-25-36',
    'piercings': 'Navel',
    'tattoos': 'Lower back',
    'career_length': '2007-'
}

# G√©n√©rer les tags
tags = engine.generate_tags(metadata)
print(f"Tags g√©n√©r√©s: {', '.join(tags)}")
# Output: Tags g√©n√©r√©s: Latina, Blonde, Big Boobs, Pierced, Tattooed, MILF
```

### Exemple 3 : Nettoyage d'Awards

```python
from stashmaster_unified import AwardsCleaner

cleaner = AwardsCleaner()

# Awards bruts
raw_awards = """
AVN AWARDS2012Winner: Unsung Starlet of the Year2014Nominee: Unsung Starlet of the Year
2015Nominee: Fan Award: Best Boobs
"""

# Nettoyer
cleaned = cleaner.clean_awards(raw_awards)
print(cleaned)
```

Output:
```
AVN AWARDS

2012
  Winner: Unsung Starlet of the Year

2014
  Nominee: Unsung Starlet of the Year

2015
  Nominee: Fan Award: Best Boobs
```

### Exemple 4 : G√©n√©ration de Bio

```python
from stashmaster_unified import BioGenerator

generator = BioGenerator()

# M√©tadonn√©es du performer
metadata = {
    'name': 'Bridgette B',
    'birthdate': 'October 15, 1983',
    'birthplace': 'Barcelona, Spain',
    'ethnicity': 'Caucasian',
    'hair_color': 'Blonde',
    'measurements': '34DD-27-34',
    'height': '173 cm',
    'weight': '129 lbs',
    'career_start': '2007',
    'aliases': ['Bridget B', 'Bridgette', 'Spanish Doll']
}

# G√©n√©rer bio Google
bio = generator.generate_google_bio('Bridgette B', metadata)
print(f"Bio g√©n√©r√©e ({len(bio)} caract√®res):\n{bio}")
```

## üîß Exemples Avanc√©s

### Exemple 5 : Fusion de Donn√©es Complexes

```python
from scrapers import DataMerger

merger = DataMerger()

# Donn√©es de 3 sources diff√©rentes
sources = [
    {
        'source': 'iafd',
        'name': 'Bridgette B',
        'birthdate': 'October 15, 1983',
        'ethnicity': 'Caucasian',
        'hair_color': 'Blonde',
        'measurements': '34DD-27-34'
    },
    {
        'source': 'freeones',
        'name': 'Bridgette B',
        'birthdate': 'October 15, 1983',
        'ethnicity': 'Caucasian',
        'hair_color': 'Blonde',  # Conflit
        'height': '173 cm'
    },
    {
        'source': 'babepedia',
        'name': 'Bridgette B',
        'birthdate': 'October 15, 1983',
        'ethnicity': 'Caucasian',
        'hair_color': 'Brown',  # Conflit
        'weight': '129 lbs'
    }
]

# Fusionner
confirmed, conflicts = merger.merge_data(sources)

print("=== Donn√©es Confirm√©es ===")
for field, info in confirmed.items():
    sources_str = ', '.join(info['sources'])
    print(f"{field}: {info['value']} ({info['count']} sources: {sources_str})")

print("\n=== Conflits ===")
for field, values in conflicts.items():
    print(f"\n{field}:")
    for v in values:
        print(f"  - {v['value']} ({v['count']} sources: {', '.join(v['sources'])})")
```

### Exemple 6 : Scraping avec Gestion d'Erreurs

```python
from scrapers import IAFDScraper
import requests

scraper = IAFDScraper()

urls = [
    "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
    "https://www.iafd.com/person.rme/perfid=invalid/gender=f/invalid.htm"
]

for url in urls:
    print(f"\nScraping: {url}")
    try:
        data = scraper.scrape_performer(url)
        if data:
            print(f"  ‚úÖ Succ√®s: {data.get('name', 'Unknown')}")
            print(f"  Champs: {len(data)}")
        else:
            print("  ‚ùå √âchec: Aucune donn√©e retourn√©e")
    except requests.RequestException as e:
        print(f"  ‚ùå Erreur r√©seau: {e}")
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
```

### Exemple 7 : Cr√©ation d'un Scraper Personnalis√©

```python
from scrapers import ScraperBase
from typing import Dict

class MonSiteScraper(ScraperBase):
    """Scraper pour mon site personnalis√©"""
    
    def scrape_performer(self, url: str) -> Dict:
        """Scrape un performer depuis mon site"""
        soup = self.get_page(url)
        if not soup:
            return {}
        
        data = {
            'source': 'monsite',
            'url': url
        }
        
        try:
            # Extraire le nom
            name_elem = soup.find('h1', class_='performer-name')
            if name_elem:
                data['name'] = name_elem.text.strip()
            
            # Extraire la date de naissance
            birthday_elem = soup.find('span', class_='birthday')
            if birthday_elem:
                data['birthdate'] = birthday_elem.text.strip()
            
            # Ajouter d'autres extractions...
            
        except Exception as e:
            print(f"Erreur: {e}")
        
        return data

# Utilisation
scraper = MonSiteScraper()
data = scraper.scrape_performer("https://monsite.com/performer/123")
print(data)
```

### Exemple 8 : Int√©gration avec l'Interface

```python
# Dans votre propre script
from stashmaster_unified import MainWindow
import tkinter as tk

# Cr√©er et configurer la fen√™tre
app = MainWindow()

# Pr√©-remplir des donn√©es (exemple)
app.metadata_entries['name'].insert(0, "Bridgette B")
app.urls_text.insert('1.0', "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm")

# Lancer l'application
app.mainloop()
```

## üéì Cas d'Usage R√©els

### Cas 1 : Workflow Complet Automatis√©

```python
#!/usr/bin/env python3
"""
Workflow automatis√© complet pour un performer
"""

from scrapers import ScraperOrchestrator
from stashmaster_unified import TagRulesEngine, BioGenerator
import json

def process_performer(name: str, urls: list) -> dict:
    """Traite compl√®tement un performer"""
    print(f"\n{'='*50}")
    print(f"Traitement de: {name}")
    print('='*50)
    
    # 1. Scraping
    print("\n1. Scraping des sources...")
    orchestrator = ScraperOrchestrator()
    confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)
    print(f"   ‚úÖ {num_sources} source(s) scrap√©e(s)")
    print(f"   ‚úÖ {len(confirmed)} champ(s) confirm√©(s)")
    print(f"   ‚ö†Ô∏è  {len(conflicts)} conflit(s)")
    
    # 2. Pr√©parer les m√©tadonn√©es
    print("\n2. Pr√©paration des m√©tadonn√©es...")
    metadata = {key: info['value'] for key, info in confirmed.items()}
    metadata['name'] = name
    
    # 3. G√©n√©rer les tags
    print("\n3. G√©n√©ration des tags...")
    tag_engine = TagRulesEngine()
    tags = tag_engine.generate_tags(metadata)
    metadata['tags'] = tags
    print(f"   ‚úÖ {len(tags)} tag(s) g√©n√©r√©(s): {', '.join(tags)}")
    
    # 4. G√©n√©rer la bio
    print("\n4. G√©n√©ration de la bio...")
    bio_generator = BioGenerator()
    bio = bio_generator.generate_google_bio(name, metadata)
    metadata['bio'] = bio
    print(f"   ‚úÖ Bio g√©n√©r√©e ({len(bio)} caract√®res)")
    
    # 5. Sauvegarder
    print("\n5. Sauvegarde...")
    filename = f"data/performers/{name.lower().replace(' ', '_')}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"   ‚úÖ Sauvegard√©: {filename}")
    
    return metadata

# Exemple d'utilisation
if __name__ == "__main__":
    performer_data = process_performer(
        name="Bridgette B",
        urls=[
            "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
            "https://www.freeones.xxx/bridgette-b"
        ]
    )
    
    print("\n" + "="*50)
    print("‚úÖ Traitement termin√© avec succ√®s!")
    print("="*50)
```

### Cas 2 : Batch Processing de Plusieurs Performers

```python
#!/usr/bin/env python3
"""
Traitement par lots de plusieurs performers
"""

import json
from pathlib import Path
from scrapers import ScraperOrchestrator
from stashmaster_unified import TagRulesEngine, BioGenerator

def batch_process(performers_file: str):
    """Traite plusieurs performers depuis un fichier JSON"""
    
    # Charger la liste
    with open(performers_file, 'r') as f:
        performers = json.load(f)
    
    print(f"Traitement de {len(performers)} performer(s)...\n")
    
    orchestrator = ScraperOrchestrator()
    tag_engine = TagRulesEngine()
    bio_generator = BioGenerator()
    
    results = {
        'success': [],
        'failed': [],
        'partial': []
    }
    
    for i, performer in enumerate(performers, 1):
        name = performer['name']
        urls = performer['urls']
        
        print(f"\n[{i}/{len(performers)}] {name}")
        print("-" * 40)
        
        try:
            # Scraping
            confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)
            
            if num_sources == 0:
                print("  ‚ùå Aucune source valide")
                results['failed'].append(name)
                continue
            
            # M√©tadonn√©es
            metadata = {key: info['value'] for key, info in confirmed.items()}
            metadata['name'] = name
            
            # Tags
            tags = tag_engine.generate_tags(metadata)
            metadata['tags'] = tags
            
            # Bio
            bio = bio_generator.generate_google_bio(name, metadata)
            metadata['bio'] = bio
            
            # Sauvegarder
            filename = f"data/performers/{name.lower().replace(' ', '_')}.json"
            Path("data/performers").mkdir(parents=True, exist_ok=True)
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            if len(conflicts) > 0:
                print(f"  ‚ö†Ô∏è  Succ√®s partiel ({len(conflicts)} conflits)")
                results['partial'].append(name)
            else:
                print("  ‚úÖ Succ√®s complet")
                results['success'].append(name)
        
        except Exception as e:
            print(f"  ‚ùå Erreur: {e}")
            results['failed'].append(name)
    
    # R√©sum√©
    print("\n" + "="*50)
    print("R√âSUM√â")
    print("="*50)
    print(f"‚úÖ Succ√®s complet: {len(results['success'])}")
    print(f"‚ö†Ô∏è  Succ√®s partiel: {len(results['partial'])}")
    print(f"‚ùå √âchecs: {len(results['failed'])}")
    
    return results

# Exemple d'utilisation
if __name__ == "__main__":
    # Cr√©er un fichier performers_list.json avec:
    # [
    #   {
    #     "name": "Performer 1",
    #     "urls": ["url1", "url2"]
    #   },
    #   ...
    # ]
    
    results = batch_process("performers_list.json")
```

### Cas 3 : Validation et Correction Semi-Automatique

```python
#!/usr/bin/env python3
"""
Validation et correction semi-automatique des donn√©es
"""

from scrapers import ScraperOrchestrator
from stashmaster_unified import TagRulesEngine

def validate_and_correct(urls: list) -> dict:
    """Valide et propose des corrections"""
    
    orchestrator = ScraperOrchestrator()
    confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)
    
    print("="*50)
    print("VALIDATION DES DONN√âES")
    print("="*50)
    
    # Donn√©es confirm√©es
    print("\n‚úÖ Donn√©es confirm√©es:")
    for field, info in confirmed.items():
        print(f"  {field}: {info['value']}")
        print(f"    Sources: {', '.join(info['sources'])}")
    
    # Conflits √† r√©soudre
    if conflicts:
        print("\n‚ö†Ô∏è  CONFLITS √Ä R√âSOUDRE:")
        corrections = {}
        
        for field, values in conflicts.items():
            print(f"\n  {field}:")
            for i, v in enumerate(values, 1):
                print(f"    [{i}] {v['value']} ({', '.join(v['sources'])})")
            
            # Demander √† l'utilisateur de choisir
            while True:
                choice = input(f"  Choisir [1-{len(values)}] ou [s]kip: ")
                if choice.lower() == 's':
                    break
                try:
                    idx = int(choice) - 1
                    if 0 <= idx < len(values):
                        corrections[field] = values[idx]['value']
                        print(f"    ‚úÖ {field} = {values[idx]['value']}")
                        break
                except ValueError:
                    pass
                print("    ‚ùå Choix invalide")
        
        # Appliquer les corrections
        for field, value in corrections.items():
            confirmed[field] = {
                'value': value,
                'note': 'Corrig√© manuellement'
            }
    
    # R√©sultat final
    final_data = {key: info['value'] for key, info in confirmed.items()}
    
    print("\n" + "="*50)
    print("DONN√âES FINALES")
    print("="*50)
    for field, value in final_data.items():
        print(f"  {field}: {value}")
    
    return final_data

# Exemple
if __name__ == "__main__":
    urls = [
        "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
        "https://www.freeones.xxx/bridgette-b"
    ]
    
    data = validate_and_correct(urls)
```

## üîó Int√©grations

### Int√©gration avec Stash

```python
import requests
import json

class StashAPI:
    """Client pour l'API Stash"""
    
    def __init__(self, url="http://localhost:9999", api_key=None):
        self.url = url
        self.api_key = api_key
    
    def create_performer(self, performer_data: dict) -> dict:
        """Cr√©e un performer dans Stash"""
        # GraphQL mutation
        mutation = """
        mutation PerformerCreate($input: PerformerCreateInput!) {
          performerCreate(input: $input) {
            id
            name
          }
        }
        """
        
        variables = {
            "input": {
                "name": performer_data.get('name'),
                "birthdate": performer_data.get('birthdate'),
                "ethnicity": performer_data.get('ethnicity'),
                "hair_color": performer_data.get('hair_color'),
                "height": performer_data.get('height'),
                "measurements": performer_data.get('measurements'),
                "tags": performer_data.get('tags', [])
            }
        }
        
        response = requests.post(
            f"{self.url}/graphql",
            json={"query": mutation, "variables": variables}
        )
        
        return response.json()

# Utilisation
stash = StashAPI()
result = stash.create_performer(performer_data)
print(f"Performer cr√©√©: {result}")
```

---

Ces exemples couvrent les cas d'usage les plus courants. Pour plus d'informations, consultez le [README.md](README.md) et la documentation des modules.


============================================================
[9/124] gui\dvd_frame.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DVDFrame - Interface de gestion des DVDs / Groupes
"""

import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox
import threading
from typing import Dict, List, Optional

class DVDFrame(ttk.Frame):
    def __init__(self, parent, dvd_id: Optional[str] = None):
        super().__init__(parent)
        self.dvd_id = dvd_id
        
        # Services
        from services.config_manager import ConfigManager
        from services.database import StashDatabase
        from services.scrapers import ScraperOrchestrator
        
        config = ConfigManager()
        self.db = StashDatabase(config.get('database_path'))
        self.scraper = ScraperOrchestrator()
        
        # UI Attributes for linting
        self.notebook: Optional[ttk.Notebook] = None
        self.tab_metadata: Optional[ttk.Frame] = None
        self.urls_text: Optional[scrolledtext.ScrolledText] = None
        
        self.stash_data: Dict = {}
        self.field_vars: Dict[str, Dict[str, tk.Variable]] = {}
        self.fields = [
            ('name', 'Titre'),
            ('date', 'Date'),
            ('studio', 'Studio'),
            ('director', 'R√©alisateur'),
            ('duration', 'Dur√©e'),
            ('rating', 'Note'),
            ('tags', 'Tags'),
        ]
        
        self._setup_ttk_styles()
        self._create_widgets()
        if dvd_id:
            self._load_from_stash()

    def _setup_ttk_styles(self):
        style = ttk.Style()
        style.map('Valid.TCombobox', fieldbackground=[('readonly', '#d4edda'), ('!disabled', '#d4edda')])
        style.map('Invalid.TCombobox', fieldbackground=[('readonly', '#f8d7da'), ('!disabled', '#f8d7da')])
        style.map('Empty.TCombobox', fieldbackground=[('readonly', '#fff3cd'), ('!disabled', '#fff3cd')])
        style.map('Normal.TCombobox', fieldbackground=[('readonly', 'white'), ('!disabled', 'white')])

    def _create_widgets(self):
        # Initialiser les variables de champ
        for field, label in self.fields:
            self.field_vars[field] = {
                'check': tk.BooleanVar(value=True),
                'stash': tk.StringVar(),
                'main': tk.StringVar(),
                'source': tk.StringVar()
            }

        # Onglets
        self.notebook = ttk.Notebook(self)
        self.notebook.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        self.tab_metadata = ttk.Frame(self.notebook)
        self.notebook.add(self.tab_metadata, text="üìÄ M√©tadonn√©es")
        
        self._setup_metadata_tab()
        
        # Toolbar
        toolbar = ttk.Frame(self, padding=10)
        toolbar.pack(side=tk.BOTTOM, fill=tk.X)
        ttk.Button(toolbar, text="üíæ Sauvegarder dans Stash", command=self._save_to_stash).pack(side=tk.RIGHT, padx=5)
        ttk.Button(toolbar, text="üîç Tout Scraper", command=self._scrape_all).pack(side=tk.LEFT, padx=5)

    def _setup_metadata_tab(self):
        if not self.tab_metadata: return
        container = ttk.Frame(self.tab_metadata, padding=10)
        container.pack(fill=tk.BOTH, expand=True)
        
        # Headers
        headers = ["", "Champ", "Valeur Stash", "Modification", "R√©sultat Scrapers"]
        for i, h in enumerate(headers):
            lbl = ttk.Label(container, text=h, font=('Segoe UI', 9, 'bold'))
            lbl.grid(row=0, column=i, padx=5, pady=5, sticky="w")
        
        # Lignes de champs
        for i, (field, label) in enumerate(self.fields, start=1):
            # Checkbox
            ttk.Checkbutton(container, variable=self.field_vars[field]['check']).grid(row=i, column=0, padx=5)
            # Label
            ttk.Label(container, text=label).grid(row=i, column=1, padx=5, sticky="w")
            # Stash Value (Read-only)
            ent_stash = ttk.Entry(container, textvariable=self.field_vars[field]['stash'], state='readonly', width=30)
            ent_stash.grid(row=i, column=2, padx=5, pady=2, sticky="w")
            # Main Input (Validation)
            ent_main = ttk.Combobox(container, textvariable=self.field_vars[field]['main'], width=40)
            ent_main.grid(row=i, column=3, padx=5, pady=2, sticky="we")
            # Source Result (Read-only)
            ent_source = ttk.Entry(container, textvariable=self.field_vars[field]['source'], state='readonly', width=30)
            ent_source.grid(row=i, column=4, padx=5, pady=2, sticky="we")
            
            self.field_vars[field]['widget'] = ent_main
            
            # Monitoring de changement pour validation visuelle
            self.field_vars[field]['main'].trace_add("write", lambda *args, f=field: self._validate_field(f))

        # URL Area
        url_frame = ttk.LabelFrame(container, text="URLs Sources (IAFD, AdultEmpire, Data18...)", padding=10)
        url_frame.grid(row=len(self.fields)+1, column=0, columnspan=5, sticky="we", pady=15)
        self.urls_text = scrolledtext.ScrolledText(url_frame, height=4, wrap=tk.WORD)
        self.urls_text.pack(fill=tk.X, expand=True)

        container.columnconfigure(3, weight=10)
        container.columnconfigure(2, weight=5)
        container.columnconfigure(4, weight=5)

    def _load_from_stash(self):
        if not self.dvd_id: return
        data = self.db.get_group_metadata(self.dvd_id)
        if data:
            self.stash_data = data
            for field, _ in self.fields:
                val = str(data.get(field, '')) if data.get(field) is not None else ''
                self.field_vars[field]['stash'].set(val)
                self.field_vars[field]['main'].set(val)
                self._validate_field(field)

    def _validate_field(self, field):
        f = self.field_vars[field]
        main_val = f['main'].get().strip()
        stash_val = f['stash'].get().strip()
        is_checked = f['check'].get()
        combo = f.get('widget')
        
        if not is_checked:
            color = "white"
        elif not main_val:
            color = "#fff3cd"
        elif main_val.lower() == stash_val.lower():
            color = "#d4edda"
        else:
            color = "#f8d7da"

        if isinstance(combo, ttk.Combobox):
            style_map = {
                "white": "Normal.TCombobox",
                "#fff3cd": "Empty.TCombobox",
                "#d4edda": "Valid.TCombobox",
                "#f8d7da": "Invalid.TCombobox"
            }
            combo.configure(style=style_map.get(color, "Normal.TCombobox"))

    def _scrape_all(self):
        if not self.urls_text: return
        urls = self.urls_text.get('1.0', tk.END).strip().split('\n')
        urls = [u.strip() for u in urls if u.strip()]
        if not urls:
            messagebox.showwarning("Scraping", "Veuillez entrer au moins une URL.")
            return

        def run_scrape():
            results = self.scraper.scrape_dvd_multi(urls)
            if not results:
                self.after(0, lambda: messagebox.showinfo("Scraping", "Aucun r√©sultat trouv√©."))
                return
            
            from services.scrapers import DataMerger
            confirmed, _ = DataMerger.merge_data(results)
            
            merged_data = {k: v['value'] for k, v in confirmed.items()}
            self.after(0, lambda: self._apply_results(merged_data))

        threading.Thread(target=run_scrape, daemon=True).start()

    def _apply_results(self, data):
        for field, _ in self.fields:
            if field in data and data[field]:
                val = str(data[field])
                self.field_vars[field]['source'].set(val)
                # Utiliser .get() sur BooleanVar pour v√©rifier si coch√©
                if self.field_vars[field]['check'].get():
                    self.field_vars[field]['main'].set(val)
                
                # Update Combobox values
                stash_val = self.field_vars[field]['stash'].get().strip()
                all_vals = sorted(list(set([v for v in [stash_val, val] if v])))
                widget = self.field_vars[field].get('widget')
                if isinstance(widget, ttk.Combobox):
                    widget['values'] = all_vals
        
        # Injection des URLs de sc√®nes (Sp√©cifique Data18)
        if 'scenes' in data and data['scenes'] and self.dvd_id:
            self._inject_scene_urls(data['scenes'])

    def _inject_scene_urls(self, scraped_scenes: List[Dict]):
        """Associe les URLs Data18 aux sc√®nes Stash du DVD"""
        stash_scenes = self.db.get_scenes_for_group(self.dvd_id)
        if not stash_scenes: return
        
        count: int = 0
        for s_scene in scraped_scenes:
            scraped_title = s_scene.get('title', '').lower()
            scraped_url = s_scene.get('url')
            if not scraped_url: continue
            
            # Recherche de match par titre (tr√®s basique pour l'instant)
            for db_scene in stash_scenes:
                db_title = db_scene.get('title', '').lower()
                # Match si le titre est contenu l'un dans l'autre (grossier mais souvent efficace sur Data18)
                if db_title and scraped_title and (db_title in scraped_title or scraped_title in db_title):
                    if self.db.add_scene_url(db_scene['id'], scraped_url):
                        count += 1
                    break
        
        if count > 0:
            print(f"Inject√© {count} URLs de sc√®nes depuis Data18.")
            # Optionnel: Notification discr√®te

    def _save_to_stash(self):
        if not self.dvd_id: return
        updates = {f: var_dict['main'].get() for f, var_dict in self.field_vars.items()}
        if self.db.save_group_metadata(self.dvd_id, updates):
            messagebox.showinfo("Sauvegarde", "DVD mis √† jour avec succ√®s.")
            self._load_from_stash()
        else:
            messagebox.showerror("Sauvegarde", "Erreur lors de la sauvegarde.")


============================================================
[10/124] gui\performer_frame.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PerformerFrame - Interface de gestion des performers
"""

import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox
import threading
import re
from typing import Dict, List, Optional, Tuple, Any

# Imports locaux
from utils.tag_engine import TagRulesEngine
from utils.awards_cleaner import AwardsCleaner
from services.bio_generator import BioGenerator
from services.database import StashDatabase
from services.config_manager import ConfigManager
from services.scrapers import ScraperOrchestrator
from services.source_finder import SourceFinderWidget
from services.url_validator import URLValidatorWidget
from services.url_manager import URLManager # Nouvelle importation
from gui.url_verification_dialog import URLVerificationDialog # Nouvelle importation

# utilitaires pour URL (nettoyage / fusion)
from utils.url_utils import clean_urls_list, merge_urls_by_domain

class PerformerFrame(ttk.Frame):
    """Frame principal pour la gestion des performers"""
    
    def __init__(self, parent, performer_id: Optional[str] = None):
        super().__init__(parent)
        self.performer_id = performer_id
        
        # Initialisation des services
        self.config = ConfigManager()
        self.db = StashDatabase(self.config.get("database_path"))
        self.tag_rules = TagRulesEngine()
        self.awards_cleaner = AwardsCleaner()
        self.bio_generator = BioGenerator()
        self.orchestrator = ScraperOrchestrator()
        self.url_manager = URLManager() # Initialisation URLManager
        
        # Donn√©es
        self.stash_data: Dict[str, Any] = {}
        self.field_vars: Dict[str, Dict[str, Any]] = {}
        
        # Widgets
        self.notebook: ttk.Notebook = None # type: ignore
        self.metadata_tab: ttk.Frame = None # type: ignore
        self.advanced_tab: ttk.Frame = None # type: ignore
        self.bio_tab: ttk.Frame = None # type: ignore
        self.bio_text: scrolledtext.ScrolledText = None # type: ignore
        self.char_label: ttk.Label = None # type: ignore
        self.progress_bar: ttk.Progressbar = None # type: ignore
        self.status_label: ttk.Label = None # type: ignore
        self.url_tree: ttk.Treeview = None # type: ignore
        self.fields = [
            ("Nom", "name"),
            ("Aliases", "aliases"),
            ("Date Naissance", "birthdate"),
            ("Lieu Naissance", "birthplace"),
            ("Date D√©c√®s", "deathdate"),
            ("Pays", "country"),
            ("Ethnicit√©", "ethnicity"),
            ("Cheveux", "hair_color"),
            ("Yeux", "eye_color"),
            ("Taille (cm)", "height"),
            ("Poids (kg)", "weight"),
            ("Mesures", "measurements"),
            ("Poitrine", "fake_tits"),
            ("Ann√©es activit√©", "career_length"),
        ]
        
        self.source_labels = {}
        self.bio_valid_var = tk.BooleanVar(value=True)

        # Bio UI (4 sous-onglets)
        self._bio_notebook: Optional[ttk.Notebook] = None
        self._bio_slots: List[str] = ["", "", "", ""]  # 0=bio_raw, 1=trivia, 2=google, 3=ollama
        self._bio_merge_content: str = ""
        self._merge_vars: List[tk.BooleanVar] = []

        self._bio_raw_text: Optional[scrolledtext.ScrolledText] = None
        self._bio_trivia_disp: Optional[scrolledtext.ScrolledText] = None
        self._bio_google_text: Optional[scrolledtext.ScrolledText] = None
        self._bio_ollama_text: Optional[scrolledtext.ScrolledText] = None
        self._bio_merge_text: Optional[scrolledtext.ScrolledText] = None

        self._lbl_scrape_chars: Optional[ttk.Label] = None
        self._lbl_scrape_chars2: Optional[ttk.Label] = None
        self._lbl_google_chars: Optional[ttk.Label] = None
        self._lbl_ollama_chars: Optional[ttk.Label] = None
        self._lbl_merge_chars: Optional[ttk.Label] = None
        self._lbl_url_count: Optional[ttk.Label] = None
        self._lbl_award_count: Optional[ttk.Label] = None

        self._ollama_status: Optional[ttk.Label] = None
        self._merge_status: Optional[ttk.Label] = None
        
        self._setup_ttk_styles()
        self._create_widgets()
        
        # Diff√©rer le chargement pour que la mainloop soit d√©marr√©e
        if performer_id:
            self.after(100, self._load_from_stash)

    def _setup_ttk_styles(self):
        style = ttk.Style()
        # On d√©finit des styles pour les Combobox bas√©s sur la validation
        style.map('Valid.TCombobox', fieldbackground=[('readonly', '#d4edda'), ('!disabled', '#d4edda')])
        style.map('Invalid.TCombobox', fieldbackground=[('readonly', '#f8d7da'), ('!disabled', '#f8d7da')])
        style.map('Empty.TCombobox', fieldbackground=[('readonly', '#fff3cd'), ('!disabled', '#fff3cd')])
        style.map('Normal.TCombobox', fieldbackground=[('readonly', 'white'), ('!disabled', 'white')])

    def _create_widgets(self):
        """Cr√©e l'interface de l'onglet Performer"""
        # Notebook pour les sous-onglets
        self.notebook = ttk.Notebook(self)
        self.notebook.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        # Onglets
        self.metadata_tab = self._create_metadata_tab()
        self.advanced_tab = self._create_advanced_tab()
        self.bio_tab = self._create_bio_tab()
        
        self.notebook.add(self.metadata_tab, text="üìã M√©tadonn√©es")
        self.notebook.add(self.advanced_tab, text="‚öôÔ∏è Tags & D√©tails")
        self.notebook.add(self.bio_tab, text="üìù Biographie")
        
        # Barre d'outils
        self._create_toolbar()

    def _get_field_values(self) -> Dict[str, Any]:
        """R√©cup√®re toutes les valeurs de validation actuelles"""
        values = {}
        for k, v in self.field_vars.items():
            if v.get('is_multiline'):
                # Handle tk.Text widget
                val = v['entry'].get('1.0', tk.END).strip()
                if k == 'aliases':
                    values[k] = [a.strip() for a in re.split(r'[,\n\r]+', val) if a.strip()]
                elif k == 'discovered_urls' or k == 'urls':
                    values[k] = [u.strip() for u in re.split(r'[,\n\r\s]+', val) if u.strip()]
                else:
                    values[k] = val
            else:
                # Handle tk.StringVar
                values[k] = v['main'].get().strip()
        # Ajouter explicitement la biographie (details)
        values['details'] = self._get_best_bio_text()
            
        return values

    def _get_best_bio_text(self) -> str:
        merge_content = (getattr(self, '_bio_merge_content', '') or '').strip()
        if merge_content:
            return merge_content

        ollama = (self._bio_ollama_text.get('1.0', tk.END).strip()
                  if getattr(self, '_bio_ollama_text', None) else '').strip()
        if ollama:
            return ollama

        google = (self._bio_google_text.get('1.0', tk.END).strip()
                  if getattr(self, '_bio_google_text', None) else '').strip()
        if google:
            return google

        # fallback legacy
        if getattr(self, 'bio_text', None):
            try:
                return self.bio_text.get('1.0', tk.END).strip()
            except Exception:
                pass
        return ""

    def _refresh_bio_counters(self):
        try:
            urls_count = 0
            if 'urls' in self.field_vars and self.field_vars['urls'].get('is_multiline'):
                urls_raw = self.field_vars['urls']['entry'].get('1.0', tk.END).strip()
                urls = [u.strip() for u in re.split(r'[\,\n\r\s]+', urls_raw) if u.strip()]
                urls_count = len(clean_urls_list(urls))

            awards_count = 0
            if 'awards' in self.field_vars and self.field_vars['awards'].get('is_multiline'):
                awards_raw = self.field_vars['awards']['entry'].get('1.0', tk.END).strip()
                awards_count = len([l for l in awards_raw.splitlines() if l.strip()])

            if self._lbl_url_count:
                self._lbl_url_count.config(text=f"URLs : {urls_count}")
            if self._lbl_award_count:
                self._lbl_award_count.config(text=f"Awards : {awards_count}")
        except Exception:
            pass

    def _bio_update_chars(self, slot_idx: int, widget: scrolledtext.ScrolledText, label: ttk.Label):
        text = widget.get('1.0', tk.END).strip()
        self._bio_slots[slot_idx] = text
        label.config(text=f"Caract√®res : {len(text)}")

    def _bio_clear(self, slot_idx: int):
        mapping = {
            2: self._bio_google_text,
            3: self._bio_ollama_text,
        }
        widget = mapping.get(slot_idx)
        if not widget:
            return
        widget.delete('1.0', tk.END)
        self._bio_slots[slot_idx] = ""
        if slot_idx == 2 and self._lbl_google_chars:
            self._lbl_google_chars.config(text="Caract√®res : 0")
        if slot_idx == 3 and self._lbl_ollama_chars:
            self._lbl_ollama_chars.config(text="Caract√®res : 0")

    def _update_raw_content(self, bio_raw: Optional[str] = None, trivia: Optional[str] = None):
        # Update internal slot values
        if bio_raw is not None:
            self._bio_slots[0] = str(bio_raw).strip()
        if trivia is not None:
            self._bio_slots[1] = str(trivia).strip()

        if self._bio_raw_text:
            self._bio_raw_text.config(state='normal')
            self._bio_raw_text.delete('1.0', tk.END)
            self._bio_raw_text.insert('1.0', self._bio_slots[0])
            self._bio_raw_text.config(state='disabled')
            if self._lbl_scrape_chars:
                self._lbl_scrape_chars.config(text=f"Caract√®res : {len(self._bio_slots[0])}")

        if self._bio_trivia_disp:
            self._bio_trivia_disp.config(state='normal')
            self._bio_trivia_disp.delete('1.0', tk.END)
            self._bio_trivia_disp.insert('1.0', self._bio_slots[1])
            self._bio_trivia_disp.config(state='disabled')
            if self._lbl_scrape_chars2:
                self._lbl_scrape_chars2.config(text=f"Caract√®res : {len(self._bio_slots[1])}")

        self._refresh_bio_counters()

    def _create_metadata_tab(self) -> ttk.Frame:
        frame = ttk.Frame(self.notebook)
        
        # Style pour les entr√©es textuelles multi-lignes
        # Canvas pour le scroll
        canvas = tk.Canvas(frame, bg="#f5f5f5", highlightthickness=0)
        scrollbar = ttk.Scrollbar(frame, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas, padding=15)
        
        scrollable_frame.bind("<Configure>", lambda e: canvas.configure(scrollregion=canvas.bbox("all")))
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # Header de la grille
        
        # Header de la grille
        headers_labels = ["Scrap", "Champ", "Valeur Stash"]
        # Sources fixes (priorit√© DataMerger)
        source_names = ["IAFD", "FreeOnes", "TheNude", "Babepedia", "Boobpedia", "XXXBios"]
        
        header_fonts = ('Segoe UI', 10, 'bold')
        for col, text in enumerate(headers_labels):
            ttk.Label(scrollable_frame, text=text, font=header_fonts).grid(row=1, column=col, padx=10, pady=10, sticky="w")
        
        for i, name in enumerate(source_names):
            lbl = ttk.Label(scrollable_frame, text=name, font=header_fonts)
            lbl.grid(row=1, column=3 + i, padx=10, pady=10)
            self.source_labels[name] = lbl
        
        # Validation / Edit √† la fin
        ttk.Label(scrollable_frame, text="Validation / Edit", font=header_fonts).grid(row=1, column=3 + len(source_names), padx=10, pady=10, sticky="w")

        for i, (label, key) in enumerate(self.fields, start=1):
            # 0: Checkbox
            var_check = tk.BooleanVar(value=True)
            cb = ttk.Checkbutton(scrollable_frame, variable=var_check)
            cb.grid(row=i+1, column=0, padx=5, pady=5)
            
            # 1: Label
            ttk.Label(scrollable_frame, text=label, font=('Segoe UI', 9)).grid(row=i+1, column=1, sticky="w", padx=10)
            
            # Determine widget type
            is_multiline = key in ['aliases', 'awards', 'discovered_urls', 'trivia', 'tattoos', 'details']
            if key == 'aliases':
                row_height = 10
            else:
                row_height = 3 if is_multiline else 1
            
            # 2: Stash Value
            var_stash = tk.StringVar()
            if is_multiline:
                entry_stash = tk.Text(scrollable_frame, width=30, height=row_height, font=('Segoe UI', 9), bg="#eeeeee", relief=tk.FLAT)
                entry_stash.grid(row=i+1, column=2, padx=2, pady=2, sticky="nsew")
                entry_stash.configure(state="disabled")
            else:
                entry_stash = ttk.Entry(scrollable_frame, textvariable=var_stash, width=30, state="readonly")
                entry_stash.grid(row=i+1, column=2, padx=2, pady=2, sticky="we")
            
            # 3-6: Sources (D√©cal√© √† col 3-6)
            vars_sources = []
            widgets_sources = []
            for col_offset, _ in enumerate(source_names):
                col = 3 + col_offset
                v_src = tk.StringVar()
                if is_multiline:
                    e_src = tk.Text(scrollable_frame, width=20, height=row_height, font=('Segoe UI', 9), bg="#f9f9f9", relief=tk.FLAT)
                    e_src.grid(row=i+1, column=col, padx=2, pady=2, sticky="nsew")
                    e_src.configure(state="disabled")
                else:
                    e_src = ttk.Entry(scrollable_frame, textvariable=v_src, width=20, state="readonly")
                    e_src.grid(row=i+1, column=col, padx=2, pady=2, sticky="we")
                vars_sources.append(v_src)
                widgets_sources.append(e_src)
            
            # 7: Main Input (Validation) - √Ä LA FIN
            var_main = tk.StringVar()
            final_col = 3 + len(source_names)
            if is_multiline:
                entry_main = tk.Text(scrollable_frame, width=45, height=row_height, font=('Segoe UI', 9), relief=tk.FLAT, highlightthickness=1)
                entry_main.grid(row=i+1, column=final_col, padx=2, pady=2, sticky="nsew")
            else:
                entry_main = ttk.Combobox(scrollable_frame, textvariable=var_main, width=45)
                entry_main.grid(row=i+1, column=final_col, padx=2, pady=2, sticky="we")
            
            # Cache vars and widgets
            self.field_vars[key] = {
                'check': var_check,
                'stash': var_stash,
                'main': var_main,
                'entry': entry_main, # This is the widget
                'sources': vars_sources,
                'source_widgets': widgets_sources,
                'stash_widget': entry_stash,
                'is_multiline': is_multiline
            }
            
            # Traces for validation
            def make_callback(k):
                return lambda *args: self._update_validation(k)

            if is_multiline:
                # Text widgets need event binding
                entry_main.bind("<KeyRelease>", make_callback(key))
            else:
                var_check.trace_add("write", make_callback(key))
                var_main.trace_add("write", make_callback(key))
                var_stash.trace_add("write", make_callback(key))
            
            # Allow row to grow for multiline
            if is_multiline:
                scrollable_frame.rowconfigure(i+1, weight=1)
            
        # Configure grid expansion
        final_col = 3 + len(source_names)
        scrollable_frame.columnconfigure(final_col, weight=10) # Validation column takes most space
        scrollable_frame.columnconfigure(2, weight=5) # Stash column
        for c in range(3, final_col):
            scrollable_frame.columnconfigure(c, weight=4) # Source columns
        
        # Expand row for content
        scrollable_frame.rowconfigure(len(self.fields) + 1, weight=1)
        
        # S'assurer que le canvas occupe toute la largeur
        frame.columnconfigure(0, weight=1)
        canvas.grid(row=0, column=0, sticky="nsew")
        scrollbar.grid(row=0, column=1, sticky="ns")
        frame.rowconfigure(0, weight=1)
        
        return frame

    def _create_advanced_tab(self) -> ttk.Frame:
        frame = ttk.Frame(self.notebook)
        frame.columnconfigure(0, weight=1)
        frame.rowconfigure(0, weight=1)
        
        canvas = tk.Canvas(frame, bg="#f0f0f0")
        scrollbar = ttk.Scrollbar(frame, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas, padding=20)
        
        scrollable_frame.bind("<Configure>", lambda e: canvas.configure(scrollregion=canvas.bbox("all")))
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # Disposition en 3 Colonnes
        # Col 1: Tags, Tatouages, Piercings
        col1 = ttk.Frame(scrollable_frame)
        col1.grid(row=0, column=0, sticky="nsew", padx=10)
        
        # Col 2: Awards
        col2 = ttk.Frame(scrollable_frame)
        col2.grid(row=0, column=1, sticky="nsew", padx=10)
        
        # Col 3: URLs
        col3 = ttk.Frame(scrollable_frame)
        col3.grid(row=0, column=2, sticky="nsew", padx=10)
        
        scrollable_frame.columnconfigure(0, weight=1)
        scrollable_frame.columnconfigure(1, weight=1)
        scrollable_frame.columnconfigure(2, weight=1)
        
        # --- Colonne 1 ---
        # Tags
        lf_tags = ttk.LabelFrame(col1, text=" Tags ", padding=10)
        lf_tags.pack(fill=tk.BOTH, expand=True, pady=5)
        self._setup_advanced_field(lf_tags, "tags", has_gen=True)
        
        # Tatouages (ex Tattoos)
        lf_tats = ttk.LabelFrame(col1, text=" Tatouages ", padding=10)
        lf_tats.pack(fill=tk.BOTH, expand=True, pady=5)
        self._setup_advanced_field(lf_tats, "tattoos")
        
        # Piercings
        lf_pierce = ttk.LabelFrame(col1, text=" Piercings ", padding=10)
        lf_pierce.pack(fill=tk.BOTH, expand=True, pady=5)
        self._setup_advanced_field(lf_pierce, "piercings")
        
        # --- Colonne 2 ---
        # Awards
        lf_awards = ttk.LabelFrame(col2, text=" Awards / Prix ", padding=10)
        lf_awards.pack(fill=tk.BOTH, expand=True, pady=5)
        self._setup_advanced_field(lf_awards, "awards")
        
        # --- Colonne 3 ---
        # URLs
        lf_urls = ttk.LabelFrame(col3, text=" URLs (Fusionn√©es & Uniques) ", padding=10)
        lf_urls.pack(fill=tk.BOTH, expand=True, pady=5)
        self._setup_advanced_field(lf_urls, "urls")
        
        # URLs D√©couvertes (cach√© ou secondaire ?) - On le garde pour la compatibilit√©
        self.field_vars['discovered_urls'] = {'is_multiline': True, 'entry': tk.Text(frame, height=1), 'check': tk.BooleanVar(), 'stash': tk.StringVar(), 'main': tk.StringVar(), 'stash_widget': tk.Text(frame)}

        canvas.grid(row=0, column=0, sticky="nsew")
        scrollbar.grid(row=0, column=1, sticky="ns")
        
        return frame

    def _setup_advanced_field(self, parent, key, has_gen=False):
        """Helper pour cr√©er les blocs de l'onglet avanc√©"""
        header_frame = ttk.Frame(parent)
        header_frame.pack(fill=tk.X)
        
        var_check = tk.BooleanVar(value=True)
        ttk.Checkbutton(header_frame, text="Valider", variable=var_check).pack(side=tk.LEFT)
        
        if has_gen and key == "tags":
            ttk.Button(header_frame, text="‚ú® G√©n√©rer Tags", command=self._refresh_tags).pack(side=tk.RIGHT)

        ttk.Label(parent, text="Actuel dans Stash:", font=('Segoe UI', 8, 'italic')).pack(anchor="w", pady=(5,0))
        entry_stash = tk.Text(parent, height=2, font=('Segoe UI', 9), bg="#eeeeee", relief=tk.FLAT)
        entry_stash.pack(fill=tk.X, pady=(0, 5))
        entry_stash.configure(state="disabled")

        ttk.Label(parent, text="Validation / Edit:", font=('Segoe UI', 8, 'italic')).pack(anchor="w")
        txt = tk.Text(parent, height=8, font=('Segoe UI', 9), relief=tk.FLAT, highlightthickness=1)
        txt.pack(fill=tk.BOTH, expand=True, pady=5)
        # when editing URLs, keep them clean/validated automatically
        if key == "urls":
            txt.bind('<KeyRelease>', self._on_urls_modified)
        
        var_stash = tk.StringVar()
        var_main = tk.StringVar()
        
        self.field_vars[key] = {
            'check': var_check,
            'stash': var_stash,
            'main': var_main,
            'entry': txt,
            'sources': [],
            'source_widgets': [],
            'stash_widget': entry_stash,
            'is_multiline': True
        }
        txt.bind("<KeyRelease>", lambda e, k=key: self._update_validation(k))

    def _create_bio_tab(self) -> ttk.Frame:
        frame = ttk.Frame(self.notebook, padding=10)
        frame.columnconfigure(0, weight=1)
        frame.rowconfigure(0, weight=1)

        self._bio_notebook = ttk.Notebook(frame)
        self._bio_notebook.grid(row=0, column=0, sticky='nsew')

        # --- Tab 0: Scrapp√© ---
        tab_scrape = ttk.Frame(self._bio_notebook, padding=10)
        tab_scrape.columnconfigure(0, weight=1)
        tab_scrape.rowconfigure(1, weight=1)
        tab_scrape.rowconfigure(3, weight=1)

        header = ttk.Frame(tab_scrape)
        header.grid(row=0, column=0, sticky='ew', pady=(0, 8))
        self._lbl_url_count = ttk.Label(header, text='URLs : 0')
        self._lbl_url_count.pack(side=tk.LEFT, padx=(0, 10))
        self._lbl_award_count = ttk.Label(header, text='Awards : 0')
        self._lbl_award_count.pack(side=tk.LEFT)

        lf_raw = ttk.LabelFrame(tab_scrape, text=' Bio scrapp√©e ', padding=6)
        lf_raw.grid(row=1, column=0, sticky='nsew')
        lf_raw.columnconfigure(0, weight=1)
        lf_raw.rowconfigure(0, weight=1)
        self._bio_raw_text = scrolledtext.ScrolledText(lf_raw, wrap=tk.WORD, height=10)
        self._bio_raw_text.grid(row=0, column=0, sticky='nsew')
        self._bio_raw_text.config(state='disabled')
        self._lbl_scrape_chars = ttk.Label(tab_scrape, text='Caract√®res : 0')
        self._lbl_scrape_chars.grid(row=2, column=0, sticky=tk.E, pady=(2, 8))

        lf_trivia = ttk.LabelFrame(tab_scrape, text=' Trivia scrapp√©e ', padding=6)
        lf_trivia.grid(row=3, column=0, sticky='nsew')
        lf_trivia.columnconfigure(0, weight=1)
        lf_trivia.rowconfigure(0, weight=1)
        self._bio_trivia_disp = scrolledtext.ScrolledText(lf_trivia, wrap=tk.WORD, height=8)
        self._bio_trivia_disp.grid(row=0, column=0, sticky='nsew')
        self._bio_trivia_disp.config(state='disabled')
        self._lbl_scrape_chars2 = ttk.Label(tab_scrape, text='Caract√®res : 0')
        self._lbl_scrape_chars2.grid(row=4, column=0, sticky=tk.E, pady=(2, 0))

        # --- Tab 1: Google ---
        tab_google = ttk.Frame(self._bio_notebook, padding=10)
        tab_google.columnconfigure(0, weight=1)
        tab_google.rowconfigure(1, weight=1)
        btns_g = ttk.Frame(tab_google)
        btns_g.grid(row=0, column=0, sticky='ew', pady=(0, 6))
        ttk.Button(btns_g, text='üìù G√©n√©rer Google', command=self._bio_generate_google).pack(side=tk.LEFT)
        ttk.Button(btns_g, text='üßΩ Effacer', command=lambda: self._bio_clear(2)).pack(side=tk.LEFT, padx=6)
        ttk.Checkbutton(btns_g, text='Valider la Bio', variable=self.bio_valid_var).pack(side=tk.RIGHT)

        self._bio_google_text = scrolledtext.ScrolledText(tab_google, wrap=tk.WORD)
        self._bio_google_text.grid(row=1, column=0, sticky='nsew')
        self._lbl_google_chars = ttk.Label(tab_google, text='Caract√®res : 0')
        self._lbl_google_chars.grid(row=2, column=0, sticky=tk.E)
        self._bio_google_text.bind(
            '<KeyRelease>',
            lambda e: self._bio_update_chars(2, self._bio_google_text, self._lbl_google_chars),
        )

        # --- Tab 2: Ollama ---
        tab_ollama = ttk.Frame(self._bio_notebook, padding=10)
        tab_ollama.columnconfigure(0, weight=1)
        tab_ollama.rowconfigure(2, weight=1)
        top_o = ttk.Frame(tab_ollama)
        top_o.grid(row=0, column=0, sticky='ew', pady=(0, 6))
        ttk.Button(top_o, text='ü§ñ G√©n√©rer Ollama', command=self._bio_generate_ollama).pack(side=tk.LEFT)
        ttk.Button(top_o, text='üßΩ Effacer', command=lambda: self._bio_clear(3)).pack(side=tk.LEFT, padx=6)
        self._ollama_status = ttk.Label(top_o, text='')
        self._ollama_status.pack(side=tk.RIGHT)

        self._bio_ollama_text = scrolledtext.ScrolledText(tab_ollama, wrap=tk.WORD)
        self._bio_ollama_text.grid(row=2, column=0, sticky='nsew')
        self._lbl_ollama_chars = ttk.Label(tab_ollama, text='Caract√®res : 0')
        self._lbl_ollama_chars.grid(row=3, column=0, sticky=tk.E)
        self._bio_ollama_text.bind(
            '<KeyRelease>',
            lambda e: self._bio_update_chars(3, self._bio_ollama_text, self._lbl_ollama_chars),
        )

        # --- Tab 3: Raffiner/Fusionner ---
        tab_merge = ttk.Frame(self._bio_notebook, padding=10)
        tab_merge.columnconfigure(0, weight=1)
        tab_merge.rowconfigure(4, weight=1)

        src_box = ttk.LabelFrame(tab_merge, text=' Sources √† fusionner ', padding=8)
        src_box.grid(row=0, column=0, sticky='ew', pady=(0, 8))
        self._merge_vars = [tk.BooleanVar(value=True), tk.BooleanVar(value=True), tk.BooleanVar(value=True), tk.BooleanVar(value=True)]
        ttk.Checkbutton(src_box, text='Scrapp√© (bio)', variable=self._merge_vars[0]).pack(side=tk.LEFT, padx=6)
        ttk.Checkbutton(src_box, text='Scrapp√© (trivia)', variable=self._merge_vars[1]).pack(side=tk.LEFT, padx=6)
        ttk.Checkbutton(src_box, text='Google', variable=self._merge_vars[2]).pack(side=tk.LEFT, padx=6)
        ttk.Checkbutton(src_box, text='Ollama', variable=self._merge_vars[3]).pack(side=tk.LEFT, padx=6)

        prompt_lf = ttk.LabelFrame(tab_merge, text=' Directives IA (style, ton, taille, fusion...) ', padding=6)
        prompt_lf.grid(row=1, column=0, sticky='ew', pady=(0, 8))
        self.bio_prompt_text = tk.Text(prompt_lf, height=4, font=('Segoe UI', 9))
        self.bio_prompt_text.pack(fill=tk.X, expand=True)
        self.bio_prompt_text.insert('1.0', 'Ton professionnel, fran√ßais, environ 3000 caract√®res. Z√âRO liste √† puces. Fusionner proprement les sources s√©lectionn√©es.')

        actions = ttk.Frame(tab_merge)
        actions.grid(row=2, column=0, sticky='ew', pady=(0, 6))
        ttk.Button(actions, text='üîÄ Fusionner', command=self._bio_do_merge).pack(side=tk.LEFT)
        ttk.Button(actions, text='‚ú® Raffiner (Ollama)', command=self._bio_do_refine).pack(side=tk.LEFT, padx=6)
        ttk.Button(actions, text='‚úÖ Appliquer ‚Üí Ollama', command=self._bio_apply_merge).pack(side=tk.LEFT)
        self._merge_status = ttk.Label(actions, text='')
        self._merge_status.pack(side=tk.RIGHT)

        self._bio_merge_text = scrolledtext.ScrolledText(tab_merge, wrap=tk.WORD)
        self._bio_merge_text.grid(row=4, column=0, sticky='nsew')
        self._lbl_merge_chars = ttk.Label(tab_merge, text='Caract√®res : 0')
        self._lbl_merge_chars.grid(row=5, column=0, sticky=tk.E)
        self._bio_merge_text.bind(
            '<KeyRelease>',
            lambda e: self._bio_update_merge_chars(),
        )

        # Add tabs
        self._bio_notebook.add(tab_scrape, text='üìÑ Scrapp√©')
        self._bio_notebook.add(tab_google, text='üîç Google')
        self._bio_notebook.add(tab_ollama, text='ü§ñ Ollama')
        self._bio_notebook.add(tab_merge, text='üîÄ Raffiner/Fusionner')

        # Legacy aliases for compatibility with existing methods
        self.bio_text = self._bio_google_text  # type: ignore
        self.char_label = self._lbl_google_chars  # type: ignore

        self._refresh_bio_counters()
        self._update_raw_content(self._bio_slots[0], self._bio_slots[1])

        return frame

    def load_performer(self, performer_data: Dict[str, Any]):
        """Charge les donn√©es d'un performer dans l'interface."""
        # 1. Pr√©-traitement URL Manager Interactif
        if performer_data.get("name"):
            # R√©cup√©ration URLs existantes
            raw_urls = performer_data.get("urls", [])
            if isinstance(raw_urls, str):
                raw_urls = [u.strip() for u in raw_urls.splitlines() if u.strip()]
            elif raw_urls is None:
                raw_urls = []
            
            # Lancement de la fen√™tre de v√©rification interactive
            # Utilisation de la classe URLVerificationDialog import√©e
            if isinstance(raw_urls, str):
                raw_urls = [u.strip() for u in raw_urls.splitlines() if u.strip()]
            
            dlg = URLVerificationDialog(self, self.url_manager, raw_urls, performer_data["name"])
            self.wait_window(dlg)
            
            if dlg.final_urls is not None:
                performer_data["urls"] = dlg.final_urls
            # Sinon, on garde les URLs d'origine (si l'utilisateur a ferm√© sans finir ?)

        self.stash_data = performer_data
        
        # Reset fields
        for key, field_info in self.field_vars.items():
            entry = field_info["entry"]
            if hasattr(entry, 'delete'):
                if isinstance(entry, (tk.Text, scrolledtext.ScrolledText)):
                    entry.delete('1.0', tk.END)
                else:
                    entry.delete(0, tk.END)
        
        # Populate fields
        for key, val in performer_data.items():
            if key in self.field_vars:
                field_info = self.field_vars[key]
                entry = field_info["entry"]
                
                display_val = val
                if isinstance(val, list):
                    display_val = "\n".join(str(v) for v in val if v)
                elif val is None:
                    display_val = ""
                
                if isinstance(entry, (tk.Text, scrolledtext.ScrolledText)):
                    entry.insert('1.0', str(display_val))
                elif isinstance(entry, (ttk.Entry, ttk.Combobox)):
                    entry.insert(0, str(display_val))
        
        self._refresh_bio_counters()
        self._update_raw_content(performer_data.get("bio_raw"), performer_data.get("trivia"))


    def _bio_update_merge_chars(self):
        if not self._bio_merge_text or not self._lbl_merge_chars:
            return
        text = self._bio_merge_text.get('1.0', tk.END).strip()
        self._bio_merge_content = text
        self._lbl_merge_chars.config(text=f"Caract√®res : {len(text)}")

    def _bio_generate_google(self):
        metadata = self._get_field_values()
        name = self.field_vars.get('name', {}).get('main').get() if 'name' in self.field_vars else ''
        # Injecter la bio Stash existante + bio scrap√©e + trivia dans les m√©tadonn√©es
        metadata['bio_raw']   = self._bio_slots[0] or self.stash_data.get('details', '')
        metadata['trivia']    = metadata.get('trivia', '') or self._bio_slots[1]
        metadata['stash_bio'] = self.stash_data.get('details', '')
        bio = self.bio_generator.generate_google_bio(name, metadata)
        if self._bio_google_text and self._lbl_google_chars:
            self._bio_google_text.delete('1.0', tk.END)
            self._bio_google_text.insert('1.0', bio or '')
            self._bio_update_chars(2, self._bio_google_text, self._lbl_google_chars)
        if self._bio_notebook:
            self._bio_notebook.select(1)

    def _bio_generate_ollama(self):
        prompt = self.bio_prompt_text.get('1.0', tk.END).strip() if getattr(self, 'bio_prompt_text', None) else ''
        name = self.field_vars.get('name', {}).get('main').get() if 'name' in self.field_vars else ''

        def run():
            try:
                if self._ollama_status:
                    self.after(0, lambda: self._ollama_status.config(text='G√©n√©ration...'))
                metadata = self._get_field_values()
                metadata['bio_raw'] = self._bio_slots[0]
                metadata['trivia'] = metadata.get('trivia', '') or self._bio_slots[1]
                bio = self.bio_generator.generate_ollama_bio(name, metadata, custom_prompt=prompt)
                if bio and self._bio_ollama_text and self._lbl_ollama_chars:
                    def apply():
                        self._bio_ollama_text.delete('1.0', tk.END)
                        self._bio_ollama_text.insert('1.0', bio)
                        self._bio_update_chars(3, self._bio_ollama_text, self._lbl_ollama_chars)
                        if self._ollama_status:
                            self._ollama_status.config(text='')
                        if self._bio_notebook:
                            self._bio_notebook.select(2)
                    self.after(0, apply)
                else:
                    self.after(0, lambda: (self._ollama_status.config(text='') if self._ollama_status else None,
                                           messagebox.showerror('Ollama', 'Erreur lors de la g√©n√©ration avec Ollama.')))
            except Exception:
                self.after(0, lambda: (self._ollama_status.config(text='') if self._ollama_status else None,
                                       messagebox.showerror('Ollama', 'Erreur lors de la g√©n√©ration avec Ollama.')))

        threading.Thread(target=run, daemon=True).start()

    def _bio_do_merge(self):
        prompt = self.bio_prompt_text.get('1.0', tk.END).strip() if getattr(self, 'bio_prompt_text', None) else ''
        sources = []
        if self._merge_vars and self._merge_vars[0].get() and self._bio_slots[0].strip():
            sources.append('BIO SCRAPP√âE:\n' + self._bio_slots[0].strip())
        if self._merge_vars and self._merge_vars[1].get() and self._bio_slots[1].strip():
            sources.append('TRIVIA SCRAPP√âE:\n' + self._bio_slots[1].strip())
        if self._merge_vars and self._merge_vars[2].get() and self._bio_google_text:
            t = self._bio_google_text.get('1.0', tk.END).strip()
            if t:
                sources.append('BIO GOOGLE:\n' + t)
        if self._merge_vars and self._merge_vars[3].get() and self._bio_ollama_text:
            t = self._bio_ollama_text.get('1.0', tk.END).strip()
            if t:
                sources.append('BIO OLLAMA:\n' + t)

        current = "\n\n---\n\n".join(sources).strip()
        if not current:
            messagebox.showwarning('Fusion', 'Aucune source s√©lectionn√©e ou disponible.')
            return

        def run():
            try:
                if self._merge_status:
                    self.after(0, lambda: self._merge_status.config(text='Fusion...'))
                merged = self.bio_generator.refine_bio(current, prompt)
                if merged and self._bio_merge_text and self._lbl_merge_chars:
                    def apply():
                        self._bio_merge_text.delete('1.0', tk.END)
                        self._bio_merge_text.insert('1.0', merged)
                        self._bio_merge_content = merged
                        self._lbl_merge_chars.config(text=f"Caract√®res : {len(merged.strip())}")
                        if self._merge_status:
                            self._merge_status.config(text='')
                        if self._bio_notebook:
                            self._bio_notebook.select(3)
                    self.after(0, apply)
                else:
                    self.after(0, lambda: (self._merge_status.config(text='') if self._merge_status else None,
                                           messagebox.showerror('Fusion', 'Erreur lors de la fusion avec Ollama.')))
            except Exception:
                self.after(0, lambda: (self._merge_status.config(text='') if self._merge_status else None,
                                       messagebox.showerror('Fusion', 'Erreur lors de la fusion avec Ollama.')))

        threading.Thread(target=run, daemon=True).start()

    def _bio_do_refine(self):
        current = self._get_best_bio_text()
        prompt = self.bio_prompt_text.get('1.0', tk.END).strip() if getattr(self, 'bio_prompt_text', None) else ''
        if not current:
            messagebox.showwarning('IA', 'Aucune biographie √† raffiner.')
            return

        def run():
            try:
                if self._merge_status:
                    self.after(0, lambda: self._merge_status.config(text='Raffinage...'))
                refined = self.bio_generator.refine_bio(current, prompt)
                if refined and self._bio_merge_text and self._lbl_merge_chars:
                    def apply():
                        self._bio_merge_text.delete('1.0', tk.END)
                        self._bio_merge_text.insert('1.0', refined)
                        self._bio_merge_content = refined
                        self._lbl_merge_chars.config(text=f"Caract√®res : {len(refined.strip())}")
                        if self._merge_status:
                            self._merge_status.config(text='')
                        if self._bio_notebook:
                            self._bio_notebook.select(3)
                    self.after(0, apply)
                else:
                    self.after(0, lambda: (self._merge_status.config(text='') if self._merge_status else None,
                                           messagebox.showerror('Ollama', 'Erreur lors du raffinage avec Ollama.')))
            except Exception:
                self.after(0, lambda: (self._merge_status.config(text='') if self._merge_status else None,
                                       messagebox.showerror('Ollama', 'Erreur lors du raffinage avec Ollama.')))

        threading.Thread(target=run, daemon=True).start()

    def _bio_apply_merge(self):
        merged = (getattr(self, '_bio_merge_content', '') or '').strip()
        if not merged or not self._bio_ollama_text or not self._lbl_ollama_chars:
            return
        self._bio_ollama_text.delete('1.0', tk.END)
        self._bio_ollama_text.insert('1.0', merged)
        self._bio_update_chars(3, self._bio_ollama_text, self._lbl_ollama_chars)
        if self._bio_notebook:
            self._bio_notebook.select(2)

    def _load_from_stash(self):
        """Charge les donn√©es du performer depuis la base Stash"""
        if not self.performer_id:
            return
            
        data = self.db.get_performer_metadata(self.performer_id)
        if not data:
            messagebox.showerror("Erreur", "Impossible de charger les donn√©es du performer.")
            return

        # 0. V√©rification interactive des URLs AVANT de charger dans l'interface
        if data.get("name"):
            # R√©cup√©ration URLs existantes
            raw_urls = data.get("urls", [])
            if isinstance(raw_urls, str):
                raw_urls = [u.strip() for u in raw_urls.splitlines() if u.strip()]
            elif raw_urls is None:
                raw_urls = []
            
            # Lancement de la fen√™tre de v√©rification interactive
            dlg = URLVerificationDialog(self, self.url_manager, raw_urls, data["name"])
            self.wait_window(dlg)
            
            if dlg.final_urls is not None:
                data["urls"] = dlg.final_urls
            # Sinon, on garde les URLs d'origine (si l'utilisateur a ferm√© sans finir)

        self.stash_data = data
        
        # 1. Remplir les champs de m√©tadonn√©es
        for key, vars in self.field_vars.items():
            val = data.get(key)
            if val is None: val = ""
            
            # Normalisation Date de Naissance / D√©c√®s (YYYY-MM-DD)
            if key in ['birthdate', 'deathdate'] and val:
                try:
                    from datetime import datetime
                    # Supposer format Stash ou autre et forcer ISO
                    # Simple regex ou dateutil si dispo, ici on tente un parse basique
                    for fmt in ("%Y-%m-%d", "%d/%m/%Y", "%Y"):
                        try:
                            val = datetime.strptime(str(val).split('T')[0], fmt).strftime("%Y-%m-%d")
                            break
                        except: continue
                except: pass

            if not isinstance(val, str):
                if isinstance(val, list):
                    # Normalisation URLs (Doublons + Tri)
                    if key == 'urls':
                        val = self._sort_urls(list(dict.fromkeys(val)))
                    val = "\n".join(map(str, val))
                else:
                    val = str(val)

            if key == 'country' and val:
                val = self._normalize_country(val)
                
            if vars.get('is_multiline'):
                st = vars['stash_widget']
                st.configure(state="normal")
                st.delete('1.0', tk.END)
                st.insert('1.0', val)
                st.configure(state="disabled")
                
                et = vars['entry']
                if not et.get('1.0', tk.END).strip():
                    et.delete('1.0', tk.END)
                    et.insert('1.0', val)
            else:
                vars['stash'].set(val)
                if not vars['main'].get():
                    vars['main'].set(val)
            
            self._update_validation(key)

        # 2. Bio / D√©tails
        details = data.get('details') or ""
        if self._bio_google_text and self._lbl_google_chars:
            self._bio_google_text.delete('1.0', tk.END)
            self._bio_google_text.insert('1.0', str(details))
            self._bio_update_chars(2, self._bio_google_text, self._lbl_google_chars)
        self._bio_merge_content = ""
        if self._bio_merge_text and self._lbl_merge_chars:
            self._bio_merge_text.delete('1.0', tk.END)
            self._lbl_merge_chars.config(text='Caract√®res : 0')
        self._refresh_bio_counters()

        # 3. URLs
        stash_urls = self.stash_data.get('urls', [])
        # clean duplicates & empties on load
        if isinstance(stash_urls, list):
            cleaned = clean_urls_list(stash_urls)
            self.stash_data['urls'] = cleaned
            stash_urls = cleaned
        self._highlight_missing_sources(stash_urls)
        # update widget content if any (loop below handles this too but we
        # might want to ensure it's normalized)
        self._start_automatic_validation()

    def _create_toolbar(self):
        toolbar = ttk.Frame(self, padding=5)
        toolbar.pack(side=tk.BOTTOM, fill=tk.X)
        
        # Conteneur pour centrer ou r√©partir si besoin
        btn_frame = ttk.Frame(toolbar)
        btn_frame.pack(fill=tk.X)
        
        ttk.Button(btn_frame, text="üíæ Sauvegarder dans Stash", command=self._save_to_stash).pack(side=tk.RIGHT, padx=5)
        ttk.Button(btn_frame, text="üîç Scraper Tout", command=self._scrape_all).pack(side=tk.LEFT, padx=5)
        
        # Barre de progression
        self.status_label = ttk.Label(btn_frame, text="Pr√™t", font=('Segoe UI', 9))
        self.status_label.pack(side=tk.RIGHT, padx=10)
        self.progress_bar = ttk.Progressbar(btn_frame, orient=tk.HORIZONTAL, length=150, mode='determinate')
        self.progress_bar.pack(side=tk.RIGHT, padx=10)

        # Nouveaux outils V2
        ttk.Button(btn_frame, text="üßπ Nettoyer URLs", command=self._clean_urls).pack(side=tk.LEFT, padx=5)
        ttk.Button(btn_frame, text="üîó Valider URLs", command=self._open_url_validator).pack(side=tk.LEFT, padx=5)
        ttk.Button(btn_frame, text="üîé Chercher Sources", command=self._open_source_finder).pack(side=tk.LEFT, padx=5)

    def _open_url_validator(self):
        """Ouvre le validateur d'URL pour ce performer"""
        db_path = self.config.get("database_path")
        stash_url = self.config.get("stash_url", "http://localhost:9999")
        
        widget = URLValidatorWidget(self, db_path=db_path, stash_url=stash_url, performer_id=self.performer_id)
        widget.show()

    def _open_source_finder(self):
        """Ouvre le chercheur de sources pour ce performer"""
        if not self.stash_data:
            messagebox.showwarning("Attention", "Veuillez d'abord charger les donn√©es du performer.")
            return

        name = self.stash_data.get("name", "")
        aliases = self.stash_data.get("aliases", [])
        urls = self.stash_data.get("urls", [])
        
        def on_selected(new_urls):
            if not new_urls:
                return
            # On ajoute les nouvelles URLs √† la liste actuelle
            current_urls = list(self.stash_data.get("urls", []))
            # On attend un dict de SourceFinderWidget: {source: url}
            if isinstance(new_urls, dict):
                new_urls_list = list(new_urls.values())
            else:
                new_urls_list = list(new_urls)
            
            merged = merge_urls_by_domain(current_urls, new_urls_list)
            merged = clean_urls_list(merged)
            self.stash_data["urls"] = merged  # D√©dupliquer
            messagebox.showinfo("Succ√®s", f"URLs ajout√©es localement. N'oubliez pas de sauvegarder.")

        widget = SourceFinderWidget(self, name=name, aliases=aliases, existing_urls=urls, on_urls_selected=on_selected)
        widget.show()

    def _update_validation(self, key: str):
        """Met √† jour la couleur de fond de l'entry selon la validit√©"""
        if key not in self.field_vars:
            return
            
        f = self.field_vars[key]
        entry = f['entry']
        is_checked = f['check'].get()
        
        if f.get('is_multiline'):
            main_val = entry.get('1.0', tk.END).strip()
            stash_val = f['stash_widget'].get('1.0', tk.END).strip()
        else:
            main_val = f['main'].get().strip()
            stash_val = f['stash'].get().strip()
        
        if not is_checked:
            color = "white"
        elif not main_val:
            color = "#fff3cd" # Jaune (vide mais coch√©)
        elif main_val.lower() == stash_val.lower():
            color = "#d4edda" # Vert (identique)
        else:
            color = "#f8d7da" # Rouge (diff√©rent)
            
        # Application de la couleur s√©curis√©e
        if isinstance(entry, tk.Text):
            entry.configure(bg=color)
        else:
            # Pour tous les widgets ttk (Combobox, Entry, etc.)
            style_map = {
                "white": "Normal.TCombobox",
                "#fff3cd": "Empty.TCombobox",
                "#d4edda": "Valid.TCombobox",
                "#f8d7da": "Invalid.TCombobox"
            }
            # Un mappage g√©n√©rique pour les styles ttk
            try:
                # Si c'est un Entry, on utilise Normal.TEntry etc (√† d√©finir si besoin)
                # Mais ici on cible principalement les Combobox
                if isinstance(entry, ttk.Combobox):
                    entry.configure(style=style_map.get(color, "Normal.TCombobox"))
                else:
                    # Fallback s√©curis√© pour √©viter le crash -bg
                    pass
            except:
                pass

        # Keep Bio counters in sync
        if key in ("urls", "awards"):
            self._refresh_bio_counters()

    def _update_count(self, event=None):
        count = len(self.bio_text.get('1.0', tk.END).strip())
        self.char_label.config(text=f"Caract√®res : {count}")

    def _refresh_tags(self):
        metadata = self._get_field_values()
        tags = self.tag_rules.generate_tags(metadata)
        entry = self.field_vars['tags']['entry']
        entry.delete('1.0', tk.END)
        entry.insert('1.0', ', '.join(tags))
        self._update_validation('tags')

    def _gen_bio_google(self):
        self._bio_generate_google()

    def _gen_bio_ollama(self):
        self._bio_generate_ollama()

    def _refine_bio_ollama(self):
        self._bio_do_refine()

    def _apply_bio(self, bio):
        if self._bio_google_text and self._lbl_google_chars:
            self._bio_google_text.delete('1.0', tk.END)
            self._bio_google_text.insert('1.0', bio)
            self._bio_update_chars(2, self._bio_google_text, self._lbl_google_chars)

    def _save_to_stash(self):
        """Sauvegarde les modifications dans Stash"""
        if not self.performer_id:
            messagebox.showerror("Sauvegarde", "Aucun performer n'est charg√©.")
            return

        updates = self._get_field_values()
        updates['details'] = self._get_best_bio_text()
        
        # S'assurer que les d√©couvertes d'URL sont incluses si modifi√©es
        if 'urls' in updates:
            updates['discovered_urls'] = updates['urls']
        
        if self.db.save_performer_metadata(self.performer_id, updates):
            # Mettre √† jour aussi l'URL principale si trouv√©e? 
            # Stash a une colonne 'url' unique sur la table performers.
            messagebox.showinfo("Sauvegarde", "Performer mis √† jour avec succ√®s dans Stash.")
            # Recharger pour rafra√Æchir les colonnes 'Stash'
            self._load_from_stash()
        else:
            messagebox.showerror("Sauvegarde", "Erreur lors de la sauvegarde dans la base de donn√©es.")

    def _scrape_all(self):
        """Orchestre le scraping multi-sources avec barre de progression"""
        urls = self.stash_data.get('urls', [])
        performer_name = self.stash_data.get('name', '')

        # On lance le scraping m√™me sans URLs (Boobpedia + XXXBios seront auto-construits)
        if not urls and not performer_name:
            messagebox.showwarning("Scraping", "Aucune URL et aucun nom trouv√© pour ce performer.")
            return

        def update_progress(current, total, source_name):
            self.after(0, lambda: self._update_ui_progress(current, total, source_name))

        def run():
            # Reset UI
            self.after(0, lambda: self.progress_bar.configure(value=0))
            self.after(0, lambda: self.status_label.configure(text="Initialisation..."))
            
            # Scraping avec auto-d√©couverte Boobpedia/XXXBios
            results = self.orchestrator.scrape_all(urls, progress_callback=update_progress, performer_name=performer_name)
            
            if not results:
                self.after(0, lambda: self.status_label.configure(text="√âchec"))
                self.after(0, lambda *_: messagebox.showinfo("Scraping", "Aucune donn√©e trouv√©e sur les sources."))
                return

            # Validation des URLs d√©couvertes
            self.after(0, lambda: self.status_label.configure(text="Validation URLs..."))
            all_discovered = []
            for res in results:
                d_urls = res.get("discovered_urls", [])
                if isinstance(d_urls, list):
                    all_discovered.extend(d_urls)
            
            all_discovered = list(dict.fromkeys(all_discovered))
            
            if all_discovered:
                from services.url_validator import URLValidator, URLStatus
                validator = URLValidator(timeout=5)
                entries = [{"url": u, "performer_id": self.performer_id or 0, "name": "Discovered", "position": 0} for u in all_discovered]
                valid_results = validator.validate_urls(entries)
                alive_urls = [r.url for r in valid_results if r.status in (URLStatus.ACTIVE, URLStatus.AMBIGUOUS, URLStatus.REDIRECT, URLStatus.WHITELISTED)]
                
                for res in results:
                    res["discovered_urls"] = alive_urls

            self.after(0, lambda *_: self._apply_scrape_results(results))

        threading.Thread(target=run, daemon=True).start()

    def _update_ui_progress(self, current, total, source_name):
        """Met √† jour les widgets de progression (appel√© via .after)"""
        if self.progress_bar and self.status_label:
            val = (current / total) * 100 if total > 0 else 0
            self.progress_bar.configure(value=val)
            self.status_label.configure(text=f"Scraping {source_name}...")
            if current == total:
                self.status_label.configure(text="Termin√©")


    def _apply_scrape_results(self, results: List[Dict]):
        """Affiche les r√©sultats du scraping dans la grille et agr√®ge les URLs"""
        # 1. Agr√©gation des URLs de toutes les sources + URLs Stash actuelles
        all_discovered = []
        for res in results:
            urls = res.get("discovered_urls", [])
            if isinstance(urls, list):
                all_discovered.extend(urls)
        
        # Ajouter l'existant pour d√©doublonnage global
        stash_urls = self.stash_data.get('urls', [])
        
        # D√©duplication et Tri Hi√©rarchique (Core sources first)
        all_discovered = merge_urls_by_domain(stash_urls, all_discovered)
        
        # Mettre √† jour l'en-t√™te (rouge si une source manque)
        self._highlight_missing_sources(all_discovered)
        
        # Dispatcher les r√©seaux sociaux si pr√©sents dans les r√©sultats
        for res in results:
            socials = res.get("socials", {})
            for s_key, s_val in socials.items():
                if s_key in res: continue # D√©j√† pr√©sent ?
                # Normaliser twitter -> x? Non, on garde twitter pour la cl√© interne
                if s_key == "x": s_key = "twitter"
                if s_val: res[s_key] = s_val
        
        # 2. Mise √† jour de la grille (limit√© aux 6 sources principales configur√©es)
        source_order = ["IAFD", "FreeOnes", "TheNude", "Babepedia", "Boobpedia", "XXXBios"]
        # R√©ordonner les r√©sultats selon source_order pour que chaque source tombe dans la bonne colonne
        ordered_results = []
        result_by_source = {r.get('source', ''): r for r in results}
        for sname in source_order:
            ordered_results.append(result_by_source.get(sname, {}))
        limit = len(source_order)
        for source_idx in range(limit):
            res = ordered_results[source_idx]
            source_name = res.get('source', source_order[source_idx])
            
            for key, fields in self.field_vars.items():
                if key == "discovered_urls":
                    continue
                    
                val = res.get(key, "")
                
                # Normalisation sp√©cifique
                if key == 'country' and val:
                    val = self._normalize_country(val)
                elif key in ['birthdate', 'deathdate'] and val:
                    # (Re-use normalization logic)
                    try:
                        from datetime import datetime
                        for fmt in ("%Y-%m-%d", "%d/%m/%Y", "%Y"):
                            try:
                                val = datetime.strptime(str(val).split('T')[0], fmt).strftime("%Y-%m-%d")
                                break
                            except: continue
                    except: pass
                # Cas sp√©cial : pour la ligne URLs, on veut l'URL source
                if key == "urls" and res.get('url'):
                    val = res['url']
                if isinstance(val, list):
                    val = "\n".join(map(str, val)) if fields.get('is_multiline') else ", ".join(map(str, val))
                
                # Cas sp√©cial : Socials si pr√©sents dans un dict
                if key == "socials":
                    # On dispatch les socials vers leurs champs respectifs
                    pass # Sera g√©r√© apr√®s la boucle key ou via key direct

                # Mettre √† jour la colonne source correspondante
                if fields.get('is_multiline'):
                    widgets = fields.get('source_widgets', [])
                    if source_idx < len(widgets):
                        w = widgets[source_idx]
                        w.configure(state="normal")
                        w.delete('1.0', tk.END)
                        w.insert('1.0', str(val))
                        w.configure(state="disabled")
                else:
                    fields['sources'][source_idx].set(str(val))
                
                # Auto-remplissage si case coch√©e et vide
                is_empty = False
                if fields.get('is_multiline'):
                    is_empty = not fields['entry'].get('1.0', tk.END).strip()
                else:
                    is_empty = not fields['main'].get().strip()

                # Fusion automatique des aliases (ne pas √©craser)
                if key == 'aliases' and fields['check'].get() and fields.get('is_multiline') and val:
                    try:
                        current_text = fields['entry'].get('1.0', tk.END).strip()
                        current_aliases = [a.strip() for a in re.split(r'[\n\r,]+', current_text) if a.strip()]
                        incoming_aliases = [a.strip() for a in re.split(r'[\n\r,]+', str(val)) if a.strip()]
                        merged = []
                        seen = set()
                        for a in (current_aliases + incoming_aliases):
                            k = a.casefold()
                            if k in seen:
                                continue
                            seen.add(k)
                            merged.append(a)
                        fields['entry'].delete('1.0', tk.END)
                        fields['entry'].insert('1.0', "\n".join(merged))
                        self._update_validation('aliases')
                        is_empty = False
                    except Exception:
                        pass

                if fields['check'].get() and is_empty and val:
                    if fields.get('is_multiline'):
                        fields['entry'].delete('1.0', tk.END)
                        fields['entry'].insert('1.0', str(val))
                    else:
                        fields['main'].set(str(val))
                
                # Mise √† jour des valeurs de la liste d√©roulante (Combobox)
                if not fields.get('is_multiline') and isinstance(fields['entry'], ttk.Combobox):
                    stash_val = fields['stash'].get().strip()
                    source_vals = [s.get().strip() for s in fields['sources']]
                    all_vals = sorted(list(set([v for v in ([stash_val] + source_vals) if v])))
                    fields['entry']['values'] = all_vals

        # 3. Remplissage du champ global URLs avec les d√©couvertes tri√©es
        if "urls" in self.field_vars and all_discovered:
            fields = self.field_vars["urls"]
            # Fusion intelligente : on garde l'existant d√©j√† √©dit√© + d√©couvertes
            current_text = fields['entry'].get('1.0', tk.END).strip()
            current_urls = [u.strip() for u in re.split(r'[,\n\r\s]+', current_text) if u.strip()]
            
            # merge+clean sans doublons
            combined = merge_urls_by_domain(current_urls, all_discovered)
            combined = clean_urls_list(combined)
            
            fields['entry'].delete('1.0', tk.END)
            fields['entry'].insert('1.0', "\n".join(combined))
            self._update_validation("urls")

        # 4. Relancer la validation automatique des URLs Stash
        self._start_automatic_validation()

        # 5. Traduction automatique (French/QC) pour les champs texte riches
        def run_translation():
            fields_to_translate = {
                'trivia': 'Trivia',
                'tattoos': 'Tatouages',
                'piercings': 'Piercings'
            }
            
            for key, label in fields_to_translate.items():
                if key not in self.field_vars: continue
                
                # R√©cup√©rer la valeur actuelle dans le widget Main
                v = self.field_vars[key]
                current_val = ""
                if v.get('is_multiline'):
                    current_val = v['entry'].get('1.0', tk.END).strip()
                else:
                    current_val = v['main'].get().strip()
                
                if current_val and current_val.lower() != 'none':
                    translated = self.bio_generator.translate_hybrid(current_val, label)
                    if translated and translated != current_val:
                        def update_ui(k=key, t=translated):
                            v_ui = self.field_vars[k]
                            if v_ui.get('is_multiline'):
                                v_ui['entry'].delete('1.0', tk.END)
                                v_ui['entry'].insert('1.0', t)
                            else:
                                v_ui['main'].set(t)
                            self._update_validation(k)
                        self.after(0, update_ui)

        threading.Thread(target=run_translation, daemon=True).start()

        # 6. Sync bio_raw / trivia scrapp√©s dans l'onglet Bio (si pr√©sents)
        bio_raw = ""
        trivia = ""
        try:
            for res in results:
                if not bio_raw and res.get('bio_raw'):
                    bio_raw = str(res.get('bio_raw') or '').strip()
                if not trivia and res.get('trivia'):
                    trivia = str(res.get('trivia') or '').strip()
                if bio_raw and trivia:
                    break
        except Exception:
            pass
        if bio_raw or trivia:
            self._update_raw_content(bio_raw=bio_raw, trivia=trivia)

        messagebox.showinfo("Scraping", f"Scraping termin√© ({len(results)} sources). {len(all_discovered)} URLs agr√©g√©es.")

    def _sort_urls(self, urls: List[str]) -> List[str]:
        """Trie les URLs : sources recherch√©es d'abord (IAFD, FreeOnes, etc.)"""
        # Cette m√©thode n'est plus utilis√©e par le Treeview, mais peut √™tre utile ailleurs.
        # Si elle n'est plus utilis√©e du tout, elle peut √™tre supprim√©e.
        core_dirs = ["iafd.com", "freeones.com", "thenude.com", "babepedia.com"]
        
        def sort_key(url):
            url_lower = url.lower()
            for i, domain in enumerate(core_dirs):
                if domain in url_lower:
                    return i
            return 99 # Autres sources apr√®s
            
        return sorted(list(set(urls)), key=sort_key)

    def _populate_url_tree(self, urls):
        pass # Ancien Treeview, m√©thode obsol√®te

    def _clean_urls(self):
        """Internal helper for the advanced tab that normalizes the list of
        URLs currently displayed: removes blank lines, trims and discards
        duplicates."""
        if 'urls' not in self.field_vars:
            return
        widget = self.field_vars['urls']['entry']
        raw = widget.get('1.0', tk.END)
        urls = [u for u in raw.splitlines()]
        cleaned = clean_urls_list(urls)
        widget.delete('1.0', tk.END)
        widget.insert('1.0', "\n".join(cleaned))

    def _on_urls_modified(self, event=None):
        # if the user is editing URLs manually, keep them tidy and revalidate
        self._clean_urls()
        self._refresh_bio_counters()
        self._start_automatic_validation()

    def _start_automatic_validation(self):
        """Lance la validation automatique des URLs Stash (champ texte)."""
        # make sure urls are cleaned first
        self._clean_urls()

        # On regarde si on a le champ 'urls' (onglet avanc√©)
        if 'urls' not in self.field_vars:
            return
            
        widget = self.field_vars['urls']['entry']
        text = widget.get('1.0', tk.END).strip()
        if not text:
            return
            
        urls_to_check = text.split('\n')
        
        def validate():
            from services.url_validator import URLStatus
            from services.url_validator import URLValidator
            validator = URLValidator(timeout=5)
            
            entries = [{"url": u, "performer_id": self.performer_id or 0, "name": "Stash", "position": 0} for u in urls_to_check]
            results = validator.validate_urls(entries)
            
            # Mettre √† jour l'UI avec des tags de couleur
            for i, res in enumerate(results):
                status = res.status
                tag = "url_ok"
                if status in (URLStatus.DEAD, URLStatus.ERROR):
                    tag = "url_error"
                elif status == URLStatus.REDIRECT:
                    tag = "url_warning"
                
                # Appliquer le tag √† la ligne correspondante
                line_start = f"{i+1}.0"
                line_end = f"{i+1}.end"
                self.after(0, lambda w=widget, t=tag, s=line_start, e=line_end: self._apply_url_tag(w, t, s, e))

            # remove dead/error URLs from the widget and re-clean
            from utils.url_utils import clean_urls_list, filter_live_urls
            live_urls = filter_live_urls(urls_to_check, results)
            live_urls = clean_urls_list(live_urls)
            if live_urls != urls_to_check:
                # rewrite the text on the main thread
                self.after(0, lambda: widget.delete('1.0', tk.END))
                self.after(0, lambda: widget.insert('1.0', "\n".join(live_urls)))
                # recolor lines if list shortened
        

        import threading
        threading.Thread(target=validate, daemon=True).start()

    def _apply_url_tag(self, widget, tag, start, end):
        """Applique un tag de couleur √† une ligne du widget Text"""
        widget.tag_add(tag, start, end)
        # Configurer les couleurs des tags si pas encore fait
        widget.tag_configure("url_ok", foreground="green")
        widget.url_error_color = widget.tag_configure("url_error", foreground="red")
        widget.tag_configure("url_warning", foreground="orange")

    def _update_url_row(self, item_id, status, redirect, tag):
        current_values = list(self.url_tree.item(item_id)['values'])
        current_values[1] = status
        current_values[2] = redirect
        self.url_tree.item(item_id, values=current_values, tags=(tag,))
        
        # Configurer les couleurs des tags
        self.url_tree.tag_configure("ok", foreground="green")
        self.url_tree.tag_configure("error", foreground="red")
        self.url_tree.tag_configure("warning", foreground="orange")

    def _open_selected_url(self):
        """Ouvre l'URL s√©lectionn√©e dans le navigateur"""
        import webbrowser
        selected = self.url_tree.selection()
        if not selected:
            return
        url = self.url_tree.item(selected[0])['values'][0]
        webbrowser.open(url)

    def _normalize_country(self, country: str) -> str:
        """Convertit un nom de pays en code ISO 2 lettres"""
        if not country or len(country) == 2:
            return country
            
        mapping = {
            "united states": "US", "usa": "US",
            "united kingdom": "UK", "uk": "UK",
            "france": "FR", "germany": "DE", "spain": "ES",
            "italy": "IT", "canada": "CA", "brazil": "BR",
            "russia": "RU", "japan": "JP", "australia": "AU",
            "hungary": "HU", "czech republic": "CZ", "poland": "PL",
            "netherlands": "NL", "belgium": "BE", "switzerland": "CH",
        }
        return mapping.get(country.lower().strip(), country)

    def _highlight_missing_sources(self, urls: List[str]):
        """Surligne en rouge le nom des sources manquantes dans les URLs Stash"""
        if not hasattr(self, 'source_labels'):
            return
            
        # Priorit√©s
        core_dirs = {
            "IAFD": "iafd.com",
            "FreeOnes": "freeones.com",
            "TheNude": "thenude.com",
            "Babepedia": "babepedia.com",
            "Boobpedia": "boobpedia.com",
            "XXXBios": "xxxbios.com"
        }
        
        url_text = "\n".join(urls).lower()
        
        for name, domain in core_dirs.items():
            lbl = self.source_labels.get(name)
            if lbl:
                if domain in url_text:
                    lbl.configure(foreground="black")
                else:
                    lbl.configure(foreground="red")
    def _delete_selected_url(self):
        """Supprime l'URL s√©lectionn√©e de la liste locale"""
        selected = self.url_tree.selection()
        if not selected:
            return
        if messagebox.askyesno("Confirmation", "Supprimer cette URL de la liste locale ?"):
            self.url_tree.delete(selected[0])
            # Note: cela ne supprime pas de Stash tant qu'on n'a pas sauvegard√©


============================================================
[11/124] gui\scene_frame.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SceneFrame - Interface de gestion des Sc√®nes
"""

import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox
from typing import Dict, List, Optional

class SceneFrame(ttk.Frame):
    """Frame principal pour la gestion des sc√®nes"""
    
    def __init__(self, parent, scene_id: Optional[str] = None):
        super().__init__(parent)
        self.scene_id = scene_id
        self.metadata = {}
        
        # Services
        from services.config_manager import ConfigManager
        from services.database import StashDatabase
        from services.scrapers import ScraperOrchestrator
        
        config = ConfigManager()
        self.db = StashDatabase(config.get('database_path'))
        self.scraper = ScraperOrchestrator()
        
        # Definition of fields moved to __init__ as an instance variable
        self.fields = [
            ("Titre:", "title"),
            ("Studio:", "studio"),
            ("Date:", "date"),
            ("Code:", "code"),
            ("R√©alisateur:", "director"),
            ("Rating:", "rating"),
            ("Tags (virgules):", "tags"),
            ("Performers (virgules):", "performers"),
        ]
        
        self._setup_ttk_styles()
        self._create_widgets()
        
        if scene_id:
            self._load_from_stash()

    def _setup_ttk_styles(self):
        style = ttk.Style()
        style.map('Valid.TCombobox', fieldbackground=[('readonly', '#d4edda'), ('!disabled', '#d4edda')])
        style.map('Invalid.TCombobox', fieldbackground=[('readonly', '#f8d7da'), ('!disabled', '#f8d7da')])
        style.map('Empty.TCombobox', fieldbackground=[('readonly', '#fff3cd'), ('!disabled', '#fff3cd')])
        style.map('Normal.TCombobox', fieldbackground=[('readonly', 'white'), ('!disabled', 'white')])

    def _create_widgets(self):
        # Header
        header = ttk.Frame(self, padding=10)
        header.pack(fill=tk.X)
        ttk.Label(header, text=f"Sc√®ne ID: {self.scene_id or 'Nouvelle'}", font=('Segoe UI', 12, 'bold')).pack(side=tk.LEFT)
        
        # Main content
        content = ttk.Frame(self, padding=10)
        content.pack(fill=tk.BOTH, expand=True)
        
        # Grid layout for fields (Standardized to Stash/Validation/Source pattern)
        fields = [
            ("Titre:", "title"),
            ("Studio:", "studio"),
            ("Date:", "date"),
            ("Code:", "code"),
            ("R√©alisateur:", "director"),
            ("Rating:", "rating"),
            ("Tags (virgules):", "tags"),
            ("Performers (virgules):", "performers"),
        ]
        
        headers = ["Valider", "Champ", "Valeur Stash", "Validation / Edit", "Sources"]
        for col, text in enumerate(headers):
            ttk.Label(content, text=text, font=('Segoe UI', 9, 'bold')).grid(row=0, column=col, padx=5, pady=5, sticky="w")

        self.field_vars = {}
        for i, (label, key) in enumerate(fields, start=1):
            # 0: Checkbox
            var_check = tk.BooleanVar(value=True)
            ttk.Checkbutton(content, variable=var_check).grid(row=i, column=0, padx=5)
            
            # 1: Label
            ttk.Label(content, text=label).grid(row=i, column=1, sticky="w", padx=5)
            
            # 2: Valeur Stash (Read-only)
            var_stash = tk.StringVar()
            ttk.Entry(content, textvariable=var_stash, state="readonly", width=30).grid(row=i, column=2, padx=5, sticky="we")
            
            # 3: Validation / Edit (Combobox)
            var_main = tk.StringVar()
            combo = ttk.Combobox(content, textvariable=var_main, width=40)
            combo.grid(row=i, column=3, padx=5, sticky="we")
            
            # 4: Source (Read-only)
            var_src = tk.StringVar()
            ttk.Entry(content, textvariable=var_src, state="readonly", width=30).grid(row=i, column=4, padx=5, sticky="we")
            
            # Monitoring
            var_main.trace_add("write", lambda *args, k=key: self._validate_field(k))
            
            self.field_vars[key] = {
                'check': var_check,
                'stash': var_stash,
                'main': var_main,
                'source': var_src,
                'widget': combo
            }
            
        content.columnconfigure(3, weight=10)
        content.columnconfigure(2, weight=5)
        content.columnconfigure(4, weight=5)
        
        # Details / Synopsis
        ttk.Label(content, text="Synopsis / D√©tails:").grid(row=len(fields)+1, column=0, sticky="nw", padx=5, pady=5)
        self.details_text = scrolledtext.ScrolledText(content, height=10, wrap=tk.WORD)
        self.details_text.grid(row=len(fields)+1, column=1, columnspan=4, sticky="wense", padx=5, pady=5)
        
        # URL Scraping area
        url_frame = ttk.LabelFrame(content, text="URLs de la Sc√®ne", padding=10)
        url_frame.grid(row=len(fields)+2, column=0, columnspan=5, sticky="we", pady=10)
        
        self.urls_text = scrolledtext.ScrolledText(url_frame, height=4, wrap=tk.WORD)
        self.urls_text.pack(fill=tk.X, expand=True)
        
        # Toolbar
        toolbar = ttk.Frame(self, padding=10)
        toolbar.pack(side=tk.BOTTOM, fill=tk.X)
        
        ttk.Button(toolbar, text="üíæ Sauvegarder", command=self._save).pack(side=tk.RIGHT, padx=5)
        ttk.Button(toolbar, text="üîç Scraper Sc√®ne", command=self._scrape).pack(side=tk.LEFT, padx=5)

    def _load_from_stash(self):
        if not self.scene_id: return
        data = self.db.get_scene_metadata(self.scene_id)
        if data:
            self.metadata = data
            for label, key in self.fields:
                val = str(data.get(key, '')) if data.get(key) is not None else ''
                self.field_vars[key]['stash'].set(val)
                self.field_vars[key]['main'].set(val)
                self._validate_field(key)
            
            # Details
            self.details_text.delete('1.0', tk.END)
            self.details_text.insert('1.0', data.get('details', ''))

    def _validate_field(self, field):
        f = self.field_vars[field]
        main_val = f['main'].get().strip()
        stash_val = f['stash'].get().strip()
        is_checked = f['check'].get()
        combo = f.get('widget')
        
        if not is_checked:
            color = "white"
        elif not main_val:
            color = "#fff3cd"
        elif main_val.lower() == stash_val.lower():
            color = "#d4edda"
        else:
            color = "#f8d7da"

        if isinstance(combo, ttk.Combobox):
            style_map = {
                "white": "Normal.TCombobox",
                "#fff3cd": "Empty.TCombobox",
                "#d4edda": "Valid.TCombobox",
                "#f8d7da": "Invalid.TCombobox"
            }
            combo.configure(style=style_map.get(color, "Normal.TCombobox"))

    def _scrape(self):
        messagebox.showinfo("Scraping", "Scraping de la sc√®ne √† venir...")

    def _save(self):
        messagebox.showinfo("Sauvegarde", "Donn√©es de la sc√®ne sauvegard√©es dans Stash.")


============================================================
[12/124] gui\url_verification_dialog.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, messagebox
import threading
import webbrowser
import time
import requests
from services.scrapers import HEADERS

class URLVerificationDialog(tk.Toplevel):
    """
    Interface interactive pour la v√©rification s√©quentielle des URLs prioritaires.
    """
    
    def __init__(self, parent, url_manager, existing_urls, performer_name):
        super().__init__(parent)
        self.url_manager = url_manager
        self.existing_urls = existing_urls
        self.performer_name = performer_name
        self.final_urls = None
        
        self.title(f"V√©rification des URLs - {performer_name}")
        self.geometry("650x450")
        self.transient(parent)
        self.grab_set()
        
        # Centrer
        self.update_idletasks()
        w = self.winfo_width()
        h = self.winfo_height()
        x = (self.winfo_screenwidth() // 2) - (w // 2)
        y = (self.winfo_screenheight() // 2) - (h // 2)
        self.geometry(f'{w}x{h}+{x}+{y}')
        
        # Main Container
        main_frame = ttk.Frame(self, padding=20)
        main_frame.pack(fill="both", expand=True)

        # Header
        self.header_lbl = ttk.Label(main_frame, text="D√©marrage...", font=("Segoe UI", 12, "bold"))
        self.header_lbl.pack(pady=(0, 10))
        
        self.status_lbl = ttk.Label(main_frame, text="Pr√©paration...", font=("Segoe UI", 10))
        self.status_lbl.pack(pady=5)
        
        # URL Input Area
        input_frame = ttk.LabelFrame(main_frame, text="URL Actuelle", padding=10)
        input_frame.pack(fill="x", pady=10)
        
        self.url_var = tk.StringVar()
        self.url_entry = ttk.Entry(input_frame, textvariable=self.url_var, font=("Consolas", 10))
        self.url_entry.pack(fill="x", pady=5)
        
        # Action Buttons for Input
        action_frame = ttk.Frame(input_frame)
        action_frame.pack(fill="x", pady=5)
        
        ttk.Button(action_frame, text="Ouvrir le lien", command=self.open_current_url).pack(side="left", padx=5)
        ttk.Button(action_frame, text="Tester le lien", command=self.test_current_url).pack(side="left", padx=5)
        
        # Info / Feedback
        self.info_lbl = ttk.Label(main_frame, text="", foreground="gray", wraplength=600)
        self.info_lbl.pack(pady=10)
        
        # Navigation Buttons
        nav_frame = ttk.Frame(main_frame)
        nav_frame.pack(side="bottom", fill="x", pady=20)
        
        self.btn_ignore = ttk.Button(nav_frame, text="Ignorer / Passer", command=self.on_ignore)
        self.btn_ignore.pack(side="left")
        
        self.btn_search = ttk.Button(nav_frame, text="üîç Rechercher Auto", command=self.on_auto_search)
        self.btn_search.pack(side="left", padx=10)
        
        self.btn_confirm = ttk.Button(nav_frame, text="‚úÖ Confirmer & Suivant", command=self.on_confirm)
        self.btn_confirm.pack(side="right")
        
        # Bouton pour fermer sans attendre (visible seulement en fin de processus)
        self.btn_close_now = ttk.Button(nav_frame, text="‚è≠Ô∏è Terminer Maintenant", command=self.force_close)
        # Initialement cach√©
        
        self.progress = ttk.Progressbar(main_frame, mode="determinate")
        self.progress.pack(side="bottom", fill="x", pady=5)
        
        # State
        self.priority_order = self.url_manager.PRIORITY_ORDER # List of (domain, scraper)
        self.current_step_idx = -1
        self.priority_results = [None] * len(self.priority_order)
        self.other_urls_buffer = []

        # Start process
        self.after(500, self.start_process)

    def start_process(self):
        # 1. Sort existing URLs
        self.sorted_slots = [None] * len(self.priority_order)
        self.other_urls_buffer = []
        
        domain_map = {domain: i for i, (domain, _) in enumerate(self.priority_order)}
        
        for url in self.existing_urls:
            url = url.strip()
            if not url: continue
            
            # Simple domain check
key="***MASKED***"
            idx = domain_map.get(key)
            
            if idx is not None:
                if self.sorted_slots[idx] is None:
                    # Check profile URL if needed? Or accept first one?
                    if self.url_manager.is_profile_url(url, key):
                        self.sorted_slots[idx] = url
                    else:
                        self.other_urls_buffer.append(url)
                else:
                    self.other_urls_buffer.append(url)
            else:
                self.other_urls_buffer.append(url)
        
        self.next_step()

    def next_step(self):
        self.current_step_idx += 1
        
        if self.current_step_idx >= len(self.priority_order):
            self.finish_process()
            return
            
        domain, scraper = self.priority_order[self.current_step_idx]
        self.header_lbl.config(text=f"Source {self.current_step_idx + 1}/{len(self.priority_order)} : {domain}")
        self.progress["value"] = ((self.current_step_idx) / len(self.priority_order)) * 100
        
        existing = self.sorted_slots[self.current_step_idx]
        
        if existing:
            self.url_var.set(existing)
            self.status_lbl.config(text="URL pr√©sente. Validation en cours...", foreground="blue")
            self.info_lbl.config(text="Une URL existe d√©j√† pour ce domaine. V√©rification de sa validit√©...")
            self.test_current_url(auto=True)
        else:
            self.url_var.set("")
            self.status_lbl.config(text="URL manquante.", foreground="orange")
            self.info_lbl.config(text=f"Aucune URL trouv√©e pour {domain}. Lancement de la recherche automatique...")
            self.on_auto_search()

    def test_current_url(self, auto=False):
        url = self.url_var.get().strip()
        if not url:
            if not auto: messagebox.showwarning("Attention", "Aucune URL √† tester.")
            return

        self.set_busy(True)
        def run():
            is_ok = self.url_manager.is_url_reachable(url)
            # Utiliser winfo_exists() pour v√©rifier que le widget existe encore
            if self.winfo_exists():
                self.after(0, lambda: self.show_test_result(is_ok, auto))
        threading.Thread(target=run, daemon=True).start()

    def show_test_result(self, is_ok, auto):
        self.set_busy(False)
        if is_ok:
            self.status_lbl.config(text="URL Valide (200 OK)", foreground="green")
            self.info_lbl.config(text="Le lien fonctionne correctement. Vous pouvez confirmer.")
        else:
            self.status_lbl.config(text="URL Invalide / inaccessible", foreground="red")
            self.info_lbl.config(text="Le lien ne r√©pond pas. Essayez de rechercher une nouvelle URL.")
            if auto:
                # If auto-check failed on existing URL, suggest search?
                pass

    def on_auto_search(self):
        domain, _ = self.priority_order[self.current_step_idx]
        self.set_busy(True)
        self.status_lbl.config(text=f"Recherche sur {domain}...", foreground="blue")
        
        def run():
            found = self.url_manager.search_url_for_domain(domain, self.performer_name)
            # Utiliser winfo_exists() pour v√©rifier que le widget existe encore
            if self.winfo_exists():
                self.after(0, lambda: self.show_search_result(found))
            
        threading.Thread(target=run, daemon=True).start()

    def show_search_result(self, found_url):
        self.set_busy(False)
        if found_url:
            self.url_var.set(found_url)
            self.status_lbl.config(text="URL trouv√©e !", foreground="green")
            self.info_lbl.config(text=f"Trouv√© : {found_url}\nVeuillez confirmer si cela correspond au profil.")
            # Auto-test validity?
            self.test_current_url(auto=True)
        else:
            self.status_lbl.config(text="Aucun r√©sultat.", foreground="red")
            self.info_lbl.config(text="La recherche automatique n'a rien donn√©. Vous pouvez entrer une URL manuellement ou passer.")

    def on_confirm(self):
        url = self.url_var.get().strip()
        # Allow empty confirmation? Means "None"
        self.priority_results[self.current_step_idx] = url if url else None
        self.next_step()

    def on_ignore(self):
        self.priority_results[self.current_step_idx] = None
        self.next_step()

    def open_current_url(self):
        url = self.url_var.get().strip()
        if url: webbrowser.open(url)
    
    def force_close(self):
        """Ferme imm√©diatement sans attendre la validation des URLs secondaires"""
        # Construct final list avec ce qu'on a d√©j√†
        final_list = []
        for u in self.priority_results:
            if u: final_list.append(u)
        # Ajouter les URLs secondaires sans validation (trop lent)
        seen = set(final_list)
        for u in self.other_urls_buffer:
            if u not in seen:
                final_list.append(u)
                seen.add(u)
        
        self.final_urls = final_list[:50]
        print(f"[URLVerificationDialog] Fermeture forc√©e - {len(self.final_urls)} URLs")
        self.destroy()

    def set_busy(self, busy):
        state = "disabled" if busy else "normal"
        self.btn_confirm.config(state=state)
        self.btn_search.config(state=state)
        self.btn_ignore.config(state=state)
        self.url_entry.config(state=state)
        if busy:
            self.config(cursor="wait")
        else:
            self.config(cursor="")

    def finish_process(self):
        """Finalise le processus et ferme la fen√™tre"""
        self.header_lbl.config(text="Finalisation...")
        self.status_lbl.config(text="Construction de la liste finale...", foreground="black")
        self.progress["value"] = 100
        
        print(f"[URLVerificationDialog] Finalisation - URLs prioritaires: {len([u for u in self.priority_results if u])}")
        print(f"[URLVerificationDialog] URLs secondaires: {len(self.other_urls_buffer)}")
        
        # Construct final list: Priorities first, then others (sans validation suppl√©mentaire)
        final_list = []
        seen = set()
        
        # Ajouter les URLs prioritaires valid√©es
        for u in self.priority_results:
            if u and u not in seen:
                final_list.append(u)
                seen.add(u)
        
        # Ajouter les URLs secondaires (sans re-validation pour √©viter les blocages)
        # Les 6 sources prioritaires sont d√©j√† valid√©es, c'est suffisant
        for u in self.other_urls_buffer:
            if u not in seen:
                final_list.append(u)
                seen.add(u)
                if len(final_list) >= 50:  # Limite √† 50 URLs totales
                    break
        
        self.final_urls = final_list[:50]
        print(f"[URLVerificationDialog] Liste finale: {len(self.final_urls)} URLs")
        
        # Fermeture imm√©diate
        self.after(100, self.destroy)



============================================================
[13/124] inspect_custom_fields.py
------------------------------------------------------------
import sqlite3
import os

db_path = "H:/Stash/stash-go.sqlite"
if not os.path.exists(db_path):
    db_path = "data/database.sqlite"

conn = sqlite3.connect(db_path)
cur = conn.cursor()

# Get all tables
cur.execute("SELECT name FROM sqlite_master WHERE type='table'")
tables = [t[0] for t in cur.fetchall()]
print(f"Tables: {', '.join(tables)}")

# Check performers columns
cur.execute("PRAGMA table_info(performers)")
p_cols = [c[1] for c in cur.fetchall()]
print(f"Performer columns: {', '.join(p_cols)}")

# check for any table with 'custom' in it
custom_tables = [t for t in tables if 'custom' in t.lower()]
print(f"Custom field tables: {', '.join(custom_tables)}")

for t in custom_tables:
    print(f"\n--- {t} ---")
    cur.execute(f"PRAGMA table_info({t})")
    for c in cur.fetchall(): print(c)
    cur.execute(f"SELECT * FROM {t} LIMIT 10")
    for r in cur.fetchall(): print(r)

conn.close()


============================================================
[14/124] inspect_db.py
------------------------------------------------------------
import sqlite3
import os

db_path = "H:/Stash/stash-go.sqlite"
if not os.path.exists(db_path):
    db_path = "data/database.sqlite"

if not os.path.exists(db_path):
    print(f"Database not found at {db_path}")
    exit(1)

conn = sqlite3.connect(db_path)
cur = conn.cursor()

print(f"--- Schema for table 'performers' ({db_path}) ---")
cur.execute("PRAGMA table_info(performers)")
columns = cur.fetchall()
for col in columns:
    print(col)

print("\n--- List of all tables ---")
cur.execute("SELECT name FROM sqlite_master WHERE type='table'")
tables = cur.fetchall()
for table in tables:
    print(table[0])

# Check for custom fields related tables
for t in ["performer_custom_fields", "custom_fields"]:
    if any(t == table[0] for table in tables):
        print(f"\n--- Schema for table '{t}' ---")
        cur.execute(f"PRAGMA table_info({t})")
        cols = cur.fetchall()
        for c in cols:
            print(c)
        
        cur.execute(f"SELECT * FROM {t} LIMIT 5")
        rows = cur.fetchall()
        print(f"Sample data from {t}:")
        for r in rows:
            print(r)

conn.close()


============================================================
[15/124] Legacy\.gitignore
------------------------------------------------------------
__pycache__/
*.pyc
.env
.venv
.idea/
.vscode/


============================================================
[16/124] Legacy\check_db.py
------------------------------------------------------------
# check_db.py ‚Äî v√©rification rapide DB
import sqlite3, sys

DB_PATH = r"F:\stash\stash-go.sqlite"  # adapter selon votre installation

conn = sqlite3.connect(DB_PATH)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

# V√©rifier un performer (remplacer 42 par un ID r√©el test√©)
performer_id = 42

print(f"=== Performer {performer_id} ===")
cur.execute("SELECT name, details, tattoos, piercings FROM performers WHERE id=?",
            (performer_id,))
row = cur.fetchone()
if row:
    print(f"  Name: {row['name']}")
    print(f"  Details: {row['details'][:80] if row['details'] else 'VIDE'}...")
    print(f"  Tattoos: {row['tattoos']}")

cur.execute("SELECT url FROM performer_urls WHERE performer_id=?", (performer_id,))
urls = [r['url'] for r in cur.fetchall()]
print(f"  URLs: {urls}")

cur.execute("""
    SELECT t.name FROM tags t
    JOIN performers_tags pt ON pt.tag_id = t.id
    WHERE pt.performer_id=?
""", (performer_id,))
tags = [r['name'] for r in cur.fetchall()]
print(f"  Tags: {tags[:10]}")

# V√©rifier un group (remplacer 5 par un ID r√©el test√©)
group_id = 5
print(f"\n=== Group {group_id} ===")
cur.execute("SELECT name, director, description FROM groups WHERE id=?", (group_id,))
row = cur.fetchone()
if row:
    print(f"  Name: {row['name']}")
    print(f"  Director: {row['director']}")

# Sc√®nes avec URLs
cur.execute("""
    SELECT s.title, su.url
    FROM groups_scenes gs
    JOIN scenes s ON s.id = gs.scene_id
    LEFT JOIN scene_urls su ON su.scene_id = s.id
    WHERE gs.group_id=?
    LIMIT 5
""", (group_id,))
for row in cur.fetchall():
    print(f"  Sc√®ne: {row['title']} ‚Üí {row['url']}")

conn.close()

============================================================
[17/124] Legacy\config\__init__.py
------------------------------------------------------------


============================================================
[18/124] Legacy\config\settings.yaml
------------------------------------------------------------
# Configuration de StashMaster V2
phase1_fields:
  - Name
  - Disambiguation
  - Aliases
  - Birthdate
  - Deathdate
  - Country
  - Ethnicity
  - Hair Color
  - Eye Color
  - Height
  - Weight
  - Measurements
  - Fake Tits
  - Career Length
phase2_fields:
  - Bio
  - Trivia
  - Awards
  - Tattoos
  - Piercings
  - Tags
  - URLs
  - Details

# ‚îÄ‚îÄ Groups (DVDs) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
group_phase1_fields:
  - Title
  - Aliases
  - Date
  - Studio
  - Director
  - Duration
  - Description
  - Tags
  - URLs

group_sources_priority:
  - data18          # P1 ‚Äî tags, sc√®nes, synopsis d√©taill√©s
  - iafd_dvd        # P2 ‚Äî r√©f√©rence US/classiques, cast, dates fiables
  - adultdvdempire  # P3 ‚Äî synopsis, covers
  - jeedoo          # P4 ‚Äî productions europ√©ennes uniquement

group_phase2_sources:  # sources avec URLs de sc√®nes individuelles
  - data18
  - adultdvdempire


============================================================
[19/124] Legacy\files\stashmaster-v2\stashmaster-v2\.gitignore
------------------------------------------------------------
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
ENV/
env/
.venv

# IDEs
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store

# Data
data/
*.sqlite
*.db

# Logs
*.log

# Test
.coverage
.pytest_cache/
htmlcov/

# Configuration locale
config.local.json
*.local.*

# Temporary files
*.tmp
*.temp
.cache/


============================================================
[20/124] Legacy\files\stashmaster-v2\stashmaster-v2\CHANGELOG.md
------------------------------------------------------------
# Changelog

Toutes les modifications notables du projet sont document√©es dans ce fichier.

## [2.0.0] - 2026-02-25

### Ajout√©
- ‚ú® **Interface unifi√©e** : Fusion compl√®te des Phase 1 et Phase 2 en une seule GUI
- üè∑Ô∏è **Syst√®me de tags intelligent** : G√©n√©ration automatique bas√©e sur des r√®gles m√©tadonn√©es
  - Tags bas√©s sur l'ethnicit√© (Caucasian, Latina, Asian, Ebony)
  - Tags bas√©s sur la couleur de cheveux (Blonde, Brunette, Redhead, Black Hair)
  - Tags bas√©s sur les mesures (Big Boobs, Small Boobs)
  - Tags pour piercings et tattoos
  - Tag MILF bas√© sur l'√¢ge de carri√®re
- üìù **Champs multilignes** pour Piercings, Tattoos et URLs
- ü™ü **Fen√™tre Trivia & Awards d√©di√©e** avec :
  - Scraping cibl√© depuis IAFD
  - Affichage s√©par√© des requ√™tes et r√©sultats
  - Nettoyage automatique des awards (1 par ligne)
- üìÑ **G√©n√©ration de bio automatique** avec 2 modes :
  - Bio Google : Template de 3000 caract√®res professionnel
  - Bio Ollama : IA locale avec prompt personnalis√©
- üîÑ **ScraperOrchestrator** : Scraping multi-sources avec fusion intelligente
- ‚úÖ **DataMerger** : D√©tection automatique des donn√©es confirm√©es et conflits
- üßπ **AwardsCleaner** : Formatage intelligent des awards
- üìä **Onglets organis√©s** : M√©tadonn√©es, Champs Avanc√©s, Bio

### Modifi√©
- üîß **Tags** : Ne sont plus scrap√©s, uniquement g√©n√©r√©s par r√®gles
- üìã **Interface** : Onglets au lieu de fen√™tres s√©par√©es
- üéØ **Workflow** : Simplifi√© et plus intuitif
- üíæ **Architecture** : Code modulaire avec s√©paration des responsabilit√©s

### Supprim√©
- ‚ùå Scraping de tags depuis les sources (remplac√© par g√©n√©ration automatique)
- ‚ùå Fen√™tres multiples (remplac√© par onglets)

### Technique
- üêç Python 3.8+ requis
- üì¶ D√©pendances : requests, beautifulsoup4, lxml
- ü§ñ Support optionnel d'Ollama pour g√©n√©ration IA
- üèóÔ∏è Architecture MVC am√©lior√©e

### Documentation
- üìñ README complet avec guide d'utilisation
- üéì Documentation des r√®gles de tags
- üí° Exemples et FAQ
- üõ†Ô∏è Guide de configuration avanc√©e

---

## [1.0.0] - Version Pr√©c√©dente

### Fonctionnalit√©s
- Interface Phase 1 : M√©tadonn√©es usuelles avec scraping
- Interface Phase 2 : Champs avanc√©s s√©par√©s
- Scraping basique depuis IAFD et autres sources
- Tags scrap√©s depuis les sources
- Bio manuelle

### Limitations
- Deux fen√™tres s√©par√©es
- Tags scrap√©s pas toujours coh√©rents
- Pas de g√©n√©ration automatique de bio
- Awards bruts non format√©s
- Workflow moins fluide

---

## √Ä venir

### [2.1.0] - Planifi√©
- [ ] Base de donn√©es SQLite int√©gr√©e
- [ ] Export vers Stash
- [ ] Import depuis fichiers JSON
- [ ] Historique des modifications
- [ ] Undo/Redo
- [ ] Raccourcis clavier
- [ ] Th√®mes dark/light
- [ ] Support multi-langues

### [2.2.0] - En r√©flexion
- [ ] Scraping d'images
- [ ] D√©tection automatique de doublons
- [ ] Suggestions intelligentes
- [ ] API REST pour int√©grations
- [ ] Plugin system
- [ ] Scraping de sc√®nes/films
- [ ] Statistiques et graphiques

---

**Format du Changelog** : [Keep a Changelog](https://keepachangelog.com/)  
**Versioning** : [Semantic Versioning](https://semver.org/)


============================================================
[21/124] Legacy\files\stashmaster-v2\stashmaster-v2\config.json
------------------------------------------------------------
{
  "scrapers": {
    "timeout": 10,
    "retry_count": 3,
    "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
  },
  "bio_generation": {
    "google_template_length": 3000,
    "ollama_url": "http://localhost:11434/api/generate",
    "ollama_model": "llama2",
    "ollama_timeout": 120
  },
  "tag_rules": {
    "ethnicity_tags": {
      "caucasian": "Caucasian",
      "latin": "Latina",
      "cuban": "Latina",
      "asian": "Asian",
      "ebony": "Ebony",
      "african": "Ebony"
    },
    "hair_color_tags": {
      "blonde": "Blonde",
      "blond": "Blonde",
      "brown": "Brunette",
      "brunette": "Brunette",
      "red": "Redhead",
      "auburn": "Redhead",
      "black": "Black Hair"
    },
    "measurements_thresholds": {
      "big_boobs_min": 36,
      "small_boobs_max": 32
    },
    "career_length_thresholds": {
      "milf_years": 10
    }
  },
  "sources": {
    "iafd": {
      "enabled": true,
      "priority": 1
    },
    "freeones": {
      "enabled": true,
      "priority": 2
    },
    "babepedia": {
      "enabled": true,
      "priority": 3
    },
    "thenude": {
      "enabled": true,
      "priority": 4
    }
  },
  "ui": {
    "window_width": 1200,
    "window_height": 900,
    "theme": "default"
  },
  "data": {
    "performers_dir": "data/performers",
    "database_path": "data/database.sqlite"
  }
}


============================================================
[22/124] Legacy\files\stashmaster-v2\stashmaster-v2\CONTRIBUTING.md
------------------------------------------------------------
# Guide de Contribution

Merci de votre int√©r√™t pour contribuer √† StashMaster V2 ! üéâ

## üìã Table des Mati√®res

- [Code de Conduite](#code-de-conduite)
- [Comment Contribuer](#comment-contribuer)
- [D√©veloppement](#d√©veloppement)
- [Standards de Code](#standards-de-code)
- [Tests](#tests)
- [Documentation](#documentation)

## ü§ù Code de Conduite

- Soyez respectueux envers tous les contributeurs
- Fournissez des critiques constructives
- Concentrez-vous sur ce qui est le mieux pour le projet
- Acceptez les feedbacks avec gr√¢ce

## üí° Comment Contribuer

### Rapporter des Bugs

Avant de cr√©er une issue :
1. V√©rifiez si le bug n'a pas d√©j√† √©t√© rapport√©
2. Utilisez la derni√®re version du code
3. Testez avec une installation propre

Pour rapporter un bug, incluez :
- **Description claire** du probl√®me
- **√âtapes pour reproduire** le bug
- **Comportement attendu** vs. comportement observ√©
- **Screenshots** si applicable
- **Environnement** : OS, version Python, d√©pendances
- **Logs d'erreur** si disponibles

### Sugg√©rer des Am√©liorations

Pour sugg√©rer une nouvelle fonctionnalit√© :
1. V√©rifiez si elle n'est pas d√©j√† planifi√©e (voir CHANGELOG)
2. Cr√©ez une issue avec le label "enhancement"
3. D√©crivez clairement :
   - Le probl√®me que √ßa r√©sout
   - Comment √ßa devrait fonctionner
   - Des exemples d'utilisation
   - Des alternatives consid√©r√©es

### Soumettre des Pull Requests

1. **Fork** le projet
2. **Cr√©ez une branche** pour votre fonctionnalit√©
   ```bash
   git checkout -b feature/ma-super-feature
   ```
3. **Committez** vos changements
   ```bash
   git commit -m "feat: ajout de ma super feature"
   ```
4. **Pushez** vers la branche
   ```bash
   git push origin feature/ma-super-feature
   ```
5. **Ouvrez une Pull Request**

## üõ†Ô∏è D√©veloppement

### Configuration de l'Environnement

```bash
# Cloner le repository
git clone https://github.com/votre-username/stashmaster-v2.git
cd stashmaster-v2

# Cr√©er un environnement virtuel
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer les d√©pendances
pip install -r requirements.txt

# Installer les d√©pendances de d√©veloppement (si disponibles)
pip install -r requirements-dev.txt
```

### Structure du Projet

```
stashmaster-v2/
‚îÇ
‚îú‚îÄ‚îÄ stashmaster_unified.py    # Application principale
‚îÇ   ‚îú‚îÄ‚îÄ MainWindow            # GUI principale
‚îÇ   ‚îú‚îÄ‚îÄ TriviaAwardsWindow    # Fen√™tre Trivia/Awards
‚îÇ   ‚îú‚îÄ‚îÄ BioGenerationWindow   # Fen√™tre g√©n√©ration de bio
‚îÇ   ‚îú‚îÄ‚îÄ TagRulesEngine        # Moteur de tags
‚îÇ   ‚îú‚îÄ‚îÄ AwardsCleaner         # Nettoyeur d'awards
‚îÇ   ‚îî‚îÄ‚îÄ BioGenerator          # G√©n√©rateur de bio
‚îÇ
‚îú‚îÄ‚îÄ scrapers.py               # Modules de scraping
‚îÇ   ‚îú‚îÄ‚îÄ ScraperBase           # Classe de base
‚îÇ   ‚îú‚îÄ‚îÄ IAFDScraper           # Scraper IAFD
‚îÇ   ‚îú‚îÄ‚îÄ FreeonesScraper       # Scraper Freeones
‚îÇ   ‚îú‚îÄ‚îÄ BabepaediaScraper     # Scraper Babepedia
‚îÇ   ‚îú‚îÄ‚îÄ TheNudeScraper        # Scraper TheNude
‚îÇ   ‚îú‚îÄ‚îÄ DataMerger            # Fusionneur de donn√©es
‚îÇ   ‚îî‚îÄ‚îÄ ScraperOrchestrator   # Orchestrateur
‚îÇ
‚îú‚îÄ‚îÄ test_stashmaster.py       # Tests unitaires
‚îú‚îÄ‚îÄ config.json               # Configuration
‚îú‚îÄ‚îÄ requirements.txt          # D√©pendances
‚îú‚îÄ‚îÄ README.md                 # Documentation
‚îú‚îÄ‚îÄ CHANGELOG.md              # Historique des versions
‚îî‚îÄ‚îÄ CONTRIBUTING.md           # Ce fichier
```

### Lancer l'Application en Mode D√©veloppement

```bash
# Mode normal
python3 stashmaster_unified.py

# Avec logs de debug (√† impl√©menter)
python3 stashmaster_unified.py --debug

# Avec un performer sp√©cifique (√† impl√©menter)
python3 stashmaster_unified.py --performer "Bridgette B"
```

## üìù Standards de Code

### Style Python

Suivez [PEP 8](https://www.python.org/dev/peps/pep-0008/) :

```python
# Bonnes pratiques
class MyClass:
    """Docstring pour la classe"""
    
    def my_method(self, param1: str, param2: int) -> bool:
        """Docstring pour la m√©thode
        
        Args:
            param1: Description du param√®tre 1
            param2: Description du param√®tre 2
            
        Returns:
            Description du retour
        """
        # Code ici
        return True

# Imports group√©s
import sys
import os
from typing import Dict, List

import requests
from bs4 import BeautifulSoup

from scrapers import IAFDScraper
```

### Nommage

- **Classes** : PascalCase (`TagRulesEngine`)
- **Fonctions/M√©thodes** : snake_case (`generate_tags`)
- **Constantes** : UPPER_CASE (`MAX_RETRIES`)
- **Variables priv√©es** : pr√©fixe `_` (`_internal_method`)

### Docstrings

Utilisez le format Google :

```python
def scrape_performer(self, url: str) -> Dict:
    """Scrape les donn√©es d'un performer.
    
    Args:
        url: L'URL de la page du performer
        
    Returns:
        Dictionnaire contenant les m√©tadonn√©es du performer
        
    Raises:
        ValueError: Si l'URL est invalide
        RequestException: Si le scraping √©choue
        
    Examples:
        >>> scraper.scrape_performer("https://example.com/performer")
        {'name': 'John Doe', 'birthdate': '1990-01-01'}
    """
    pass
```

### Type Hints

Utilisez les type hints pour am√©liorer la lisibilit√© :

```python
from typing import Dict, List, Optional, Tuple

def merge_data(self, sources: List[Dict]) -> Tuple[Dict, Dict]:
    """Fusionne les donn√©es de plusieurs sources"""
    pass

def get_performer(self, id: int) -> Optional[Dict]:
    """R√©cup√®re un performer par ID"""
    pass
```

## üß™ Tests

### Lancer les Tests

```bash
# Tous les tests
python3 test_stashmaster.py

# Tests sp√©cifiques
python3 -m unittest test_stashmaster.TestTagRulesEngine

# Avec couverture (si coverage install√©)
coverage run test_stashmaster.py
coverage report
```

### √âcrire des Tests

```python
import unittest

class TestMyFeature(unittest.TestCase):
    def setUp(self):
        """Pr√©paration avant chaque test"""
        self.engine = TagRulesEngine()
    
    def tearDown(self):
        """Nettoyage apr√®s chaque test"""
        pass
    
    def test_my_feature(self):
        """Test de ma fonctionnalit√©"""
        result = self.engine.generate_tags({})
        self.assertIsInstance(result, list)
        self.assertEqual(len(result), 0)
```

### Couverture de Tests

Visez au minimum :
- 80% de couverture pour le code principal
- 60% pour les scrapers (d√©pendent de sources externes)
- 100% pour les utilitaires critiques (TagRulesEngine, DataMerger)

## üìö Documentation

### Documenter le Code

- **Classes** : Docstring avec description, attributs
- **M√©thodes** : Docstring avec Args, Returns, Raises
- **Modules** : Docstring d'en-t√™te avec description g√©n√©rale

### Mettre √† Jour la Documentation

Lors de l'ajout de fonctionnalit√©s :
1. **README.md** : Ajouter dans la section correspondante
2. **CHANGELOG.md** : Documenter le changement
3. **Docstrings** : Commenter le code
4. **config.json** : Ajouter les nouvelles options

## üîÄ Workflow Git

### Branches

- `main` : Code stable, production
- `develop` : D√©veloppement en cours
- `feature/*` : Nouvelles fonctionnalit√©s
- `bugfix/*` : Corrections de bugs
- `hotfix/*` : Corrections urgentes

### Messages de Commit

Utilisez [Conventional Commits](https://www.conventionalcommits.org/) :

```bash
# Format
<type>(<scope>): <description>

[corps optionnel]

[footer(s) optionnel(s)]

# Exemples
feat(tags): ajout de la r√®gle MILF bas√©e sur l'√¢ge
fix(scraper): correction du parsing IAFD
docs(readme): mise √† jour des instructions d'installation
test(tags): ajout de tests pour les tags d'ethnicit√©
refactor(bio): am√©lioration de la g√©n√©ration Google
style(ui): correction de l'alignement des boutons
```

Types :
- `feat` : Nouvelle fonctionnalit√©
- `fix` : Correction de bug
- `docs` : Documentation
- `style` : Formatage (pas de changement de code)
- `refactor` : Refactoring
- `test` : Ajout de tests
- `chore` : Maintenance

## üéØ Priorit√©s de D√©veloppement

Consultez le [CHANGELOG.md](CHANGELOG.md) pour voir les fonctionnalit√©s planifi√©es.

### Court Terme (v2.1)
- Base de donn√©es SQLite
- Export vers Stash
- Import JSON
- Historique des modifications

### Moyen Terme (v2.2)
- Scraping d'images
- D√©tection de doublons
- API REST
- Plugin system

### Long Terme (v3.0)
- Interface web
- Multi-utilisateurs
- Synchronisation cloud
- Mobile app

## ‚ùì Questions ?

N'h√©sitez pas √† :
- Ouvrir une issue pour discuter
- Rejoindre les discussions
- Contacter les mainteneurs

## üôè Remerciements

Merci √† tous les contributeurs qui aident √† am√©liorer StashMaster V2 !

---

**Happy Coding!** üöÄ


============================================================
[23/124] Legacy\files\stashmaster-v2\stashmaster-v2\data\README.md
------------------------------------------------------------
# Dossier Data

Ce dossier contient toutes les donn√©es sauvegard√©es par StashMaster V2.

## Structure

```
data/
‚îú‚îÄ‚îÄ performers/          # JSON files des performers
‚îÇ   ‚îú‚îÄ‚îÄ performer_1.json
‚îÇ   ‚îú‚îÄ‚îÄ performer_2.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ database.sqlite     # Base de donn√©es (futur)
```

## Format JSON des Performers

```json
{
  "name": "Performer Name",
  "aliases": ["Alias 1", "Alias 2"],
  "birthdate": "January 1, 1990",
  "birthplace": "City, Country",
  "ethnicity": "Caucasian",
  "hair_color": "Blonde",
  "eye_color": "Blue",
  "height": "170 cm",
  "weight": "55 kg",
  "measurements": "34DD-25-36",
  "tattoos": "Description of tattoos",
  "piercings": "Description of piercings",
  "career_length": "2010-",
  "tags": ["Tag1", "Tag2", "Tag3"],
  "urls": [
    "https://source1.com/...",
    "https://source2.com/..."
  ],
  "trivia": "Interesting facts...",
  "awards": "Awards and nominations...",
  "bio": "Full biography (3000 characters)..."
}
```

## Sauvegarde et Restauration

### Sauvegarde manuelle
Copiez simplement le dossier `data/` pour cr√©er une sauvegarde.

### Restauration
Remplacez le dossier `data/` par votre sauvegarde.

## Notes

- Les fichiers JSON sont encod√©s en UTF-8
- Les noms de fichiers sont en minuscules avec underscores
- La base de donn√©es SQLite sera ajout√©e dans une version future


============================================================
[24/124] Legacy\files\stashmaster-v2\stashmaster-v2\EXAMPLES.md
------------------------------------------------------------
# Exemples d'Utilisation

Ce document contient des exemples pratiques pour utiliser StashMaster V2.

## üìã Exemples Basiques

### Exemple 1 : Scraping Simple

```python
# Dans un script Python
from scrapers import ScraperOrchestrator

# Cr√©er l'orchestrateur
orchestrator = ScraperOrchestrator()

# URLs √† scraper
urls = [
    "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
    "https://www.freeones.xxx/bridgette-b"
]

# Scraper et fusionner
confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)

# Afficher les r√©sultats
print(f"Donn√©es de {num_sources} source(s)")
print("\nConfirm√©es:")
for field, info in confirmed.items():
    print(f"  {field}: {info['value']}")

print("\nConflits:")
for field, values in conflicts.items():
    print(f"  {field}:")
    for v in values:
        print(f"    - {v['value']} ({', '.join(v['sources'])})")
```

### Exemple 2 : G√©n√©ration de Tags

```python
from stashmaster_unified import TagRulesEngine

# Cr√©er le moteur de r√®gles
engine = TagRulesEngine()

# M√©tadonn√©es d'exemple
metadata = {
    'ethnicity': 'Latina',
    'hair_color': 'Blonde',
    'measurements': '36DD-25-36',
    'piercings': 'Navel',
    'tattoos': 'Lower back',
    'career_length': '2007-'
}

# G√©n√©rer les tags
tags = engine.generate_tags(metadata)
print(f"Tags g√©n√©r√©s: {', '.join(tags)}")
# Output: Tags g√©n√©r√©s: Latina, Blonde, Big Boobs, Pierced, Tattooed, MILF
```

### Exemple 3 : Nettoyage d'Awards

```python
from stashmaster_unified import AwardsCleaner

cleaner = AwardsCleaner()

# Awards bruts
raw_awards = """
AVN AWARDS2012Winner: Unsung Starlet of the Year2014Nominee: Unsung Starlet of the Year
2015Nominee: Fan Award: Best Boobs
"""

# Nettoyer
cleaned = cleaner.clean_awards(raw_awards)
print(cleaned)
```

Output:
```
AVN AWARDS

2012
  Winner: Unsung Starlet of the Year

2014
  Nominee: Unsung Starlet of the Year

2015
  Nominee: Fan Award: Best Boobs
```

### Exemple 4 : G√©n√©ration de Bio

```python
from stashmaster_unified import BioGenerator

generator = BioGenerator()

# M√©tadonn√©es du performer
metadata = {
    'name': 'Bridgette B',
    'birthdate': 'October 15, 1983',
    'birthplace': 'Barcelona, Spain',
    'ethnicity': 'Caucasian',
    'hair_color': 'Blonde',
    'measurements': '34DD-27-34',
    'height': '173 cm',
    'weight': '129 lbs',
    'career_start': '2007',
    'aliases': ['Bridget B', 'Bridgette', 'Spanish Doll']
}

# G√©n√©rer bio Google
bio = generator.generate_google_bio('Bridgette B', metadata)
print(f"Bio g√©n√©r√©e ({len(bio)} caract√®res):\n{bio}")
```

## üîß Exemples Avanc√©s

### Exemple 5 : Fusion de Donn√©es Complexes

```python
from scrapers import DataMerger

merger = DataMerger()

# Donn√©es de 3 sources diff√©rentes
sources = [
    {
        'source': 'iafd',
        'name': 'Bridgette B',
        'birthdate': 'October 15, 1983',
        'ethnicity': 'Caucasian',
        'hair_color': 'Blonde',
        'measurements': '34DD-27-34'
    },
    {
        'source': 'freeones',
        'name': 'Bridgette B',
        'birthdate': 'October 15, 1983',
        'ethnicity': 'Caucasian',
        'hair_color': 'Blonde',  # Conflit
        'height': '173 cm'
    },
    {
        'source': 'babepedia',
        'name': 'Bridgette B',
        'birthdate': 'October 15, 1983',
        'ethnicity': 'Caucasian',
        'hair_color': 'Brown',  # Conflit
        'weight': '129 lbs'
    }
]

# Fusionner
confirmed, conflicts = merger.merge_data(sources)

print("=== Donn√©es Confirm√©es ===")
for field, info in confirmed.items():
    sources_str = ', '.join(info['sources'])
    print(f"{field}: {info['value']} ({info['count']} sources: {sources_str})")

print("\n=== Conflits ===")
for field, values in conflicts.items():
    print(f"\n{field}:")
    for v in values:
        print(f"  - {v['value']} ({v['count']} sources: {', '.join(v['sources'])})")
```

### Exemple 6 : Scraping avec Gestion d'Erreurs

```python
from scrapers import IAFDScraper
import requests

scraper = IAFDScraper()

urls = [
    "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
    "https://www.iafd.com/person.rme/perfid=invalid/gender=f/invalid.htm"
]

for url in urls:
    print(f"\nScraping: {url}")
    try:
        data = scraper.scrape_performer(url)
        if data:
            print(f"  ‚úÖ Succ√®s: {data.get('name', 'Unknown')}")
            print(f"  Champs: {len(data)}")
        else:
            print("  ‚ùå √âchec: Aucune donn√©e retourn√©e")
    except requests.RequestException as e:
        print(f"  ‚ùå Erreur r√©seau: {e}")
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
```

### Exemple 7 : Cr√©ation d'un Scraper Personnalis√©

```python
from scrapers import ScraperBase
from typing import Dict

class MonSiteScraper(ScraperBase):
    """Scraper pour mon site personnalis√©"""
    
    def scrape_performer(self, url: str) -> Dict:
        """Scrape un performer depuis mon site"""
        soup = self.get_page(url)
        if not soup:
            return {}
        
        data = {
            'source': 'monsite',
            'url': url
        }
        
        try:
            # Extraire le nom
            name_elem = soup.find('h1', class_='performer-name')
            if name_elem:
                data['name'] = name_elem.text.strip()
            
            # Extraire la date de naissance
            birthday_elem = soup.find('span', class_='birthday')
            if birthday_elem:
                data['birthdate'] = birthday_elem.text.strip()
            
            # Ajouter d'autres extractions...
            
        except Exception as e:
            print(f"Erreur: {e}")
        
        return data

# Utilisation
scraper = MonSiteScraper()
data = scraper.scrape_performer("https://monsite.com/performer/123")
print(data)
```

### Exemple 8 : Int√©gration avec l'Interface

```python
# Dans votre propre script
from stashmaster_unified import MainWindow
import tkinter as tk

# Cr√©er et configurer la fen√™tre
app = MainWindow()

# Pr√©-remplir des donn√©es (exemple)
app.metadata_entries['name'].insert(0, "Bridgette B")
app.urls_text.insert('1.0', "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm")

# Lancer l'application
app.mainloop()
```

## üéì Cas d'Usage R√©els

### Cas 1 : Workflow Complet Automatis√©

```python
#!/usr/bin/env python3
"""
Workflow automatis√© complet pour un performer
"""

from scrapers import ScraperOrchestrator
from stashmaster_unified import TagRulesEngine, BioGenerator
import json

def process_performer(name: str, urls: list) -> dict:
    """Traite compl√®tement un performer"""
    print(f"\n{'='*50}")
    print(f"Traitement de: {name}")
    print('='*50)
    
    # 1. Scraping
    print("\n1. Scraping des sources...")
    orchestrator = ScraperOrchestrator()
    confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)
    print(f"   ‚úÖ {num_sources} source(s) scrap√©e(s)")
    print(f"   ‚úÖ {len(confirmed)} champ(s) confirm√©(s)")
    print(f"   ‚ö†Ô∏è  {len(conflicts)} conflit(s)")
    
    # 2. Pr√©parer les m√©tadonn√©es
    print("\n2. Pr√©paration des m√©tadonn√©es...")
    metadata = {key: info['value'] for key, info in confirmed.items()}
    metadata['name'] = name
    
    # 3. G√©n√©rer les tags
    print("\n3. G√©n√©ration des tags...")
    tag_engine = TagRulesEngine()
    tags = tag_engine.generate_tags(metadata)
    metadata['tags'] = tags
    print(f"   ‚úÖ {len(tags)} tag(s) g√©n√©r√©(s): {', '.join(tags)}")
    
    # 4. G√©n√©rer la bio
    print("\n4. G√©n√©ration de la bio...")
    bio_generator = BioGenerator()
    bio = bio_generator.generate_google_bio(name, metadata)
    metadata['bio'] = bio
    print(f"   ‚úÖ Bio g√©n√©r√©e ({len(bio)} caract√®res)")
    
    # 5. Sauvegarder
    print("\n5. Sauvegarde...")
    filename = f"data/performers/{name.lower().replace(' ', '_')}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"   ‚úÖ Sauvegard√©: {filename}")
    
    return metadata

# Exemple d'utilisation
if __name__ == "__main__":
    performer_data = process_performer(
        name="Bridgette B",
        urls=[
            "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
            "https://www.freeones.xxx/bridgette-b"
        ]
    )
    
    print("\n" + "="*50)
    print("‚úÖ Traitement termin√© avec succ√®s!")
    print("="*50)
```

### Cas 2 : Batch Processing de Plusieurs Performers

```python
#!/usr/bin/env python3
"""
Traitement par lots de plusieurs performers
"""

import json
from pathlib import Path
from scrapers import ScraperOrchestrator
from stashmaster_unified import TagRulesEngine, BioGenerator

def batch_process(performers_file: str):
    """Traite plusieurs performers depuis un fichier JSON"""
    
    # Charger la liste
    with open(performers_file, 'r') as f:
        performers = json.load(f)
    
    print(f"Traitement de {len(performers)} performer(s)...\n")
    
    orchestrator = ScraperOrchestrator()
    tag_engine = TagRulesEngine()
    bio_generator = BioGenerator()
    
    results = {
        'success': [],
        'failed': [],
        'partial': []
    }
    
    for i, performer in enumerate(performers, 1):
        name = performer['name']
        urls = performer['urls']
        
        print(f"\n[{i}/{len(performers)}] {name}")
        print("-" * 40)
        
        try:
            # Scraping
            confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)
            
            if num_sources == 0:
                print("  ‚ùå Aucune source valide")
                results['failed'].append(name)
                continue
            
            # M√©tadonn√©es
            metadata = {key: info['value'] for key, info in confirmed.items()}
            metadata['name'] = name
            
            # Tags
            tags = tag_engine.generate_tags(metadata)
            metadata['tags'] = tags
            
            # Bio
            bio = bio_generator.generate_google_bio(name, metadata)
            metadata['bio'] = bio
            
            # Sauvegarder
            filename = f"data/performers/{name.lower().replace(' ', '_')}.json"
            Path("data/performers").mkdir(parents=True, exist_ok=True)
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            if len(conflicts) > 0:
                print(f"  ‚ö†Ô∏è  Succ√®s partiel ({len(conflicts)} conflits)")
                results['partial'].append(name)
            else:
                print("  ‚úÖ Succ√®s complet")
                results['success'].append(name)
        
        except Exception as e:
            print(f"  ‚ùå Erreur: {e}")
            results['failed'].append(name)
    
    # R√©sum√©
    print("\n" + "="*50)
    print("R√âSUM√â")
    print("="*50)
    print(f"‚úÖ Succ√®s complet: {len(results['success'])}")
    print(f"‚ö†Ô∏è  Succ√®s partiel: {len(results['partial'])}")
    print(f"‚ùå √âchecs: {len(results['failed'])}")
    
    return results

# Exemple d'utilisation
if __name__ == "__main__":
    # Cr√©er un fichier performers_list.json avec:
    # [
    #   {
    #     "name": "Performer 1",
    #     "urls": ["url1", "url2"]
    #   },
    #   ...
    # ]
    
    results = batch_process("performers_list.json")
```

### Cas 3 : Validation et Correction Semi-Automatique

```python
#!/usr/bin/env python3
"""
Validation et correction semi-automatique des donn√©es
"""

from scrapers import ScraperOrchestrator
from stashmaster_unified import TagRulesEngine

def validate_and_correct(urls: list) -> dict:
    """Valide et propose des corrections"""
    
    orchestrator = ScraperOrchestrator()
    confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)
    
    print("="*50)
    print("VALIDATION DES DONN√âES")
    print("="*50)
    
    # Donn√©es confirm√©es
    print("\n‚úÖ Donn√©es confirm√©es:")
    for field, info in confirmed.items():
        print(f"  {field}: {info['value']}")
        print(f"    Sources: {', '.join(info['sources'])}")
    
    # Conflits √† r√©soudre
    if conflicts:
        print("\n‚ö†Ô∏è  CONFLITS √Ä R√âSOUDRE:")
        corrections = {}
        
        for field, values in conflicts.items():
            print(f"\n  {field}:")
            for i, v in enumerate(values, 1):
                print(f"    [{i}] {v['value']} ({', '.join(v['sources'])})")
            
            # Demander √† l'utilisateur de choisir
            while True:
                choice = input(f"  Choisir [1-{len(values)}] ou [s]kip: ")
                if choice.lower() == 's':
                    break
                try:
                    idx = int(choice) - 1
                    if 0 <= idx < len(values):
                        corrections[field] = values[idx]['value']
                        print(f"    ‚úÖ {field} = {values[idx]['value']}")
                        break
                except ValueError:
                    pass
                print("    ‚ùå Choix invalide")
        
        # Appliquer les corrections
        for field, value in corrections.items():
            confirmed[field] = {
                'value': value,
                'note': 'Corrig√© manuellement'
            }
    
    # R√©sultat final
    final_data = {key: info['value'] for key, info in confirmed.items()}
    
    print("\n" + "="*50)
    print("DONN√âES FINALES")
    print("="*50)
    for field, value in final_data.items():
        print(f"  {field}: {value}")
    
    return final_data

# Exemple
if __name__ == "__main__":
    urls = [
        "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
        "https://www.freeones.xxx/bridgette-b"
    ]
    
    data = validate_and_correct(urls)
```

## üîó Int√©grations

### Int√©gration avec Stash

```python
import requests
import json

class StashAPI:
    """Client pour l'API Stash"""
    
    def __init__(self, url="http://localhost:9999", api_key=None):
        self.url = url
        self.api_key = api_key
    
    def create_performer(self, performer_data: dict) -> dict:
        """Cr√©e un performer dans Stash"""
        # GraphQL mutation
        mutation = """
        mutation PerformerCreate($input: PerformerCreateInput!) {
          performerCreate(input: $input) {
            id
            name
          }
        }
        """
        
        variables = {
            "input": {
                "name": performer_data.get('name'),
                "birthdate": performer_data.get('birthdate'),
                "ethnicity": performer_data.get('ethnicity'),
                "hair_color": performer_data.get('hair_color'),
                "height": performer_data.get('height'),
                "measurements": performer_data.get('measurements'),
                "tags": performer_data.get('tags', [])
            }
        }
        
        response = requests.post(
            f"{self.url}/graphql",
            json={"query": mutation, "variables": variables}
        )
        
        return response.json()

# Utilisation
stash = StashAPI()
result = stash.create_performer(performer_data)
print(f"Performer cr√©√©: {result}")
```

---

Ces exemples couvrent les cas d'usage les plus courants. Pour plus d'informations, consultez le [README.md](README.md) et la documentation des modules.


============================================================
[25/124] Legacy\files\stashmaster-v2\stashmaster-v2\README.md
------------------------------------------------------------
# StashMaster V2 - Interface Unifi√©e

Application Python/Tkinter pour la gestion et le scraping de m√©tadonn√©es de performers, avec g√©n√©ration automatique de biographies.

## üéØ Caract√©ristiques Principales

### Interface Unifi√©e
- **Fusion Phase 1 & Phase 2** : Une seule GUI pour toutes les op√©rations
- **Organisation par onglets** : M√©tadonn√©es, Champs Avanc√©s, Bio
- **Workflow intuitif** : Scraping ‚Üí Validation ‚Üí G√©n√©ration Bio

### Syst√®me de Tags Intelligent
- ‚úÖ **G√©n√©ration automatique** bas√©e sur des r√®gles m√©tadonn√©es
- ‚úÖ **PAS de scraping de tags** depuis les sources
- ‚úÖ **R√®gles intelligentes** : ethnicit√©, couleur de cheveux, mesures, piercings, tattoos, √¢ge

### Champs Optimis√©s
- **Champs simple ligne** : Nom, Aliases, Dates, Pays, etc.
- **Champs multilignes** :
  - üìù Piercings
  - üìù Tattoos
  - üîó URLs (une par ligne)

### Trivia & Awards
- **Fen√™tre d√©di√©e** avec requ√™te et r√©sultats s√©par√©s
- **Scraping cibl√©** depuis IAFD et autres sources
- **Nettoyage automatique** : 1 award par ligne
- **Format structur√©** : Ann√©e ‚Üí C√©r√©monie ‚Üí Awards

### G√©n√©ration de Bio Automatique
- **Bio Google** : 3000 caract√®res, format professionnel (bas√© sur mod√®le)
- **Bio Ollama** : G√©n√©ration IA locale optionnelle
- **Prompt personnalis√©** : Directives pr√©cises pour l'IA
- **Choix flexible** : Cases √† cocher pour type de bio

## üìã Installation

### Pr√©requis
```bash
# Python 3.8 ou sup√©rieur
python --version

# Tkinter (normalement inclus avec Python)
# Sur Ubuntu/Debian si besoin :
sudo apt-get install python3-tk
```

### Installation des d√©pendances
```bash
# Installer les packages Python requis
pip install -r requirements.txt
```

### Installation d'Ollama (optionnel)
Si vous voulez utiliser la g√©n√©ration de bio avec IA locale :

```bash
# T√©l√©charger et installer Ollama depuis https://ollama.ai
# Puis t√©l√©charger un mod√®le
ollama pull llama2
```

## üöÄ Utilisation

### Lancement
```bash
python stashmaster_unified.py
```

### Workflow Complet

#### 1. Saisie des URLs
- Ouvrir l'onglet **"Champs Avanc√©s"**
- Coller les URLs des sources (une par ligne) :
  ```
  https://www.iafd.com/person.rme/perfid=...
  https://www.freeones.xxx/...
  https://www.babepedia.com/...
  ```

#### 2. Scraping
- Menu **"Actions" ‚Üí "Scraper & Lancer le flux Bio IA"**
- L'application scrape automatiquement toutes les URLs
- Affiche les r√©sultats avec :
  - ‚úÖ Donn√©es confirm√©es (m√™me valeur de plusieurs sources)
  - üÜï Nouvelles donn√©es (une seule source)
  - ‚ö†Ô∏è Conflits (valeurs diff√©rentes entre sources)

#### 3. Validation des M√©tadonn√©es
- V√©rifier et compl√©ter les champs dans l'onglet **"M√©tadonn√©es"**
- Les valeurs confirm√©es sont pr√©-remplies
- R√©soudre les conflits manuellement si n√©cessaire

#### 4. G√©n√©ration des Tags
- Onglet **"Champs Avanc√©s"**
- Cliquer sur **"üîÑ G√©n√©rer Tags"**
- Les tags sont cr√©√©s automatiquement selon les r√®gles :
  - Ethnicit√© ‚Üí Caucasian, Latina, Asian, Ebony
  - Cheveux ‚Üí Blonde, Brunette, Redhead, Black Hair
  - Mesures ‚Üí Big Boobs, Small Boobs
  - Piercings ‚Üí Pierced
  - Tattoos ‚Üí Tattooed
  - Carri√®re ‚Üí MILF (si > 10 ans)

#### 5. Trivia & Awards
- Menu **"Actions" ‚Üí "Trivia & Awards..."**
- Fen√™tre d√©di√©e s'ouvre avec deux sections :
  
  **Trivia**
  - Cliquer **"Scraper Trivia"**
  - Les anecdotes sont r√©cup√©r√©es et affich√©es
  
  **Awards**
  - Cliquer **"Scraper Awards"**
  - Tous les prix/nominations sont list√©s
  - Cliquer **"Nettoyer Awards"** pour formater (1 par ligne)
  
- **"Appliquer et continuer"** pour sauvegarder

#### 6. G√©n√©ration de Bio
- Menu **"Actions" ‚Üí "G√©n√©rer Bio..."** ou onglet **"Bio"**
- Fen√™tre de g√©n√©ration s'ouvre avec 3 options :

  **Option 1 : Bio Google (recommand√©)**
  - ‚úÖ G√©n√©ration automatique instantan√©e
  - ‚úÖ Format professionnel de 3000 caract√®res
  - ‚úÖ Structure avec sections : Introduction, Origines, Carri√®re, Vie Personnelle, Apparence, Prix
  - ‚úÖ Bas√© sur le mod√®le BioGooglemodele.txt
  
  **Option 2 : Bio Ollama**
  - G√©n√©ration avec IA locale (Ollama doit √™tre install√©)
  - Prompt par d√©faut optimis√©
  
  **Option 3 : Bio Ollama avec prompt personnalis√©**
  - √âcrire vos directives pr√©cises dans le champ
  - Contr√¥le total sur le style et le contenu
  
- Cliquer **"G√©n√©rer la Bio"**
- V√©rifier le compteur de caract√®res
- **"Appliquer"** pour ins√©rer dans l'onglet Bio

#### 7. Sauvegarde
- Bouton **"üíæ Sauvegarder"** en bas √† droite
- Toutes les donn√©es sont sauvegard√©es

## üìä Architecture

### Structure des Fichiers
```
stashmaster_unified/
‚îÇ
‚îú‚îÄ‚îÄ stashmaster_unified.py    # Application principale
‚îú‚îÄ‚îÄ scrapers.py                # Modules de scraping
‚îú‚îÄ‚îÄ requirements.txt           # D√©pendances Python
‚îú‚îÄ‚îÄ README.md                  # Ce fichier
‚îÇ
‚îî‚îÄ‚îÄ data/                      # Donn√©es sauvegard√©es (√† cr√©er)
    ‚îú‚îÄ‚îÄ performers/            # JSON des performers
    ‚îî‚îÄ‚îÄ database.sqlite        # Base de donn√©es (futur)
```

### Composants Principaux

#### `MainWindow`
Interface principale unifi√©e avec 3 onglets :
- üìã M√©tadonn√©es : Champs de base
- ‚öôÔ∏è Champs Avanc√©s : Tags, Piercings, Tattoos, URLs
- üìù Bio : Biographie finale

#### `TriviaAwardsWindow`
Fen√™tre d√©di√©e pour :
- Scraping et affichage des trivia
- Scraping et nettoyage des awards
- Format structur√© : 1 award par ligne

#### `BioGenerationWindow`
Fen√™tre de g√©n√©ration avec :
- Choix du type de bio (Google/Ollama)
- Champ pour prompt personnalis√©
- Pr√©visualisation et compteur de caract√®res

#### `TagRulesEngine`
Moteur de r√®gles pour g√©n√©rer les tags automatiquement selon :
- Les m√©tadonn√©es collect√©es (ethnicit√©, cheveux, mesures)
- Les attributs physiques (piercings, tattoos)
- L'√¢ge de carri√®re

#### `AwardsCleaner`
Nettoyeur d'awards pour :
- Formater les awards (1 par ligne)
- Organiser par ann√©e et c√©r√©monie
- Distinguer Winner vs Nominee

#### `BioGenerator`
G√©n√©rateur de biographies avec 2 modes :
- **Google Bio** : Template de 3000 caract√®res
- **Ollama Bio** : IA locale avec prompt personnalis√©

#### `ScraperOrchestrator`
Orchestre le scraping de plusieurs sources :
- IAFD
- Freeones
- Babepedia
- TheNude

#### `DataMerger`
Fusionne intelligemment les donn√©es de plusieurs sources :
- D√©tecte les valeurs confirm√©es (consensus)
- Identifie les nouvelles donn√©es (source unique)
- Signale les conflits (valeurs diff√©rentes)

## üé® R√®gles de Tags

Les tags sont g√©n√©r√©s automatiquement selon ces r√®gles :

### Ethnicit√©
| M√©tadonn√©e | Tag G√©n√©r√© |
|------------|------------|
| Caucasian  | Caucasian  |
| Cuban, Latin, Latina | Latina |
| Asian | Asian |
| Ebony, African | Ebony |

### Couleur de Cheveux
| M√©tadonn√©e | Tag G√©n√©r√© |
|------------|------------|
| Blonde, Blond | Blonde |
| Brown, Brunette | Brunette |
| Red, Auburn | Redhead |
| Black | Black Hair |

### Mesures
| Condition | Tag G√©n√©r√© |
|-----------|------------|
| Tour de poitrine ‚â• 36" | Big Boobs |
| Tour de poitrine ‚â§ 32" | Small Boobs |

### Attributs
| M√©tadonn√©e | Tag G√©n√©r√© |
|------------|------------|
| Piercings (non vide) | Pierced |
| Tattoos (non vide) | Tattooed |
| Carri√®re > 10 ans | MILF |

## üìù Format de Bio Google

La bio g√©n√©r√©e suit ce template de 3000 caract√®res :

```markdown
### [Nom] : L'√©toile charismatique au parcours diversifi√©

**Introduction**
Contexte, d√©but de carri√®re, pseudonymes...

**üìÖ Origines et Premiers Pas**
Lieu de naissance, origines, d√©but de carri√®re...

**üèÜ Carri√®re et Filmographie**
√âvolution, studios, performances, apog√©e...

**üí° Faits Int√©ressants & Vie Personnelle**
Personnalit√©, trivia, vie priv√©e...

**üëó Apparence et Style**
Description physique, mesures, tatouages, piercings...

**üèÜ Prix et Distinctions**
Awards, nominations, reconnaissance...

**Conclusion rapide**
R√©sum√©, impact, h√©ritage...
```

## üîß Configuration Avanc√©e

### Personnaliser les R√®gles de Tags
Modifier la classe `TagRulesEngine` dans `stashmaster_unified.py` :

```python
@staticmethod
def generate_tags(metadata: Dict) -> List[str]:
    tags = []
    
    # Ajouter vos r√®gles personnalis√©es ici
    if condition:
        tags.append('YourTag')
    
    return list(set(tags))
```

### Ajouter un Nouveau Scraper
Cr√©er une nouvelle classe dans `scrapers.py` :

```python
class NewSourceScraper(ScraperBase):
    def scrape_performer(self, url: str) -> Dict:
        # Votre code de scraping
        return data
```

Puis l'enregistrer dans `ScraperOrchestrator` :

```python
self.scrapers['newsource'] = NewSourceScraper()
```

### Personnaliser le Template de Bio
Modifier la m√©thode `generate_google_bio` dans la classe `BioGenerator`.

## ‚ùì FAQ

### Les tags ne se g√©n√®rent pas automatiquement ?
‚Üí V√©rifiez que vous avez bien rempli les champs de base (ethnicit√©, cheveux, mesures) et cliquez sur "üîÑ G√©n√©rer Tags"

### Ollama ne fonctionne pas ?
‚Üí V√©rifiez qu'Ollama est install√© et en cours d'ex√©cution :
```bash
ollama serve
```

### Les awards ne sont pas nettoy√©s correctement ?
‚Üí Utilisez le bouton "Nettoyer Awards" apr√®s le scraping pour formater automatiquement

### Comment r√©soudre les conflits de donn√©es ?
‚Üí Les conflits sont affich√©s lors du scraping. Choisissez manuellement la valeur correcte ou conservez celle de la source la plus fiable (g√©n√©ralement IAFD)

### La bio est trop longue/courte ?
‚Üí Bio Google : ~3000 caract√®res (fixe)
‚Üí Bio Ollama : Ajustez dans le prompt personnalis√© : "√âcris une bio de [X] caract√®res..."

## üìÑ Licence

Ce projet est fourni tel quel pour usage personnel.

## ü§ù Contribution

Pour toute am√©lioration ou correction :
1. Cr√©er une branche pour votre fonctionnalit√©
2. Commiter vos changements
3. Cr√©er une Pull Request

## üìÆ Support

Pour toute question ou probl√®me, cr√©er une issue sur le repository.

---

**Version** : 2.0  
**Date** : F√©vrier 2026  
**Statut** : Production


============================================================
[26/124] Legacy\files\stashmaster-v2\stashmaster-v2\requirements.txt
------------------------------------------------------------
requests==2.31.0
beautifulsoup4==4.12.3
lxml==5.1.0


============================================================
[27/124] Legacy\files\stashmaster-v2\stashmaster-v2\scrapers.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Scrapers pour diff√©rentes sources de donn√©es
"""

import requests
from bs4 import BeautifulSoup
import re
from typing import Dict, List, Optional, Tuple
from datetime import datetime


class ScraperBase:
    """Classe de base pour tous les scrapers"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def get_page(self, url: str, timeout: int = 10) -> Optional[BeautifulSoup]:
        """R√©cup√®re et parse une page web"""
        try:
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except Exception as e:
            print(f"Erreur lors du scraping de {url}: {e}")
            return None


class IAFDScraper(ScraperBase):
    """Scraper pour IAFD.com"""
    
    def scrape_performer(self, url: str) -> Dict:
        """Scrape les donn√©es d'un performer depuis IAFD"""
        soup = self.get_page(url)
        if not soup:
            return {}
        
        data = {
            'source': 'iafd',
            'url': url
        }
        
        try:
            # Nom
            name_elem = soup.find('h1')
            if name_elem:
                data['name'] = name_elem.text.strip()
            
            # Informations de base
            info_box = soup.find('div', class_='biodata')
            if info_box:
                rows = info_box.find_all('p')
                for row in rows:
                    text = row.text.strip()
                    
                    # Date de naissance
                    if 'Born' in text or 'Birthday' in text:
                        date_match = re.search(r'(\w+ \d+, \d{4})', text)
                        if date_match:
                            data['birthdate'] = date_match.group(1)
                    
                    # Lieu de naissance
                    if 'Birthplace' in text:
                        place = text.split('Birthplace:')[-1].strip()
                        data['birthplace'] = place
                    
                    # Ethnicit√©
                    if 'Ethnicity' in text:
                        ethnicity = text.split('Ethnicity:')[-1].strip()
                        data['ethnicity'] = ethnicity
                    
                    # Cheveux
                    if 'Hair Color' in text:
                        hair = text.split('Hair Color:')[-1].strip()
                        data['hair_color'] = hair
                    
                    # Yeux
                    if 'Eye Color' in text:
                        eyes = text.split('Eye Color:')[-1].strip()
                        data['eye_color'] = eyes
                    
                    # Taille
                    if 'Height' in text:
                        height = text.split('Height:')[-1].strip()
                        data['height'] = height
                    
                    # Poids
                    if 'Weight' in text:
                        weight = text.split('Weight:')[-1].strip()
                        data['weight'] = weight
                    
                    # Mesures
                    if 'Measurements' in text:
                        measurements = text.split('Measurements:')[-1].strip()
                        data['measurements'] = measurements
                    
                    # Tatouages
                    if 'Tattoos' in text:
                        tattoos = text.split('Tattoos:')[-1].strip()
                        data['tattoos'] = tattoos
                    
                    # Piercings
                    if 'Piercings' in text:
                        piercings = text.split('Piercings:')[-1].strip()
                        data['piercings'] = piercings
                    
                    # Ann√©es actives
                    if 'Years Active' in text:
                        years = text.split('Years Active:')[-1].strip()
                        data['career_length'] = years
            
            # Aliases
            aliases_section = soup.find('p', string=re.compile('Also Known As', re.I))
            if aliases_section:
                aliases_text = aliases_section.text.replace('Also Known As:', '').strip()
                aliases = [a.strip() for a in aliases_text.split(',')]
                data['aliases'] = aliases
            
        except Exception as e:
            print(f"Erreur parsing IAFD: {e}")
        
        return data
    
    def scrape_awards(self, url: str) -> List[Dict]:
        """Scrape les awards depuis IAFD"""
        soup = self.get_page(url)
        if not soup:
            return []
        
        awards = []
        
        try:
            # Chercher la section awards
            awards_section = soup.find('div', {'id': 'awards'})
            if not awards_section:
                awards_section = soup.find('h2', string=re.compile('Awards', re.I))
                if awards_section:
                    awards_section = awards_section.find_next('div')
            
            if awards_section:
                # Parser les awards
                award_items = awards_section.find_all(['p', 'li'])
                current_year = None
                current_ceremony = None
                
                for item in award_items:
                    text = item.text.strip()
                    
                    # D√©tecter l'ann√©e
                    year_match = re.match(r'^(19\d{2}|20\d{2})$', text)
                    if year_match:
                        current_year = year_match.group(1)
                        continue
                    
                    # D√©tecter le type de c√©r√©monie
                    if any(ceremony in text.upper() for ceremony in ['AVN', 'XBIZ', 'XRCO', 'NIGHTMOVES']):
                        current_ceremony = text
                        continue
                    
                    # C'est un award
                    if current_year and text:
                        award = {
                            'year': current_year,
                            'ceremony': current_ceremony or 'Unknown',
                            'award': text,
                            'winner': 'Winner' in text or 'Won' in text
                        }
                        awards.append(award)
        
        except Exception as e:
            print(f"Erreur scraping awards IAFD: {e}")
        
        return awards


class FreeonesScraper(ScraperBase):
    """Scraper pour Freeones.xxx"""
    
    def scrape_performer(self, url: str) -> Dict:
        """Scrape les donn√©es d'un performer depuis Freeones"""
        soup = self.get_page(url)
        if not soup:
            return {}
        
        data = {
            'source': 'freeones',
            'url': url
        }
        
        try:
            # Nom
            name_elem = soup.find('h1', class_='profile-header-name')
            if name_elem:
                data['name'] = name_elem.text.strip()
            
            # Bio section
            bio_section = soup.find('div', class_='profile-meta-list')
            if bio_section:
                items = bio_section.find_all('div', class_='profile-meta-item')
                for item in items:
                    label_elem = item.find('span', class_='profile-meta-label')
                    value_elem = item.find('span', class_='profile-meta-value')
                    
                    if label_elem and value_elem:
                        label = label_elem.text.strip().lower()
                        value = value_elem.text.strip()
                        
                        if 'born' in label or 'birth' in label:
                            data['birthdate'] = value
                        elif 'ethnicity' in label:
                            data['ethnicity'] = value
                        elif 'hair' in label:
                            data['hair_color'] = value
                        elif 'eye' in label:
                            data['eye_color'] = value
                        elif 'height' in label:
                            data['height'] = value
                        elif 'weight' in label:
                            data['weight'] = value
                        elif 'measure' in label:
                            data['measurements'] = value
                        elif 'tattoo' in label:
                            data['tattoos'] = value
                        elif 'piercing' in label:
                            data['piercings'] = value
            
            # Aliases
            aliases_elem = soup.find('div', class_='profile-aliases')
            if aliases_elem:
                aliases = [a.text.strip() for a in aliases_elem.find_all('a')]
                data['aliases'] = aliases
        
        except Exception as e:
            print(f"Erreur parsing Freeones: {e}")
        
        return data


class BabepaediaScraper(ScraperBase):
    """Scraper pour Babepedia.com"""
    
    def scrape_performer(self, url: str) -> Dict:
        """Scrape les donn√©es d'un performer depuis Babepedia"""
        soup = self.get_page(url)
        if not soup:
            return {}
        
        data = {
            'source': 'babepedia',
            'url': url
        }
        
        try:
            # Nom
            name_elem = soup.find('h1', class_='firstHeading')
            if name_elem:
                data['name'] = name_elem.text.strip()
            
            # Infobox (similaire √† Wikipedia)
            infobox = soup.find('table', class_='infobox')
            if infobox:
                rows = infobox.find_all('tr')
                for row in rows:
                    th = row.find('th')
                    td = row.find('td')
                    
                    if th and td:
                        label = th.text.strip().lower()
                        value = td.text.strip()
                        
                        if 'born' in label:
                            data['birthdate'] = value
                        elif 'birth' in label and 'place' in label:
                            data['birthplace'] = value
                        elif 'ethnic' in label:
                            data['ethnicity'] = value
                        elif 'hair' in label:
                            data['hair_color'] = value
                        elif 'eye' in label:
                            data['eye_color'] = value
                        elif 'height' in label:
                            data['height'] = value
                        elif 'weight' in label:
                            data['weight'] = value
                        elif 'measure' in label:
                            data['measurements'] = value
        
        except Exception as e:
            print(f"Erreur parsing Babepedia: {e}")
        
        return data


class TheNudeScraper(ScraperBase):
    """Scraper pour TheNude.com"""
    
    def scrape_performer(self, url: str) -> Dict:
        """Scrape les donn√©es d'un performer depuis TheNude"""
        soup = self.get_page(url)
        if not soup:
            return {}
        
        data = {
            'source': 'thenude',
            'url': url
        }
        
        try:
            # Nom
            name_elem = soup.find('h1')
            if name_elem:
                data['name'] = name_elem.text.strip()
            
            # Donn√©es biographiques
            bio_divs = soup.find_all('div', class_='bio-item')
            for div in bio_divs:
                label_elem = div.find('strong')
                if label_elem:
                    label = label_elem.text.strip().lower()
                    value = div.text.replace(label_elem.text, '').strip()
                    
                    if 'born' in label or 'birth' in label:
                        data['birthdate'] = value
                    elif 'ethnic' in label:
                        data['ethnicity'] = value
                    elif 'hair' in label:
                        data['hair_color'] = value
                    elif 'eye' in label:
                        data['eye_color'] = value
                    elif 'height' in label:
                        data['height'] = value
                    elif 'weight' in label:
                        data['weight'] = value
                    elif 'measure' in label:
                        data['measurements'] = value
        
        except Exception as e:
            print(f"Erreur parsing TheNude: {e}")
        
        return data


class DataMerger:
    """Fusionne les donn√©es de plusieurs sources"""
    
    @staticmethod
    def merge_data(sources_data: List[Dict]) -> Tuple[Dict, Dict]:
        """
        Fusionne les donn√©es de plusieurs sources
        Retourne: (donn√©es confirm√©es, donn√©es en conflit)
        """
        confirmed = {}
        conflicts = {}
        
        # Compter les occurrences de chaque valeur pour chaque champ
        field_values = {}
        
        for source_data in sources_data:
            source_name = source_data.get('source', 'unknown')
            for field, value in source_data.items():
                if field in ['source', 'url']:
                    continue
                
                if not value:
                    continue
                
                if field not in field_values:
                    field_values[field] = {}
                
                value_str = str(value).strip().lower()
                if value_str not in field_values[field]:
                    field_values[field][value_str] = {
                        'count': 0,
                        'sources': [],
                        'original_value': value
                    }
                
                field_values[field][value_str]['count'] += 1
                field_values[field][value_str]['sources'].append(source_name)
        
        # D√©terminer les valeurs confirm√©es et les conflits
        for field, values in field_values.items():
            if len(values) == 1:
                # Une seule valeur trouv√©e
                value_info = list(values.values())[0]
                confirmed[field] = {
                    'value': value_info['original_value'],
                    'count': value_info['count'],
                    'sources': value_info['sources']
                }
            else:
                # Plusieurs valeurs diff√©rentes = conflit
                sorted_values = sorted(values.items(), 
key="***MASKED***"
                                     reverse=True)
                
                if sorted_values[0][1]['count'] > sorted_values[1][1]['count']:
                    # Une valeur majoritaire
                    confirmed[field] = {
                        'value': sorted_values[0][1]['original_value'],
                        'count': sorted_values[0][1]['count'],
                        'sources': sorted_values[0][1]['sources'],
                        'note': 'Valeur majoritaire'
                    }
                    # Garder les autres valeurs dans les conflits
                    conflicts[field] = [
                        {
                            'value': v[1]['original_value'],
                            'count': v[1]['count'],
                            'sources': v[1]['sources']
                        }
                        for v in sorted_values[1:]
                    ]
                else:
                    # √âgalit√© = vrai conflit
                    conflicts[field] = [
                        {
                            'value': v[1]['original_value'],
                            'count': v[1]['count'],
                            'sources': v[1]['sources']
                        }
                        for v in sorted_values
                    ]
        
        return confirmed, conflicts


class ScraperOrchestrator:
    """Orchestre le scraping depuis plusieurs sources"""
    
    def __init__(self):
        self.scrapers = {
            'iafd': IAFDScraper(),
            'freeones': FreeonesScraper(),
            'babepedia': BabepaediaScraper(),
            'thenude': TheNudeScraper()
        }
        self.merger = DataMerger()
    
    def scrape_urls(self, urls: List[str]) -> Tuple[Dict, Dict, int]:
        """
        Scrape plusieurs URLs et fusionne les r√©sultats
        Retourne: (donn√©es confirm√©es, conflits, nombre de sources)
        """
        sources_data = []
        
        for url in urls:
            scraper = self._get_scraper_for_url(url)
            if scraper:
                data = scraper.scrape_performer(url)
                if data:
                    sources_data.append(data)
        
        if not sources_data:
            return {}, {}, 0
        
        confirmed, conflicts = self.merger.merge_data(sources_data)
        return confirmed, conflicts, len(sources_data)
    
    def _get_scraper_for_url(self, url: str) -> Optional[ScraperBase]:
        """Retourne le scraper appropri√© pour une URL"""
        url_lower = url.lower()
        
        if 'iafd.com' in url_lower:
            return self.scrapers['iafd']
        elif 'freeones' in url_lower:
            return self.scrapers['freeones']
        elif 'babepedia' in url_lower:
            return self.scrapers['babepedia']
        elif 'thenude' in url_lower:
            return self.scrapers['thenude']
        
        return None
    
    def scrape_awards(self, urls: List[str]) -> List[Dict]:
        """Scrape les awards de toutes les sources"""
        all_awards = []
        
        for url in urls:
            if 'iafd.com' in url.lower():
                awards = self.scrapers['iafd'].scrape_awards(url)
                all_awards.extend(awards)
        
        return all_awards


if __name__ == "__main__":
    # Test
    orchestrator = ScraperOrchestrator()
    
    # Exemple d'URLs
    test_urls = [
        "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
    ]
    
    confirmed, conflicts, num_sources = orchestrator.scrape_urls(test_urls)
    
    print(f"\n=== Donn√©es confirm√©es de {num_sources} source(s) ===")
    for field, info in confirmed.items():
        print(f"{field}: {info['value']} (sources: {', '.join(info['sources'])})")
    
    if conflicts:
        print(f"\n=== Conflits d√©tect√©s ===")
        for field, values in conflicts.items():
            print(f"\n{field}:")
            for v in values:
                print(f"  - {v['value']} ({v['count']} source(s): {', '.join(v['sources'])})")


============================================================
[28/124] Legacy\files\stashmaster-v2\stashmaster-v2\stashmaster_unified.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
StashMaster V2 - Interface Unifi√©e
Fusion des Phase 1 et Phase 2 avec g√©n√©ration automatique de bio
"""

import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox, simpledialog
import requests
from bs4 import BeautifulSoup
import re
import json
from datetime import datetime
from typing import Dict, List, Optional
import threading


class TagRulesEngine:
    """Moteur de r√®gles pour g√©n√©rer des tags bas√©s sur les m√©tadonn√©es"""
    
    @staticmethod
    def generate_tags(metadata: Dict) -> List[str]:
        """G√©n√®re des tags bas√©s sur les m√©tadonn√©es collect√©es"""
        tags = []
        
        # Tags bas√©s sur l'ethnicit√©
        ethnicity = metadata.get('ethnicity', '').lower()
        if ethnicity:
            if 'caucasian' in ethnicity:
                tags.append('Caucasian')
            elif 'latin' in ethnicity or 'cuban' in ethnicity:
                tags.append('Latina')
            elif 'asian' in ethnicity:
                tags.append('Asian')
            elif 'ebony' in ethnicity or 'african' in ethnicity:
                tags.append('Ebony')
        
        # Tags bas√©s sur la couleur de cheveux
        hair_color = metadata.get('hair_color', '').lower()
        if 'blonde' in hair_color or 'blond' in hair_color:
            tags.append('Blonde')
        elif 'brown' in hair_color or 'brunette' in hair_color:
            tags.append('Brunette')
        elif 'red' in hair_color:
            tags.append('Redhead')
        elif 'black' in hair_color:
            tags.append('Black Hair')
        
        # Tags bas√©s sur les mesures
        measurements = metadata.get('measurements', '')
        if measurements:
            # Extraire la taille des seins (premi√®re valeur)
            match = re.match(r'(\d+)', measurements)
            if match:
                size = int(match.group(1))
                if size >= 36:
                    tags.append('Big Boobs')
                elif size <= 32:
                    tags.append('Small Boobs')
        
        # Tags bas√©s sur les piercings
        piercings = metadata.get('piercings', '').lower()
        if piercings and piercings != 'none':
            tags.append('Pierced')
        
        # Tags bas√©s sur les tattoos
        tattoos = metadata.get('tattoos', '').lower()
        if tattoos and tattoos != 'none':
            tags.append('Tattooed')
        
        # Tags bas√©s sur l'√¢ge de carri√®re
        career_start = metadata.get('career_start', '')
        if career_start:
            try:
                year = int(career_start.split('-')[0])
                current_year = datetime.now().year
                if current_year - year > 10:
                    tags.append('MILF')
            except:
                pass
        
        return list(set(tags))  # √âliminer les doublons


class AwardsCleaner:
    """Nettoie et formate les awards pour avoir 1 par ligne"""
    
    @staticmethod
    def clean_awards(raw_awards: str) -> str:
        """Nettoie le texte brut des awards pour avoir un format lisible"""
        if not raw_awards:
            return ""
        
        # S√©parer par les num√©ros d'ann√©e
        lines = []
        current_year = None
        
        # Pattern pour d√©tecter les ann√©es
        year_pattern = re.compile(r'\b(19\d{2}|20\d{2})\b')
        
        # Pattern pour d√©tecter les types d'awards
        award_types = ['AVN AWARDS', 'XBIZ AWARDS', 'NIGHTMOVES', 'XRCO AWARDS']
        
        text = raw_awards
        for award_type in award_types:
            text = text.replace(award_type, f'\n{award_type}\n')
        
        # Diviser en lignes et nettoyer
        for line in text.split('\n'):
            line = line.strip()
            if not line:
                continue
            
            # D√©tecter si c'est une ann√©e
            if re.match(r'^\d{4}$', line):
                current_year = line
                lines.append(f'\n{current_year}')
                continue
            
            # D√©tecter si c'est un type d'award
            if any(award_type in line.upper() for award_type in award_types):
                lines.append(f'\n{line}')
                continue
            
            # D√©tecter Winner ou Nominee
            if line.startswith('Winner:') or line.startswith('Nominee:'):
                lines.append(f'  {line}')
            elif current_year and not line.startswith(' '):
                lines.append(f'  {line}')
            else:
                lines.append(line)
        
        return '\n'.join(lines)


class BioGenerator:
    """G√©n√©rateur de biographies avec Google Search et Ollama"""
    
    def __init__(self):
        self.ollama_url = "http://localhost:11434/api/generate"
    
    def generate_google_bio(self, performer_name: str, metadata: Dict) -> str:
        """G√©n√®re une bio de 3000 caract√®res style Google"""
        
        # Template bas√© sur BioGooglemodele.txt
        template = f"""### {performer_name} : L'√©toile charismatique au parcours diversifi√©

**Introduction**
N√©e le {metadata.get('birthdate', '[date]')} √† {metadata.get('birthplace', '[lieu]')}, {performer_name} a marqu√© de son empreinte l'industrie du divertissement pour adultes d√®s son entr√©e en sc√®ne en {metadata.get('career_start', '[ann√©e]')}. Reconnue pour son charisme naturel et son √©nergie captivante, elle a rapidement acquis une notori√©t√© significative. Au fil de sa carri√®re, elle a adopt√© plusieurs pseudonymes, tels que {', '.join(metadata.get('aliases', []))}, qui ont tous contribu√© √† forger son image polyvalente et √† laisser un impact m√©morable dans le secteur.

**üìÖ Origines et Premiers Pas**
Issue d'une famille d'origine {metadata.get('ethnicity', '[origine]')} et ayant grandi dans le vibrant paysage de {metadata.get('birthplace', '[lieu]')}, la vie de {performer_name} avant son immersion dans l'industrie est envelopp√©e d'une certaine discr√©tion. Les informations d√©taill√©es concernant son enfance ou son parcours scolaire ne sont pas largement divulgu√©es publiquement, soulignant une volont√© de pr√©server sa sph√®re priv√©e. C'est √† l'√¢ge de {metadata.get('career_start_age', '[√¢ge]')} ans, en {metadata.get('career_start', '[ann√©e]')}, qu'elle a franchi le seuil du monde du divertissement pour adultes, un choix qui allait d√©finir une d√©cennie de sa vie professionnelle et la propulser sur le devant de la sc√®ne internationale.

**üèÜ Carri√®re et Filmographie**
La trajectoire professionnelle de {performer_name} a d√©but√© avec une force consid√©rable, la menant √† collaborer avec certains des plus grands noms de l'industrie. D√®s les premi√®res ann√©es de sa carri√®re, elle a √©t√© une pr√©sence r√©guli√®re sur des plateformes de renom. Ces partenariats pr√©coces lui ont permis d'acqu√©rir une visibilit√© rapide et de se b√¢tir une solide r√©putation en tant qu'interpr√®te polyvalente.

Son √©volution l'a ensuite amen√©e √† diversifier ses r√¥les et √† travailler avec d'autres studios influents. Elle a su s'adapter √† diff√©rents types de sc√®nes, d√©montrant une gamme de performances qui ont plu √† un large public. Bien que sa carri√®re en sc√®nes explicites ait connu son apog√©e autour de {metadata.get('peak_years', '[p√©riode]')}, sa vaste filmographie et la qualit√© constante de ses prestations lui ont assur√© une place de choix parmi les √©toiles de sa g√©n√©ration.

**üí° Faits Int√©ressants & Vie Personnelle**
Au-del√† de l'√©cran, {performer_name} est r√©put√©e pour sa personnalit√© authentique et son approche terre-√†-terre. La sph√®re de sa vie personnelle reste, comme il est courant dans cette industrie, relativement priv√©e. {metadata.get('trivia', '')}

**üëó Apparence et Style**
{performer_name} est souvent caract√©ris√©e par une beaut√© distinctive, ancr√©e dans ses origines {metadata.get('ethnicity', '[origine]')}. Elle arbore typiquement une chevelure {metadata.get('hair_color', '[couleur]')}, souvent longue et soyeuse, qui encadre un visage expressif et une silhouette g√©n√©ralement {metadata.get('body_type', 'fine et athl√©tique')}. Son style sur sc√®ne est marqu√© par une √©nergie palpable et une capacit√© √† incarner des personnages vari√©s avec cr√©dibilit√©.

Mesures physiques : {metadata.get('measurements', '[mesures]')} - Taille : {metadata.get('height', '[taille]')} - Poids : {metadata.get('weight', '[poids]')}
Tatouages : {metadata.get('tattoos', 'Information non disponible')}
Piercings : {metadata.get('piercings', 'Information non disponible')}

**üèÜ Prix et Distinctions**
La reconnaissance de l'industrie n'a pas tard√© √† se manifester pour {performer_name}, qui a √©t√© honor√©e de nombreuses nominations au cours de sa carri√®re. {metadata.get('awards_summary', '')}

**Conclusion rapide**
En somme, {performer_name} demeure une figure embl√©matique et respect√©e de l'industrie pour adultes. Son parcours, caract√©ris√© par une entr√©e remarqu√©e en {metadata.get('career_start', '[ann√©e]')} et une carri√®re diversifi√©e sous plusieurs alias, a laiss√© une impression durable. Son professionnalisme, son charme et sa capacit√© √† captiver le public continuent d'√™tre salu√©s par ses fans et les connaisseurs du milieu, confirmant son statut d'√©toile marquante de sa g√©n√©ration."""
        
        # Limiter √† environ 3000 caract√®res
        if len(template) > 3000:
            # Couper intelligemment
            template = template[:2950] + "..."
        
        return template
    
    def generate_ollama_bio(self, performer_name: str, metadata: Dict, custom_prompt: str = "") -> Optional[str]:
        """G√©n√®re une bio avec Ollama"""
        try:
            # Construire le prompt
            if custom_prompt:
                prompt = custom_prompt
            else:
                prompt = f"""√âcris une biographie professionnelle de 3000 caract√®res pour {performer_name}, 
                actrice de l'industrie du divertissement pour adultes.
                
                Informations disponibles :
                - Nom : {performer_name}
                - Aliases : {', '.join(metadata.get('aliases', []))}
                - Date de naissance : {metadata.get('birthdate', 'Non disponible')}
                - Lieu de naissance : {metadata.get('birthplace', 'Non disponible')}
                - Ethnicit√© : {metadata.get('ethnicity', 'Non disponible')}
                - D√©but de carri√®re : {metadata.get('career_start', 'Non disponible')}
                - Mesures : {metadata.get('measurements', 'Non disponible')}
                
                La biographie doit √™tre :
                - Professionnelle et respectueuse
                - Structur√©e avec des sections claires
                - D'environ 3000 caract√®res
                - En fran√ßais
                """
            
            # Appel √† Ollama
            response = requests.post(
                self.ollama_url,
                json={
                    "model": "llama2",
                    "prompt": prompt,
                    "stream": False
                },
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '')
            else:
                return None
        except Exception as e:
            print(f"Erreur Ollama: {e}")
            return None


class TriviaAwardsWindow(tk.Toplevel):
    """Fen√™tre s√©par√©e pour Trivia et Awards avec requ√™te et r√©sultats"""
    
    def __init__(self, parent, performer_name: str, urls: List[str]):
        super().__init__(parent)
        self.title(f"Trivia & Awards ‚Äî {performer_name}")
        self.geometry("1000x700")
        
        self.performer_name = performer_name
        self.urls = urls
        self.awards_cleaner = AwardsCleaner()
        
        self._create_widgets()
    
    def _create_widgets(self):
        # Frame principal
        main_frame = ttk.Frame(self, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        self.columnconfigure(0, weight=1)
        self.rowconfigure(0, weight=1)
        main_frame.columnconfigure(0, weight=1)
        main_frame.rowconfigure(2, weight=1)
        main_frame.rowconfigure(4, weight=1)
        
        # Section Trivia
        trivia_label = ttk.Label(main_frame, text="üìù Trivia", font=('Segoe UI', 12, 'bold'))
        trivia_label.grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        
        # Boutons de scraping pour Trivia
        trivia_btn_frame = ttk.Frame(main_frame)
        trivia_btn_frame.grid(row=1, column=0, sticky=tk.W, pady=(0, 5))
        
        ttk.Button(trivia_btn_frame, text="Scraper Trivia", 
                   command=self._scrape_trivia).pack(side=tk.LEFT, padx=(0, 5))
        
        # Champ Trivia (multiligne)
        self.trivia_text = scrolledtext.ScrolledText(main_frame, height=8, wrap=tk.WORD)
        self.trivia_text.grid(row=2, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        
        # Section Awards
        awards_label = ttk.Label(main_frame, text="üèÜ Awards & Nominations", 
                                 font=('Segoe UI', 12, 'bold'))
        awards_label.grid(row=3, column=0, sticky=tk.W, pady=(10, 5))
        
        # Boutons de scraping pour Awards
        awards_btn_frame = ttk.Frame(main_frame)
        awards_btn_frame.grid(row=4, column=0, sticky=tk.W, pady=(0, 5))
        
        ttk.Button(awards_btn_frame, text="Scraper Awards", 
                   command=self._scrape_awards).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(awards_btn_frame, text="Nettoyer Awards", 
                   command=self._clean_awards).pack(side=tk.LEFT, padx=(0, 5))
        
        # Champ Awards (multiligne)
        self.awards_text = scrolledtext.ScrolledText(main_frame, height=15, wrap=tk.WORD)
        self.awards_text.grid(row=5, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        
        # Boutons d'action
        btn_frame = ttk.Frame(main_frame)
        btn_frame.grid(row=6, column=0, sticky=tk.E, pady=(10, 0))
        
        ttk.Button(btn_frame, text="Annuler", command=self.destroy).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(btn_frame, text="Appliquer et continuer", 
                   command=self._apply_and_continue).pack(side=tk.LEFT)
    
    def _scrape_trivia(self):
        """Scrape les trivia depuis les URLs"""
        self.trivia_text.delete('1.0', tk.END)
        self.trivia_text.insert('1.0', "Scraping en cours...\n")
        
        def scrape():
            trivia_items = []
            for url in self.urls:
                if 'iafd.com' in url:
                    trivia = self._scrape_iafd_trivia(url)
                    if trivia:
                        trivia_items.extend(trivia)
            
            # Afficher les r√©sultats
            self.after(0, lambda: self._display_trivia(trivia_items))
        
        thread = threading.Thread(target=scrape)
        thread.daemon = True
        thread.start()
    
    def _scrape_iafd_trivia(self, url: str) -> List[str]:
        """Scrape les trivia depuis IAFD"""
        try:
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Chercher la section trivia (√† adapter selon la structure r√©elle)
            trivia = []
            # Code de scraping √† adapter selon la structure du site
            
            return trivia
        except Exception as e:
            print(f"Erreur scraping trivia: {e}")
            return []
    
    def _display_trivia(self, trivia_items: List[str]):
        """Affiche les trivia scrap√©s"""
        self.trivia_text.delete('1.0', tk.END)
        if trivia_items:
            for item in trivia_items:
                self.trivia_text.insert(tk.END, f"‚Ä¢ {item}\n")
        else:
            self.trivia_text.insert(tk.END, "Aucun trivia trouv√©.")
    
    def _scrape_awards(self):
        """Scrape les awards depuis les URLs"""
        self.awards_text.delete('1.0', tk.END)
        self.awards_text.insert('1.0', "Scraping en cours...\n")
        
        def scrape():
            awards_text = ""
            for url in self.urls:
                if 'iafd.com' in url:
                    awards = self._scrape_iafd_awards(url)
                    if awards:
                        awards_text += awards + "\n\n"
            
            # Afficher les r√©sultats
            self.after(0, lambda: self._display_awards(awards_text))
        
        thread = threading.Thread(target=scrape)
        thread.daemon = True
        thread.start()
    
    def _scrape_iafd_awards(self, url: str) -> str:
        """Scrape les awards depuis IAFD"""
        try:
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Chercher la section awards (√† adapter selon la structure r√©elle)
            awards = ""
            # Code de scraping √† adapter
            
            return awards
        except Exception as e:
            print(f"Erreur scraping awards: {e}")
            return ""
    
    def _display_awards(self, awards_text: str):
        """Affiche les awards scrap√©s"""
        self.awards_text.delete('1.0', tk.END)
        if awards_text:
            self.awards_text.insert(tk.END, awards_text)
        else:
            self.awards_text.insert(tk.END, "Aucun award trouv√©.")
    
    def _clean_awards(self):
        """Nettoie les awards pour avoir 1 par ligne"""
        raw_text = self.awards_text.get('1.0', tk.END)
        cleaned_text = self.awards_cleaner.clean_awards(raw_text)
        self.awards_text.delete('1.0', tk.END)
        self.awards_text.insert('1.0', cleaned_text)
    
    def _apply_and_continue(self):
        """Applique les modifications et ferme la fen√™tre"""
        # R√©cup√©rer les donn√©es
        self.trivia_data = self.trivia_text.get('1.0', tk.END).strip()
        self.awards_data = self.awards_text.get('1.0', tk.END).strip()
        self.destroy()
    
    def get_data(self) -> Dict:
        """Retourne les donn√©es collect√©es"""
        return {
            'trivia': getattr(self, 'trivia_data', ''),
            'awards': getattr(self, 'awards_data', '')
        }


class BioGenerationWindow(tk.Toplevel):
    """Fen√™tre pour la g√©n√©ration de bio"""
    
    def __init__(self, parent, performer_name: str, metadata: Dict):
        super().__init__(parent)
        self.title(f"G√©n√©ration de Bio ‚Äî {performer_name}")
        self.geometry("900x800")
        
        self.performer_name = performer_name
        self.metadata = metadata
        self.bio_generator = BioGenerator()
        
        self._create_widgets()
    
    def _create_widgets(self):
        # Frame principal
        main_frame = ttk.Frame(self, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        self.columnconfigure(0, weight=1)
        self.rowconfigure(0, weight=1)
        main_frame.columnconfigure(0, weight=1)
        main_frame.rowconfigure(3, weight=1)
        
        # Options de g√©n√©ration
        options_frame = ttk.LabelFrame(main_frame, text="Options de g√©n√©ration", padding="10")
        options_frame.grid(row=0, column=0, sticky=(tk.W, tk.E), pady=(0, 10))
        options_frame.columnconfigure(1, weight=1)
        
        # Choix du type de bio
        self.bio_type_var = tk.StringVar(value="google")
        
        ttk.Radiobutton(options_frame, text="Bio Google (3000 car. automatique)", 
                        variable=self.bio_type_var, value="google").grid(row=0, column=0, sticky=tk.W)
        
        ttk.Radiobutton(options_frame, text="Bio Ollama (avec IA locale)", 
                        variable=self.bio_type_var, value="ollama").grid(row=1, column=0, sticky=tk.W)
        
        ttk.Radiobutton(options_frame, text="Bio Ollama avec prompt personnalis√©", 
                        variable=self.bio_type_var, value="ollama_custom").grid(row=2, column=0, sticky=tk.W)
        
        # Section prompt personnalis√©
        prompt_frame = ttk.LabelFrame(main_frame, text="Prompt personnalis√© (optionnel)", padding="10")
        prompt_frame.grid(row=1, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        prompt_frame.columnconfigure(0, weight=1)
        prompt_frame.rowconfigure(1, weight=1)
        
        ttk.Label(prompt_frame, text="Entrez vos directives pr√©cises pour la g√©n√©ration de la bio :").grid(
            row=0, column=0, sticky=tk.W, pady=(0, 5))
        
        self.prompt_text = scrolledtext.ScrolledText(prompt_frame, height=6, wrap=tk.WORD)
        self.prompt_text.grid(row=1, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # Bouton de g√©n√©ration
        ttk.Button(main_frame, text="G√©n√©rer la Bio", 
                   command=self._generate_bio).grid(row=2, column=0, pady=(0, 10))
        
        # R√©sultat
        result_frame = ttk.LabelFrame(main_frame, text="Bio g√©n√©r√©e", padding="10")
        result_frame.grid(row=3, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        result_frame.columnconfigure(0, weight=1)
        result_frame.rowconfigure(0, weight=1)
        
        self.bio_text = scrolledtext.ScrolledText(result_frame, wrap=tk.WORD)
        self.bio_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # Label pour le compteur de caract√®res
        self.char_count_label = ttk.Label(result_frame, text="Caract√®res : 0")
        self.char_count_label.grid(row=1, column=0, sticky=tk.E, pady=(5, 0))
        
        # Boutons d'action
        btn_frame = ttk.Frame(main_frame)
        btn_frame.grid(row=4, column=0, sticky=tk.E, pady=(10, 0))
        
        ttk.Button(btn_frame, text="Annuler", command=self.destroy).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(btn_frame, text="Appliquer", 
                   command=self._apply_bio).pack(side=tk.LEFT)
    
    def _generate_bio(self):
        """G√©n√®re la bio selon le type choisi"""
        bio_type = self.bio_type_var.get()
        
        self.bio_text.delete('1.0', tk.END)
        self.bio_text.insert('1.0', "G√©n√©ration en cours...\n")
        
        def generate():
            bio = ""
            if bio_type == "google":
                bio = self.bio_generator.generate_google_bio(self.performer_name, self.metadata)
            elif bio_type in ["ollama", "ollama_custom"]:
                custom_prompt = ""
                if bio_type == "ollama_custom":
                    custom_prompt = self.prompt_text.get('1.0', tk.END).strip()
                bio = self.bio_generator.generate_ollama_bio(
                    self.performer_name, self.metadata, custom_prompt)
                if bio is None:
                    bio = "Erreur: Ollama n'est pas disponible ou n'a pas r√©pondu."
            
            # Afficher le r√©sultat
            self.after(0, lambda: self._display_bio(bio))
        
        thread = threading.Thread(target=generate)
        thread.daemon = True
        thread.start()
    
    def _display_bio(self, bio: str):
        """Affiche la bio g√©n√©r√©e"""
        self.bio_text.delete('1.0', tk.END)
        self.bio_text.insert('1.0', bio)
        
        # Mettre √† jour le compteur de caract√®res
        char_count = len(bio)
        self.char_count_label.config(text=f"Caract√®res : {char_count}")
    
    def _apply_bio(self):
        """Applique la bio et ferme la fen√™tre"""
        self.generated_bio = self.bio_text.get('1.0', tk.END).strip()
        self.destroy()
    
    def get_bio(self) -> str:
        """Retourne la bio g√©n√©r√©e"""
        return getattr(self, 'generated_bio', '')


class MainWindow(tk.Tk):
    """Fen√™tre principale unifi√©e - Fusion Phase 1 et Phase 2"""
    
    def __init__(self):
        super().__init__()
        
        self.title("StashMaster V2 - Performer")
        self.geometry("1200x900")
        
        self.tag_rules = TagRulesEngine()
        self.metadata = {}
        
        self._create_widgets()
        self._create_menu()
    
    def _create_menu(self):
        """Cr√©e le menu principal"""
        menubar = tk.Menu(self)
        self.config(menu=menubar)
        
        # Menu Fichier
        file_menu = tk.Menu(menubar, tearoff=0)
        menubar.add_cascade(label="Fichier", menu=file_menu)
        file_menu.add_command(label="Nouveau", command=self._new_performer)
        file_menu.add_command(label="Ouvrir...", command=self._open_performer)
        file_menu.add_separator()
        file_menu.add_command(label="Quitter", command=self.quit)
        
        # Menu Actions
        actions_menu = tk.Menu(menubar, tearoff=0)
        menubar.add_cascade(label="Actions", menu=actions_menu)
        actions_menu.add_command(label="Scraper & Lancer le flux Bio IA", 
                                 command=self._start_scraping)
        actions_menu.add_command(label="Trivia & Awards...", 
                                 command=self._open_trivia_awards)
        actions_menu.add_command(label="G√©n√©rer Bio...", 
                                 command=self._open_bio_generator)
    
    def _create_widgets(self):
        """Cr√©e l'interface principale"""
        # Notebook pour les onglets
        notebook = ttk.Notebook(self)
        notebook.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        # Onglet 1: M√©tadonn√©es usuelles
        self.metadata_frame = self._create_metadata_tab()
        notebook.add(self.metadata_frame, text="üìã M√©tadonn√©es")
        
        # Onglet 2: Champs avanc√©s
        self.advanced_frame = self._create_advanced_tab()
        notebook.add(self.advanced_frame, text="‚öôÔ∏è Champs Avanc√©s")
        
        # Onglet 3: Bio
        self.bio_frame = self._create_bio_tab()
        notebook.add(self.bio_frame, text="üìù Bio")
        
        # Barre d'outils en bas
        self._create_toolbar()
    
    def _create_metadata_tab(self) -> ttk.Frame:
        """Cr√©e l'onglet des m√©tadonn√©es de base"""
        frame = ttk.Frame()
        
        # Frame avec scrollbar
        canvas = tk.Canvas(frame)
        scrollbar = ttk.Scrollbar(frame, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # Champs de m√©tadonn√©es
        fields = [
            ("Name:", "name"),
            ("Aliases:", "aliases"),
            ("Birthdate:", "birthdate"),
            ("Deathdate:", "deathdate"),
            ("Country:", "country"),
            ("Ethnicity:", "ethnicity"),
            ("Hair Color:", "hair_color"),
            ("Eye Color:", "eye_color"),
            ("Height:", "height"),
            ("Weight:", "weight"),
            ("Measurements:", "measurements"),
            ("Fake Tits:", "fake_tits"),
            ("Career Length:", "career_length"),
        ]
        
        self.metadata_entries = {}
        
        for i, (label, key) in enumerate(fields):
            ttk.Label(scrollable_frame, text=label).grid(row=i, column=0, sticky=tk.W, 
                                                          padx=5, pady=3)
            entry = ttk.Entry(scrollable_frame, width=50)
            entry.grid(row=i, column=1, sticky=(tk.W, tk.E), padx=5, pady=3)
            self.metadata_entries[key] = entry
        
        scrollable_frame.columnconfigure(1, weight=1)
        
        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
        
        return frame
    
    def _create_advanced_tab(self) -> ttk.Frame:
        """Cr√©e l'onglet des champs avanc√©s"""
        frame = ttk.Frame()
        frame.columnconfigure(0, weight=1)
        
        row = 0
        
        # Tags - CHAMP SIMPLE LIGNE (g√©n√©r√©s automatiquement)
        ttk.Label(frame, text="Tags (g√©n√©r√©s automatiquement):", 
                  font=('Segoe UI', 10, 'bold')).grid(row=row, column=0, sticky=tk.W, 
                                                       padx=5, pady=(10, 2))
        row += 1
        
        self.tags_entry = ttk.Entry(frame, state='readonly')
        self.tags_entry.grid(row=row, column=0, sticky=(tk.W, tk.E), padx=5, pady=(0, 10))
        row += 1
        
        # Piercings - CHAMP MULTILIGNE
        ttk.Label(frame, text="Piercings:", 
                  font=('Segoe UI', 10, 'bold')).grid(row=row, column=0, sticky=tk.W, 
                                                       padx=5, pady=(10, 2))
        row += 1
        
        self.piercings_text = scrolledtext.ScrolledText(frame, height=4, wrap=tk.WORD)
        self.piercings_text.grid(row=row, column=0, sticky=(tk.W, tk.E), padx=5, pady=(0, 10))
        row += 1
        
        # Tattoos - CHAMP MULTILIGNE
        ttk.Label(frame, text="Tattoos:", 
                  font=('Segoe UI', 10, 'bold')).grid(row=row, column=0, sticky=tk.W, 
                                                       padx=5, pady=(10, 2))
        row += 1
        
        self.tattoos_text = scrolledtext.ScrolledText(frame, height=4, wrap=tk.WORD)
        self.tattoos_text.grid(row=row, column=0, sticky=(tk.W, tk.E), padx=5, pady=(0, 10))
        row += 1
        
        # URLs - CHAMP MULTILIGNE
        ttk.Label(frame, text="URLs:", 
                  font=('Segoe UI', 10, 'bold')).grid(row=row, column=0, sticky=tk.W, 
                                                       padx=5, pady=(10, 2))
        row += 1
        
        self.urls_text = scrolledtext.ScrolledText(frame, height=6, wrap=tk.WORD)
        self.urls_text.grid(row=row, column=0, sticky=(tk.W, tk.E), padx=5, pady=(0, 10))
        row += 1
        
        # Bouton pour g√©n√©rer les tags
        ttk.Button(frame, text="üîÑ G√©n√©rer Tags", 
                   command=self._generate_tags).grid(row=row, column=0, pady=10)
        
        return frame
    
    def _create_bio_tab(self) -> ttk.Frame:
        """Cr√©e l'onglet de la bio"""
        frame = ttk.Frame()
        frame.columnconfigure(0, weight=1)
        frame.rowconfigure(1, weight=1)
        
        # Boutons d'action
        btn_frame = ttk.Frame(frame)
        btn_frame.grid(row=0, column=0, sticky=tk.W, padx=5, pady=5)
        
        ttk.Button(btn_frame, text="üìù G√©n√©rer Bio...", 
                   command=self._open_bio_generator).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(btn_frame, text="üóëÔ∏è Effacer", 
                   command=self._clear_bio).pack(side=tk.LEFT)
        
        # Champ de bio
        self.bio_text = scrolledtext.ScrolledText(frame, wrap=tk.WORD)
        self.bio_text.grid(row=1, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), 
                          padx=5, pady=(0, 5))
        
        # Compteur de caract√®res
        self.bio_char_label = ttk.Label(frame, text="Caract√®res : 0")
        self.bio_char_label.grid(row=2, column=0, sticky=tk.E, padx=5, pady=(0, 5))
        
        # Binding pour mettre √† jour le compteur
        self.bio_text.bind('<KeyRelease>', self._update_bio_char_count)
        
        return frame
    
    def _create_toolbar(self):
        """Cr√©e la barre d'outils en bas"""
        toolbar = ttk.Frame(self)
        toolbar.pack(side=tk.BOTTOM, fill=tk.X, padx=5, pady=5)
        
        ttk.Button(toolbar, text="Tout s√©lectionner", 
                   command=self._select_all).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(toolbar, text="S√©lectionner vid√©os", 
                   command=self._select_videos).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(toolbar, text="Suivant / Traiter", 
                   command=self._process_next).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(toolbar, text="Retour", 
                   command=self._go_back).pack(side=tk.LEFT)
        
        # Spacer
        ttk.Frame(toolbar).pack(side=tk.LEFT, expand=True)
        
        ttk.Button(toolbar, text="üíæ Sauvegarder", 
                   command=self._save).pack(side=tk.RIGHT, padx=(5, 0))
    
    def _generate_tags(self):
        """G√©n√®re les tags automatiquement bas√©s sur les m√©tadonn√©es"""
        # R√©cup√©rer les m√©tadonn√©es actuelles
        metadata = {
            'ethnicity': self.metadata_entries['ethnicity'].get(),
            'hair_color': self.metadata_entries['hair_color'].get(),
            'measurements': self.metadata_entries['measurements'].get(),
            'piercings': self.piercings_text.get('1.0', tk.END).strip(),
            'tattoos': self.tattoos_text.get('1.0', tk.END).strip(),
            'career_length': self.metadata_entries['career_length'].get(),
        }
        
        # G√©n√©rer les tags
        tags = self.tag_rules.generate_tags(metadata)
        
        # Afficher les tags
        self.tags_entry.config(state='normal')
        self.tags_entry.delete(0, tk.END)
        self.tags_entry.insert(0, ', '.join(tags))
        self.tags_entry.config(state='readonly')
        
        messagebox.showinfo("Tags g√©n√©r√©s", 
                           f"{len(tags)} tag(s) g√©n√©r√©(s) automatiquement.")
    
    def _update_bio_char_count(self, event=None):
        """Met √† jour le compteur de caract√®res de la bio"""
        text = self.bio_text.get('1.0', tk.END)
        count = len(text.strip())
        self.bio_char_label.config(text=f"Caract√®res : {count}")
    
    def _clear_bio(self):
        """Efface la bio"""
        if messagebox.askyesno("Confirmation", 
                              "√ätes-vous s√ªr de vouloir effacer la bio ?"):
            self.bio_text.delete('1.0', tk.END)
            self._update_bio_char_count()
    
    def _start_scraping(self):
        """Lance le flux de scraping complet"""
        messagebox.showinfo("Scraping", 
                           "Fonction de scraping √† impl√©menter...")
    
    def _open_trivia_awards(self):
        """Ouvre la fen√™tre Trivia & Awards"""
        # R√©cup√©rer le nom et les URLs
        performer_name = self.metadata_entries['name'].get()
        if not performer_name:
            messagebox.showwarning("Attention", "Veuillez entrer un nom de performer.")
            return
        
        urls_text = self.urls_text.get('1.0', tk.END).strip()
        urls = [url.strip() for url in urls_text.split('\n') if url.strip()]
        
        if not urls:
            messagebox.showwarning("Attention", "Veuillez entrer au moins une URL.")
            return
        
        # Ouvrir la fen√™tre
        window = TriviaAwardsWindow(self, performer_name, urls)
        self.wait_window(window)
        
        # R√©cup√©rer les donn√©es
        data = window.get_data()
        # Stocker dans les m√©tadonn√©es
        self.metadata['trivia'] = data.get('trivia', '')
        self.metadata['awards'] = data.get('awards', '')
        
        messagebox.showinfo("Trivia & Awards", 
                           "Donn√©es collect√©es avec succ√®s.")
    
    def _open_bio_generator(self):
        """Ouvre la fen√™tre de g√©n√©ration de bio"""
        # R√©cup√©rer le nom
        performer_name = self.metadata_entries['name'].get()
        if not performer_name:
            messagebox.showwarning("Attention", "Veuillez entrer un nom de performer.")
            return
        
        # Pr√©parer les m√©tadonn√©es
        metadata = {
            'name': performer_name,
            'aliases': self.metadata_entries['aliases'].get().split(','),
            'birthdate': self.metadata_entries['birthdate'].get(),
            'birthplace': self.metadata_entries['country'].get(),
            'ethnicity': self.metadata_entries['ethnicity'].get(),
            'hair_color': self.metadata_entries['hair_color'].get(),
            'measurements': self.metadata_entries['measurements'].get(),
            'height': self.metadata_entries['height'].get(),
            'weight': self.metadata_entries['weight'].get(),
            'tattoos': self.tattoos_text.get('1.0', tk.END).strip(),
            'piercings': self.piercings_text.get('1.0', tk.END).strip(),
            'trivia': self.metadata.get('trivia', ''),
            'awards': self.metadata.get('awards', ''),
            'career_length': self.metadata_entries['career_length'].get(),
        }
        
        # Ouvrir la fen√™tre
        window = BioGenerationWindow(self, performer_name, metadata)
        self.wait_window(window)
        
        # R√©cup√©rer la bio g√©n√©r√©e
        bio = window.get_bio()
        if bio:
            self.bio_text.delete('1.0', tk.END)
            self.bio_text.insert('1.0', bio)
            self._update_bio_char_count()
            messagebox.showinfo("Bio g√©n√©r√©e", "La bio a √©t√© g√©n√©r√©e avec succ√®s.")
    
    def _new_performer(self):
        """Nouveau performer"""
        # Effacer tous les champs
        for entry in self.metadata_entries.values():
            entry.delete(0, tk.END)
        self.piercings_text.delete('1.0', tk.END)
        self.tattoos_text.delete('1.0', tk.END)
        self.urls_text.delete('1.0', tk.END)
        self.bio_text.delete('1.0', tk.END)
        self.tags_entry.config(state='normal')
        self.tags_entry.delete(0, tk.END)
        self.tags_entry.config(state='readonly')
        self.metadata = {}
    
    def _open_performer(self):
        """Ouvre un performer existant"""
        messagebox.showinfo("Ouvrir", "Fonction √† impl√©menter...")
    
    def _select_all(self):
        """S√©lectionne tous les champs"""
        pass
    
    def _select_videos(self):
        """S√©lectionne les vid√©os"""
        pass
    
    def _process_next(self):
        """Traite le performer suivant"""
        pass
    
    def _go_back(self):
        """Retour"""
        pass
    
    def _save(self):
        """Sauvegarde les donn√©es"""
        messagebox.showinfo("Sauvegarde", "Donn√©es sauvegard√©es (√† impl√©menter).")


def main():
    app = MainWindow()
    app.mainloop()


if __name__ == "__main__":
    main()


============================================================
[29/124] Legacy\files\stashmaster-v2\stashmaster-v2\test_stashmaster.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Tests unitaires pour StashMaster V2
"""

import unittest
import sys
import os

# Ajouter le r√©pertoire parent au path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from stashmaster_unified import TagRulesEngine, AwardsCleaner
from scrapers import DataMerger


class TestTagRulesEngine(unittest.TestCase):
    """Tests pour le moteur de g√©n√©ration de tags"""
    
    def setUp(self):
        self.engine = TagRulesEngine()
    
    def test_ethnicity_tags(self):
        """Test des tags d'ethnicit√©"""
        # Test Caucasian
        metadata = {'ethnicity': 'Caucasian'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Caucasian', tags)
        
        # Test Latina
        metadata = {'ethnicity': 'Cuban'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Latina', tags)
        
        # Test Asian
        metadata = {'ethnicity': 'Asian'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Asian', tags)
    
    def test_hair_color_tags(self):
        """Test des tags de couleur de cheveux"""
        # Test Blonde
        metadata = {'hair_color': 'Blonde'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Blonde', tags)
        
        # Test Brunette
        metadata = {'hair_color': 'Brown'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Brunette', tags)
        
        # Test Redhead
        metadata = {'hair_color': 'Red'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Redhead', tags)
    
    def test_measurements_tags(self):
        """Test des tags bas√©s sur les mesures"""
        # Test Big Boobs
        metadata = {'measurements': '38DD-27-34'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Big Boobs', tags)
        
        # Test Small Boobs
        metadata = {'measurements': '32A-24-32'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Small Boobs', tags)
    
    def test_piercings_tags(self):
        """Test des tags de piercings"""
        metadata = {'piercings': 'Navel, Tongue'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Pierced', tags)
        
        # Pas de tag si "none"
        metadata = {'piercings': 'none'}
        tags = self.engine.generate_tags(metadata)
        self.assertNotIn('Pierced', tags)
    
    def test_tattoos_tags(self):
        """Test des tags de tattoos"""
        metadata = {'tattoos': 'Lower back, Right shoulder'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Tattooed', tags)
        
        # Pas de tag si "none"
        metadata = {'tattoos': 'none'}
        tags = self.engine.generate_tags(metadata)
        self.assertNotIn('Tattooed', tags)
    
    def test_combined_tags(self):
        """Test de la g√©n√©ration de tags combin√©s"""
        metadata = {
            'ethnicity': 'Latina',
            'hair_color': 'Blonde',
            'measurements': '36DD-25-36',
            'piercings': 'Navel',
            'tattoos': 'Lower back'
        }
        tags = self.engine.generate_tags(metadata)
        
        self.assertIn('Latina', tags)
        self.assertIn('Blonde', tags)
        self.assertIn('Big Boobs', tags)
        self.assertIn('Pierced', tags)
        self.assertIn('Tattooed', tags)
        
        # V√©rifier qu'il n'y a pas de doublons
        self.assertEqual(len(tags), len(set(tags)))


class TestAwardsCleaner(unittest.TestCase):
    """Tests pour le nettoyeur d'awards"""
    
    def setUp(self):
        self.cleaner = AwardsCleaner()
    
    def test_clean_simple_awards(self):
        """Test du nettoyage d'awards simples"""
        raw = """
        2012
        Winner: Unsung Starlet of the Year
        2013
        Nominee: Best Boobs
        """
        
        cleaned = self.cleaner.clean_awards(raw)
        
        # V√©rifier la pr√©sence des ann√©es
        self.assertIn('2012', cleaned)
        self.assertIn('2013', cleaned)
        
        # V√©rifier la pr√©sence des awards
        self.assertIn('Winner', cleaned)
        self.assertIn('Nominee', cleaned)
    
    def test_clean_awards_with_ceremony(self):
        """Test du nettoyage avec type de c√©r√©monie"""
        raw = """
        AVN AWARDS
        2012
        Winner: Unsung Starlet of the Year
        XBIZ AWARDS
        2013
        Nominee: Best Performer
        """
        
        cleaned = self.cleaner.clean_awards(raw)
        
        # V√©rifier la pr√©sence des c√©r√©monies
        self.assertIn('AVN AWARDS', cleaned)
        self.assertIn('XBIZ AWARDS', cleaned)
    
    def test_clean_empty_awards(self):
        """Test avec des awards vides"""
        raw = ""
        cleaned = self.cleaner.clean_awards(raw)
        self.assertEqual(cleaned, "")


class TestDataMerger(unittest.TestCase):
    """Tests pour le fusionneur de donn√©es"""
    
    def setUp(self):
        self.merger = DataMerger()
    
    def test_merge_identical_data(self):
        """Test de fusion de donn√©es identiques"""
        sources = [
            {
                'source': 'iafd',
                'name': 'Bridgette B',
                'ethnicity': 'Caucasian'
            },
            {
                'source': 'freeones',
                'name': 'Bridgette B',
                'ethnicity': 'Caucasian'
            }
        ]
        
        confirmed, conflicts = self.merger.merge_data(sources)
        
        # Les donn√©es identiques doivent √™tre confirm√©es
        self.assertIn('name', confirmed)
        self.assertEqual(confirmed['name']['value'], 'Bridgette B')
        self.assertEqual(confirmed['name']['count'], 2)
        
        # Pas de conflits
        self.assertEqual(len(conflicts), 0)
    
    def test_merge_conflicting_data(self):
        """Test de fusion de donn√©es conflictuelles"""
        sources = [
            {
                'source': 'iafd',
                'hair_color': 'Blonde'
            },
            {
                'source': 'freeones',
                'hair_color': 'Brown'
            }
        ]
        
        confirmed, conflicts = self.merger.merge_data(sources)
        
        # Doit y avoir un conflit
        self.assertIn('hair_color', conflicts)
        self.assertEqual(len(conflicts['hair_color']), 2)
    
    def test_merge_majority_data(self):
        """Test de fusion avec valeur majoritaire"""
        sources = [
            {'source': 'iafd', 'ethnicity': 'Caucasian'},
            {'source': 'freeones', 'ethnicity': 'Caucasian'},
            {'source': 'thenude', 'ethnicity': 'White'}
        ]
        
        confirmed, conflicts = self.merger.merge_data(sources)
        
        # La valeur majoritaire doit √™tre confirm√©e
        self.assertIn('ethnicity', confirmed)
        self.assertEqual(confirmed['ethnicity']['count'], 2)
    
    def test_merge_unique_data(self):
        """Test de fusion de donn√©es uniques"""
        sources = [
            {
                'source': 'iafd',
                'name': 'Bridgette B',
                'birthdate': 'October 15, 1983'
            },
            {
                'source': 'freeones',
                'name': 'Bridgette B'
            }
        ]
        
        confirmed, conflicts = self.merger.merge_data(sources)
        
        # Les donn√©es uniques doivent √™tre confirm√©es
        self.assertIn('birthdate', confirmed)
        self.assertEqual(confirmed['birthdate']['count'], 1)


class TestScrapers(unittest.TestCase):
    """Tests pour les scrapers (n√©cessitent une connexion internet)"""
    
    def test_iafd_url_detection(self):
        """Test de d√©tection d'URL IAFD"""
        from scrapers import ScraperOrchestrator
        
        orchestrator = ScraperOrchestrator()
        url = "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm"
        
        scraper = orchestrator._get_scraper_for_url(url)
        self.assertIsNotNone(scraper)
        self.assertEqual(type(scraper).__name__, 'IAFDScraper')
    
    def test_freeones_url_detection(self):
        """Test de d√©tection d'URL Freeones"""
        from scrapers import ScraperOrchestrator
        
        orchestrator = ScraperOrchestrator()
        url = "https://www.freeones.xxx/bridgette-b"
        
        scraper = orchestrator._get_scraper_for_url(url)
        self.assertIsNotNone(scraper)
        self.assertEqual(type(scraper).__name__, 'FreeonesScraper')


def run_tests():
    """Lance tous les tests"""
    # Cr√©er une suite de tests
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()
    
    # Ajouter tous les tests
    suite.addTests(loader.loadTestsFromTestCase(TestTagRulesEngine))
    suite.addTests(loader.loadTestsFromTestCase(TestAwardsCleaner))
    suite.addTests(loader.loadTestsFromTestCase(TestDataMerger))
    suite.addTests(loader.loadTestsFromTestCase(TestScrapers))
    
    # Lancer les tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # Retourner le statut
    return 0 if result.wasSuccessful() else 1


if __name__ == '__main__':
    sys.exit(run_tests())


============================================================
[30/124] Legacy\gui\__init__.py
------------------------------------------------------------


============================================================
[31/124] Legacy\gui\app.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
import sv_ttk
from gui.performer_frame import PerformerFrame
# Placeholders for future modules
from gui.group_frame import GroupFrame
# from gui.scene_frame import SceneFrame

def launch_app(module, stash_id):
    root = tk.Tk()
    root.title(f"StashMaster V2 - {module}")
    
    # Maximiser la fen√™tre au d√©marrage (Windows)
    try:
        root.state('zoomed')
    except:
        # Fallback pour d'autres OS (Linux/Mac)
        w, h = root.winfo_screenwidth(), root.winfo_screenheight()
        root.geometry(f"{w}x{h}+0+0")

    # Appliquer le th√®me moderne
    sv_ttk.set_theme("dark")

    if module == "Performer":
        frame = PerformerFrame(root, stash_id)
    elif module == "Group":
        frame = GroupFrame(root, stash_id)
    # elif module == "Scene":
    #     frame = SceneFrame(root, stash_id)
    else:
        import tkinter.messagebox as mb
        mb.showerror("Erreur", f"Module inconnu: {module}")
        root.destroy()
        return
    frame.pack(fill=tk.BOTH, expand=True)
    root.mainloop()


============================================================
[32/124] Legacy\gui\bio_studio_window.py
------------------------------------------------------------
"""
BioStudioWindow ‚Äî GUI 2/3 du flux Bio IA
=========================================
G√©n√©ration de biographie en plein √©cran.
Gemini (gauche) et Ollama (droite) c√¥te √† c√¥te.

Layout :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  HEADER : performer + breadcrumb 2/3                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  CONTEXTE PERFORMER (bandeau compact 2 lignes)                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ ü§ñ Google Gemini ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ ‚öôÔ∏è Ollama (local) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  [üöÄ G√©n√©rer]  [üîÑ Retry] ‚îÇ  ‚îÇ  [‚öôÔ∏è Affiner]  [‚ö° G√©n√©rer]    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                           ‚îÇ  ‚îÇ                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Zone texte √©ditable      ‚îÇ  ‚îÇ  Zone texte √©ditable            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  (r√©sultat Gemini)        ‚îÇ  ‚îÇ  (r√©sultat Ollama)              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                           ‚îÇ  ‚îÇ                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  [üìã Copier] [Utiliser ‚Üí] ‚îÇ  ‚îÇ  [üìã Copier] [Utiliser ‚Üí]      ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  [‚Üê Retour]  [Annuler]    Compteurs chars       [‚Üí Valider ‚Üí]       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""
import tkinter as tk
from tkinter import ttk, messagebox
import threading
import platform

from services.bio_generator import BioGenerator

# ‚îÄ‚îÄ Palette ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
P = {
    "bg":         "#13131f",
    "surface":    "#1e1e30",
    "card":       "#22223a",
    "card_hdr":   "#2c2c4a",
    "border":     "#3a3a58",
    "accent":     "#7c6af7",
    "gemini_bg":  "#0d1a2e",
    "gemini_hdr": "#1a2a4a",
    "gemini_acc": "#4a8af0",
    "ollama_bg":  "#1e1204",
    "ollama_hdr": "#2e2010",
    "ollama_acc": "#e8954a",
    "success":    "#4caf7d",
    "danger":     "#c05050",
    "text":       "#e8e8f5",
    "muted":      "#8888aa",
    "dim":        "#55557a",
}

FH1  = ("Segoe UI", 14, "bold")
FH2  = ("Segoe UI", 11, "bold")
FH3  = ("Segoe UI", 9,  "bold")
FB   = ("Segoe UI", 10)
FSM  = ("Segoe UI", 8)
FSMB = ("Segoe UI", 8,  "bold")
FMONO= ("Consolas", 9)


class BioStudioWindow(tk.Toplevel):
    """
    Fen√™tre 2/3 : g√©n√©ration de bio via Gemini et/ou Ollama.
    Retourne `result` dict avec 'bio' str, ou None si annul√©.
    """
    def __init__(self, parent, db_data, stash_ctx, merged_data,
                 scraped_results, checked_fields, review_result):
        super().__init__(parent)
        self.title("üé® Studio Bio IA ‚Äî √âtape 2/3")
        self.configure(bg=P["bg"])

        self.db_data         = db_data
        self.stash_ctx       = stash_ctx
        self.merged_data     = merged_data
        self.scraped_results = scraped_results
        self.checked_fields  = checked_fields
        self.review_result   = review_result   # r√©sultat GUI 1
        self.result          = None            # {'bio': str}

        self._bio_gen  = BioGenerator()
        self._ctx      = None
        self._busy_g   = False   # Gemini en cours
        self._busy_o   = False   # Ollama en cours

        _fullscreen(self)
        self.transient(parent)
        self.grab_set()

        self._build_ui()
        self._prepare_context()

        self.bind("<Escape>", lambda _: self._cancel())
        self.wait_window()

    # ‚îÄ‚îÄ Contexte IA ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _prepare_context(self):
        """Construit le contexte IA depuis les donn√©es fusionn√©es + s√©lections GUI 1."""
        try:
            # Injecter les s√©lections de la GUI 1 dans merged_data
            md = dict(self.merged_data)
            rv = self.review_result or {}

            if rv.get("awards"):
                md.setdefault("awards", {})["merged"] = rv["awards"]
            if rv.get("tattoos") is not None:
                md.setdefault("tattoos", {})["merged"] = rv["tattoos"]
            if rv.get("piercings") is not None:
                md.setdefault("piercings", {})["merged"] = rv["piercings"]
            if rv.get("urls"):
                md.setdefault("urls", {})["merged"] = rv["urls"]
            if rv.get("trivia"):
                md.setdefault("trivia", {})["suggestion"] = rv["trivia"]
                md["trivia"]["by_source"] = md["trivia"].get("by_source", {})
                md["trivia"]["by_source"]["_user_"] = rv["trivia"]

            all_fields = list(set(self.checked_fields + [
                "Name", "Birthdate", "Country", "Ethnicity",
                "Hair Color", "Eye Color", "Measurements", "Height",
                "Weight", "Fake Tits", "Aliases", "Career Length",
                "Tattoos", "Piercings", "Awards", "URLs", "Details",
            ]))

            self._ctx = self._bio_gen.build_context_from_v2(
                db_data=self.db_data,
                stash_ctx=self.stash_ctx,
                scraped_results=self.scraped_results,
                merged_data=md,
                checked_fields=all_fields,
            )
            self._set_context_banner()
        except Exception as e:
            print(f"[BioStudio] Erreur contexte : {e}")

    # ‚îÄ‚îÄ UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_ui(self):
        self._build_header()
        self._build_context_banner()
        self._build_studio()
        self._build_footer()

    def _build_header(self):
        hdr = tk.Frame(self, bg=P["accent"])
        hdr.pack(fill=tk.X)
        tk.Frame(hdr, bg="#9a88ff", height=3).pack(fill=tk.X)

        row = tk.Frame(hdr, bg=P["accent"], pady=7)
        row.pack(fill=tk.X, padx=12)

        tk.Label(row, text="üé®", font=("Segoe UI", 18),
                 fg="white", bg=P["accent"]).pack(side=tk.LEFT, padx=(0, 8))

        name = self.db_data.get("name", "Performer inconnu")
        tk.Label(row, text=f"Studio Bio IA ‚Äî {name}",
                 font=FH1, fg="white", bg=P["accent"]).pack(side=tk.LEFT)

        # Breadcrumb
        bc = tk.Frame(row, bg=P["accent"])
        bc.pack(side=tk.RIGHT, padx=12)
        for num, lbl, active in [("1","Donn√©es",False),("2","Bio IA",True),("3","Valider",False)]:
            bg = "#5a48c8" if active else P["accent"]
            fg = "white"   if active else "#9980cc"
            tk.Label(bc, text=f" {num} ", font=FSMB, fg=fg,
                     bg=bg, padx=6, pady=3).pack(side=tk.LEFT, padx=1)
            tk.Label(bc, text=lbl, font=FSM, fg=fg,
                     bg=P["accent"]).pack(side=tk.LEFT, padx=(0,8))

    def _build_context_banner(self):
        """Bandeau compact montrant le contexte performer."""
        self._banner_frame = tk.Frame(self, bg=P["surface"], pady=4)
        self._banner_frame.pack(fill=tk.X)
        tk.Frame(self, bg=P["border"], height=1).pack(fill=tk.X)

        self._banner_inner = tk.Frame(self._banner_frame, bg=P["surface"])
        self._banner_inner.pack(fill=tk.X, padx=12)

    def _set_context_banner(self):
        """Remplit le bandeau contexte une fois le contexte charg√©."""
        for w in self._banner_inner.winfo_children():
            w.destroy()
        if not self._ctx:
            return

        db  = self.db_data
        ctx = self._ctx

        def pill(label, value, color=None):
            if not value:
                return
            f = tk.Frame(self._banner_inner, bg=P["card_hdr"])
            f.pack(side=tk.LEFT, padx=3, pady=2)
            tk.Label(f, text=f" {label} ", font=FSMB,
                     fg=P["muted"], bg=P["card_hdr"]).pack(side=tk.LEFT)
            tk.Label(f, text=f" {str(value)[:40]} ", font=FSM,
                     fg=color or P["text"], bg=P["card"]).pack(side=tk.LEFT)

        pill("üë§", ctx.get("name"))
        pill("üéÇ", ctx.get("birthdate"))
        pill("üåç", ctx.get("country"))
        pill("üë§", ctx.get("ethnicity"))
        pill("üíá", ctx.get("hair_color"))
        pill("üëÅ",  ctx.get("eye_color"))
        pill("üìê", ctx.get("measurements"))
        pill("üé¨", f"{ctx.get('scene_count',0)} sc√®nes", P["gemini_acc"])
        pill("üè¢", ", ".join(ctx.get("studios",[])[:3]))
        pill("üèÜ", f"{len(ctx.get('awards_fr',[]))} awards",
             P["ollama_acc"] if ctx.get("awards_fr") else None)

        tk.Frame(self, bg=P["border"], height=1).pack(fill=tk.X)

    def _build_studio(self):
        """Zone centrale : deux panneaux c√¥te √† c√¥te."""
        studio = tk.Frame(self, bg=P["bg"])
        studio.pack(fill=tk.BOTH, expand=True, padx=8, pady=6)
        studio.grid_columnconfigure(0, weight=1)
        studio.grid_columnconfigure(1, weight=1)
        studio.grid_rowconfigure(0, weight=1)

        self._build_gemini_panel(studio)
        self._build_ollama_panel(studio)

    # ‚îÄ‚îÄ Panneau Gemini ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_gemini_panel(self, parent):
        panel = tk.Frame(parent, bg=P["gemini_hdr"], bd=0)
        panel.grid(row=0, column=0, sticky="nsew", padx=(0, 4))

        # En-t√™te Gemini
        ghdr = tk.Frame(panel, bg=P["gemini_acc"], pady=8)
        ghdr.pack(fill=tk.X)
        tk.Label(ghdr, text="  ü§ñ  Google Gemini",
                 font=FH2, fg="white", bg=P["gemini_acc"]).pack(side=tk.LEFT, padx=8)
        self._gemini_status = tk.Label(ghdr, text="Pr√™t",
                                        font=FSM, fg="#cce", bg=P["gemini_acc"])
        self._gemini_status.pack(side=tk.RIGHT, padx=8)
        self._gemini_progress = ttk.Progressbar(ghdr, mode="indeterminate", length=100)
        self._gemini_progress.pack(side=tk.RIGHT, padx=4)

        # Boutons
        gbtn = tk.Frame(panel, bg=P["gemini_hdr"], pady=6)
        gbtn.pack(fill=tk.X, padx=8)

        self._btn_gemini_gen = _studio_btn(
            gbtn, "üöÄ G√©n√©rer", P["gemini_acc"],
            self._run_gemini
        )
        self._btn_gemini_retry = _studio_btn(
            gbtn, "üîÑ R√©g√©n√©rer", P["card_hdr"],
            self._run_gemini, state=tk.DISABLED
        )
        self._gemini_char_lbl = tk.Label(
            gbtn, text="", font=FSM, fg=P["muted"], bg=P["gemini_hdr"]
        )
        self._gemini_char_lbl.pack(side=tk.RIGHT, padx=8)

        tk.Frame(panel, bg=P["gemini_acc"], height=2).pack(fill=tk.X)

        # Zone texte
        txt_f = tk.Frame(panel, bg=P["gemini_bg"])
        txt_f.pack(fill=tk.BOTH, expand=True)

        self._txt_gemini = tk.Text(
            txt_f, wrap=tk.WORD, font=FB,
            bg=P["gemini_bg"], fg=P["text"],
            insertbackground=P["text"],
            relief=tk.FLAT, padx=12, pady=10,
            undo=True,
        )
        sb = ttk.Scrollbar(txt_f, command=self._txt_gemini.yview)
        self._txt_gemini.configure(yscrollcommand=sb.set)
        self._txt_gemini.bind("<KeyRelease>",
                              lambda e: self._update_char(self._txt_gemini,
                                                          self._gemini_char_lbl))
        sb.pack(side=tk.RIGHT, fill=tk.Y)
        self._txt_gemini.pack(fill=tk.BOTH, expand=True)
        self._txt_gemini.insert("1.0",
            "üí° Cliquez sur [üöÄ G√©n√©rer] pour cr√©er une biographie via Google Gemini.\n\n"
            "Le r√©sultat sera directement √©ditable ici.")

        # Footer panneau
        gfoot = tk.Frame(panel, bg=P["gemini_hdr"], pady=6)
        gfoot.pack(fill=tk.X, padx=8)
        _studio_btn(gfoot, "üìã Copier",
                    P["card_hdr"], lambda: self._copy(self._txt_gemini))
        _studio_btn(gfoot, "‚Üí Utiliser cette bio",
                    P["success"],  lambda: self._use_bio(self._txt_gemini))

    # ‚îÄ‚îÄ Panneau Ollama ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_ollama_panel(self, parent):
        panel = tk.Frame(parent, bg=P["ollama_hdr"], bd=0)
        panel.grid(row=0, column=1, sticky="nsew", padx=(4, 0))

        # En-t√™te Ollama
        ohdr = tk.Frame(panel, bg=P["ollama_acc"], pady=8)
        ohdr.pack(fill=tk.X)
        tk.Label(ohdr, text="  ‚öôÔ∏è  Ollama (local)",
                 font=FH2, fg="white", bg=P["ollama_acc"]).pack(side=tk.LEFT, padx=8)
        self._ollama_status = tk.Label(ohdr, text="Pr√™t",
                                        font=FSM, fg="#ffe", bg=P["ollama_acc"])
        self._ollama_status.pack(side=tk.RIGHT, padx=8)
        self._ollama_progress = ttk.Progressbar(ohdr, mode="indeterminate", length=100)
        self._ollama_progress.pack(side=tk.RIGHT, padx=4)

        # Boutons
        obtn = tk.Frame(panel, bg=P["ollama_hdr"], pady=6)
        obtn.pack(fill=tk.X, padx=8)

        self._btn_ollama_refine = _studio_btn(
            obtn, "‚öôÔ∏è Affiner Gemini", P["ollama_acc"],
            self._run_ollama_refine
        )
        self._btn_ollama_gen = _studio_btn(
            obtn, "‚ö° G√©n√©rer seul", P["card_hdr"],
            self._run_ollama_gen
        )
        self._ollama_char_lbl = tk.Label(
            obtn, text="", font=FSM, fg=P["muted"], bg=P["ollama_hdr"]
        )
        self._ollama_char_lbl.pack(side=tk.RIGHT, padx=8)

        tk.Frame(panel, bg=P["ollama_acc"], height=2).pack(fill=tk.X)

        # Zone texte
        txt_f = tk.Frame(panel, bg=P["ollama_bg"])
        txt_f.pack(fill=tk.BOTH, expand=True)

        self._txt_ollama = tk.Text(
            txt_f, wrap=tk.WORD, font=FB,
            bg=P["ollama_bg"], fg=P["text"],
            insertbackground=P["text"],
            relief=tk.FLAT, padx=12, pady=10,
            undo=True,
        )
        sb = ttk.Scrollbar(txt_f, command=self._txt_ollama.yview)
        self._txt_ollama.configure(yscrollcommand=sb.set)
        self._txt_ollama.bind("<KeyRelease>",
                              lambda e: self._update_char(self._txt_ollama,
                                                          self._ollama_char_lbl))
        sb.pack(side=tk.RIGHT, fill=tk.Y)
        self._txt_ollama.pack(fill=tk.BOTH, expand=True)
        self._txt_ollama.insert("1.0",
            "üí° [‚öôÔ∏è Affiner Gemini] : prend la bio Gemini et la corrige.\n"
            "[‚ö° G√©n√©rer seul] : g√©n√®re une bio ind√©pendante via Ollama.\n\n"
            "G√©n√©rez d'abord avec Gemini (gauche) pour obtenir la meilleure qualit√©.")

        # Footer panneau
        ofoot = tk.Frame(panel, bg=P["ollama_hdr"], pady=6)
        ofoot.pack(fill=tk.X, padx=8)
        _studio_btn(ofoot, "üìã Copier",
                    P["card_hdr"], lambda: self._copy(self._txt_ollama))
        _studio_btn(ofoot, "‚Üí Utiliser cette bio",
                    P["success"],  lambda: self._use_bio(self._txt_ollama))

    # ‚îÄ‚îÄ Footer principal ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_footer(self):
        tk.Frame(self, bg=P["border"], height=1).pack(fill=tk.X)
        bar = tk.Frame(self, bg=P["surface"], pady=10)
        bar.pack(fill=tk.X, padx=12)

        _action_btn(bar, "‚Üê Retour",  P["dim"],     self._go_back, side=tk.LEFT)
        _action_btn(bar, "‚úñ Annuler", P["danger"],  self._cancel,  side=tk.LEFT, padx=6)

        _action_btn(bar, "‚Üí Valider sans bio",
                    P["card_hdr"], self._proceed_no_bio, side=tk.RIGHT)
        _action_btn(bar, "‚Üí Continuer vers Validation",
                    P["success"], self._proceed_with_best, side=tk.RIGHT, padx=8)

    # ‚îÄ‚îÄ G√©n√©ration Gemini ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _run_gemini(self):
        if self._busy_g:
            return
        if not self._bio_gen.gemini_key:
            messagebox.showerror(
                "Cl√© Gemini manquante",
                "Fichier .gemini_key introuvable √† la racine du projet V2."
            )
            return
        self._set_gemini_busy(True, "G√©n√©ration en cours‚Ä¶")

        def _worker():
            try:
                bio = self._bio_gen.generate_gemini_bio(self._ctx)
                self.after(0, self._on_gemini_done, bio)
            except Exception as e:
                self.after(0, self._on_gemini_done, None, str(e))

        threading.Thread(target=_worker, daemon=True).start()

    def _on_gemini_done(self, bio, error=None):
        self._set_gemini_busy(False)
        if bio:
            self._set_text(self._txt_gemini, bio)
            self._update_char(self._txt_gemini, self._gemini_char_lbl)
            self._gemini_status.config(text=f"‚úì {len(bio)} chars")
            self._btn_gemini_retry.config(state=tk.NORMAL)
        else:
            self._gemini_status.config(text="‚úó √âchec")
            msg = f"G√©n√©ration Gemini √©chou√©e.\n{error or ''}"
            messagebox.showwarning("Gemini", msg)

    def _set_gemini_busy(self, busy, msg=""):
        self._busy_g = busy
        state = tk.DISABLED if busy else tk.NORMAL
        self._btn_gemini_gen.config(state=state)
        if busy:
            self._gemini_progress.start(10)
            self._gemini_status.config(text=msg)
        else:
            self._gemini_progress.stop()

    # ‚îÄ‚îÄ G√©n√©ration Ollama ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _run_ollama_refine(self):
        """Affine la bio Gemini."""
        base = self._txt_gemini.get("1.0", tk.END).strip()
        if len(base) < 100:
            messagebox.showinfo("Ollama", "G√©n√©rez d'abord une bio avec Gemini.")
            return
        self._run_ollama(base_bio=base)

    def _run_ollama_gen(self):
        """G√©n√®re ind√©pendamment."""
        self._run_ollama(base_bio=None)

    def _run_ollama(self, base_bio=None):
        if self._busy_o:
            return
        self._set_ollama_busy(True, "Ollama en cours‚Ä¶")

        def _worker():
            try:
                bio = self._bio_gen.generate_ollama_bio(
                    self._ctx, base_bio=base_bio
                )
                self.after(0, self._on_ollama_done, bio)
            except Exception as e:
                self.after(0, self._on_ollama_done, None, str(e))

        threading.Thread(target=_worker, daemon=True).start()

    def _on_ollama_done(self, bio, error=None):
        self._set_ollama_busy(False)
        if bio:
            self._set_text(self._txt_ollama, bio)
            self._update_char(self._txt_ollama, self._ollama_char_lbl)
            self._ollama_status.config(text=f"‚úì {len(bio)} chars")
        else:
            self._ollama_status.config(text="‚úó √âchec")
            messagebox.showwarning(
                "Ollama",
                "G√©n√©ration Ollama √©chou√©e.\n"
                "V√©rifiez que le serveur Ollama est lanc√© (ollama serve).\n"
                f"{error or ''}"
            )

    def _set_ollama_busy(self, busy, msg=""):
        self._busy_o = busy
        state = tk.DISABLED if busy else tk.NORMAL
        self._btn_ollama_refine.config(state=state)
        self._btn_ollama_gen.config(state=state)
        if busy:
            self._ollama_progress.start(10)
            self._ollama_status.config(text=msg)
        else:
            self._ollama_progress.stop()

    # ‚îÄ‚îÄ Actions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _set_text(self, widget, text):
        widget.delete("1.0", tk.END)
        widget.insert("1.0", text)

    def _update_char(self, widget, lbl):
        n = len(widget.get("1.0", tk.END).strip())
        color = P["success"] if 2500 <= n <= 4000 else P["muted"]
        lbl.config(text=f"{n} chars", fg=color)

    def _copy(self, widget):
        text = widget.get("1.0", tk.END).strip()
        if text:
            self.clipboard_clear()
            self.clipboard_append(text)

    def _use_bio(self, widget):
        """S√©lectionne cette bio et passe √† la validation."""
        bio = widget.get("1.0", tk.END).strip()
        if len(bio) < 100:
            messagebox.showwarning("Bio trop courte",
                                   "La biographie est vide ou trop courte.")
            return
        self.result = {"bio": bio}
        self.destroy()

    def _proceed_with_best(self):
        """Choisit la bio la plus longue disponible."""
        gemini = self._txt_gemini.get("1.0", tk.END).strip()
        ollama = self._txt_ollama.get("1.0", tk.END).strip()

        best = ""
        if len(gemini) > 200:
            best = gemini
        if len(ollama) > 200:
            # Prendre la plus longue (souvent Ollama affine mieux)
            if len(ollama) > len(best):
                best = ollama

        if not best:
            messagebox.showwarning("Aucune bio",
                                   "G√©n√©rez d'abord une biographie.")
            return
        self.result = {"bio": best}
        self.destroy()

    def _proceed_no_bio(self):
        self.result = {"bio": ""}
        self.destroy()

    def _go_back(self):
        """Retourne √† GUI 1 sans r√©sultat."""
        self.result = None
        self.destroy()

    def _cancel(self):
        self.result = None
        self.destroy()


# ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _fullscreen(win):
    if platform.system() == "Windows":
        win.state("zoomed")
    elif platform.system() == "Linux":
        try:
            win.attributes("-zoomed", True)
        except Exception:
            win.geometry(f"{win.winfo_screenwidth()}x{win.winfo_screenheight()}+0+0")
    else:
        win.attributes("-fullscreen", True)


def _studio_btn(parent, text, bg, cmd, state=tk.NORMAL):
    b = tk.Button(
        parent, text=text, command=cmd, state=state,
        font=FH3, bg=bg, fg="white",
        relief=tk.FLAT, padx=10, pady=5, cursor="hand2",
        activebackground=bg, activeforeground="white",
    )
    b.pack(side=tk.LEFT, padx=4)
    return b


def _action_btn(parent, text, bg, cmd, side=tk.RIGHT, padx=6):
    b = tk.Button(
        parent, text=text, command=cmd,
        font=FH3, bg=bg, fg="white",
        relief=tk.FLAT, padx=16, pady=8, cursor="hand2",
        activebackground=bg, activeforeground="white",
    )
    b.pack(side=side, padx=padx)


============================================================
[33/124] Legacy\gui\bio_wizard.py
------------------------------------------------------------
"""
BioWizard - Fen√™tre d√©di√©e √† la g√©n√©ration de biographies en 3 √©tapes.
1. G√©n√©ration Google Gemini
2. Affinage Ollama
3. Validation et injection
"""
import tkinter as tk
from tkinter import ttk, messagebox
import threading

from services.bio_generator import BioGenerator

class BioWizard(tk.Toplevel):
    def __init__(self, parent, db_data, stash_ctx, merged_data, scraped_results, checked_fields):
        super().__init__(parent)
        self.title("Assistant de G√©n√©ration de Biographie IA")
        self.geometry("1000x750")
        self.minsize(800, 600)

        self.transient(parent)
        self.grab_set()

        # Stockage
        self.db_data = db_data
        self.stash_ctx = stash_ctx
        self.merged_data = merged_data
        self.scraped_results = scraped_results
        self.checked_fields = checked_fields
        self.final_bio = None  # Le r√©sultat final sera stock√© ici

        self._build_ui()
        self.wait_window()

    def _build_ui(self):
        # Barre de progression
        self.progress_frame = ttk.Frame(self)
        self.progress_label = ttk.Label(self.progress_frame, text="Pr√™t", font=("Segoe UI", 9, "italic"))
        self.progress_label.pack(side=tk.LEFT, padx=10)
        self.progress_bar = ttk.Progressbar(self.progress_frame, mode="indeterminate", length=200)
        self.progress_bar.pack(side=tk.LEFT, padx=5)
        
        self.notebook = ttk.Notebook(self)
        self.notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)

        self.tab1_google = ttk.Frame(self.notebook, padding=10)
        self.tab2_ollama = ttk.Frame(self.notebook, padding=10)
        self.tab3_validate = ttk.Frame(self.notebook, padding=10)

        self.notebook.add(self.tab1_google, text="√âtape 1 : Google Gemini")
        self.notebook.add(self.tab2_ollama, text="√âtape 2 : Affinage Ollama", state="disabled")
        self.notebook.add(self.tab3_validate, text="√âtape 3 : Validation", state="disabled")

        self._create_google_tab()
        self._create_ollama_tab()
        self._create_validate_tab()

    def _create_google_tab(self):
        frame = self.tab1_google
        frame.grid_columnconfigure(0, weight=1)
        frame.grid_rowconfigure(1, weight=1)
        
        action_frame = ttk.Frame(frame)
        action_frame.grid(row=0, column=0, sticky=tk.NSEW, pady=(0, 10))
        
        self.btn_gen_gemini = ttk.Button(action_frame, text="üöÄ Lancer la g√©n√©ration Gemini", command=self._run_gemini_generation)
        self.btn_gen_gemini.pack(side=tk.LEFT)
        
        self.btn_copy_to_ollama = ttk.Button(action_frame, text="Continuer vers l'√©tape 2 ‚û°", state=tk.DISABLED, command=self._copy_to_ollama)
        self.btn_copy_to_ollama.pack(side=tk.LEFT, padx=10)

        self.txt_gemini_result = tk.Text(frame, wrap=tk.WORD, font=("Segoe UI", 10))
        self.txt_gemini_result.grid(row=1, column=0, sticky=tk.NSEW)
        self.txt_gemini_result.insert("1.0", "Cliquez sur 'Lancer la g√©n√©ration' pour cr√©er une biographie avec Google Gemini...")
        self.txt_gemini_result.config(state=tk.DISABLED)

    def _create_ollama_tab(self):
        frame = self.tab2_ollama
        frame.grid_columnconfigure(1, weight=1)
        frame.grid_rowconfigure(2, weight=1)
        
        action_frame = ttk.Frame(frame)
        action_frame.grid(row=0, column=0, columnspan=2, sticky=tk.NSEW, pady=(0, 10))
        self.btn_refine_ollama = ttk.Button(action_frame, text="‚öôÔ∏è Lancer l'affinage Ollama", command=self._run_ollama_refinement)
        self.btn_refine_ollama.pack(side=tk.LEFT)
        
        self.btn_copy_to_validate = ttk.Button(action_frame, text="Continuer vers la validation ‚û°", state=tk.DISABLED, command=self._copy_to_validation)
        self.btn_copy_to_validate.pack(side=tk.LEFT, padx=10)

        ttk.Label(frame, text="Biographie de base (Gemini)").grid(row=1, column=0, columnspan=2, sticky=tk.W)
        self.txt_ollama_input = tk.Text(frame, wrap=tk.WORD, height=8, font=("Segoe UI", 10), state=tk.DISABLED, relief=tk.SUNKEN)
        self.txt_ollama_input.grid(row=2, column=0, columnspan=2, sticky=tk.NSEW, pady=(0, 10))

        ttk.Label(frame, text="Biographie affin√©e (Ollama)").grid(row=3, column=0, columnspan=2, sticky=tk.W)
        self.txt_ollama_result = tk.Text(frame, wrap=tk.WORD, font=("Segoe UI", 10))
        self.txt_ollama_result.grid(row=4, column=0, columnspan=2, sticky=tk.NSEW)

    def _create_validate_tab(self):
        frame = self.tab3_validate
        frame.grid_columnconfigure(0, weight=1)
        frame.grid_rowconfigure(1, weight=1)
        
        action_frame = ttk.Frame(frame)
        action_frame.grid(row=0, column=0, sticky=tk.NSEW, pady=(0, 10))
        self.btn_inject = ttk.Button(action_frame, text="‚úÖ Valider et Utiliser cette Bio", command=self._inject_bio)
        self.btn_inject.pack(side=tk.LEFT)

        self.txt_final_bio = tk.Text(frame, wrap=tk.WORD, font=("Segoe UI", 10))
        self.txt_final_bio.grid(row=1, column=0, sticky=tk.NSEW)

    def _run_gemini_generation(self):
        self._show_progress("G√©n√©ration Gemini en cours...")
        self.btn_gen_gemini.config(state=tk.DISABLED)
        self.btn_copy_to_ollama.config(state=tk.DISABLED)

        def _do_generate():
            try:
                bio_gen = BioGenerator()
                ctx = bio_gen.build_context_from_v2(self.db_data, self.stash_ctx, self.scraped_results, self.merged_data, self.checked_fields)
                gemini_bio = bio_gen.generate_gemini_bio(ctx)

                def _update_ui():
                    self.txt_gemini_result.config(state=tk.NORMAL)
                    self.txt_gemini_result.delete("1.0", tk.END)
                    if gemini_bio:
                        self.txt_gemini_result.insert("1.0", gemini_bio)
                        self.btn_copy_to_ollama.config(state=tk.NORMAL)
                    else:
                        self.txt_gemini_result.insert("1.0", "La g√©n√©ration Gemini a √©chou√©. V√©rifiez la console pour les erreurs (cl√© API, etc.).")
                    self.txt_gemini_result.config(state=tk.DISABLED)
                    self.btn_gen_gemini.config(state=tk.NORMAL)

                self.after(0, _update_ui)
            except Exception as e:
                self.after(0, lambda: messagebox.showerror("Erreur Gemini", str(e)))
            finally:
                self.after(0, self._hide_progress)
        
        threading.Thread(target=_do_generate, daemon=True).start()

    def _copy_to_ollama(self):
        gemini_text = self.txt_gemini_result.get("1.0", tk.END)
        self.txt_ollama_input.config(state=tk.NORMAL)
        self.txt_ollama_input.delete("1.0", tk.END)
        self.txt_ollama_input.insert("1.0", gemini_text)
        self.txt_ollama_input.config(state=tk.DISABLED)
        self.txt_ollama_result.delete("1.0", tk.END) # Clear previous results
        self.notebook.tab(1, state="normal")
        self.notebook.select(self.tab2_ollama)

    def _run_ollama_refinement(self):
        gemini_bio = self.txt_ollama_input.get("1.0", tk.END).strip()
        if not gemini_bio or "√©chou√©" in gemini_bio:
            messagebox.showwarning("Bio de base manquante", "La biographie de base (Gemini) est n√©cessaire pour l'affinage.")
            return

        self._show_progress("Affinage Ollama en cours...")
        self.btn_refine_ollama.config(state=tk.DISABLED)
        self.btn_copy_to_validate.config(state=tk.DISABLED)

        def _do_refine():
            try:
                bio_gen = BioGenerator()
                ctx = bio_gen.build_context_from_v2(self.db_data, self.stash_ctx, self.scraped_results, self.merged_data, self.checked_fields)
                ollama_bio = bio_gen.generate_ollama_bio(ctx, gemini_bio)

                def _update_ui():
                    self.txt_ollama_result.delete("1.0", tk.END)
                    if ollama_bio:
                        self.txt_ollama_result.insert("1.0", ollama_bio)
                        self.btn_copy_to_validate.config(state=tk.NORMAL)
                    else:
                         self.txt_ollama_result.insert("1.0", "L'affinage Ollama a √©chou√©. V√©rifiez que le serveur Ollama est bien lanc√©.")
                
                self.after(0, _update_ui)
            except Exception as e:
                self.after(0, lambda: messagebox.showerror("Erreur Ollama", str(e)))
            finally:
                self.after(0, self._hide_progress)
                self.after(0, self.btn_refine_ollama.config, {'state': tk.NORMAL})
        
        threading.Thread(target=_do_refine, daemon=True).start()
    
    def _copy_to_validation(self):
        ollama_text = self.txt_ollama_result.get("1.0", tk.END)
        self.txt_final_bio.delete("1.0", tk.END)
        self.txt_final_bio.insert("1.0", ollama_text)
        self.notebook.tab(2, state="normal")
        self.notebook.select(self.tab3_validate)

    def _inject_bio(self):
        """Stocke la bio finale et ferme la fen√™tre."""
        self.final_bio = self.txt_final_bio.get("1.0", tk.END).strip()
        self.destroy()

    def _show_progress(self, message):
        self.progress_label.config(text=message)
        self.progress_frame.pack(fill=tk.X, padx=10, pady=5, before=self.notebook)
        self.progress_bar.start(10)

    def _hide_progress(self):
        self.progress_bar.stop()
        self.progress_frame.pack_forget()


============================================================
[34/124] Legacy\gui\data_review_window.py
------------------------------------------------------------
"""
DataReviewWindow ‚Äî GUI 1/3 du flux Bio IA
==========================================
Affiche en plein √©cran toutes les donn√©es scrap√©es (sauf bio) pour r√©vision :
Trivia ¬∑ Awards ¬∑ Tatouages ¬∑ Piercings ¬∑ Tags ¬∑ URLs

Layout :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  HEADER : performer + scraping stats + breadcrumb 1/3                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  R√âSUM√â PERFORMER (bandeau horizontal) : identit√© + apparence rapide  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  COL GAUCHE              ‚îÇ  COL DROITE                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ üìù Trivia ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îå‚îÄ‚îÄ üèÜ Awards ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  texte √©ditable     ‚îÇ ‚îÇ  ‚îÇ  liste cochable                       ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ üé® Tatouages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îå‚îÄ‚îÄ üíâ Piercings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  liste cochable     ‚îÇ ‚îÇ  ‚îÇ  liste cochable                       ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îå‚îÄ‚îÄ üè∑Ô∏è Tags ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ üîó URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ  grille de badges cochables           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  liste cochable     ‚îÇ ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  [Annuler]   [Tout cocher]   [S√©lect. vides]       [‚Üí G√©n√©rer Bio]   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""
import tkinter as tk
from tkinter import ttk, messagebox
import platform

# ‚îÄ‚îÄ Palette ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
P = {
    "bg":        "#13131f",
    "surface":   "#1e1e30",
    "card":      "#22223a",
    "card_hdr":  "#2c2c4a",
    "border":    "#3a3a58",
    "accent":    "#7c6af7",
    "success":   "#4caf7d",
    "danger":    "#c05050",
    "text":      "#e8e8f5",
    "muted":     "#8888aa",
    "dim":       "#55557a",
    "trivia_bg": "#1a1a2e",
    "award_bg":  "#1e2a1e",
    "tattoo_bg": "#2a1e1e",
    "tag_sel":   "#2a3a4a",
    "tag_unsel": "#1e1e2a",
    "url_bg":    "#1e1e28",
}

FH1  = ("Segoe UI", 14, "bold")
FH2  = ("Segoe UI", 11, "bold")
FH3  = ("Segoe UI", 9,  "bold")
FB   = ("Segoe UI", 10)
FSM  = ("Segoe UI", 8)
FSMB = ("Segoe UI", 8, "bold")
FMON = ("Consolas", 9)


def _lbl(parent, text, font=FB, fg=None, bg=None, **kw):
    return tk.Label(parent, text=text, font=font,
                    fg=fg or P["text"], bg=bg or P["surface"], **kw)


def _sep(parent, bg=None):
    return tk.Frame(parent, bg=bg or P["border"], height=1)


class _Card(tk.Frame):
    """Carte √† en-t√™te color√© avec zone de contenu."""
    def __init__(self, parent, title, icon="", accent=None, **kw):
        super().__init__(parent, bg=P["card"], bd=0, **kw)
        self._accent = accent or P["accent"]

        # Bande couleur lat√©rale
        tk.Frame(self, bg=self._accent, width=3).pack(side=tk.LEFT, fill=tk.Y)

        inner = tk.Frame(self, bg=P["card"])
        inner.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # En-t√™te
        hdr = tk.Frame(inner, bg=P["card_hdr"], pady=5)
        hdr.pack(fill=tk.X)
        _lbl(hdr, f"  {icon}  {title}" if icon else f"  {title}",
             font=FH3, fg=P["text"], bg=P["card_hdr"],
             anchor="w").pack(side=tk.LEFT, padx=4)
        self._count_lbl = _lbl(hdr, "", font=FSM, fg=P["muted"],
                                bg=P["card_hdr"])
        self._count_lbl.pack(side=tk.RIGHT, padx=8)

        _sep(inner, P["border"]).pack(fill=tk.X)

        self._body = tk.Frame(inner, bg=P["card"], padx=6, pady=4)
        self._body.pack(fill=tk.BOTH, expand=True)

    def body(self):
        return self._body

    def set_count(self, n, label=""):
        self._count_lbl.config(text=f"{n} {label}" if n else "")


class DataReviewWindow(tk.Toplevel):
    """
    Fen√™tre 1/3 : r√©vision des donn√©es scrap√©es.
    Retourne `result` dict ou None si annul√©.
    """
    def __init__(self, parent, db_data, stash_ctx, merged_data,
                 scraped_results, checked_fields):
        super().__init__(parent)
        self.title("üìã R√©vision des donn√©es ‚Äî √âtape 1/3")
        self.configure(bg=P["bg"])

        self.db_data         = db_data
        self.stash_ctx       = stash_ctx
        self.merged_data     = merged_data
        self.scraped_results = scraped_results
        self.checked_fields  = checked_fields

        self.result = None          # dict s√©lections ou None

        # Widgets de s√©lection
        self._trivia_text   = None
        self._award_vars    = []    # [(BooleanVar, str)]
        self._tattoo_vars   = []    # [(BooleanVar, dict)]
        self._piercing_vars = []    # [(BooleanVar, dict)]
        self._tag_vars      = []    # [(BooleanVar, str)]
        self._url_vars      = []    # [(BooleanVar, str, str)]  (key, url)

        _fullscreen(self)
        self.transient(parent)
        self.grab_set()

        self._build_ui()
        self.bind("<Escape>", lambda _: self._cancel())
        self.wait_window()

    # ‚îÄ‚îÄ UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_ui(self):
        self._build_header()
        self._build_performer_banner()
        self._build_body()
        self._build_footer()

    def _build_header(self):
        hdr = tk.Frame(self, bg=P["accent"], pady=0)
        hdr.pack(fill=tk.X)
        tk.Frame(hdr, bg="#9a88ff", height=3).pack(fill=tk.X)

        row = tk.Frame(hdr, bg=P["accent"], pady=7)
        row.pack(fill=tk.X, padx=12)

        _lbl(row, "üìã", font=("Segoe UI", 18), fg="white",
             bg=P["accent"]).pack(side=tk.LEFT, padx=(0, 8))

        name = self.db_data.get("name", "Performer inconnu")
        _lbl(row, f"R√©vision des donn√©es ‚Äî {name}",
             font=FH1, fg="white", bg=P["accent"]).pack(side=tk.LEFT)

        # Breadcrumb
        bc = tk.Frame(row, bg=P["accent"])
        bc.pack(side=tk.RIGHT, padx=12)
        for i, (num, lbl, active) in enumerate([
            ("1", "Donn√©es", True),
            ("2", "Bio IA",  False),
            ("3", "Valider", False),
        ]):
            fg   = "white"  if active else "#9980cc"
            bgc  = "#5a48c8" if active else P["accent"]
            tk.Label(bc, text=f" {num} ", font=FSMB, fg=fg,
                     bg=bgc, padx=6, pady=3).pack(side=tk.LEFT, padx=1)
            tk.Label(bc, text=lbl, font=FSM, fg=fg,
                     bg=P["accent"]).pack(side=tk.LEFT, padx=(0, 8))

    def _build_performer_banner(self):
        """Bandeau horizontal r√©sumant le performer."""
        banner = tk.Frame(self, bg=P["surface"], pady=6)
        banner.pack(fill=tk.X, padx=0)
        _sep(banner).pack(fill=tk.X)

        inner = tk.Frame(banner, bg=P["surface"])
        inner.pack(fill=tk.X, padx=14, pady=4)

        db = self.db_data
        ctx = self.stash_ctx or {}

        def pill(parent, label, value, accent=None):
            if not value:
                return
            f = tk.Frame(parent, bg=P["card_hdr"])
            f.pack(side=tk.LEFT, padx=4, pady=2)
            tk.Label(f, text=f" {label} ", font=FSMB,
                     fg=P["muted"], bg=P["card_hdr"]).pack(side=tk.LEFT)
            tk.Label(f, text=f" {value} ", font=FSM,
                     fg=accent or P["text"], bg=P["card"]).pack(side=tk.LEFT)

        pill(inner, "üéÇ", db.get("birthdate"))
        pill(inner, "üåç", db.get("country"))
        pill(inner, "üë§", db.get("ethnicity"))
        pill(inner, "üíá", db.get("hair_color"))
        pill(inner, "üëÅ", db.get("eye_color"))
        pill(inner, "üìê", db.get("measurements"))
        pill(inner, "üé¨", str(ctx.get("scene_count", "")) + " sc√®nes"
             if ctx.get("scene_count") else "", P["accent"])
        if ctx.get("studios"):
            pill(inner, "üè¢", ", ".join(ctx["studios"][:3]))

        _sep(banner).pack(fill=tk.X)

    def _build_body(self):
        body = tk.Frame(self, bg=P["bg"])
        body.pack(fill=tk.BOTH, expand=True, padx=10, pady=6)
        body.grid_columnconfigure(0, weight=1)
        body.grid_columnconfigure(1, weight=1)
        body.grid_rowconfigure(0, weight=2)
        body.grid_rowconfigure(1, weight=1)
        body.grid_rowconfigure(2, weight=1)

        # ‚îÄ‚îÄ Colonne gauche ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # Ligne 0 : Trivia (haut)
        self._build_trivia(body, row=0, col=0)
        # Ligne 1 : Tatouages
        self._build_body_art(body, "tattoos",   "üé®", "Tatouages",
                             "#c05050", row=1, col=0)
        # Ligne 2 : URLs
        self._build_urls(body, row=2, col=0)

        # ‚îÄ‚îÄ Colonne droite ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # Ligne 0 : Awards
        self._build_awards(body, row=0, col=1)
        # Ligne 1 : Piercings
        self._build_body_art(body, "piercings", "üíâ", "Piercings",
                             "#5080c0", row=1, col=1)
        # Ligne 2 : Tags
        self._build_tags(body, row=2, col=1)

    # ‚îÄ‚îÄ Sections ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_trivia(self, parent, row, col):
        card = _Card(parent, "Trivia / Informations", "üìù",
                     accent="#8060d0")
        card.grid(row=row, column=col, sticky="nsew", padx=5, pady=5)

        by_source = self.merged_data.get("trivia", {}).get("by_source", {})
        PRIORITY  = ["freeones", "thenude", "babepedia"]

        if not by_source:
            _lbl(card.body(), "Aucun trivia trouv√©.",
                 fg=P["muted"], bg=P["card"]).pack(anchor="w")
            return

        # S√©lecteur de source
        top = tk.Frame(card.body(), bg=P["card"])
        top.pack(fill=tk.X, pady=(0, 4))

        _lbl(top, "Source :", font=FH3, fg=P["muted"],
             bg=P["card"]).pack(side=tk.LEFT, padx=4)

        best = next((s for s in PRIORITY if s in by_source), None) or \
               list(by_source.keys())[0]
        self._trivia_src_var = tk.StringVar(value=best)

        for src in by_source:
            n = len(by_source[src])
            rb = tk.Radiobutton(
                top, text=f"{src.upper()} ({n}c)",
                variable=self._trivia_src_var, value=src,
                font=FSM, fg=P["text"], bg=P["card"],
                selectcolor=P["card_hdr"],
                activebackground=P["card"], relief=tk.FLAT,
                command=self._update_trivia_preview,
            )
            rb.pack(side=tk.LEFT, padx=4)

        # Zone texte √©ditable
        txt_f = tk.Frame(card.body(), bg=P["card"])
        txt_f.pack(fill=tk.BOTH, expand=True)

        self._trivia_text = tk.Text(
            txt_f, wrap=tk.WORD, font=FB,
            bg=P["trivia_bg"], fg=P["text"],
            insertbackground=P["text"],
            relief=tk.FLAT, padx=6, pady=6,
        )
        sb = ttk.Scrollbar(txt_f, command=self._trivia_text.yview)
        self._trivia_text.configure(yscrollcommand=sb.set)
        sb.pack(side=tk.RIGHT, fill=tk.Y)
        self._trivia_text.pack(fill=tk.BOTH, expand=True)

        self._trivia_sources = by_source
        self._update_trivia_preview()

        card.set_count(len(by_source), "sources")

    def _update_trivia_preview(self):
        if self._trivia_text is None:
            return
        src  = self._trivia_src_var.get()
        text = self._trivia_sources.get(src, "")
        self._trivia_text.delete("1.0", tk.END)
        self._trivia_text.insert("1.0", text)

    def _build_awards(self, parent, row, col):
        data   = self.merged_data.get("awards", {})
        merged = data.get("merged", [])
        srcs   = data.get("sources", {})

        card = _Card(parent, "Awards & Nominations", "üèÜ",
                     accent="#c0a020")
        card.grid(row=row, column=col, sticky="nsew", padx=5, pady=5)
        card.set_count(len(merged), "awards")

        if not merged:
            _lbl(card.body(), "Aucun award trouv√©.",
                 fg=P["muted"], bg=P["card"]).pack(anchor="w")
            return

        # Infos sources
        src_txt = "  ".join(
            f"[{s.upper()}: {len(a)}]" for s, a in srcs.items() if a
        )
        _lbl(card.body(), src_txt, font=FSM, fg=P["muted"],
             bg=P["card"]).pack(anchor="w", pady=(0, 4))

        # Zone scrollable
        sf = _scrolled_frame(card.body(), bg=P["award_bg"])

        self._award_vars = []
        for award in merged:
            var = tk.BooleanVar(value=True)
            self._award_vars.append((var, award))
            _checkbutton(sf, award, var, bg=P["award_bg"])

        # Boutons tout/rien
        _check_buttons(card.body(), self._award_vars)

    def _build_body_art(self, parent, field, icon, title, accent,
                        row, col):
        data   = self.merged_data.get(field, {})
        merged = data.get("merged", [])
        db_val = data.get("db_value", "")

        card = _Card(parent, title, icon, accent=accent)
        card.grid(row=row, column=col, sticky="nsew", padx=5, pady=5)
        card.set_count(len(merged))

        if db_val:
            _lbl(card.body(),
                 f"Stash actuel : {str(db_val)[:80]}",
                 font=FSM, fg=P["muted"], bg=P["card"]).pack(anchor="w")

        if not merged:
            _lbl(card.body(), f"Aucun {title.lower()} trouv√©.",
                 fg=P["muted"], bg=P["card"]).pack(anchor="w")
            var_attr = f"_{field}_vars"
            setattr(self, var_attr, [])
            return

        sf = _scrolled_frame(card.body(), bg=P["tattoo_bg"])

        vars_list = []
        for item in merged:
            pos  = item.get("position", "?")
            desc = item.get("description", "")
            lbl  = f"{pos}" + (f"  ({desc})" if desc else "")
            var  = tk.BooleanVar(value=True)
            vars_list.append((var, item))
            _checkbutton(sf, lbl, var, bg=P["tattoo_bg"])

        var_attr = f"_{field}_vars"
        setattr(self, var_attr, vars_list)
        _check_buttons(card.body(), vars_list)

    def _build_tags(self, parent, row, col):
        data       = self.merged_data.get("tags", {})
        merged     = data.get("merged", [])
        stash_tags = {t.lower() for t in (self.db_data.get("tags") or [])}

        card = _Card(parent, "Tags", "üè∑Ô∏è", accent="#3a7a9a")
        card.grid(row=row, column=col, sticky="nsew", padx=5, pady=5)
        card.set_count(len(merged))

        if not merged:
            _lbl(card.body(), "Aucun tag trouv√©.",
                 fg=P["muted"], bg=P["card"]).pack(anchor="w")
            return

        # Grille de badges cliquables
        grid = tk.Frame(card.body(), bg=P["card"])
        grid.pack(fill=tk.BOTH, expand=True)

        self._tag_vars = []
        for tag in merged:
            already = tag.lower() in stash_tags
            var = tk.BooleanVar(value=True)
            self._tag_vars.append((var, tag))

            btn_frame = tk.Frame(grid, bg=P["card"])
            btn_frame.pack(side=tk.LEFT, padx=2, pady=2)

            def _toggle(v=var, bf=btn_frame, t=tag, al=already):
                v.set(not v.get())
                _refresh_tag_btn(bf, t, v.get(), al)

            _refresh_tag_btn(btn_frame, tag, True, already)
            btn_frame.bind("<Button-1>", lambda e, fn=_toggle: fn())
            for child in btn_frame.winfo_children():
                child.bind("<Button-1>", lambda e, fn=_toggle: fn())

        # Boutons
        btn_row = tk.Frame(card.body(), bg=P["card"])
        btn_row.pack(fill=tk.X, pady=4)
        _small_btn(btn_row, "‚úì Tous",
                   lambda: [v.set(True) or _refresh_all_tags(self._tag_vars)
                             for v, _ in self._tag_vars])
        _small_btn(btn_row, "‚úó Aucun",
                   lambda: [v.set(False) or _refresh_all_tags(self._tag_vars)
                             for v, _ in self._tag_vars])
        _small_btn(btn_row, "Nouveaux",
                   lambda: self._select_new_tags(stash_tags))

    def _select_new_tags(self, stash_tags):
        for var, tag in self._tag_vars:
            var.set(tag.lower() not in stash_tags)

    def _build_urls(self, parent, row, col):
        data       = self.merged_data.get("urls", {})
        merged     = data.get("merged", {})
        stash_urls = set(self.db_data.get("urls") or [])

        card = _Card(parent, "URLs & R√©seaux sociaux", "üîó",
                     accent="#3a6a9a")
        card.grid(row=row, column=col, sticky="nsew", padx=5, pady=5)
        card.set_count(len(merged))

        if not merged:
            _lbl(card.body(), "Aucune URL trouv√©e.",
                 fg=P["muted"], bg=P["card"]).pack(anchor="w")
            return

        sf = _scrolled_frame(card.body(), bg=P["url_bg"])

        ICONS = {
            "iafd": "üé¨", "freeones": "üåê", "babepedia": "üìö",
            "thenude": "üì∑", "twitter": "üê¶", "instagram": "üì∏",
            "onlyfans": "üíé", "facebook": "üëç", "tiktok": "üéµ",
        }

        self._url_vars = []
        for key in sorted(merged):
            url  = merged[key]
            icon = ICONS.get(key.lower(), "üîó")
            var  = tk.BooleanVar(value=True)
            self._url_vars.append((var, key, url))

            row_f = tk.Frame(sf, bg=P["url_bg"])
            row_f.pack(fill=tk.X, pady=1)

            cb = tk.Checkbutton(row_f, variable=var, bg=P["url_bg"],
                                fg=P["text"], selectcolor=P["card"],
                                activebackground=P["url_bg"], relief=tk.FLAT)
            cb.pack(side=tk.LEFT)

            tk.Label(row_f, text=f"{icon} {key:<14}",
                     font=FSMB, fg=P["text"], bg=P["url_bg"]).pack(side=tk.LEFT)
            tk.Label(row_f, text=url, font=FSM,
                     fg=P["muted"], bg=P["url_bg"]).pack(side=tk.LEFT)

    # ‚îÄ‚îÄ Footer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_footer(self):
        _sep(self, P["border"]).pack(fill=tk.X)
        bar = tk.Frame(self, bg=P["surface"], pady=10)
        bar.pack(fill=tk.X, padx=12)

        _action_btn(bar, "‚úñ Annuler", P["danger"],
                    self._cancel, side=tk.LEFT)

        tk.Frame(bar, bg=P["surface"], width=12).pack(side=tk.LEFT)

        _action_btn(bar, "‚úì Tout cocher", P["dim"],
                    self._check_all, side=tk.LEFT)
        _action_btn(bar, "‚≠ï Tout d√©cocher", P["dim"],
                    self._uncheck_all, side=tk.LEFT, padx=4)

        _action_btn(bar, "‚Üí G√©n√©rer la Bio IA", P["success"],
                    self._proceed, side=tk.RIGHT)

    # ‚îÄ‚îÄ Actions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _check_all(self):
        for v, _ in self._award_vars:
            v.set(True)
        for v, _ in self._tattoo_vars:
            v.set(True)
        for v, _ in self._piercing_vars:
            v.set(True)
        for v, _ in self._tag_vars:
            v.set(True)
        for v, *_ in self._url_vars:
            v.set(True)

    def _uncheck_all(self):
        for v, _ in self._award_vars:
            v.set(False)
        for v, _ in self._tattoo_vars:
            v.set(False)
        for v, _ in self._piercing_vars:
            v.set(False)
        for v, _ in self._tag_vars:
            v.set(False)
        for v, *_ in self._url_vars:
            v.set(False)

    def _proceed(self):
        """Collecte les s√©lections et ferme."""
        self.result = {
            "trivia":   self._trivia_text.get("1.0", tk.END).strip()
                        if self._trivia_text else "",
            "awards":   [a  for v, a in self._award_vars    if v.get()],
            "tattoos":  [i  for v, i in self._tattoo_vars   if v.get()],
            "piercings":[i  for v, i in self._piercing_vars if v.get()],
            "tags":     [t  for v, t in self._tag_vars      if v.get()],
            "urls":     {k: u for v, k, u in self._url_vars if v.get()},
        }
        self.destroy()

    def _cancel(self):
        self.result = None
        self.destroy()


# ‚îÄ‚îÄ Helpers UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _fullscreen(win):
    if platform.system() == "Windows":
        win.state("zoomed")
    elif platform.system() == "Linux":
        try:
            win.attributes("-zoomed", True)
        except Exception:
            win.geometry(f"{win.winfo_screenwidth()}x{win.winfo_screenheight()}+0+0")
    else:
        win.attributes("-fullscreen", True)


def _scrolled_frame(parent, bg=None):
    """Frame scrollable verticalement."""
    bg = bg or P["card"]
    wrapper = tk.Frame(parent, bg=bg)
    wrapper.pack(fill=tk.BOTH, expand=True)

    canvas = tk.Canvas(wrapper, bg=bg, highlightthickness=0)
    sb     = ttk.Scrollbar(wrapper, orient=tk.VERTICAL, command=canvas.yview)
    inner  = tk.Frame(canvas, bg=bg)

    inner.bind("<Configure>",
               lambda e: canvas.configure(scrollregion=canvas.bbox("all")))
    win_id = canvas.create_window((0, 0), window=inner, anchor="nw")
    canvas.configure(yscrollcommand=sb.set)
    canvas.bind("<Configure>",
                lambda e: canvas.itemconfig(win_id, width=e.width))
    canvas.bind_all("<MouseWheel>",
                    lambda e: canvas.yview_scroll(int(-1*(e.delta/120)), "units"))

    sb.pack(side=tk.RIGHT, fill=tk.Y)
    canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
    return inner


def _checkbutton(parent, text, var, bg=None):
    bg = bg or P["card"]
    cb = tk.Checkbutton(
        parent, text=text, variable=var,
        font=FSM, fg=P["text"], bg=bg,
        selectcolor=P["card_hdr"],
        activebackground=bg, relief=tk.FLAT,
        wraplength=360, justify="left", anchor="w",
    )
    cb.pack(anchor="w", padx=4, pady=1)


def _check_buttons(parent, vars_list, extra_btns=None):
    row = tk.Frame(parent, bg=P["card"])
    row.pack(fill=tk.X, pady=4)
    _small_btn(row, "‚úì Tous",
               lambda: [v.set(True) for v, _ in vars_list])
    _small_btn(row, "‚úó Aucun",
               lambda: [v.set(False) for v, _ in vars_list])
    if extra_btns:
        for txt, cmd in extra_btns:
            _small_btn(row, txt, cmd)


def _small_btn(parent, text, cmd, **kw):
    b = tk.Button(
        parent, text=text, command=cmd,
        font=FSM, bg=P["card_hdr"], fg=P["text"],
        relief=tk.FLAT, padx=8, pady=3, cursor="hand2",
        activebackground=P["border"], activeforeground=P["text"],
    )
    b.pack(side=tk.LEFT, padx=3, **kw)


def _action_btn(parent, text, bg, cmd, side=tk.RIGHT, padx=6):
    b = tk.Button(
        parent, text=text, command=cmd,
        font=FH3, bg=bg, fg="white",
        relief=tk.FLAT, padx=16, pady=8, cursor="hand2",
        activebackground=bg, activeforeground="white",
    )
    b.pack(side=side, padx=padx)


def _refresh_tag_btn(frame, tag, selected, already):
    for w in frame.winfo_children():
        w.destroy()
    bg = P["tag_sel"] if selected else P["tag_unsel"]
    fg = P["text"]    if selected else P["muted"]
    pfx = "‚úì " if already else ("+ " if selected else "")
    lbl = tk.Label(frame, text=f"{pfx}{tag}", font=FSMB,
                   fg=fg, bg=bg, padx=8, pady=4, cursor="hand2")
    lbl.pack()


def _refresh_all_tags(tag_vars):
    pass  # Simplification : badges se rafra√Æchissent au prochain clic


============================================================
[35/124] Legacy\gui\group_frame.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
from gui.group_phase1 import GroupPhase1Frame
from gui.group_phase2 import GroupPhase2Frame

class GroupFrame(ttk.Frame):
    def __init__(self, parent, stash_id):
        super().__init__(parent)
        self.stash_id = stash_id
        self.current_frame = None
        
        # For now, just a label
        label = ttk.Label(self, text=f"Group Frame for ID: {self.stash_id}")
        label.pack(pady=20, padx=20)

        self.goto_phase1()

    def goto_phase1(self):
        if self.current_frame:
            self.current_frame.destroy()
        
        self.current_frame = GroupPhase1Frame(self, self, self.stash_id)
        self.current_frame.pack(fill=tk.BOTH, expand=True)

    def goto_phase2(self, group_data, scenes_data):
        if self.current_frame:
            self.current_frame.destroy()
            
        self.current_frame = GroupPhase2Frame(self, self, self.stash_id, group_data, scenes_data)
        self.current_frame.pack(fill=tk.BOTH, expand=True)

    def return_to_menu(self):
        # Fermer la fen√™tre actuelle
        self.master.destroy()
        # Relancer le launcher
        try:
            from gui.launcher import start_launcher
            start_launcher()
        except Exception as e:
            print(f"Erreur lors du retour au menu: {e}")


============================================================
[36/124] Legacy\gui\group_phase1.py
------------------------------------------------------------
"""
Placeholder for Group Phase 1 Frame.
The content for this file is in PLAN_GROUPS_V2.md, which was not provided.
"""
import tkinter as tk
from tkinter import ttk, messagebox
import threading
import yaml

from services.db import GroupDB
from services.group_phase1_scraper import GroupPhase1ScraperService
from services.group_phase1_merger import GroupPhase1Merger
from gui.phase1_conflict_dialog import Phase1ConflictDialog
from services.phase2_scraper import Phase2ScraperService


class GroupPhase1Frame(ttk.Frame):
    def __init__(self, parent, controller, group_id):
        super().__init__(parent)
        self.controller = controller
        self.group_id = group_id

        self._group_data = None
        self._scenes_data = [] # Scenes associ√©es au group

        self.field_checkboxes = {}
        self.fields = {}

        self.create_ui()
        self._load_data()

    def create_ui(self):
        # Header
        header_frame = ttk.Frame(self)
        header_frame.pack(fill=tk.X, padx=10, pady=5)
        ttk.Label(header_frame, text=f"Group ID: {self.group_id}",
                  font=("Segoe UI", 14, "bold")).pack(side=tk.LEFT)
        ttk.Button(header_frame, text="Retour Launcher",
                   command=self.controller.return_to_menu).pack(side=tk.RIGHT)

        # ScrolledFrame pour les champs du Group
        self.main_canvas = tk.Canvas(self, highlightthickness=0)
        self.main_scrollbar = ttk.Scrollbar(self, orient=tk.VERTICAL, command=self.main_canvas.yview)
        self.main_scrollable_frame = ttk.Frame(self.main_canvas)

        self.main_scrollable_frame.bind(
            "<Configure>",
            lambda e: self.main_canvas.configure(
                scrollregion=self.main_canvas.bbox("all")
            )
        )
        self.main_canvas.create_window((0, 0), window=self.main_scrollable_frame, anchor="nw")
        self.main_canvas.configure(yscrollcommand=self.main_scrollbar.set)

        self.main_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.main_canvas.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Section Group Details
        group_details_frame = ttk.LabelFrame(self.main_scrollable_frame, text="Group Details (Phase 1)", padding=10)
        group_details_frame.pack(fill=tk.X, padx=10, pady=5)

        # TODO: Charger les champs depuis settings.yaml
        self.fields_list = [
            "Title", "Aliases", "Date", "Studio", "Director",
            "Duration", "Description", "Tags", "URLs"
        ]

        for i, field in enumerate(self.fields_list):
            row = i
            var = tk.BooleanVar(value=True)
            self.field_checkboxes[field] = var
            checkbox = ttk.Checkbutton(group_details_frame, variable=var, text="")
            checkbox.grid(row=row, column=0, sticky=tk.W, padx=(5, 0), pady=2)

            ttk.Label(group_details_frame, text=f"{field}:").grid(row=row, column=1, sticky=tk.W, padx=5, pady=2)
            entry = tk.Text(group_details_frame, height=2, width=60, font=("Segoe UI", 9))
            entry.grid(row=row, column=2, sticky=tk.EW, padx=5, pady=2)
            self.fields[field] = entry

        group_details_frame.grid_columnconfigure(2, weight=1)

        # Boutons d'action
        action_frame = ttk.Frame(self.main_scrollable_frame)
        action_frame.pack(fill=tk.X, padx=10, pady=5)
        ttk.Button(action_frame, text="üîé Analyser & Phase 2", command=self._run_phase1).pack(side=tk.LEFT, padx=5)

        # Section Sc√®nes Associ√©es
        self.scenes_frame = ttk.LabelFrame(self.main_scrollable_frame, text="Sc√®nes Associ√©es au Group", padding=10)
        self.scenes_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        ttk.Label(self.scenes_frame, text="Chargement des sc√®nes...", font=("Segoe UI", 9, "italic")).pack()

    def _load_data(self):
        try:
            gid = int(self.group_id)
        except ValueError:
            gid = self.group_id

        db = GroupDB()
        self._group_data = db.get_group_by_id(gid)
        self._scenes_data = db.get_group_scenes(gid)
        db.close()

        print(f"DEBUG: Loaded group data for ID {gid}: {bool(self._group_data)}")
        print(f"DEBUG: Loaded {len(self._scenes_data)} scenes")

        if self._group_data:
            for field_name, entry_widget in self.fields.items():
db_key="***MASKED***"
                value = self._group_data.get(db_key)
                if field_name == "Studio" and self._group_data.get("studio_name"):
                    value = self._group_data["studio_name"]
                elif field_name == "Tags" and value:
                    value = ", ".join(value)
                elif field_name == "URLs" and value:
                    value = "\n".join(value)

                if value:
                    entry_widget.config(state=tk.NORMAL)
                    entry_widget.delete("1.0", tk.END)
                    entry_widget.insert("1.0", str(value))
                    entry_widget.config(state=tk.DISABLED)

        self._display_scenes()

    def _display_scenes(self):
        for widget in self.scenes_frame.winfo_children():
            widget.destroy()

        if not self._scenes_data:
            ttk.Label(self.scenes_frame, text="Aucune sc√®ne associ√©e.", font=("Segoe UI", 9, "italic")).pack()
            return

        # Treeview pour les sc√®nes
        columns = ("index", "title", "urls")
        tree = ttk.Treeview(self.scenes_frame, columns=columns, show="headings", height=8)
        tree.heading("index", text="#")
        tree.heading("title", text="Titre Stash")
        tree.heading("urls", text="URLs Existantes")
        
        tree.column("index", width=30, anchor=tk.CENTER)
        tree.column("title", width=300, anchor=tk.W)
        tree.column("urls", width=400, anchor=tk.W)

        for s in self._scenes_data:
            urls_str = ", ".join(s.get("existing_urls", []))
            tree.insert("", tk.END, values=(
                s.get("scene_index", "?"),
                s.get("scene_title", "Sans titre"),
                urls_str
            ))
        
        tree.pack(fill=tk.BOTH, expand=True)

    def _run_phase1(self):
        """Lance le scraping Phase 1 Group."""
        checked = [f for f, var in self.field_checkboxes.items() if var.get()]
        if not checked:
            messagebox.showwarning("Attention", "Aucun champ coch√©.")
            return

        # Afficher progression
        progress_popup = tk.Toplevel(self)
        progress_popup.title("Scraping Group Phase 1...")
        progress_popup.geometry("300x100")
        prog_label = ttk.Label(progress_popup, text="Initialisation...")
        prog_label.pack(pady=20)

        def _do():
            try:
                scraper = GroupPhase1ScraperService()
                title = self.fields["Title"].get("1.0", tk.END).strip()
                year = self.fields["Date"].get("1.0", tk.END).strip()[:4] # Ann√©e
                known_urls = self.fields["URLs"].get("1.0", tk.END).strip().split("\n")
                known_urls = [u.strip() for u in known_urls if u.strip()]

                def update_prog(src, st):
                    self.after(0, lambda: prog_label.config(text=f"[{src}] {st}"))

                scraped = scraper.scrape(title, year, known_urls, progress_callback=update_prog)
                
                # Check for Data18
                has_data18 = any(r.get("_source") == "data18" for r in scraped)
                
                if not has_data18:
                    self.after(0, lambda: self._ask_data18_and_continue(scraped, checked, scraper, progress_popup))
                else:
                    self.after(0, lambda: self._finish_phase1(scraped, checked, progress_popup))

            except Exception as e:
                self.after(0, lambda: [progress_popup.destroy(), messagebox.showerror("Erreur", str(e))])

        threading.Thread(target=_do, daemon=True).start()

    def _ask_data18_and_continue(self, scraped, checked, scraper, progress_popup):
        from tkinter import simpledialog
        # Cacher la popup de progression temporairement
        progress_popup.withdraw()
        
        url = simpledialog.askstring(
            "Data18 Manquant", 
            "Data18 n'a pas √©t√© trouv√© automatiquement.\n\n"
            "Pour avoir les meilleurs r√©sultats (Tags, Sc√®nes...), collez l'URL Data18 ici :\n"
            "(Sinon, laissez vide et OK pour continuer)",
            parent=self
        )
        
        progress_popup.deiconify()
        
        if url and "data18.com" in url:
            # Relancer un petit thread pour scraper cette URL
            def _scrape_extra():
                try:
                    from services.extractors.dvd.data18_dvd import Data18DVDExtractor
                    e = Data18DVDExtractor()
                    res = e.extract_from_url(url.strip())
                    if res:
                        scraped.append(res)
                except Exception as e:
                    print(f"Error scraping extra URL: {e}")
                
                self.after(0, lambda: self._finish_phase1(scraped, checked, progress_popup))
            
            threading.Thread(target=_scrape_extra, daemon=True).start()
        else:
            self._finish_phase1(scraped, checked, progress_popup)

    def _finish_phase1(self, scraped, checked, progress_popup):
        progress_popup.destroy()
        merger = GroupPhase1Merger()
        merged = merger.merge(self._group_data, scraped, checked)
        self._show_conflict_dialog(merged, checked)

    def _show_conflict_dialog(self, merged_data: dict, checked_fields: list[str]):
        from services.group_phase1_merger import GROUP_FIELDS
        group_title = self.fields["Title"].get("1.0", tk.END).strip()
        dialog = Phase1ConflictDialog(self, group_title, merged_data, GROUP_FIELDS)
        if dialog.result:
            self._inject_phase1(dialog.result)

    def _inject_phase1(self, result: dict):
        try:
            # Pour Group Phase 1, on injecte directement via GroupDB (√† impl√©menter ou simuler)
            # On r√©utilise les champs mapp√©s
            db_updates = {}
            from services.group_phase1_merger import GROUP_FIELDS
            for field, value in result.items():
db_key="***MASKED***"
                if db_key:
                    db_updates[db_key] = value

            # Simulation d'injection (ou ajout de la m√©thode dans GroupDB)
            print(f"DEBUG: Injecting group updates: {db_updates}")
            
            # TODO: Impl√©menter GroupDB.update_group(self.group_id, db_updates)
            
            messagebox.showinfo("‚úÖ Phase 1 Termin√©e", "Les donn√©es du Group ont √©t√© mises √† jour.")
            
            # Passer √† la Phase 2
            self.controller.goto_phase2(self._group_data, self._scenes_data)

        except Exception as e:
            messagebox.showerror("Erreur Injection", str(e))




============================================================
[37/124] Legacy\gui\group_phase2.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, messagebox
import threading

from services.db import GroupDB
from services.group_phase2_scraper import GroupPhase2ScraperService
from services.group_phase2_merger import GroupPhase2Merger

STATUS_ICONS = {
    "new":         "üü¢",
    "partial":     "üü†",
    "already_present": "üîµ",
    "no_match":    "‚ö™",
}

class GroupPhase2Frame(ttk.Frame):
    def __init__(self, parent, controller, group_id, group_data, scenes_data):
        super().__init__(parent)
        self.controller = controller
        self.group_id = group_id
        self._group_data = group_data # Donn√©es Group d√©j√† scrap√©es/fusionn√©es Phase 1
        self._scenes_data = scenes_data # Liste des sc√®nes Stash du Group
        self._merged_scene_urls = [] # R√©sultat de la fusion Phase 2

        self.scene_checkboxes = [] # Pour cocher les URLs √† injecter

        self.create_ui()
        self._run_phase2()

    def create_ui(self):
        # Header
        header_frame = ttk.Frame(self)
        header_frame.pack(fill=tk.X, padx=10, pady=5)
        ttk.Label(header_frame, text=f"Group ID: {self.group_id} ‚Äî Phase 2: URLs Sc√®nes",
                  font=("Segoe UI", 14, "bold")).pack(side=tk.LEFT)
        ttk.Button(header_frame, text="Retour Phase 1",
                   command=lambda: self.controller.goto_phase1()).pack(side=tk.RIGHT)

        # Progress bar (similaire √† Performer Phase 2)
        self.progress_frame = ttk.Frame(self)
        self.progress_label = ttk.Label(self.progress_frame, text="", 
                                        font=("Segoe UI", 9, "italic"))
        self.progress_label.pack(side=tk.LEFT, padx=10)
        self.progress_bar = ttk.Progressbar(self.progress_frame, mode="indeterminate", length=200)
        self.progress_bar.pack(side=tk.LEFT, padx=5)

        # Zone principale scrollable pour les sc√®nes
        self.main_canvas = tk.Canvas(self, highlightthickness=0)
        self.main_scrollbar = ttk.Scrollbar(self, orient=tk.VERTICAL, command=self.main_canvas.yview)
        self.main_scrollable_frame = ttk.Frame(self.main_canvas)

        self.main_scrollable_frame.bind(
            "<Configure>",
            lambda e: self.main_canvas.configure(
                scrollregion=self.main_canvas.bbox("all")
            )
        )
        self.main_canvas_window = self.main_canvas.create_window((0, 0), window=self.main_scrollable_frame, anchor="nw")
        self.main_canvas.configure(yscrollcommand=self.main_scrollbar.set)
        
        # Mousewheel scrolling
        self.main_canvas.bind_all("<MouseWheel>",
                             lambda e: self.main_canvas.yview_scroll(int(-1 * (e.delta / 120)), "units"))
        
        # Resize canvas window width to match canvas width
        self.main_canvas.bind("<Configure>",
                         lambda e: self.main_canvas.itemconfig(self.main_canvas_window, width=e.width))


        self.main_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.main_canvas.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Boutons d'action
        action_frame = ttk.Frame(self)
        action_frame.pack(fill=tk.X, padx=10, pady=5)
        ttk.Button(action_frame, text="‚úî Injecter s√©lectionn√©es", command=self._inject).pack(side=tk.LEFT, padx=5)
        ttk.Button(action_frame, text="Tout s√©lectionner", command=self._select_all).pack(side=tk.LEFT, padx=5)
        ttk.Button(action_frame, text="D√©s√©lectionner tout", command=self._deselect_all).pack(side=tk.LEFT, padx=5)

    def _run_phase2(self):
        self._show_progress("Scraping URLs de sc√®nes...")

        def _do_scraping_and_merge():
            try:
                scraper = GroupPhase2ScraperService()
                
                def update_prog(src, st):
                    self.after(0, lambda: self._update_progress(f"[{src}] {st}"))

                scraped_urls_by_index = scraper.scrape(
                    group_data=self._group_data,
                    progress_callback=update_prog
                )

                merger = GroupPhase2Merger()
                self._merged_scene_urls = merger.merge(
                    self._scenes_data, scraped_urls_by_index)

                self.after(0, self._display_results)

            except Exception as e:
                self.after(0, lambda: messagebox.showerror("Erreur Phase 2", str(e)))
            finally:
                self.after(0, self._hide_progress)

        threading.Thread(target=_do_scraping_and_merge, daemon=True).start()

    def _display_results(self):
        for widget in self.main_scrollable_frame.winfo_children():
            widget.destroy()

        if not self._merged_scene_urls:
            ttk.Label(self.main_scrollable_frame, text="Aucune URL de sc√®ne trouv√©e ou fusionn√©e.",
                      font=("Segoe UI", 10, "italic")).pack(padx=10, pady=10)
            return

        # Headers du tableau
        header_frame = ttk.Frame(self.main_scrollable_frame)
        header_frame.pack(fill=tk.X, padx=5, pady=2)
        ttk.Label(header_frame, text="", width=4).pack(side=tk.LEFT) # Checkbox
        ttk.Label(header_frame, text="Statut", width=8, font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
        ttk.Label(header_frame, text="Index", width=6, font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
        ttk.Label(header_frame, text="Titre Stash", width=40, anchor=tk.W, font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
        ttk.Label(header_frame, text="Nouvelles URLs", anchor=tk.W, font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT, expand=True, fill=tk.X)

        self.scene_checkboxes = []
        for scene_data in self._merged_scene_urls:
            self._build_scene_row(self.main_scrollable_frame, scene_data)

    def _build_scene_row(self, parent_frame, scene_data):
        row_frame = ttk.Frame(parent_frame, padding=2)
        row_frame.pack(fill=tk.X, padx=5, pady=1)

        status_icon = STATUS_ICONS.get(scene_data["status"], "")

        # Cocher par d√©faut si nouvelles URLs (statut 'new' ou 'partial')
        has_new = bool(scene_data.get("new_urls"))
        var = tk.BooleanVar(value=has_new) 
        self.scene_checkboxes.append((var, scene_data)) # Stocker tuple (var, data)
        
        chk = ttk.Checkbutton(row_frame, variable=var)
        chk.pack(side=tk.LEFT)
        if not has_new:
            chk.config(state=tk.DISABLED)

        ttk.Label(row_frame, text=status_icon, width=4).pack(side=tk.LEFT)
        ttk.Label(row_frame, text=str(scene_data.get("scene_index", "?")), width=6).pack(side=tk.LEFT)
        ttk.Label(row_frame, text=scene_data.get("scene_title", "Sans titre"), width=40, anchor=tk.W).pack(side=tk.LEFT)

        urls_frame = ttk.Frame(row_frame)
        urls_frame.pack(side=tk.LEFT, expand=True, fill=tk.X)

        new_urls = scene_data.get("new_urls", {})
        existing = scene_data.get("existing_urls", [])

        if new_urls:
            for src, url in new_urls.items():
                ttk.Label(urls_frame, text=f"‚ûï {src.upper()}: {url}", anchor=tk.W, font=("Segoe UI", 8, "bold"), foreground="green").pack(fill=tk.X)
        
        if existing:
            count = len(existing)
            ttk.Label(urls_frame, text=f"Existing: {count} URLs", anchor=tk.W, foreground="gray", font=("Segoe UI", 8)).pack(fill=tk.X)
        
        if not new_urls and not existing:
             ttk.Label(urls_frame, text="‚Äî", anchor=tk.W, foreground="gray", font=("Segoe UI", 8)).pack(fill=tk.X)

    def _inject(self):
        selected_urls_to_inject = []
        for var, scene_data in self.scene_checkboxes:
            if var.get():
                for url_src, url_val in scene_data["new_urls"].items():
                    selected_urls_to_inject.append({
                        "scene_id": scene_data["scene_id"],
                        "url": url_val,
                        "source": url_src,
                    })
        
        if not selected_urls_to_inject:
            messagebox.showwarning("Attention", "Aucune URL s√©lectionn√©e √† injecter.")
            return

        try:
            db = GroupDB()
            # appel √† la m√©thode inject_scene_urls que j'ai ajout√©e dans db.py
            db.inject_scene_urls(selected_urls_to_inject) 
            db.close()
            
            messagebox.showinfo("‚úÖ Injection Phase 2", f"{len(selected_urls_to_inject)} URLs de sc√®nes inject√©es.")

            # Optionnel : Recharger ou fermer
            # self._run_phase2() 

        except Exception as e:
            messagebox.showerror("Erreur injection", str(e))

    def _select_all(self):
        for var, _ in self.scene_checkboxes:
            if str(var['state']) != tk.DISABLED:
                var.set(True)

    def _deselect_all(self):
        for var, _ in self.scene_checkboxes:
            var.set(False)

    def _show_progress(self, message):
        self.progress_frame.pack(fill=tk.X, padx=10, pady=2, before=self.main_canvas)
        self.progress_bar.start(10)
        self.progress_label.config(text=message)

    def _update_progress(self, message):
        self.progress_label.config(text=message)

    def _hide_progress(self):
        self.progress_bar.stop()
        self.progress_frame.pack_forget()


============================================================
[38/124] Legacy\gui\launcher.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, simpledialog, messagebox
import sv_ttk
from gui.app import launch_app

def center_window(window, width=400, height=300):
    screen_width = window.winfo_screenwidth()
    screen_height = window.winfo_screenheight()
    x = (screen_width - width) // 2
    y = (screen_height - height) // 2
    window.geometry(f'{width}x{height}+{x}+{y}')

def start_launcher():
    root = tk.Tk()
    root.title("StashMaster V2 - Launcher")
    
    # Appliquer le th√®me moderne sombre
    sv_ttk.set_theme("dark")
    
    center_window(root, 400, 250)
    
    ttk.Label(root, text="StashMaster V2", font=("Segoe UI", 16, "bold")).pack(pady=(20, 10))
    ttk.Label(root, text="S√©lectionnez un module :", font=("Segoe UI", 10)).pack(pady=5)
    
    def on_select(module):
        root.withdraw()
        # Utiliser un prompt simple pour l'ID
        stash_id = simpledialog.askstring("Entrer l'ID", f"Entrez l'ID pour le module {module} :", parent=root)
        if not stash_id:
            messagebox.showwarning("ID requis", "Vous devez entrer un ID valide pour continuer.")
            root.deiconify()
            return
        root.destroy()
        # Lancer l'application principale maximis√©e
        launch_app(module, stash_id)
        
    ttk.Button(root, text="Performer", width=25, command=lambda: on_select("Performer")).pack(pady=5)
    ttk.Button(root, text="Group / DVD", width=25, command=lambda: on_select("Group")).pack(pady=5)
    ttk.Button(root, text="Scene", width=25, command=lambda: on_select("Scene")).pack(pady=5)
    
    root.mainloop()

if __name__ == "__main__":
    start_launcher()

============================================================
[39/124] Legacy\gui\performer_base.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk

class PerformerBaseFrame(ttk.Frame):
    def __init__(self, parent, controller, stash_id):
        super().__init__(parent)
        self.controller = controller
        self.stash_id = stash_id
        self.fields = {}
        self.field_checkboxes = {}
        # To be defined in subclasses
        self.fields_list = [] 
        self.db_mapping = {}

    def create_header(self, title, buttons_config):
        bar = ttk.Frame(self, padding=5)
        bar.pack(fill=tk.X)
        
        ttk.Label(bar, text=f"{title} | ID: {self.stash_id}", font=("Segoe UI", 12, "bold")).pack(side=tk.LEFT, padx=5)
        
        btn_frame = ttk.Frame(bar)
        btn_frame.pack(side=tk.RIGHT, padx=5)
        
        for text, command in buttons_config:
            ttk.Button(btn_frame, text=text, command=command).pack(side=tk.LEFT, padx=2)

    def select_all_fields(self):
        for var in self.field_checkboxes.values():
            var.set(True)

    def select_empty_fields(self):
        for field, entry in self.fields.items():
            if field in self.field_checkboxes and entry:
                val = ""
                if isinstance(entry, tk.Entry):
                    val = entry.get().strip()
                elif isinstance(entry, tk.Text):
                    val = entry.get("1.0", tk.END).strip()
                
                if not val:
                    self.field_checkboxes[field].set(True)
                else:
                    self.field_checkboxes[field].set(False)

    def load_data(self):
        try:
            from services.db import PerformerDB
            db = PerformerDB()
            data = db.get_performer_by_id(self.stash_id)
            db.close()
        except Exception as e:
            print(f"Erreur DB: {e}")
            data = None
        
        if not data:
            return

        for field, db_key in self.db_mapping.items():
            entry = self.fields.get(field)
            if entry and db_key in data:
                value = data[db_key]
                if isinstance(value, (list, tuple)):
                    if field == "URLs":
                        value = "\n".join(value)
                    else:
                        value = ", ".join(value)
                
                if isinstance(entry, tk.Entry):
                    entry.delete(0, tk.END)
                    entry.insert(0, str(value) if value is not None else "")
                elif isinstance(entry, tk.Text):
                    entry.delete('1.0', tk.END)
                    entry.insert('1.0', str(value) if value is not None else "")


============================================================
[40/124] Legacy\gui\performer_frame.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
from gui.performer_phase1 import PerformerPhase1Frame
from gui.performer_phase2 import PerformerPhase2Frame

class PerformerFrame(ttk.Frame):
    def __init__(self, parent, stash_id):
        super().__init__(parent)
        self.stash_id = stash_id
        self.current_frame = None
        self.phase1_data = {} # Pour stocker les donn√©es r√©solues de la phase 1
        
        self.goto_phase1()

    def goto_phase1(self):
        if self.current_frame:
            self.current_frame.destroy()
        
        self.current_frame = PerformerPhase1Frame(self, self, self.stash_id)
        self.current_frame.pack(fill=tk.BOTH, expand=True)

    def goto_phase2(self, phase1_updates: dict):
        """Passe √† la phase 2 en emportant les donn√©es r√©solues de la phase 1."""
        self.phase1_data = phase1_updates

        if self.current_frame:
            self.current_frame.destroy()
            
        # Initialise la phase 2 avec les donn√©es de la phase 1
        self.current_frame = PerformerPhase2Frame(self, self, self.stash_id, self.phase1_data)
        self.current_frame.pack(fill=tk.BOTH, expand=True)




    def return_to_menu(self):
        # Fermer la fen√™tre actuelle
        self.master.destroy()
        # Relancer le launcher
        try:
            from gui.launcher import start_launcher
            start_launcher()
        except Exception as e:
            print(f"Erreur lors du retour au menu: {e}")




============================================================
[41/124] Legacy\gui\performer_phase1.py
------------------------------------------------------------
import tkinter as tk
from tkinter import messagebox
from tkinter import ttk
import threading

from gui.performer_base import PerformerBaseFrame
from gui.phase1_conflict_dialog import Phase1ConflictDialog


class PerformerPhase1Frame(PerformerBaseFrame):
    def __init__(self, parent, controller, stash_id):
        super().__init__(parent, controller, stash_id)
        
        # Configuration des champs Phase 1
        self.fields_list = [
            "Name", "Aliases", "Birthdate", "Deathdate", "Country", "Ethnicity",
            "Hair Color", "Eye Color", "Height", "Weight", "Measurements", "Fake Tits", "Career Length"
        ]
        
        self.db_mapping = {
            "Name": "name",
            "Aliases": "aliases",
            "Birthdate": "birthdate",
            "Deathdate": "death_date",
            "Country": "country",
            "Ethnicity": "ethnicity",
            "Hair Color": "hair_color",
            "Eye Color": "eye_color",
            "Height": "height",
            "Weight": "weight",
            "Measurements": "measurements",
            "Fake Tits": "fake_tits",
            "Career Length": "career_length"
        }
        
        self.create_ui()
        self.load_data()

    def process_and_goto_phase2(self):
        """
        Workflow Phase 1 : Scrape, compare, et pr√©pare les donn√©es pour la Phase 2.
        AUCUNE injection en base de donn√©es n'est faite ici.
        """
        checked_fields = [f for f, var in self.field_checkboxes.items() if var.get()]
        if not checked_fields:
            self.controller.goto_phase2({}) # Passe un dict vide si pas de scraping
            return
        
        try:
            from services.db import PerformerDB
            db = PerformerDB()
            db_data = db.get_performer_by_id(self.stash_id)
            db.close()
        except Exception as e:
            messagebox.showerror("Erreur", f"Impossible de lire la DB: {e}")
            return

        if not db_data:
            messagebox.showerror("Erreur", "Performer non trouv√© dans la base.")
            return

        performer_name = db_data["name"]
        known_urls = db_data.get("urls", [])

        self.progress_frame.pack(fill=tk.X, padx=10, pady=2, after=list(self.fields.values())[-1].master)
        self.progress_bar.start(10)
        self.progress_label.config(text=f"Scraping {len(checked_fields)} champs pour {performer_name}...")

        def _do_scraping():
            try:
                from services.phase2_scraper import Phase2ScraperService
                from services.phase1_merger import Phase1Merger

                scraper = Phase2ScraperService()
                results = scraper.scrape(performer_name, known_urls=known_urls,
                                         progress_callback=lambda s, m: self.after(0, self.progress_label.config, {'text': f"[{s}] {m}"}))
                
                # Nettoyage des donn√©es scrap√©es AVANT le merge
                for res in results:
                    if res.get("hair_color"):
                        # D√©duplication et nettoyage (ex: "Blonde, Blonde" -> "Blonde")
                        parts = [p.strip() for p in res["hair_color"].replace('/', ',').split(',') if p.strip()]
                        seen = set()
                        unique = []
                        for p in parts:
                            p_cap = p.capitalize()
                            if p_cap not in seen:
                                seen.add(p_cap)
                                unique.append(p_cap)
                        res["hair_color"] = ", ".join(unique)

                merger = Phase1Merger()
                merge_results = merger.merge(db_data, results, checked_fields)
                
                # Construire l'affichage pour TOUS les champs coch√©s
                display_results = {}
                for field in checked_fields:
                    # Le merger renvoie les r√©sultats index√©s par le nom du champ (ex: "Name")
                    if field in merge_results:
                        display_results[field] = merge_results[field]
                    else:
                        # Si le merger n'a rien renvoy√©, on force l'affichage en mode "empty"
                        # pour confirmer √† l'utilisateur que le champ a √©t√© trait√© mais sans r√©sultat.
db_key="***MASKED***"
                        current_val = db_data.get(db_key) if db_key else None
                        display_results[field] = {'status': 'empty', 'db_value': current_val, 'scraped_values': {}, 'suggestion': None}

                self.after(0, self._hide_progress)

                phase1_updates = {}
                if display_results:
                    dialog = Phase1ConflictDialog(self.master, performer_name, display_results, self.db_mapping)
                    if dialog.result is not None:
                        # Le dialogue retourne les valeurs √† mettre √† jour
                        for field_name, resolved_value in dialog.result.items():
db_key="***MASKED***"
                            if db_key:
                                # Sp√©cial pour les alias, on veut une liste
                                if db_key == 'aliases' and isinstance(resolved_value, str):
                                     phase1_updates[db_key] = [a.strip() for a in resolved_value.split(',') if a.strip()]
                                elif db_key == 'hair_color' and isinstance(resolved_value, str):
                                     # Nettoyage et d√©duplication pour la couleur de cheveux
                                     parts = [p.strip() for p in resolved_value.replace('/', ',').split(',') if p.strip()]
                                     seen = set()
                                     unique_parts = []
                                     for p in parts:
                                         if p.lower() not in seen:
                                             seen.add(p.lower())
                                             unique_parts.append(p)
                                     phase1_updates[db_key] = ", ".join(unique_parts)
                                else:
                                     phase1_updates[db_key] = resolved_value
                        
                        messagebox.showinfo("Phase 1 Trait√©e", f"{len(phase1_updates)} modifications de la Phase 1 ont √©t√© pr√©par√©es pour la validation finale.")
                    else:
                        # L'utilisateur a annul√©, on ne passe aucune modification
                        messagebox.showinfo("Annul√©", "Fusion des donn√©es annul√©e. Passage en phase 2 sans appliquer les changements de la phase 1.")
                else:
                    messagebox.showinfo("Phase 1 Compl√®te", "Aucun conflit ou nouvelle donn√©e √† traiter. Passage en Phase 2.")
                
                # Passer en Phase 2 avec les donn√©es r√©solues de la phase 1
                self.after(0, lambda: self.controller.goto_phase2(phase1_updates))

            except Exception as e:
                err_msg = str(e)
                self.after(0, lambda: messagebox.showerror("Erreur Phase 1", f"Une erreur est survenue : {err_msg}"))
            finally:
                self.after(0, self._hide_progress)

        threading.Thread(target=_do_scraping, daemon=True).start()

    def _hide_progress(self):
        self.progress_bar.stop()
        self.progress_frame.pack_forget()

    def create_ui(self):
        # Header + Boutons sp√©cifiques
        buttons = [
            ("Tout s√©lectionner", self.select_all_fields),
            ("S√©lectionner vides", self.select_empty_fields),
            ("Suivant / Traiter", self.process_and_goto_phase2),
            ("Retour", self.controller.return_to_menu),
        ]
        self.create_header("Phase 1 : M√©tadonn√©es usuelles", buttons)

        # Barre de progression (cach√©e par d√©faut)
        self.progress_frame = ttk.Frame(self)
        self.progress_label = ttk.Label(self.progress_frame, text="",
                                        font=("Segoe UI", 9, "italic"))
        self.progress_label.pack(side=tk.LEFT, padx=10)
        self.progress_bar = ttk.Progressbar(self.progress_frame, mode="indeterminate", length=200)
        self.progress_bar.pack(side=tk.LEFT, padx=5)

        # Zone des champs
        f = ttk.Frame(self)
        f.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        
        # Configuration de la grille pour l'extension horizontale
        f.grid_columnconfigure(0, weight=0)  # Checkbox
        f.grid_columnconfigure(1, weight=0)  # Label
        f.grid_columnconfigure(2, weight=1)  # Entry (prend tout l'espace restant)

        for i, field in enumerate(self.fields_list):
            row = i
            # Checkbox (d√©coch√©e par d√©faut)
            var = tk.BooleanVar(value=False)
            self.field_checkboxes[field] = var
            checkbox = ttk.Checkbutton(f, variable=var, text="")
            checkbox.grid(row=row, column=0, sticky=tk.W, padx=(5, 0), pady=2)
            
            # Label
            ttk.Label(f, text=f"{field}:").grid(row=row, column=1, sticky=tk.NW, padx=5, pady=2)
            
            # Entry Widget
            entry = ttk.Entry(f, width=60)
            entry.grid(row=row, column=2, sticky=tk.EW, padx=5, pady=2)
            self.fields[field] = entry


============================================================
[42/124] Legacy\gui\performer_phase2.py
------------------------------------------------------------
"""
PerformerPhase2Frame ‚Äî Orchestre le flux 3-fen√™tres Bio IA.

Flux :
  1. Scraping Phase 2 (background thread)
  2. GUI 1 : DataReviewWindow   ‚Äî r√©vision donn√©es (sauf bio)
  3. GUI 2 : BioStudioWindow    ‚Äî g√©n√©ration bio Gemini / Ollama
  4. GUI 3 : ValidationWindow   ‚Äî validation + injection Stash

Toutes les fen√™tres s'ouvrent en plein √©cran.
"""
import tkinter as tk
from tkinter import ttk, messagebox
import threading
import copy

from gui.performer_base import PerformerBaseFrame


class PerformerPhase2Frame(PerformerBaseFrame):
    def __init__(self, parent, controller, stash_id, phase1_data: dict):
        self.phase1_data  = phase1_data
        self.phase2_data  = {}

        super().__init__(parent, controller, stash_id)

        self.fields_list = [
            "Trivia", "Awards", "Tattoos", "Piercings", "Tags", "URLs", "Details"
        ]
        self.db_mapping = {
            "Trivia":    "trivia",
            "Awards":    "awards",
            "Tattoos":   "tattoos",
            "Piercings": "piercings",
            "Tags":      "tags",
            "URLs":      "urls",
            "Details":   "details",
        }

        self._scraped_results = None
        self._merged_data     = None
        self._db_data         = None
        self._stash_context   = None

        self.create_ui()
        self.load_data()

    # ‚îÄ‚îÄ Chargement DB ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def load_data(self):
        super().load_data()
        if self.phase1_data:
            for db_key, value in self.phase1_data.items():
                field_name = next(
                    (n for n, k in self.db_mapping.items() if k == db_key), None
                )
                if field_name:
                    display = (", ".join(value) if isinstance(value, list)
                               else str(value) if value else "")
                    self._update_field_display(field_name, display)

    # ‚îÄ‚îÄ UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def create_ui(self):
        buttons = [
            ("üîé Scraper & Lancer le flux Bio IA", self.run_full_flow),
            ("Tout s√©lectionner",  self.select_all_fields),
            ("S√©lectionner vides", self.select_empty_fields),
            ("Retour Phase 1",     self.controller.goto_phase1),
        ]
        self.create_header("Phase 2 : Champs Avanc√©s + Bio IA", buttons)

        # Barre de progression (scraping)
        self.progress_frame = ttk.Frame(self)
        self.progress_label = ttk.Label(
            self.progress_frame, text="", font=("Segoe UI", 9, "italic")
        )
        self.progress_label.pack(side=tk.LEFT, padx=10)
        self.progress_bar = ttk.Progressbar(
            self.progress_frame, mode="indeterminate", length=200
        )
        self.progress_bar.pack(side=tk.LEFT, padx=5)

        # Grille des champs (lecture seule ‚Äî affichage des donn√©es actuelles)
        f = ttk.Frame(self)
        f.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        f.grid_columnconfigure(2, weight=1)

        for i, field in enumerate(self.fields_list):
            var = tk.BooleanVar(value=True)
            self.field_checkboxes[field] = var
            ttk.Checkbutton(f, variable=var, text="").grid(
                row=i, column=0, sticky=tk.W, padx=(5, 0), pady=5
            )
            ttk.Label(f, text=f"{field}:").grid(
                row=i, column=1, sticky=tk.NW, padx=5, pady=5
            )

            height = 3
            if field == "Details": height = 8
            if field in ("URLs", "Awards"): height = 5

            entry = tk.Text(f, width=60, height=height, font=("Segoe UI", 10))
            entry.grid(row=i, column=2, sticky=tk.EW, padx=5, pady=5)
            self.fields[field] = entry

    # ‚îÄ‚îÄ Flux principal ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def run_full_flow(self):
        """
        Point d'entr√©e : scrape les sources puis ouvre les 3 GUIs en s√©quence.
        """
        # Charger DB
        try:
            from services.db import PerformerDB
            db = PerformerDB()
            self._db_data      = db.get_performer_by_id(self.stash_id)
            self._stash_context = db.get_performer_context(self.stash_id)
            db.close()
        except Exception as e:
            messagebox.showerror("Erreur DB", f"Impossible de lire la base : {e}")
            return

        if not self._db_data:
            messagebox.showerror("Erreur", "Performer introuvable dans la base.")
            return

        # Donn√©es √† jour (DB + Phase 1)
        self._combined = copy.deepcopy(self._db_data)
        self._combined.update(self.phase1_data)

        performer_name = self._combined.get("name", "")
        known_urls     = self._combined.get("urls", [])

        # Afficher barre progression
        self._show_progress(f"Scraping en cours pour {performer_name}‚Ä¶")

        def _do_scraping():
            try:
                from services.phase2_scraper import Phase2ScraperService
                from services.phase2_merger  import Phase2Merger

                scraper = Phase2ScraperService()
                results = scraper.scrape(
                    performer_name,
                    known_urls=known_urls,
                    progress_callback=lambda s, m: self.after(
                        0, self.progress_label.config, {"text": f"[{s}] {m}"}
                    ),
                )
                self._scraped_results = results

                merger = Phase2Merger()
                self._merged_data = merger.merge(self._combined, results)

                self.after(0, self._on_scraping_done)

            except Exception as e:
                err = str(e)
                self.after(0, lambda: messagebox.showerror("Erreur scraping", err))
                self.after(0, self._hide_progress)

        threading.Thread(target=_do_scraping, daemon=True).start()

    def _on_scraping_done(self):
        self._hide_progress()
        checked_fields = [f for f, v in self.field_checkboxes.items() if v.get()]
        self._open_gui1(checked_fields)

    # ‚îÄ‚îÄ GUI 1 : DataReviewWindow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _open_gui1(self, checked_fields):
        from gui.data_review_window import DataReviewWindow

        win = DataReviewWindow(
            parent          = self.winfo_toplevel(),
            db_data         = self._combined,
            stash_ctx       = self._stash_context,
            merged_data     = self._merged_data,
            scraped_results = self._scraped_results or [],
            checked_fields  = checked_fields,
        )

        if win.result is None:
            # Annul√©
            return

        # Appliquer les s√©lections dans les champs UI de phase 2
        self._apply_review_result(win.result)
        self._open_gui2(checked_fields, win.result)

    # ‚îÄ‚îÄ GUI 2 : BioStudioWindow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _open_gui2(self, checked_fields, review_result):
        from gui.bio_studio_window import BioStudioWindow

        win = BioStudioWindow(
            parent          = self.winfo_toplevel(),
            db_data         = self._combined,
            stash_ctx       = self._stash_context,
            merged_data     = self._merged_data,
            scraped_results = self._scraped_results or [],
            checked_fields  = checked_fields,
            review_result   = review_result,
        )

        if win.result is None:
            # Retour ou annuler : retourner √† GUI 1
            self._open_gui1(checked_fields)
            return

        self._open_gui3(review_result, win.result)

    # ‚îÄ‚îÄ GUI 3 : ValidationWindow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _open_gui3(self, review_result, bio_result):
        from gui.validation_window import ValidationWindow

        win = ValidationWindow(
            parent        = self.winfo_toplevel(),
            db_data       = self._combined,
            stash_ctx     = self._stash_context,
            review_result = review_result,
            bio_result    = bio_result,
        )

        if win.result == "injected":
            messagebox.showinfo(
                "‚úÖ Succ√®s",
                "Les donn√©es ont √©t√© inject√©es dans Stash avec succ√®s.\n"
                "Vous pouvez continuer ou retourner au menu.",
            )
            # Rafra√Æchir l'affichage Phase 2 avec les nouvelles donn√©es
            self._refresh_display(review_result, bio_result)

        elif win.result is None:
            # Retour √† GUI 2 ‚Üí GUI 1 est perdue (simplification)
            # On relance seulement GUI 2 avec le m√™me review_result
            checked = [f for f, v in self.field_checkboxes.items() if v.get()]
            self._open_gui2(checked, review_result)

    # ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _apply_review_result(self, result: dict):
        """Met √† jour les widgets de la phase 2 avec les s√©lections GUI 1."""
        if not result:
            return

        mapping = {
            "trivia":    ("Trivia",   lambda x: x),
            "awards":    ("Awards",   lambda x: "\n".join(x) if x else ""),
            "tattoos":   ("Tattoos",  self._fmt_body_art),
            "piercings": ("Piercings",self._fmt_body_art),
            "tags":      ("Tags",     lambda x: ", ".join(x) if x else ""),
            "urls":      ("URLs",     lambda x: "\n".join(x.values()) if x else ""),
        }
        for key, (field, fmt) in mapping.items():
            val = result.get(key)
            if val is not None:
                self._update_field_display(field, fmt(val))
                self.phase2_data[key] = val

    def _fmt_body_art(self, items):
        if not items:
            return ""
        def fmt(it):
            pos  = it.get("position", "")
            desc = it.get("description", "")
            return f"{pos} ({desc})" if desc else pos
        return "; ".join(fmt(i) for i in items)

    def _refresh_display(self, review_result: dict, bio_result: dict):
        """Rafra√Æchit les champs apr√®s injection r√©ussie."""
        self._apply_review_result(review_result)
        bio = (bio_result or {}).get("bio", "")
        if bio:
            self._update_field_display("Details", bio)
            self.phase2_data["details"] = bio

    def _show_progress(self, message):
        self.progress_label.config(text=message)
        self.progress_frame.pack(fill=tk.X, padx=10, pady=2)
        self.progress_bar.start(10)

    def _hide_progress(self):
        self.progress_bar.stop()
        self.progress_frame.pack_forget()

    def _update_field_display(self, field_name: str, value: str):
        entry = self.fields.get(field_name)
        if entry and isinstance(entry, tk.Text):
            entry.config(state=tk.NORMAL)
            entry.delete("1.0", tk.END)
            if value:
                entry.insert("1.0", value)
            self.field_checkboxes.get(field_name, tk.BooleanVar()).set(bool(value))


============================================================
[43/124] Legacy\gui\phase1_conflict_dialog.py
------------------------------------------------------------
"""
Phase1ConflictDialog ‚Äî Dialogue de r√©solution des conflits Phase 1.
Affiche confirmations, nouveaux et conflits pour chaque champ coch√©.
"""
import tkinter as tk
from tkinter import ttk


# Code couleur par statut
STATUS_COLORS = {
    "confirmed": "#2ecc71",  # vert
    "new": "#3498db",        # bleu
    "conflict": "#e74c3c",   # rouge
    "empty": "#95a5a6",      # gris
}

STATUS_ICONS = {
    "confirmed": "‚úÖ",
    "new": "üÜï",
    "conflict": "‚ö†Ô∏è",
    "empty": "‚¨ú",
}


class Phase1ConflictDialog(tk.Toplevel):
    """
    Dialogue modal montrant le r√©sultat du scraping Phase 1.
    Pour chaque champ :
    - Confirm√© (vert) ‚Üí DB == source
    - Nouveau (bleu) ‚Üí DB vide, suggestion disponible
    - Conflit (rouge) ‚Üí DB ‚â† sources ‚Üí l'utilisateur choisit
    - Vide (gris) ‚Üí rien trouv√©
    """

    def __init__(self, parent, performer_name: str, merge_result: dict, db_mapping: dict):
        super().__init__(parent)
        self.performer_name = performer_name
        self.title(f"üîç R√©sultat scraping Phase 1 pour {performer_name}")
        self.merge_result = merge_result
        self.db_mapping = db_mapping
        self.result = None  # Dict final si valid√©

        self.geometry("800x600")
        self.minsize(750, 500)
        self.transient(parent)
        self.grab_set()

        # Variables de s√©lection par champ
        self.selections = {}

        self._build_ui()
        self.wait_window()

    def _build_ui(self):
        # ‚îÄ‚îÄ Compteurs en haut ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        header = ttk.Frame(self, padding=10)
        header.pack(fill=tk.X)

        counts = {"confirmed": 0, "new": 0, "conflict": 0, "empty": 0}
        for info in self.merge_result.values():
            counts[info["status"]] = counts.get(info["status"], 0) + 1

        summary = (
            f"‚úÖ Confirm√©s: {counts['confirmed']}  |  "
            f"üÜï Nouveaux: {counts['new']}  |  "
            f"‚ö†Ô∏è Conflits: {counts['conflict']}  |  "
            f"‚¨ú Vides: {counts['empty']}"
        )
        ttk.Label(header, text=summary, font=("Segoe UI", 11, "bold")).pack(anchor=tk.W)

        # ‚îÄ‚îÄ Zone scrollable ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        main = ttk.Frame(self)
        main.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        canvas = tk.Canvas(main, highlightthickness=0)
        scrollbar = ttk.Scrollbar(main, orient=tk.VERTICAL, command=canvas.yview)
        self.scroll_frame = ttk.Frame(canvas)

        self.scroll_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        canvas.create_window((0, 0), window=self.scroll_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        canvas.bind_all("<MouseWheel>",
                        lambda e: canvas.yview_scroll(int(-1 * (e.delta / 120)), "units"))

        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # ‚îÄ‚îÄ Lignes par champ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        for field_name, info in self.merge_result.items():
            self._build_field_row(field_name, info)

        # ‚îÄ‚îÄ Boutons ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        btn_frame = ttk.Frame(self, padding=10)
        btn_frame.pack(fill=tk.X)

        ttk.Button(btn_frame, text="‚úÖ Appliquer et continuer",
                   command=self._apply).pack(side=tk.RIGHT, padx=5)
        ttk.Button(btn_frame, text="‚ùå Annuler",
                   command=self.destroy).pack(side=tk.RIGHT, padx=5)

    def _build_field_row(self, field_name: str, info: dict):
        """Construire une ligne pour un champ."""
        status = info["status"]
        db_value = info.get("db_value") or ""
        scraped_values = info.get("scraped_values", {})
        suggestion = info.get("suggestion") or ""

        # Cadre principal du champ
        frame = ttk.Frame(self.scroll_frame, padding=(5, 3))
        frame.pack(fill=tk.X, padx=5, pady=2)

        # Ic√¥ne + Nom du champ
        icon = STATUS_ICONS.get(status, "")
        ttk.Label(frame, text=f"{icon} {field_name}", width=20, anchor=tk.W,
                  font=("Segoe UI", 10, "bold")).pack(side=tk.LEFT, padx=(0, 10))

        if status == "confirmed":
            # Tout va bien ‚Äî afficher simplement la valeur
            ttk.Label(frame, text=f"‚úì {db_value}",
                      font=("Segoe UI", 10)).pack(side=tk.LEFT, fill=tk.X, expand=True)
            self.selections[field_name] = tk.StringVar(value=db_value)
            
            # Afficher les sources qui confirment la valeur
            if scraped_values:
                sources = ", ".join(k.upper() for k in scraped_values.keys())
                ttk.Label(frame, text=f"[{sources}]", font=("Segoe UI", 8, "italic")).pack(side=tk.LEFT, padx=5)

        elif status == "empty":
            # Rien trouv√©
            ttk.Label(frame, text="(aucune donn√©e)",
                      font=("Segoe UI", 10, "italic")).pack(side=tk.LEFT)
            self.selections[field_name] = tk.StringVar(value="")

        elif status == "new":
            # Nouvelle valeur ‚Äî proposer la suggestion modifiable
            var = tk.StringVar(value=suggestion)
            self.selections[field_name] = var
            ttk.Label(frame, text="DB: (vide) ‚Üí",
                      font=("Segoe UI", 9, "italic")).pack(side=tk.LEFT, padx=(0, 5))
            entry = ttk.Entry(frame, textvariable=var, width=50)
            entry.pack(side=tk.LEFT, fill=tk.X, expand=True)
            # Source info
            sources = ", ".join(scraped_values.keys())
            ttk.Label(frame, text=f"[{sources}]",
                      font=("Segoe UI", 8)).pack(side=tk.LEFT, padx=5)

        elif status == "conflict":
            # Conflit ‚Äî radio buttons pour choisir
            self._build_conflict_section(frame, field_name, db_value, scraped_values, suggestion)

    def _build_conflict_section(self, parent, field_name, db_value, scraped_values, suggestion):
        """Construire la section de r√©solution de conflit."""
        # Sous-frame pour les options
        conflict_frame = ttk.Frame(parent)
        conflict_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        var = tk.StringVar(value=suggestion)
        self.selections[field_name] = var

        # Option 1 : garder la valeur DB
        ttk.Radiobutton(
            conflict_frame,
            text=f"DB: {db_value}",
            variable=var,
            value=db_value
        ).pack(anchor=tk.W)

        # Options : valeurs scrap√©es
        for source, val in scraped_values.items():
            ttk.Radiobutton(
                conflict_frame,
                text=f"{source.upper()}: {val}",
                variable=var,
                value=val
            ).pack(anchor=tk.W)

    def _apply(self):
        """Collecter les s√©lections et fermer."""
        self.result = {}
        for field_name, var in self.selections.items():
            val = var.get().strip()
            if val:
                self.result[field_name] = val
        self.destroy()


============================================================
[44/124] Legacy\gui\phase2_field_wizard.py
------------------------------------------------------------
"""
Phase2FieldWizard ‚Äî Dialogue pas-√†-pas pour la r√©solution des champs Phase 2.
Pr√©sente un champ √† la fois avec le contexte Stash.
"""
import tkinter as tk
from tkinter import ttk


# Ordre des pages du wizard
WIZARD_PAGES = [
    ("awards", "üèÜ AWARDS"),
    ("trivia", "üìù TRIVIA"),
    ("tattoos", "üé® TATTOOS"),
    ("piercings", "üíâ PIERCINGS"),
    ("tags", "üè∑Ô∏è TAGS"),
    ("urls", "üîó URLs"),
    ("details", "üìñ DETAILS (Bio)"),
]


class Phase2FieldWizard(tk.Toplevel):
    """
    Wizard pas-√†-pas : une page par champ Phase 2.
    Chaque page affiche le contexte Stash + les donn√©es scrap√©es.
    """

    def __init__(self, parent, merged_data: dict, stash_context: dict,
                 db_data: dict, scraped_results: list[dict] = None, checked_fields: list[str] | None = None):
        super().__init__(parent)
        self.title("üìã Wizard Phase 2 ‚Äî R√©solution pas-√†-pas")
        self.merged_data = merged_data
        self.stash_ctx = stash_context
        self.db_data = db_data
        self.scraped_results = scraped_results or []  # Stocker les r√©sultats bruts
        self.checked_fields = checked_fields or []   # ‚Üê AJOUTER
        self.result = None
        self.generated_bio = None

        self.geometry("950x700")
        self.minsize(900, 600)
        self.transient(parent)
        self.grab_set()

        # √âtat du wizard
        self.current_page = 0
        self.selections = {}

        # Construire les cadres
        self._build_shell()
        self._show_page(0)
        self.wait_window()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # STRUCTURE PRINCIPALE
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_shell(self):
        """Cr√©er la coquille du wizard (header, zone contenu, boutons nav)."""
        # ‚îÄ‚îÄ Header ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        self.header_frame = ttk.Frame(self, padding=10)
        self.header_frame.pack(fill=tk.X)

        self.page_title = ttk.Label(self.header_frame, text="",
                                     font=("Segoe UI", 14, "bold"))
        self.page_title.pack(side=tk.LEFT)

        self.page_counter = ttk.Label(self.header_frame, text="",
                                       font=("Segoe UI", 10))
        self.page_counter.pack(side=tk.RIGHT)

        # Progress bar
        self.progress = ttk.Progressbar(self, maximum=len(WIZARD_PAGES), length=400)
        self.progress.pack(fill=tk.X, padx=10, pady=(0, 5))

        # ‚îÄ‚îÄ Zone contenu scrollable ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        content_wrapper = ttk.Frame(self)
        content_wrapper.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        self.canvas = tk.Canvas(content_wrapper, highlightthickness=0)
        scrollbar = ttk.Scrollbar(content_wrapper, orient=tk.VERTICAL, command=self.canvas.yview)
        self.content_frame = ttk.Frame(self.canvas)

        self.content_frame.bind(
            "<Configure>",
            lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all"))
        )
        self.canvas_window = self.canvas.create_window((0, 0), window=self.content_frame, anchor="nw")
        self.canvas.configure(yscrollcommand=scrollbar.set)
        self.canvas.bind_all("<MouseWheel>",
                             lambda e: self.canvas.yview_scroll(int(-1 * (e.delta / 120)), "units"))

        # Resize canvas window width
        self.canvas.bind("<Configure>",
                         lambda e: self.canvas.itemconfig(self.canvas_window, width=e.width))

        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # ‚îÄ‚îÄ Boutons navigation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        nav_frame = ttk.Frame(self, padding=10)
        nav_frame.pack(fill=tk.X)

        self.btn_prev = ttk.Button(nav_frame, text="‚óÄ Pr√©c√©dent", command=self._prev_page)
        self.btn_prev.pack(side=tk.LEFT, padx=5)

        ttk.Button(nav_frame, text="‚ùå Annuler", command=self.destroy).pack(side=tk.LEFT, padx=5)

        self.btn_next = ttk.Button(nav_frame, text="Suivant ‚ñ∂", command=self._next_page)
        self.btn_next.pack(side=tk.RIGHT, padx=5)

        self.btn_skip = ttk.Button(nav_frame, text="‚è≠ Passer", command=self._skip_page)
        self.btn_skip.pack(side=tk.RIGHT, padx=5)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # NAVIGATION
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _prev_page(self):
        if self.current_page > 0:
            self._save_current()
            self.current_page -= 1
            self._show_page(self.current_page)

    def _next_page(self):
        self._save_current()
        if self.current_page < len(WIZARD_PAGES) - 1:
            self.current_page += 1
            self._show_page(self.current_page)
        else:
            self._finish()

    def _skip_page(self):
        """Passer sans sauvegarder le champ courant."""
        if self.current_page < len(WIZARD_PAGES) - 1:
            self.current_page += 1
            self._show_page(self.current_page)
        else:
            self._finish()

    def _show_page(self, idx):
        """Afficher la page √† l'index donn√©."""
        field_key, title = WIZARD_PAGES[idx]

        # MAJ header
        self.page_title.config(text=title)
        self.page_counter.config(text=f"√âtape {idx + 1} / {len(WIZARD_PAGES)}")
        self.progress["value"] = idx + 1

        # MAJ boutons
        self.btn_prev.config(state=tk.NORMAL if idx > 0 else tk.DISABLED)
        self.btn_next.config(text="‚úÖ Terminer" if idx == len(WIZARD_PAGES) - 1 else "Suivant ‚ñ∂")

        # Vider le contenu
        for w in self.content_frame.winfo_children():
            w.destroy()

        # Construire la page selon le champ
        builder = {
            "details": self._page_details,
            "awards": self._page_awards,
            "trivia": self._page_trivia,
            "tattoos": self._page_body_art,
            "piercings": self._page_body_art,
            "tags": self._page_tags,
            "urls": self._page_urls,
        }
        builder_fn = builder.get(field_key, lambda k: None)
        builder_fn(field_key)

        # Scroll en haut
        self.canvas.yview_moveto(0)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CONTEXTE STASH (affich√© sur chaque page)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _add_stash_context(self, parent):
        """Ajouter un panneau de contexte Stash."""
        ctx = self.stash_ctx
        if not ctx:
            return

        frame = ttk.LabelFrame(parent, text="üìä Contexte Stash", padding=8)
        frame.pack(fill=tk.X, padx=5, pady=5)

        info_parts = []
        if ctx.get("scene_count"):
            info_parts.append(f"üé¨ {ctx['scene_count']} sc√®nes")
        if ctx.get("studios"):
            studios_str = ", ".join(ctx["studios"][:10])
            if len(ctx["studios"]) > 10:
                studios_str += f" (+{len(ctx['studios']) - 10})"
            info_parts.append(f"üè¢ Studios: {studios_str}")
        if ctx.get("groups"):
            groups_str = ", ".join(ctx["groups"][:8])
            if len(ctx["groups"]) > 8:
                groups_str += f" (+{len(ctx['groups']) - 8})"
            info_parts.append(f"üìÄ Groups: {groups_str}")
        if ctx.get("collaborators"):
            top5 = [f"{c['name']} ({c['count']})" for c in ctx["collaborators"][:5]]
            info_parts.append(f"üë• Top collabs: {', '.join(top5)}")

        for part in info_parts:
            ttk.Label(frame, text=part, font=("Segoe UI", 9), wraplength=800).pack(
                anchor=tk.W, pady=1)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: DETAILS (Bio)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_details(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("details", {})
        by_source = data.get("by_source", {})
        fused = data.get("fused")
        db_bio = self.db_data.get("details") or self.db_data.get("bio") or ""

        # Contexte Stash
        self._add_stash_context(parent)

        # Valeur Stash actuelle
        if db_bio:
            stash_frame = ttk.LabelFrame(parent, text="üìã Bio actuelle (Stash)", padding=8)
            stash_frame.pack(fill=tk.X, padx=5, pady=5)
            stash_text = tk.Text(stash_frame, height=4, width=80, font=("Segoe UI", 9),
                                 wrap=tk.WORD, state=tk.DISABLED)
            stash_text.pack(fill=tk.X)
            stash_text.config(state=tk.NORMAL)
            stash_text.insert("1.0", db_bio)
            stash_text.config(state=tk.DISABLED)

        # Choix de source
        self._details_choice_frame = ttk.LabelFrame(parent, text="üîÑ Choisir la bio", padding=8)
        self._details_choice_frame.pack(fill=tk.X, padx=5, pady=5)

        options = {}
        if db_bio:
            options["stash"] = db_bio

        for source, text in by_source.items():
            options[source] = text

        if fused:
            options["_fused_"] = fused

        # Ajouter la bio IA si d√©j√† g√©n√©r√©e (persistance)
        if self.generated_bio:
            options["_ia_"] = self.generated_bio

        if not options:
            ttk.Label(self._details_choice_frame, text="Aucune bio disponible",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W)
            return

        # R√©cup√©rer la s√©lection pr√©c√©dente ou d√©faut
        prev = self.selections.get("details", {}).get("_choice")
        
        # S√©lection par d√©faut : IA si dispo, sinon Stash, sinon premi√®re source
        default = prev
        if not default:
            default = "_ia_" if self.generated_bio else ("stash" if db_bio else list(options.keys())[0])

        self._details_var = tk.StringVar(value=default)
        self._details_options = options

        for key, text in options.items():
            label = key.upper() if key != "_fused_" else "FUSION"
            length = len(text)
            ttk.Radiobutton(self._details_choice_frame, text=f"{label} ({length} car.)",
                           variable=self._details_var, value=key).pack(anchor=tk.W, padx=5, pady=2)

        # Preview
        self._details_preview = tk.Text(parent, height=8, width=80,
                                         font=("Segoe UI", 9), wrap=tk.WORD)
        self._details_preview.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)

        def update(*_):
            self._details_preview.delete("1.0", tk.END)
            self._details_preview.insert("1.0", options.get(self._details_var.get(), ""))

        self._details_var.trace_add("write", update)
        update()

        # NOUVEAU : Bouton g√©n√©ration IA
        gen_frame = ttk.LabelFrame(parent, text="‚ú® G√©n√©ration IA", padding=8)
        gen_frame.pack(fill=tk.X, padx=5, pady=5)

        self._bio_status = ttk.Label(gen_frame, text="Pr√™t (Auto)", font=("Segoe UI", 9, "italic"))
        self._bio_status.pack(side=tk.LEFT, padx=10)

        ttk.Button(
            gen_frame,
            text="üîÑ R√©g√©n√©rer (Gemini/Ollama)",
            command=self._run_bio_generation
        ).pack(side=tk.RIGHT, padx=5)

        # Lancement automatique si pas encore de bio IA
        if not self.generated_bio:
            self.after(500, self._run_bio_generation)
        else:
            self._bio_status.config(text="‚úÖ Bio IA d√©j√† disponible")

    def _run_bio_generation(self):
        """Lance la g√©n√©ration IA en thread."""
        import threading
        import copy
        self._bio_status.config(text="üöÄ G√©n√©ration auto en cours (Gemini > Ollama)...")

        def t():
            from services.bio_generator import BioGenerator
            gen = BioGenerator()

            # Utiliser les vrais champs coch√©s (Phase 1 + Phase 2 disponibles)
            all_checked = (
                self.checked_fields if self.checked_fields
                else list(self.merged_data.keys())
            )
            # Ajouter tous les champs Phase 1 potentiellement utiles.
            # Le g√©n√©rateur d√©cidera d'utiliser les specs techniques (Taille/Poids...) uniquement si les infos sont limit√©es.
            all_checked.extend(["Name", "Birthdate", "Height", "Weight", "Measurements", "Fake Tits", "Hair Color", "Eye Color", "Ethnicity", "Country", "Aliases", "Career Length"])
            
            # S'assurer que Awards et URLs sont coch√©s s'ils existent dans les donn√©es fusionn√©es
            all_checked.extend(["Awards", "URLs"])
            
            # Utiliser les donn√©es fusionn√©es, mais mettre √† jour avec les s√©lections utilisateur
            # faites dans les onglets pr√©c√©dents (puisque Details est maintenant √† la fin)
            effective_merged = copy.deepcopy(self.merged_data)
            if "awards" in self.selections:
                effective_merged.setdefault("awards", {})["merged"] = self.selections["awards"]["value"]
            
            # AJOUT : Injecter les URLs valid√©es (onglet 6) pour que l'IA connaisse les bons r√©seaux sociaux
            if "urls" in self.selections:
                effective_merged.setdefault("urls", {})["merged"] = self.selections["urls"]["value"]

            ctx = gen.build_context_from_v2(
                db_data=self.db_data,
                stash_ctx=self.stash_ctx,
                scraped_results=self.scraped_results,  # Passer les r√©sultats bruts
                merged_data=effective_merged,
                checked_fields=all_checked,
            )
            bio = gen.generate(ctx)

            def update():
                if bio:
                    self.generated_bio = bio
                    # Mise √† jour UI seulement si on est encore sur la page Details
current_key="***MASKED***"
                    if hasattr(self, "_details_options") and current_key == "details":
                        if "_ia_" not in self._details_options:
                            self._details_options["_ia_"] = bio
                            ttk.Radiobutton(
                                self._details_choice_frame,
                                text=f"ü§ñ IA GENERATED ({len(bio)} car.)",
                                variable=self._details_var,
                                value="_ia_"
                            ).pack(anchor=tk.W, padx=5, pady=2)
                        self._details_var.set("_ia_") # S√©lectionne et met √† jour la preview via trace
                    self._bio_status.config(text=f"‚úÖ Bio g√©n√©r√©e ({len(bio)} car.)")
                else:
                    self._bio_status.config(
                        text="‚ùå √âchec ‚Äî V√©rifier .gemini_key ou Ollama actif")

            self.after(0, update)

        threading.Thread(target=t, daemon=True).start()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: AWARDS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_awards(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("awards", {})
        merged = data.get("merged", [])
        sources = data.get("sources", {})

        self._add_stash_context(parent)

        if not merged:
            ttk.Label(parent, text="Aucun award trouv√©",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        # Info sources
        info_frame = ttk.LabelFrame(parent, text="üìä Sources", padding=8)
        info_frame.pack(fill=tk.X, padx=5, pady=5)
        source_info = "  ".join(f"[{s}: {len(a)}]" for s, a in sources.items() if a)
        ttk.Label(info_frame, text=source_info, font=("Segoe UI", 9)).pack(anchor=tk.W)

        # Liste cochable
        list_frame = ttk.LabelFrame(parent, text=f"üèÜ Awards trouv√©s ({len(merged)})", padding=8)
        list_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        prev = self.selections.get("awards", {}).get("_items")
        self._award_vars = []
        for i, award in enumerate(merged):
            checked = prev[i] if prev and i < len(prev) else True
            var = tk.BooleanVar(value=checked)
            self._award_vars.append((var, award))
            ttk.Checkbutton(list_frame, text=award, variable=var).pack(anchor=tk.W, padx=5)

        # Boutons tout cocher/d√©cocher
        btn_row = ttk.Frame(list_frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda: [v.set(True) for v, _ in self._award_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda: [v.set(False) for v, _ in self._award_vars]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: TRIVIA
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_trivia(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("trivia", {})
        by_source = data.get("by_source", {})

        self._add_stash_context(parent)

        if not by_source:
            ttk.Label(parent, text="Aucun trivia trouv√©",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        choice_frame = ttk.LabelFrame(parent, text="üìù Sources Trivia", padding=8)
        choice_frame.pack(fill=tk.X, padx=5, pady=5)

        prev = self.selections.get("trivia", {}).get("_choice")
        default = prev or list(by_source.keys())[0]
        self._trivia_var = tk.StringVar(value=default)
        self._trivia_sources = by_source

        for source, text in by_source.items():
            preview = text[:80] + "..." if len(text) > 80 else text
            ttk.Radiobutton(choice_frame, text=f"{source.upper()} ‚Äî {preview}",
                           variable=self._trivia_var, value=source).pack(anchor=tk.W, padx=5, pady=2)

        self._trivia_preview = tk.Text(parent, height=6, width=80,
                                        font=("Segoe UI", 9), wrap=tk.WORD)
        self._trivia_preview.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)

        def update(*_):
            self._trivia_preview.delete("1.0", tk.END)
            self._trivia_preview.insert("1.0", by_source.get(self._trivia_var.get(), ""))

        self._trivia_var.trace_add("write", update)
        update()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: TATTOOS / PIERCINGS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_body_art(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get(field_key, {})
        merged = data.get("merged", [])
        db_value = data.get("db_value", "")

        self._add_stash_context(parent)

        # Valeur actuelle Stash
        if db_value:
            stash_frame = ttk.LabelFrame(parent, text=f"üìã {field_key.title()} actuels (Stash)",
                                          padding=8)
            stash_frame.pack(fill=tk.X, padx=5, pady=5)
            ttk.Label(stash_frame, text=str(db_value), font=("Segoe UI", 9),
                      wraplength=800).pack(anchor=tk.W)

        if not merged:
            ttk.Label(parent, text=f"Aucun {field_key} trouv√©",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        list_frame = ttk.LabelFrame(parent, text=f"R√©sultat ({len(merged)} entr√©es)", padding=8)
        list_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        prev = self.selections.get(field_key, {}).get("_items")
        var_attr = f"_{field_key}_vars"
        vars_list = []
        for i, item in enumerate(merged):
            pos = item.get("position", "?")
            desc = item.get("description", "")
            label = f"{pos}" + (f" ({desc})" if desc else "")
            checked = prev[i] if prev and i < len(prev) else True
            var = tk.BooleanVar(value=checked)
            vars_list.append((var, item))
            ttk.Checkbutton(list_frame, text=label, variable=var).pack(anchor=tk.W, padx=5)

        setattr(self, var_attr, vars_list)

        btn_row = ttk.Frame(list_frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda vl=vars_list: [v.set(True) for v, _ in vl]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda vl=vars_list: [v.set(False) for v, _ in vl]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: TAGS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_tags(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("tags", {})
        merged = data.get("merged", [])
        sources = data.get("sources", {})
        stash_tags = self.db_data.get("tags", [])

        self._add_stash_context(parent)

        # Tags actuels Stash
        if stash_tags:
            stash_frame = ttk.LabelFrame(parent, text=f"üìã Tags actuels Stash ({len(stash_tags)})",
                                          padding=8)
            stash_frame.pack(fill=tk.X, padx=5, pady=5)
            ttk.Label(stash_frame, text=", ".join(stash_tags[:30]),
                      font=("Segoe UI", 9), wraplength=800).pack(anchor=tk.W)

        if not merged:
            ttk.Label(parent, text="Aucun tag scrap√©",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        # Info sources
        info = "  ".join(f"[{s}: {len(t)}]" for s, t in sources.items() if t)
        ttk.Label(parent, text=f"Union : {len(merged)} tags ‚Äî {info}",
                  font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, padx=10, pady=5)

        list_frame = ttk.LabelFrame(parent, text="üè∑Ô∏è Tags scrap√©s", padding=8)
        list_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        prev = self.selections.get("tags", {}).get("_items")
        self._tag_vars = []
        for i, tag in enumerate(merged):
            # Marquer si d√©j√† dans Stash
            in_stash = tag.lower() in [t.lower() for t in stash_tags]
            label = f"{'‚úì ' if in_stash else ''}{tag}"
            checked = prev[i] if prev and i < len(prev) else True
            var = tk.BooleanVar(value=checked)
            self._tag_vars.append((var, tag))
            ttk.Checkbutton(list_frame, text=label, variable=var).pack(anchor=tk.W, padx=5)

        btn_row = ttk.Frame(list_frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda: [v.set(True) for v, _ in self._tag_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda: [v.set(False) for v, _ in self._tag_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="Nouveaux seuls",
                   command=lambda: self._select_new_tags_only(stash_tags)).pack(side=tk.LEFT, padx=2)

    def _select_new_tags_only(self, stash_tags):
        """Cocher uniquement les tags qui ne sont PAS d√©j√† dans Stash."""
        stash_lower = {t.lower() for t in stash_tags}
        for var, tag in self._tag_vars:
            var.set(tag.lower() not in stash_lower)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: URLs
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_urls(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("urls", {})
        merged = data.get("merged", {})
        stash_urls = self.db_data.get("urls", [])

        self._add_stash_context(parent)

        # URLs actuelles Stash
        if stash_urls:
            stash_frame = ttk.LabelFrame(parent, text=f"üìã URLs actuelles Stash ({len(stash_urls)})",
                                          padding=8)
            stash_frame.pack(fill=tk.X, padx=5, pady=5)
            for url in stash_urls[:15]:
                ttk.Label(stash_frame, text=url, font=("Segoe UI", 9)).pack(anchor=tk.W)

        if not merged:
            ttk.Label(parent, text="Aucune URL scrap√©e",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        list_frame = ttk.LabelFrame(parent, text=f"üîó URLs scrap√©es ({len(merged)})", padding=8)
        list_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        prev = self.selections.get("urls", {}).get("_items")
        self._url_vars = []
        for i, (key, url) in enumerate(sorted(merged.items())):
            checked = True
            if prev:
                checked = prev.get(key, True)
            var = tk.BooleanVar(value=checked)
            self._url_vars.append((var, key, url))
            row = ttk.Frame(list_frame)
            row.pack(fill=tk.X, padx=5, pady=1)
            ttk.Checkbutton(row, variable=var).pack(side=tk.LEFT)
            ttk.Label(row, text=f"{key}:", width=12, anchor=tk.W,
                      font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
            ttk.Label(row, text=url, font=("Segoe UI", 9)).pack(side=tk.LEFT, fill=tk.X)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # SAUVEGARDE / FINITION
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _save_current(self):
        """Sauvegarder la s√©lection de la page courante."""
field_key="***MASKED***"

        if field_key == "details":
            if hasattr(self, "_details_var"):
                choice = self._details_var.get()
                text = self._details_options.get(choice, "")
                self.selections["details"] = {"_choice": choice, "value": text}

        elif field_key == "awards":
            if hasattr(self, "_award_vars"):
                items = [v.get() for v, _ in self._award_vars]
                selected = [a for v, a in self._award_vars if v.get()]
                self.selections["awards"] = {"_items": items, "value": selected}

        elif field_key == "trivia":
            if hasattr(self, "_trivia_var"):
                choice = self._trivia_var.get()
                text = self._trivia_sources.get(choice, "")
                self.selections["trivia"] = {"_choice": choice, "value": text}

        elif field_key in ("tattoos", "piercings"):
            var_attr = f"_{field_key}_vars"
            if hasattr(self, var_attr):
                vars_list = getattr(self, var_attr)
                items = [v.get() for v, _ in vars_list]
                selected = [item for v, item in vars_list if v.get()]
                self.selections[field_key] = {"_items": items, "value": selected}

        elif field_key == "tags":
            if hasattr(self, "_tag_vars"):
                items = [v.get() for v, _ in self._tag_vars]
                selected = [t for v, t in self._tag_vars if v.get()]
                self.selections["tags"] = {"_items": items, "value": selected}

        elif field_key == "urls":
            if hasattr(self, "_url_vars"):
                items = {k: v.get() for v, k, _ in self._url_vars}
                selected = {k: u for v, k, u in self._url_vars if v.get()}
                self.selections["urls"] = {"_items": items, "value": selected}

    def _finish(self):
        """Collecter toutes les s√©lections et fermer."""
        self._save_current()

        self.result = {}

        # Details
        det = self.selections.get("details", {})
        self.result["details"] = det.get("value")

        # Awards
        aw = self.selections.get("awards", {})
        self.result["awards"] = aw.get("value", [])

        # Trivia
        tr = self.selections.get("trivia", {})
        self.result["trivia"] = tr.get("value")

        # Tattoos
        tt = self.selections.get("tattoos", {})
        self.result["tattoos"] = tt.get("value", [])

        # Piercings
        pi = self.selections.get("piercings", {})
        self.result["piercings"] = pi.get("value", [])

        # Tags
        tg = self.selections.get("tags", {})
        self.result["tags"] = tg.get("value", [])

        # URLs
        ur = self.selections.get("urls", {})
        self.result["urls"] = ur.get("value", {})

        self.destroy()


============================================================
[45/124] Legacy\gui\phase2_merge_dialog.py
------------------------------------------------------------
"""
Phase2MergeDialog ‚Äî Fen√™tre modale de r√©solution des r√©sultats Phase 2.
Chaque champ a son propre mode d'affichage et de s√©lection.
"""
import tkinter as tk
from tkinter import ttk


class Phase2MergeDialog(tk.Toplevel):
    """
    Dialogue de fusion pour les champs Phase 2.
    Affiche les r√©sultats fusionn√©s et permet √† l'utilisateur
    de choisir/modifier avant application.
    """

    def __init__(self, parent, merged_data: dict):
        super().__init__(parent)
        self.title("üìã R√©sultats scraping Phase 2")
        self.merged_data = merged_data
        self.result = None  # Dict final si l'utilisateur valide

        # Configurer la fen√™tre
        self.geometry("900x700")
        self.minsize(800, 600)
        self.transient(parent)
        self.grab_set()

        # Variables de s√©lection
        self.selections = {}

        self._build_ui()
        self.wait_window()

    def _build_ui(self):
        # ‚îÄ‚îÄ Frame principal avec scroll ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        main = ttk.Frame(self, padding=10)
        main.pack(fill=tk.BOTH, expand=True)

        # Canvas + Scrollbar pour le contenu
        canvas = tk.Canvas(main, highlightthickness=0)
        scrollbar = ttk.Scrollbar(main, orient=tk.VERTICAL, command=canvas.yview)
        self.scroll_frame = ttk.Frame(canvas)

        self.scroll_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        canvas.create_window((0, 0), window=self.scroll_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        # Mousewheel scroll
        canvas.bind_all("<MouseWheel>",
                        lambda e: canvas.yview_scroll(int(-1 * (e.delta / 120)), "units"))

        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # ‚îÄ‚îÄ Sections par champ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        self._build_awards_section()
        self._build_trivia_section()
        self._build_details_section()
        self._build_body_art_section("tattoos", "üé® TATTOOS")
        self._build_body_art_section("piercings", "üíâ PIERCINGS")
        self._build_tags_section()
        self._build_urls_section()

        # ‚îÄ‚îÄ Boutons d'action ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        btn_frame = ttk.Frame(self, padding=10)
        btn_frame.pack(fill=tk.X)

        ttk.Button(btn_frame, text="‚úÖ Appliquer tout",
                   command=self._apply_all).pack(side=tk.RIGHT, padx=5)
        ttk.Button(btn_frame, text="‚ùå Annuler",
                   command=self.destroy).pack(side=tk.RIGHT, padx=5)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # AWARDS ‚Äî liste cochable
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_awards_section(self):
        data = self.merged_data.get("awards", {})
        merged = data.get("merged", [])
        sources = data.get("sources", {})

        frame = self._section_frame("üèÜ AWARDS")

        if not merged:
            ttk.Label(frame, text="Aucun award trouv√©").pack(anchor=tk.W)
            self.selections["awards"] = []
            return

        # Compteur par source
        source_info = "  ".join(f"[{s}: {len(a)}]" for s, a in sources.items() if a)
        ttk.Label(frame, text=source_info, font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, pady=(0, 5))

        # Liste cochable
        self.award_vars = []
        for award in merged:
            var = tk.BooleanVar(value=True)
            self.award_vars.append((var, award))
            ttk.Checkbutton(frame, text=award, variable=var).pack(anchor=tk.W, padx=10)

        # Boutons
        btn_row = ttk.Frame(frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout cocher",
                   command=lambda: [v.set(True) for v, _ in self.award_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Tout d√©cocher",
                   command=lambda: [v.set(False) for v, _ in self.award_vars]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TRIVIA ‚Äî s√©lecteur de source
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_trivia_section(self):
        data = self.merged_data.get("trivia", {})
        by_source = data.get("by_source", {})
        suggestion = data.get("suggestion")

        frame = self._section_frame("üìù TRIVIA")

        if not by_source:
            ttk.Label(frame, text="Aucun trivia trouv√©").pack(anchor=tk.W)
            self.selections["trivia"] = None
            return

        # Radio buttons pour chaque source
        self.trivia_var = tk.StringVar(value=list(by_source.keys())[0] if suggestion is None else
                                       next((k for k, v in by_source.items() if v == suggestion), ""))
        
        for source, text in by_source.items():
            preview = text[:100] + "..." if len(text) > 100 else text
            ttk.Radiobutton(frame, text=f"{source.upper()} ‚Äî {preview}",
                           variable=self.trivia_var, value=source).pack(anchor=tk.W, padx=10, pady=2)

        # Zone de pr√©visualisation
        self.trivia_preview = tk.Text(frame, height=4, width=80, font=("Segoe UI", 9), wrap=tk.WORD)
        self.trivia_preview.pack(fill=tk.X, padx=10, pady=5)
        
        # MAJ pr√©visualisation quand la s√©lection change
        def update_preview(*_):
            src = self.trivia_var.get()
            self.trivia_preview.delete("1.0", tk.END)
            self.trivia_preview.insert("1.0", by_source.get(src, ""))
        
        self.trivia_var.trace_add("write", update_preview)
        update_preview()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # DETAILS (Bio) ‚Äî radio source unique ou fusion
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_details_section(self):
        data = self.merged_data.get("details", {})
        by_source = data.get("by_source", {})
        fused = data.get("fused")

        frame = self._section_frame("üìñ DETAILS (Bio)")

        if not by_source:
            ttk.Label(frame, text="Aucune bio trouv√©e").pack(anchor=tk.W)
            self.selections["details"] = None
            return

        # Options
        self.details_var = tk.StringVar(value="freeones" if "freeones" in by_source else list(by_source.keys())[0])

        for source, text in by_source.items():
            length = len(text)
            ttk.Radiobutton(frame, text=f"{source.upper()} ({length} car.)",
                           variable=self.details_var, value=source).pack(anchor=tk.W, padx=10, pady=2)

        if fused:
            ttk.Radiobutton(frame, text=f"Fusion toutes sources ({len(fused)} car.)",
                           variable=self.details_var, value="_fused_").pack(anchor=tk.W, padx=10, pady=2)

        # Zone de pr√©visualisation
        self.details_preview = tk.Text(frame, height=6, width=80, font=("Segoe UI", 9), wrap=tk.WORD)
        self.details_preview.pack(fill=tk.X, padx=10, pady=5)

        def update_preview(*_):
            src = self.details_var.get()
            self.details_preview.delete("1.0", tk.END)
            if src == "_fused_":
                self.details_preview.insert("1.0", fused or "")
            else:
                self.details_preview.insert("1.0", by_source.get(src, ""))

        self.details_var.trace_add("write", update_preview)
        update_preview()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TATTOOS / PIERCINGS ‚Äî liste √©ditable
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_body_art_section(self, field: str, title: str):
        data = self.merged_data.get(field, {})
        merged = data.get("merged", [])

        frame = self._section_frame(title)

        if not merged:
            ttk.Label(frame, text=f"Aucun {field} trouv√©").pack(anchor=tk.W)
            self.selections[field] = []
            return

        # Info strat√©gie
        ttk.Label(frame, text=f"Merge auto (structur√© > flat) ‚Üí {len(merged)} entr√©es uniques",
                  font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, pady=(0, 5))

        # Liste cochable
        vars_list = []
        for item in merged:
            pos = item.get("position", "?")
            desc = item.get("description", "")
            label = f"{pos}" + (f" ({desc})" if desc else "")
            var = tk.BooleanVar(value=True)
            vars_list.append((var, item))
            ttk.Checkbutton(frame, text=label, variable=var).pack(anchor=tk.W, padx=10)

        setattr(self, f"{field}_vars", vars_list)

        # Boutons
        btn_row = ttk.Frame(frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda vl=vars_list: [v.set(True) for v, _ in vl]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda vl=vars_list: [v.set(False) for v, _ in vl]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TAGS ‚Äî liste filtrable
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_tags_section(self):
        data = self.merged_data.get("tags", {})
        merged = data.get("merged", [])
        sources = data.get("sources", {})

        frame = self._section_frame("üè∑Ô∏è TAGS")

        if not merged:
            ttk.Label(frame, text="Aucun tag trouv√©").pack(anchor=tk.W)
            self.selections["tags"] = []
            return

        # Compteur
        source_info = "  ".join(f"[{s}: {len(t)}]" for s, t in sources.items() if t)
        ttk.Label(frame, text=f"Union : {len(merged)} tags ‚Äî {source_info}",
                  font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, pady=(0, 5))

        # Filtre
        filter_frame = ttk.Frame(frame)
        filter_frame.pack(fill=tk.X, padx=10, pady=2)
        ttk.Label(filter_frame, text="Filtrer:").pack(side=tk.LEFT)
        self.tag_filter_var = tk.StringVar()
        filter_entry = ttk.Entry(filter_frame, textvariable=self.tag_filter_var, width=30)
        filter_entry.pack(side=tk.LEFT, padx=5)

        # Tags dans un frame scrollable
        tag_canvas = tk.Canvas(frame, height=150, highlightthickness=0)
        tag_scroll = ttk.Scrollbar(frame, orient=tk.VERTICAL, command=tag_canvas.yview)
        self.tag_inner = ttk.Frame(tag_canvas)

        self.tag_inner.bind("<Configure>",
                            lambda e: tag_canvas.configure(scrollregion=tag_canvas.bbox("all")))
        tag_canvas.create_window((0, 0), window=self.tag_inner, anchor="nw")
        tag_canvas.configure(yscrollcommand=tag_scroll.set)

        tag_scroll.pack(side=tk.RIGHT, fill=tk.Y)
        tag_canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=10)

        self.tag_vars = []
        for tag in merged:
            var = tk.BooleanVar(value=True)
            self.tag_vars.append((var, tag))
            ttk.Checkbutton(self.tag_inner, text=tag, variable=var).pack(anchor=tk.W)

        # Boutons
        btn_row = ttk.Frame(frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda: [v.set(True) for v, _ in self.tag_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda: [v.set(False) for v, _ in self.tag_vars]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # URLs ‚Äî tableau
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_urls_section(self):
        data = self.merged_data.get("urls", {})
        merged = data.get("merged", {})

        frame = self._section_frame("üîó URLs")

        if not merged:
            ttk.Label(frame, text="Aucune URL trouv√©e").pack(anchor=tk.W)
            self.selections["urls"] = {}
            return

        ttk.Label(frame, text=f"{len(merged)} URLs agr√©g√©es",
                  font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, pady=(0, 5))

        self.url_vars = []
        for key, url in sorted(merged.items()):
            var = tk.BooleanVar(value=True)
            self.url_vars.append((var, key, url))
            row = ttk.Frame(frame)
            row.pack(fill=tk.X, padx=10, pady=1)
            ttk.Checkbutton(row, variable=var).pack(side=tk.LEFT)
            ttk.Label(row, text=f"{key}:", width=12, anchor=tk.W,
                      font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
            ttk.Label(row, text=url, font=("Segoe UI", 9)).pack(side=tk.LEFT, fill=tk.X)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Helpers
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _section_frame(self, title: str) -> ttk.Frame:
        """Cr√©er un cadre de section avec titre."""
        sep = ttk.Separator(self.scroll_frame, orient=tk.HORIZONTAL)
        sep.pack(fill=tk.X, padx=5, pady=(10, 5))

        lbl = ttk.Label(self.scroll_frame, text=title, 
                        font=("Segoe UI", 11, "bold"))
        lbl.pack(anchor=tk.W, padx=10)

        frame = ttk.Frame(self.scroll_frame, padding=(10, 5))
        frame.pack(fill=tk.X, padx=5)
        return frame

    def _apply_all(self):
        """Collecter toutes les s√©lections et fermer."""
        self.result = {}

        # Awards
        if hasattr(self, 'award_vars'):
            self.result["awards"] = [a for v, a in self.award_vars if v.get()]
        else:
            self.result["awards"] = []

        # Trivia
        if hasattr(self, 'trivia_var'):
            src = self.trivia_var.get()
            by_source = self.merged_data.get("trivia", {}).get("by_source", {})
            self.result["trivia"] = by_source.get(src)
        else:
            self.result["trivia"] = None

        # Details
        if hasattr(self, 'details_var'):
            src = self.details_var.get()
            details_data = self.merged_data.get("details", {})
            if src == "_fused_":
                self.result["details"] = details_data.get("fused")
            else:
                self.result["details"] = details_data.get("by_source", {}).get(src)
        else:
            self.result["details"] = None

        # Tattoos
        if hasattr(self, 'tattoos_vars'):
            self.result["tattoos"] = [item for v, item in self.tattoos_vars if v.get()]
        else:
            self.result["tattoos"] = []

        # Piercings
        if hasattr(self, 'piercings_vars'):
            self.result["piercings"] = [item for v, item in self.piercings_vars if v.get()]
        else:
            self.result["piercings"] = []

        # Tags
        if hasattr(self, 'tag_vars'):
            self.result["tags"] = [t for v, t in self.tag_vars if v.get()]
        else:
            self.result["tags"] = []

        # URLs
        if hasattr(self, 'url_vars'):
            self.result["urls"] = {k: u for v, k, u in self.url_vars if v.get()}
        else:
            self.result["urls"] = {}

        self.destroy()


============================================================
[46/124] Legacy\gui\validation_window.py
------------------------------------------------------------
"""
ValidationWindow ‚Äî GUI 3/3 du flux Bio IA
==========================================
R√©capitulatif final avant injection dans Stash.

Layout :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  HEADER : performer + breadcrumb 3/3                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  R√âCAPITULATIF 40%      ‚îÇ  BIO FINALE 60%                            ‚îÇ
‚îÇ                         ‚îÇ                                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ ‚úÖ √Ä injecter ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îå‚îÄ‚îÄ ‚úçÔ∏è Biographie (√©ditable) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Trivia : ‚úì/‚úó      ‚îÇ ‚îÇ  ‚îÇ                                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Awards : N        ‚îÇ ‚îÇ  ‚îÇ  Grande zone de texte √©ditable        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Tatouages : N     ‚îÇ ‚îÇ  ‚îÇ  avec scroll                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Piercings : N     ‚îÇ ‚îÇ  ‚îÇ                                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Tags : N          ‚îÇ ‚îÇ  ‚îÇ                                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  URLs : N          ‚îÇ ‚îÇ  ‚îÇ                                       ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                         ‚îÇ  [üìã Copier] [üóë Effacer] [N chars]        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ ‚ö†Ô∏è Champs DB ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ                                            ‚îÇ
‚îÇ  ‚îÇ  Mapping injection ‚îÇ ‚îÇ                                            ‚îÇ
‚îÇ  ‚îÇ  avant/apr√®s       ‚îÇ ‚îÇ                                            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  [‚Üê Retour]  [‚úñ Annuler]              [‚úÖ INJECTER DANS STASH]       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""
import tkinter as tk
from tkinter import ttk, messagebox
import platform

# ‚îÄ‚îÄ Palette ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
P = {
    "bg":         "#13131f",
    "surface":    "#1e1e30",
    "card":       "#22223a",
    "card_hdr":   "#2c2c4a",
    "border":     "#3a3a58",
    "accent":     "#7c6af7",
    "success":    "#4caf7d",
    "success_dk": "#2a6a4a",
    "danger":     "#c05050",
    "warn":       "#d4a940",
    "text":       "#e8e8f5",
    "muted":      "#8888aa",
    "dim":        "#55557a",
    "bio_bg":     "#0d0d1e",
    "added":      "#1a3a1a",   # fond items ajout√©s
    "removed":    "#3a1a1a",   # fond items supprim√©s
}

FH1  = ("Segoe UI", 14, "bold")
FH2  = ("Segoe UI", 11, "bold")
FH3  = ("Segoe UI", 9,  "bold")
FB   = ("Segoe UI", 10)
FSM  = ("Segoe UI", 8)
FSMB = ("Segoe UI", 8,  "bold")


class ValidationWindow(tk.Toplevel):
    """
    Fen√™tre 3/3 : validation finale et injection dans Stash.
    """
    def __init__(self, parent, db_data, stash_ctx,
                 review_result, bio_result):
        super().__init__(parent)
        self.title("‚úÖ Validation & Injection ‚Äî √âtape 3/3")
        self.configure(bg=P["bg"])

        self.db_data       = db_data
        self.stash_ctx     = stash_ctx
        self.review_result = review_result   # dict depuis GUI 1
        self.bio_result    = bio_result      # dict {'bio': str} depuis GUI 2

        self.result        = None   # 'injected' | None

        _fullscreen(self)
        self.transient(parent)
        self.grab_set()

        self._build_ui()
        self._populate_summary()
        self._populate_bio()

        self.bind("<Escape>", lambda _: self._cancel())
        self.wait_window()

    # ‚îÄ‚îÄ UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_ui(self):
        self._build_header()

        body = tk.Frame(self, bg=P["bg"])
        body.pack(fill=tk.BOTH, expand=True, padx=8, pady=6)
        body.grid_columnconfigure(0, weight=38)
        body.grid_columnconfigure(1, weight=62)
        body.grid_rowconfigure(0, weight=1)

        self._build_summary_panel(body)
        self._build_bio_panel(body)

        self._build_footer()

    def _build_header(self):
        hdr = tk.Frame(self, bg=P["success"], pady=0)
        hdr.pack(fill=tk.X)
        tk.Frame(hdr, bg="#6adf9d", height=3).pack(fill=tk.X)

        row = tk.Frame(hdr, bg=P["success"], pady=7)
        row.pack(fill=tk.X, padx=12)

        tk.Label(row, text="‚úÖ", font=("Segoe UI", 18),
                 fg="white", bg=P["success"]).pack(side=tk.LEFT, padx=(0, 8))

        name = self.db_data.get("name", "Performer inconnu")
        tk.Label(row, text=f"Validation & Injection ‚Äî {name}",
                 font=FH1, fg="white", bg=P["success"]).pack(side=tk.LEFT)

        # Breadcrumb
        bc = tk.Frame(row, bg=P["success"])
        bc.pack(side=tk.RIGHT, padx=12)
        for num, lbl, active in [("1","Donn√©es",False),("2","Bio IA",False),("3","Valider",True)]:
            bg = P["success_dk"] if active else P["success"]
            fg = "white"         if active else "#aaffcc"
            tk.Label(bc, text=f" {num} ", font=FSMB, fg=fg,
                     bg=bg, padx=6, pady=3).pack(side=tk.LEFT, padx=1)
            tk.Label(bc, text=lbl, font=FSM, fg=fg,
                     bg=P["success"]).pack(side=tk.LEFT, padx=(0, 8))

    # ‚îÄ‚îÄ Panneau gauche : R√©capitulatif ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_summary_panel(self, parent):
        panel = tk.Frame(parent, bg=P["surface"])
        panel.grid(row=0, column=0, sticky="nsew", padx=(0, 5))

        _hdr(panel, "üìä R√©capitulatif des modifications")

        # Scrollable
        canvas = tk.Canvas(panel, bg=P["surface"], highlightthickness=0)
        sb     = ttk.Scrollbar(panel, orient=tk.VERTICAL, command=canvas.yview)
        self._summary_inner = tk.Frame(canvas, bg=P["surface"])

        self._summary_inner.bind("<Configure>", lambda e: canvas.configure(
            scrollregion=canvas.bbox("all")
        ))
        win = canvas.create_window((0, 0), window=self._summary_inner, anchor="nw")
        canvas.configure(yscrollcommand=sb.set)
        canvas.bind("<Configure>", lambda e: canvas.itemconfig(win, width=e.width))
        canvas.bind_all("<MouseWheel>",
                        lambda e: canvas.yview_scroll(int(-1*(e.delta/120)), "units"))

        sb.pack(side=tk.RIGHT, fill=tk.Y)
        canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

    def _populate_summary(self):
        parent = self._summary_inner
        rv = self.review_result or {}
        db = self.db_data

        def section(title, icon, items_new, items_old=None, text_mode=False):
            """Carte r√©capitulatif pour un champ."""
            card = tk.Frame(parent, bg=P["card"], pady=0)
            card.pack(fill=tk.X, padx=6, pady=4)
            tk.Frame(card, bg=P["accent"], width=3).pack(side=tk.LEFT, fill=tk.Y)

            inner = tk.Frame(card, bg=P["card"])
            inner.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

            # En-t√™te
            hf = tk.Frame(inner, bg=P["card_hdr"], pady=4)
            hf.pack(fill=tk.X)
            tk.Label(hf, text=f"  {icon}  {title}", font=FH3,
                     fg=P["text"], bg=P["card_hdr"], anchor="w").pack(side=tk.LEFT, padx=4)

            if text_mode:
                # Champ texte court
                n = len(items_new) if items_new else 0
                status = f"‚úì {n} chars" if n > 0 else "‚Äî vide"
                color  = P["success"] if n > 0 else P["muted"]
                tk.Label(hf, text=status, font=FSMB,
                         fg=color, bg=P["card_hdr"]).pack(side=tk.RIGHT, padx=8)
                if items_new:
                    preview = items_new[:120] + ("‚Ä¶" if len(items_new) > 120 else "")
                    tk.Label(inner, text=preview, font=FSM,
                             fg=P["muted"], bg=P["card"],
                             wraplength=320, justify="left",
                             anchor="w", padx=8, pady=4).pack(fill=tk.X)
            else:
                # Liste d'√©l√©ments
                n_new = len(items_new) if items_new else 0
                n_old = len(items_old) if items_old else 0
                added = n_new - n_old if n_new > n_old else 0
                status = f"+{added} ¬∑ total {n_new}" if added else f"{n_new} √©l√©ments"
                color  = P["success"] if n_new > 0 else P["muted"]
                tk.Label(hf, text=status, font=FSMB,
                         fg=color, bg=P["card_hdr"]).pack(side=tk.RIGHT, padx=8)

                body = tk.Frame(inner, bg=P["card"], padx=8, pady=4)
                body.pack(fill=tk.X)

                display = items_new[:8] if items_new else []
                for it in display:
                    label = it if isinstance(it, str) else \
                            (f"{it.get('position','')} ({it.get('description','')})"
                             if it.get("description") else it.get("position", str(it)))
                    tk.Label(body, text=f"  ‚úì {label}", font=FSM,
                             fg=P["text"], bg=P["card"], anchor="w").pack(fill=tk.X)
                if items_new and len(items_new) > 8:
                    tk.Label(body, text=f"  ‚Ä¶ +{len(items_new)-8} autres",
                             font=FSM, fg=P["muted"], bg=P["card"],
                             anchor="w").pack(fill=tk.X)
                if not items_new:
                    tk.Label(body, text="  ‚Äî aucun √©l√©ment", font=FSM,
                             fg=P["dim"], bg=P["card"], anchor="w").pack(fill=tk.X)

        # Sections
        section("Trivia",     "üìù", rv.get("trivia", ""),
                text_mode=True)
        section("Awards",     "üèÜ", rv.get("awards", []),
                items_old=db.get("awards", []))
        section("Tatouages",  "üé®", rv.get("tattoos", []),
                items_old=[])
        section("Piercings",  "üíâ", rv.get("piercings", []),
                items_old=[])
        section("Tags",       "üè∑Ô∏è", rv.get("tags", []),
                items_old=db.get("tags", []))

        # URLs : dict ‚Üí liste de strings
        urls_new = list((rv.get("urls") or {}).keys())
        urls_old = db.get("urls", [])
        section("URLs",       "üîó", urls_new, items_old=urls_old)

        # Bio
        bio = (self.bio_result or {}).get("bio", "")
        section("Biographie", "‚úçÔ∏è", bio, text_mode=True)

        # S√©parateur + info DB
        tk.Frame(parent, bg=P["border"], height=1).pack(fill=tk.X, padx=6, pady=8)

        info = tk.Frame(parent, bg=P["card"], padx=10, pady=8)
        info.pack(fill=tk.X, padx=6, pady=2)
        tk.Label(info, text="‚ÑπÔ∏è  Champs qui seront mis √† jour dans Stash :",
                 font=FH3, fg=P["muted"], bg=P["card"]).pack(anchor="w")
        fields = []
        if rv.get("trivia"):           fields.append("details (trivia)")
        if rv.get("awards"):           fields.append("custom_fields (awards)")
        if rv.get("tattoos"):          fields.append("tattoos")
        if rv.get("piercings"):        fields.append("piercings")
        if rv.get("tags"):             fields.append("tags")
        if rv.get("urls"):             fields.append("performer_urls")
        if (self.bio_result or {}).get("bio"): fields.append("details (bio)")
        for f in fields:
            tk.Label(info, text=f"  ‚Ä¢ {f}", font=FSM,
                     fg=P["text"], bg=P["card"]).pack(anchor="w")

    # ‚îÄ‚îÄ Panneau droit : Bio finale ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_bio_panel(self, parent):
        panel = tk.Frame(parent, bg=P["surface"])
        panel.grid(row=0, column=1, sticky="nsew", padx=(5, 0))

        _hdr(panel, "‚úçÔ∏è Biographie finale (√©ditable)")

        # Zone texte
        txt_f = tk.Frame(panel, bg=P["bio_bg"])
        txt_f.pack(fill=tk.BOTH, expand=True, padx=4, pady=(4, 0))

        self._txt_bio = tk.Text(
            txt_f, wrap=tk.WORD, font=("Segoe UI", 10),
            bg=P["bio_bg"], fg=P["text"],
            insertbackground=P["text"],
            relief=tk.FLAT, padx=14, pady=12,
            undo=True,
        )
        sb = ttk.Scrollbar(txt_f, command=self._txt_bio.yview)
        self._txt_bio.configure(yscrollcommand=sb.set)
        self._txt_bio.bind("<KeyRelease>", self._update_char)
        sb.pack(side=tk.RIGHT, fill=tk.Y)
        self._txt_bio.pack(fill=tk.BOTH, expand=True)

        # Barre sous l'√©diteur
        bio_bar = tk.Frame(panel, bg=P["surface"], pady=5)
        bio_bar.pack(fill=tk.X, padx=4)

        self._char_lbl = tk.Label(bio_bar, text="0 chars",
                                   font=FSM, fg=P["muted"], bg=P["surface"])
        self._char_lbl.pack(side=tk.RIGHT, padx=8)

        _mini_btn(bio_bar, "üìã Copier", self._copy_bio)
        _mini_btn(bio_bar, "üóë Effacer", self._clear_bio)
        _mini_btn(bio_bar, "‚Ü∫ Restaurer", self._restore_bio)

    def _populate_bio(self):
        bio = (self.bio_result or {}).get("bio", "")
        self._original_bio = bio
        if bio:
            self._txt_bio.insert("1.0", bio)
        else:
            self._txt_bio.insert("1.0",
                "Aucune biographie g√©n√©r√©e.\n\n"
                "Vous pouvez en saisir une manuellement ici, ou retourner √† l'√©tape 2.")
        self._update_char()

    def _update_char(self, event=None):
        n = len(self._txt_bio.get("1.0", tk.END).strip())
        color = P["success"] if 2500 <= n <= 4000 else \
                P["warn"]   if 100  <= n < 2500   else P["muted"]
        self._char_lbl.config(text=f"{n} chars", fg=color)

    def _copy_bio(self):
        text = self._txt_bio.get("1.0", tk.END).strip()
        if text:
            self.clipboard_clear()
            self.clipboard_append(text)

    def _clear_bio(self):
        if messagebox.askyesno("Effacer", "Effacer la biographie ?"):
            self._txt_bio.delete("1.0", tk.END)
            self._update_char()

    def _restore_bio(self):
        self._txt_bio.delete("1.0", tk.END)
        self._txt_bio.insert("1.0", self._original_bio or "")
        self._update_char()

    # ‚îÄ‚îÄ Footer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_footer(self):
        tk.Frame(self, bg=P["border"], height=1).pack(fill=tk.X)
        bar = tk.Frame(self, bg=P["surface"], pady=10)
        bar.pack(fill=tk.X, padx=12)

        _action_btn(bar, "‚Üê Retour",  P["dim"],    self._go_back, side=tk.LEFT)
        _action_btn(bar, "‚úñ Annuler", P["danger"], self._cancel,  side=tk.LEFT, padx=6)

        # Avertissement
        tk.Label(
            bar,
            text="‚ö†Ô∏è  L'injection modifie directement la base Stash ‚Äî op√©ration irr√©versible",
            font=FSM, fg=P["warn"], bg=P["surface"],
        ).pack(side=tk.LEFT, padx=20)

        _action_btn(bar, "‚úÖ  INJECTER DANS STASH",
                    P["success"], self._inject, side=tk.RIGHT, padx=8)

    # ‚îÄ‚îÄ Injection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _inject(self):
        bio = self._txt_bio.get("1.0", tk.END).strip()
        rv  = self.review_result or {}

        # R√©sum√© de confirmation
        parts = []
        if rv.get("trivia"):  parts.append(f"Trivia ({len(rv['trivia'])} chars)")
        if rv.get("awards"):  parts.append(f"{len(rv['awards'])} awards")
        if rv.get("tattoos"): parts.append(f"{len(rv['tattoos'])} tatouages")
        if rv.get("piercings"):parts.append(f"{len(rv['piercings'])} piercings")
        if rv.get("tags"):    parts.append(f"{len(rv['tags'])} tags")
        if rv.get("urls"):    parts.append(f"{len(rv['urls'])} URLs")
        if bio:               parts.append(f"Bio ({len(bio)} chars)")

        if not parts:
            messagebox.showwarning("Rien √† injecter",
                                   "Aucune donn√©e s√©lectionn√©e.")
            return

        confirm = messagebox.askyesno(
            "Confirmer l'injection",
            f"Vous allez injecter dans Stash :\n\n"
            + "\n".join(f"  ‚Ä¢ {p}" for p in parts)
            + f"\n\nPerformer : {self.db_data.get('name')}"
            + f"\nID : {self.db_data.get('id')}"
            + "\n\nCette action est irr√©versible. Continuer ?",
            icon="warning",
        )
        if not confirm:
            return

        try:
            self._do_injection(rv, bio)
            messagebox.showinfo(
                "‚úÖ Injection r√©ussie",
                f"Les donn√©es ont √©t√© inject√©es avec succ√®s dans Stash.\n\n"
                + "\n".join(f"  ‚úì {p}" for p in parts),
            )
            self.result = "injected"
            self.destroy()

        except Exception as e:
            messagebox.showerror(
                "Erreur d'injection",
                f"Une erreur est survenue lors de l'injection :\n\n{e}"
            )

    def _do_injection(self, rv: dict, bio: str):
        """Injecte les donn√©es dans la DB Stash."""
        from services.db import PerformerDB
        from utils.body_art_parser import parse_body_art

        performer_id = self.db_data.get("id")
        if not performer_id:
            raise ValueError("ID performer manquant")

        db = PerformerDB()
        try:
            updates = {}

            # ‚îÄ‚îÄ Bio / Details ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            if bio:
                updates["details"] = bio

            # ‚îÄ‚îÄ Tatouages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            tattoos = rv.get("tattoos", [])
            if tattoos:
                def _fmt_item(it):
                    pos  = it.get("position", "")
                    desc = it.get("description", "")
                    return f"{pos} ({desc})" if desc else pos
                updates["tattoos"] = "; ".join(_fmt_item(t) for t in tattoos)

            # ‚îÄ‚îÄ Piercings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            piercings = rv.get("piercings", [])
            if piercings:
                def _fmt_item(it):
                    pos  = it.get("position", "")
                    desc = it.get("description", "")
                    return f"{pos} ({desc})" if desc else pos
                updates["piercings"] = "; ".join(_fmt_item(p) for p in piercings)

            # ‚îÄ‚îÄ Appliquer updates performer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            if updates:
                db.update_performer(performer_id, updates)

            # ‚îÄ‚îÄ Tags ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            tags = rv.get("tags", [])
            if tags:
                db.update_performer_tags(performer_id, tags)

            # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            urls = rv.get("urls", {})
            if urls:
                url_list = list(urls.values())
                db.update_performer_urls(performer_id, url_list)

            # ‚îÄ‚îÄ Awards & Trivia ‚Üí custom fields ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            custom = []
            if rv.get("awards"):
                for a in rv["awards"]:
                    custom.append({"type": "award", "value": a})
            if rv.get("trivia"):
                custom.append({"type": "trivia", "value": rv["trivia"]})
            if custom:
                try:
                    from utils.customfield_utils import inject_custom_fields
                    inject_custom_fields(db, performer_id, custom)
                except Exception as e:
                    print(f"[Injection] Warning custom fields : {e}")

        finally:
            db.close()

    def _go_back(self):
        self.result = None
        self.destroy()

    def _cancel(self):
        self.result = None
        self.destroy()


# ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _fullscreen(win):
    if platform.system() == "Windows":
        win.state("zoomed")
    elif platform.system() == "Linux":
        try:
            win.attributes("-zoomed", True)
        except Exception:
            win.geometry(f"{win.winfo_screenwidth()}x{win.winfo_screenheight()}+0+0")
    else:
        win.attributes("-fullscreen", True)


def _hdr(parent, title):
    f = tk.Frame(parent, bg="#2c2c4a", pady=7)
    f.pack(fill=tk.X)
    tk.Label(f, text=f"  {title}", font=("Segoe UI", 11, "bold"),
             fg="#e8e8f5", bg="#2c2c4a", anchor="w").pack(side=tk.LEFT, padx=8)
    tk.Frame(parent, bg="#3a3a58", height=1).pack(fill=tk.X)


def _mini_btn(parent, text, cmd):
    b = tk.Button(parent, text=text, command=cmd,
                  font=("Segoe UI", 8), bg="#2c2c4a", fg="#e8e8f5",
                  relief=tk.FLAT, padx=8, pady=3, cursor="hand2",
                  activebackground="#3a3a58")
    b.pack(side=tk.LEFT, padx=3)


def _action_btn(parent, text, bg, cmd, side=tk.RIGHT, padx=6):
    b = tk.Button(parent, text=text, command=cmd,
                  font=("Segoe UI", 9, "bold"), bg=bg, fg="white",
                  relief=tk.FLAT, padx=16, pady=8, cursor="hand2",
                  activebackground=bg, activeforeground="white")
    b.pack(side=side, padx=padx)


============================================================
[47/124] Legacy\main.py
------------------------------------------------------------
from gui.launcher import start_launcher

if __name__ == "__main__":
    start_launcher()


============================================================
[48/124] Legacy\README.md
------------------------------------------------------------
# StashMaster V2

Gestion modulaire des m√©tadonn√©es performers (phase 1/phase 2).

- Phase 1 : m√©tadonn√©es usuelles (Nom, D√©sambigu√Øsation, Alias, Date de naissance, Date du d√©c√®s, Pays, Ethnicit√©, Couleur des cheveux, Couleur des yeux, Taille, Poids, Mensurations, Faux seins, Dur√©e de carri√®re)
- Phase 2 : champs avanc√©s (Bio, Trivia, Awards, Tattoos, Piercings, Tags, URLs, Details)

Structure pr√™te pour extensions (group, scene, etc.).


============================================================
[49/124] Legacy\requirements.txt
------------------------------------------------------------
# tkinter is part of stdlib, not a pip package
sv_ttk
requests
pillow
loguru
pydantic
pyyaml
lxml
beautifulsoup4
google-generativeai


============================================================
[50/124] Legacy\services\__init__.py
------------------------------------------------------------


============================================================
[51/124] Legacy\services\bio_generator.py
------------------------------------------------------------
"""
BioGenerator ‚Äî G√©n√®re une bio professionnelle via IA √† partir des donn√©es V2.
Utilise Gemini (API REST) ou Ollama (fallback local).
"""
import os
import json
import urllib.request

GEMINI_MODEL = "gemini-2.0-flash"  # Flash = moins cher, suffisant
GEMINI_API_URL="***MASKED***"

SYSTEM_PROMPT = """Tu es un r√©dacteur expert pour une base de donn√©es de films pour adultes.
Ton objectif est de r√©diger une biographie structur√©e et professionnelle en FRAN√áAIS (Qu√©bec) pour l'artiste, bas√©e sur les faits fournis.

Respecte scrupuleusement le format suivant (avec les √©mojis et le gras) :

### [Nom] : [Titre accrocheur]

**Introduction**
[Paragraphe introductif : identit√©, dates cl√©s, pseudonymes, r√©sum√© de carri√®re.]

**üìÖ Origines et Premiers Pas**
[D√©tails sur les origines, la jeunesse, et l'entr√©e dans l'industrie.]

**üèÜ Carri√®re et Filmographie**
[Parcours professionnel, studios majeurs, √©volution, sc√®nes notables.]

**üí° Faits Int√©ressants & Vie Personnelle**
[Vie hors cam√©ra, personnalit√©, anecdotes, r√©seaux sociaux.]

**üëó Apparence et Style**
[Attributs physiques, style, tatouages, piercings.]

**üèÜ Prix et Distinctions**
[R√©compenses et nominations. Si aucune, mentionner la popularit√©.]

**Conclusion rapide**
[Phrase de conclusion sur l'h√©ritage de l'artiste.]

R√®gles imp√©ratives :
- N'invente AUCUN fait. Base-toi uniquement sur les donn√©es fournies (contexte Stash, donn√©es scrap√©es).
- Si une information est manquante pour une section, r√©dige une phrase g√©n√©rale ou passe bri√®vement, mais garde la section.
- Tu DOIS inclure TOUTES les 7 sections (Introduction, Origines, Carri√®re, Faits, Apparence, Prix, Conclusion).
- Le ton doit √™tre informatif, fluide et agr√©able √† lire (style encyclop√©dique/journalistique).
- Utilise le fran√ßais standard du Qu√©bec.
"""


class BioGenerator:
    def __init__(self, gemini_key: str | None = None, ollama_url: str = "http://localhost:11434"):
        self.gemini_key = gemini_key or self._load_gemini_key()
        self.ollama_url = ollama_url

    def _load_gemini_key(self) -> str | None:
        # Chercher le .gemini_key √† la racine du projet
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
key_path="***MASKED***"

        if os.path.exists(key_path):
            with open(key_path, 'r') as f:
                return f.read().strip()
        
        # Fallback pour le chemin d'origine si n√©cessaire (peut √™tre retir√© plus tard)
        legacy_path = r"F:\V2\.gemini_key"
        if os.path.exists(legacy_path):
             with open(legacy_path, 'r') as f:
                return f.read().strip()

        return None

    def build_context_from_v2(
        self,
        db_data: dict,
        stash_ctx: dict,
        scraped_results: list[dict],
        merged_data: dict,
        checked_fields: list[str],
    ) -> dict:
        """
        Construit le contexte de g√©n√©ration depuis les donn√©es V2.
        
        Args:
            db_data:         donn√©es DB du performer (V2 PerformerDB)
            stash_ctx:       contexte Stash (sc√®nes, studios, collabs)
            scraped_results: r√©sultats bruts des extracteurs Phase 2
            merged_data:     r√©sultats fusionn√©s Phase2Merger
            checked_fields:  champs coch√©s par l'utilisateur
        """
        ctx = {
            "name":      db_data.get("name", "Unknown"),
            "birthdate": db_data.get("birthdate") if "Birthdate" in checked_fields else None,
            "details":   db_data.get("details"),
            "mini_bios": [],
            "trivia":    [],
            "scene_count":    stash_ctx.get("scene_count", 0),
            "studios":        stash_ctx.get("studios", [])[:10],
            "collaborators":  [c["name"] for c in stash_ctx.get("collaborators", [])[:5]],
            "awards":         [],
            "socials":        [],
            "career_length":  None,
        }

        # Bios scrap√©es (Details)
        if "Details" in checked_fields:
            for src, bio_text in merged_data.get("details", {}).get("by_source", {}).items():
                if bio_text and len(bio_text) > 50:
                    ctx["mini_bios"].append(f"[{src.upper()}] {bio_text[:600]}")

        # D√©tection "Infos limit√©es" : Si aucune bio textuelle n'est trouv√©e, on autorise plus de specs techniques
        limited_info_mode = (len(ctx["mini_bios"]) == 0)

        # Helper pour r√©cup√©rer une valeur (DB > Merged/Scraped)
        def get_val(key):
            v = db_data.get(key)
            if not v and isinstance(merged_data, dict):
                v = merged_data.get(key)
            
            # Fallback : Chercher dans les r√©sultats bruts du scraping
            if not v and scraped_results:
                for res in scraped_results:
                    if res.get(key):
                        v = res.get(key)
                        break
            return v

        # Career Length (Phase 1)
        if "Career Length" in checked_fields:
            val = get_val("career_length")
            if val: ctx["career_length"] = val

        # 1. Origines & Apparence de base (Toujours utile pour la narration)
        base_specs = []
        if "Ethnicity" in checked_fields:
            val = get_val("ethnicity")
            if val: base_specs.append(f"Ethnicity: {val}")
        if "Country" in checked_fields:
            val = get_val("country")
            if val: base_specs.append(f"Country: {val}")
        if "Hair Color" in checked_fields:
            val = get_val("hair_color")
            if val:
                if isinstance(val, list):
                    val = val[0] if val else ""
                val = str(val).split(",")[0].strip() # Garder couleur principale
                if val: base_specs.append(f"Hair: {val}")
        if "Eye Color" in checked_fields:
            val = get_val("eye_color")
            if val: base_specs.append(f"Eyes: {val}")
        
        if base_specs:
            ctx["trivia"].append("Appearance/Origins: " + ", ".join(base_specs))

        # 2. Specs Techniques (Seulement si infos limit√©es pour √©viter l'effet "fiche technique")
        specs = []
        tech_map = {
            "Height":       ("height", "Height"),
            "Weight":       ("weight", "Weight"),
            "Measurements": ("measurements", "Measurements"),
            "Fake Tits":    ("fake_tits", "Fake Tits"),
        }
        
        if limited_info_mode:
            for field, (db_key, label) in tech_map.items():
                if field in checked_fields and db_data.get(db_key):
                    specs.append(f"{label}: {db_data[db_key]}")
                if field in checked_fields:
                    val = get_val(db_key)
                    if val: specs.append(f"{label}: {val}")
            
            if specs:
                ctx["trivia"].append("Physical Stats (Use to flesh out bio if needed): " + ", ".join(specs))

        # Awards (depuis merged_data)
        if "Awards" in checked_fields:
            awards = merged_data.get("awards", {}).get("merged", [])
            if awards:
                ctx["awards"] = awards  # Stocker la liste compl√®te pour le prompt

        # Aliases
        if "Aliases" in checked_fields and db_data.get("aliases"):
            aliases = db_data["aliases"]
            if isinstance(aliases, list):
                aliases = ", ".join(aliases)
            ctx["trivia"].append(f"Aliases: {aliases}")

        # URLs (Socials)
        if "URLs" in checked_fields:
            urls_data = merged_data.get("urls", {}).get("merged", {})
            # Filtrer pour ne garder que les r√©seaux sociaux principaux
social_keys="***MASKED***"
            found_socials = []
            for k, v in urls_data.items():
                if any(s in k.lower() for s in social_keys):
                    # On envoie "Twitter: http..." pour que l'IA puisse extraire le handle si elle veut
                    found_socials.append(f"{k} ({v})")
            if found_socials:
                ctx["socials"] = found_socials

        return ctx

    def build_prompt(self, ctx: dict) -> str:
        """Construit le prompt utilisateur depuis le contexte."""
        parts = [f"Performer: {ctx['name']}"]

        if ctx.get("birthdate"):
            parts.append(f"Born: {ctx['birthdate']}")

        if ctx.get("career_length"):
            parts.append(f"Years Active: {ctx['career_length']}")

        if ctx.get("trivia"):
            parts.append("\nKnown facts:")
            parts.extend(f"  - {t}" for t in ctx["trivia"])

        if ctx.get("scene_count"):
            parts.append(f"\nStash scenes: {ctx['scene_count']}")

        if ctx.get("studios"):
            parts.append(f"Studios worked with: {', '.join(ctx['studios'][:8])}")

        if ctx.get("collaborators"):
            parts.append(f"Frequent collaborators: {', '.join(ctx['collaborators'])}")

        if ctx.get("awards"):
            parts.append("\nAwards & Nominations:")
            # On envoie les 20 premiers pour donner de la mati√®re √† la section 'Prix'
            parts.append("; ".join(ctx["awards"][:20]))

        if ctx.get("socials"):
            parts.append(f"\nSocial Media: {', '.join(ctx['socials'])}")

        if ctx.get("mini_bios"):
            parts.append("\nSource bios for reference:")
            for bio in ctx["mini_bios"][:3]:  # max 3 sources
                parts.append(f"  {bio}")

        if ctx.get("details"):
            parts.append(f"\nCurrent Stash bio: {ctx['details'][:400]}")

        parts.append("\nR√©dige la biographie en fran√ßais (Qu√©bec) selon le mod√®le.")
        return "\n".join(parts)

    def generate(self, ctx: dict) -> str | None:
        """G√©n√®re la bio. Essaie Gemini d'abord, Ollama en fallback."""
        prompt = self.build_prompt(ctx)

        if self.gemini_key:
            result = self._call_gemini(prompt)
            if result:
                return result.strip()

        return self._call_ollama(prompt)

    def _call_gemini(self, user_prompt: str) -> str | None:
        url = GEMINI_API_URL.format(model=GEMINI_MODEL, key=self.gemini_key)
        payload = {
            "system_instruction": {"parts": [{"text": SYSTEM_PROMPT}]},
            "contents": [{"parts": [{"text": user_prompt}]}],
            "generationConfig": {
                "temperature": 0.7,
                "maxOutputTokens": 300,
            }
        }
        try:
            data = json.dumps(payload).encode()
            req = urllib.request.Request(url, data=data,
                                         headers={"Content-Type": "application/json"})
            with urllib.request.urlopen(req, timeout=20) as resp:
                result = json.loads(resp.read())
            return result["candidates"][0]["content"]["parts"][0]["text"]
        except Exception as e:
            print(f"[BioGenerator] Gemini error: {e}")
            return None

    def _call_ollama(self, user_prompt: str, model: str = "dolphin-llama3") -> str | None:
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user",   "content": user_prompt},
            ],
            "stream": False,
            "options": {"temperature": 0.7, "num_predict": 300}
        }
        try:
            data = json.dumps(payload).encode()
            req = urllib.request.Request(
                f"{self.ollama_url}/api/chat", data=data,
                headers={"Content-Type": "application/json"}
            )
            with urllib.request.urlopen(req, timeout=30) as resp:
                result = json.loads(resp.read())
            return result["message"]["content"]
        except Exception as e:
            print(f"[BioGenerator] Ollama error: {e}")
            return None


============================================================
[52/124] Legacy\services\db.py
------------------------------------------------------------
import sqlite3

DB_PATH = r"H:\Stash\stash-go.sqlite"

class PerformerDB:
    def __init__(self, db_path=DB_PATH):
        self.conn = sqlite3.connect(db_path)
        self.conn.row_factory = sqlite3.Row

    def get_performer_by_id(self, performer_id):
        cur = self.conn.cursor()
        cur.execute("SELECT * FROM performers WHERE id=?", (performer_id,))
        row = cur.fetchone()
        if not row:
            return None
        data = dict(row)
        # Extraire les alias
        try:
            cur.execute("SELECT alias FROM performer_aliases WHERE performer_id=?", (performer_id,))
            aliases_rows = cur.fetchall()
            aliases = [r['alias'] for r in aliases_rows]
            print(f"DEBUG: Found aliases for ID {performer_id}: {aliases}")
        except Exception as e:
            print(f"DEBUG: Error fetching aliases: {e}")
            aliases = []
        data["aliases"] = aliases
        # Extraire les URLs
        try:
            cur.execute("SELECT url FROM performer_urls WHERE performer_id=?", (performer_id,))
            urls_rows = cur.fetchall()
            urls = [r['url'] for r in urls_rows]
            data["urls"] = urls
        except Exception as e:
            print(f"DEBUG: Error fetching URLs: {e}")
            data["urls"] = []

        # Extraire les tags
        try:
            # Jointure avec la table tags pour obtenir les noms
            query = """
                SELECT t.name 
                FROM tags t
                JOIN performers_tags pt ON pt.tag_id = t.id
                WHERE pt.performer_id = ?
            """
            cur.execute(query, (performer_id,))
            tags_rows = cur.fetchall()
            tags = [r['name'] for r in tags_rows]
            data["tags"] = tags
        except Exception as e:
            print(f"DEBUG: Error fetching tags: {e}")
            data["tags"] = []

        # Extraire la biographie (d√©tails)
        data["bio"] = data.get("details", "")

        return data

    def get_performer_context(self, performer_id):
        """Extraire le contexte Stash complet d'un performer."""
        cur = self.conn.cursor()
        ctx = {"groups": [], "studios": [], "collaborators": [], "scene_count": 0}

        # Nombre de sc√®nes
        try:
            cur.execute(
                "SELECT COUNT(*) as cnt FROM performers_scenes WHERE performer_id=?",
                (performer_id,),
            )
            row = cur.fetchone()
            ctx["scene_count"] = row["cnt"] if row else 0
        except Exception:
            pass

        # Groups (DVDs) via performers_scenes ‚Üí groups_scenes ‚Üí groups
        try:
            cur.execute(
                """
                SELECT DISTINCT g.name
                FROM groups g
                JOIN groups_scenes gs ON gs.group_id = g.id
                JOIN performers_scenes ps ON ps.scene_id = gs.scene_id
                WHERE ps.performer_id = ?
                ORDER BY g.name
                """,
                (performer_id,),
            )
            ctx["groups"] = [r["name"] for r in cur.fetchall()]
        except Exception:
            pass

        # Studios via scenes
        try:
            cur.execute(
                """
                SELECT DISTINCT st.name
                FROM studios st
                JOIN scenes s ON s.studio_id = st.id
                JOIN performers_scenes ps ON ps.scene_id = s.id
                WHERE ps.performer_id = ?
                ORDER BY st.name
                """,
                (performer_id,),
            )
            ctx["studios"] = [r["name"] for r in cur.fetchall()]
        except Exception:
            pass

        # Top collaborateurs
        try:
            cur.execute(
                """
                SELECT p2.name, COUNT(*) as cnt
                FROM performers_scenes ps1
                JOIN performers_scenes ps2 ON ps1.scene_id = ps2.scene_id
                JOIN performers p2 ON p2.id = ps2.performer_id
                WHERE ps1.performer_id = ? AND ps2.performer_id != ?
                GROUP BY p2.id
                ORDER BY cnt DESC
                LIMIT 20
                """,
                (performer_id, performer_id),
            )
            ctx["collaborators"] = [
                {"name": r["name"], "count": r["cnt"]} for r in cur.fetchall()
            ]
        except Exception:
            pass

        return ctx

    def get_known_performers(self):
        """Retourne une liste de tous les noms de performers connus."""
        cur = self.conn.cursor()
        cur.execute("SELECT name FROM performers ORDER BY name")
        rows = cur.fetchall()
        return [r['name'] for r in rows]

    def close(self):
        self.conn.close()

    def inject_performer_metadata(self, performer_id: int, updates: dict) -> None:
        """
        Met √† jour les champs Phase 2 d'un performer.
        G√®re : details, tattoos, piercings, awards, trivia, tags, urls.
        """
        cur = self.conn.cursor()

        # Champs texte directs
        DIRECT = {"details", "tattoos", "piercings", "trivia", "death_date", "awards"}
        direct = {k: v for k, v in updates.items() if k in DIRECT and v is not None}
        if direct:
            set_clause = ", ".join(f"{k}=?" for k in direct)
            vals = list(direct.values()) + [performer_id]
            cur.execute(
                f"UPDATE performers SET {set_clause}, updated_at=datetime('now') WHERE id=?",
                vals
            )

        # URLs ‚Üí table performer_urls
        for url in updates.get("urls", []):
            cur.execute(
                "SELECT COUNT(*) FROM performer_urls WHERE performer_id=? AND url=?",
                (performer_id, url)
            )
            if cur.fetchone()[0] == 0:
                cur.execute(
                    "SELECT COALESCE(MAX(position),-1)+1 FROM performer_urls WHERE performer_id=?",
                    (performer_id,)
                )
                pos = cur.fetchone()[0]
                cur.execute(
                    "INSERT INTO performer_urls (performer_id, position, url) VALUES (?,?,?)",
                    (performer_id, pos, url)
                )

        # Tags ‚Üí table performers_tags (get-or-create)
        for tag_name in updates.get("tags", []):
            cur.execute("SELECT id FROM tags WHERE name=?", (tag_name,))
            row = cur.fetchone()
            tag_id = row[0] if row else None
            if not tag_id:
                cur.execute(
                    "INSERT INTO tags (name, created_at, updated_at, ignore_auto_tag) "
                    "VALUES (?,datetime('now'),datetime('now'),0)",
                    (tag_name,)
                )
                tag_id = cur.lastrowid
            cur.execute(
                "SELECT COUNT(*) FROM performers_tags WHERE performer_id=? AND tag_id=?",
                (performer_id, tag_id)
            )
            if cur.fetchone()[0] == 0:
                cur.execute(
                    "INSERT INTO performers_tags (performer_id, tag_id) VALUES (?,?)",
                    (performer_id, tag_id)
                )

        self.conn.commit()

class GroupDB:
    def __init__(self, db_path=DB_PATH):
        self.conn = sqlite3.connect(db_path)
        self.conn.row_factory = sqlite3.Row

    def get_group_by_id(self, group_id):
        cur = self.conn.cursor()
        cur.execute("SELECT g.*, s.name as studio_name FROM groups g LEFT JOIN studios s ON g.studio_id = s.id WHERE g.id=?", (group_id,))
        row = cur.fetchone()
        if not row:
            return None
        data = dict(row)

        # Extraire les URLs
        try:
            cur.execute("SELECT url FROM group_urls WHERE group_id=?", (group_id,))
            urls_rows = cur.fetchall()
            urls = [r['url'] for r in urls_rows]
            data["urls"] = urls
        except Exception as e:
            print(f"DEBUG: Error fetching group URLs: {e}")
            data["urls"] = []

        # Extraire les tags (√âtiquettes)
        try:
            query = """
                SELECT t.name 
                FROM tags t
                JOIN groups_tags gt ON gt.tag_id = t.id
                WHERE gt.group_id = ?
            """
            cur.execute(query, (group_id,))
            tags_rows = cur.fetchall()
            tags = [r['name'] for r in tags_rows]
            data["tags"] = tags
        except Exception as e:
            print(f"DEBUG: Error fetching group tags: {e}")
            data["tags"] = []

        return data

    def get_group_scenes(self, group_id: int) -> list[dict]:
        """
        R√©cup√®re les sc√®nes associ√©es √† un groupe, y compris leurs URLs.
        """
        cur = self.conn.cursor()
        query = """
            SELECT
                s.id AS scene_id,
                gs.scene_index,
                s.title AS scene_title,
                GROUP_CONCAT(su.url) AS existing_urls
            FROM
                groups_scenes gs
            JOIN
                scenes s ON gs.scene_id = s.id
            LEFT JOIN
                scene_urls su ON s.id = su.scene_id
            WHERE
                gs.group_id = ?
            GROUP BY
                s.id, gs.scene_index, s.title
            ORDER BY
                gs.scene_index
        """
        cur.execute(query, (group_id,))
        rows = cur.fetchall()
        
        scenes = []
        for row in rows:
            scene = dict(row)
            scene['existing_urls'] = scene['existing_urls'].split(',') if scene['existing_urls'] else []
            scenes.append(scene)
        return scenes

    def inject_scene_urls(self, scene_urls_to_inject: list[dict]) -> None:
        """
        Injecte les URLs de sc√®nes dans la base de donn√©es.
        Args:
            scene_urls_to_inject: Liste de dicts avec {'scene_id': int, 'url': str, 'source': str}
        """
        cur = self.conn.cursor()
        for item in scene_urls_to_inject:
            scene_id = item['scene_id']
            url = item['url']
            
            # V√©rifier si l'URL existe d√©j√† pour cette sc√®ne
            cur.execute(
                "SELECT COUNT(*) FROM scene_urls WHERE scene_id=? AND url=?",
                (scene_id, url)
            )
            if cur.fetchone()[0] == 0:
                cur.execute(
                    "INSERT INTO scene_urls (scene_id, url) VALUES (?,?)",
                    (scene_id, url)
                )
        self.conn.commit()


    def close(self):
        self.conn.close()


============================================================
[53/124] Legacy\services\extractors\__init__.py
------------------------------------------------------------


============================================================
[54/124] Legacy\services\extractors\babepedia.py
------------------------------------------------------------
"""
Extracteur Babepedia ‚Äî source pour bio courte, tattoos/piercings
avec parsing am√©lior√©, et tags.
"""
import re

from services.extractors.base import BaseExtractor
from utils.body_art_parser import parse_body_art


class BabepediaExtractor(BaseExtractor):
    SOURCE_NAME = "babepedia"

    def build_url(self, performer_name: str) -> str | None:
        """Construire l'URL Babepedia depuis le nom."""
        # Babepedia utilise le nom with espaces ‚Üí underscores
        slug = self._normalize_name_underscore(performer_name)
        return f"https://www.babepedia.com/babe/{slug}"

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)

        tree = self._fetch_tree(url)
        if tree is None:
            return data

        # ‚îÄ‚îÄ Phase 1 ‚Äî M√©tadonn√©es ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["name"] = self._get_text(tree, '//h1/text()') or self._get_text(tree, '//h1//text()')
        
        aliases_text = self._get_stat(tree, "Aliases") or self._get_stat(tree, "Also known as")
        if aliases_text:
            data["aliases"] = [a.strip() for a in aliases_text.split(',') if a.strip()]

        data["birthdate"] = self._get_stat(tree, "Born") or self._get_stat(tree, "Birthday")
        data["country"] = self._get_stat(tree, "Birthplace")
        data["ethnicity"] = self._get_stat(tree, "Ethnicity")
        data["hair_color"] = self._get_stat(tree, "Hair color") or self._get_stat(tree, "Hair")
        data["eye_color"] = self._get_stat(tree, "Eye color") or self._get_stat(tree, "Eyes")
        data["height"] = self._get_stat(tree, "Height")
        data["weight"] = self._get_stat(tree, "Weight")
        data["measurements"] = self._get_stat(tree, "Measurements")
        data["fake_tits"] = self._get_stat(tree, "Boobs")

        # ‚îÄ‚îÄ Bio / Details ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        bio_xpaths = [
            '//p[@id="biotext"]/text()',
            '//div[@id="bio"]//text()',
            '//div[contains(@class,"bio")]//text()',
        ]
        for xpath in bio_xpaths:
            texts = tree.xpath(xpath)
            if texts:
                bio = " ".join(t.strip() for t in texts if t.strip())
                if len(bio) > 20:
                    data["details"] = bio
                    break

        # ‚îÄ‚îÄ Tattoos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tattoo_text = self._get_stat(tree, "Tattoos")
        data["tattoos"] = parse_body_art(tattoo_text) if tattoo_text else []

        # ‚îÄ‚îÄ Piercings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        piercing_text = self._get_stat(tree, "Piercings")
        data["piercings"] = parse_body_art(piercing_text) if piercing_text else []

# ‚îÄ‚îÄ Tags ‚Äî strat√©gie multi-fallback ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tags = []

        # Strat√©gie 1 : meta keywords (le plus fiable)
        meta_kw = tree.xpath('//meta[@name="keywords"]/@content')
        if meta_kw and meta_kw[0].strip():
            tags = [t.strip() for t in meta_kw[0].split(',')
                    if t.strip() and len(t.strip()) > 2]

        # Strat√©gie 2 : liens /tag/ ou /category/ dans la page
        if not tags:
            raw = tree.xpath(
                '//a[contains(@href,"/tag/") or contains(@href,"/category/")]/text()'
            )
            tags = [t.strip() for t in raw if t.strip() and len(t.strip()) > 2]

        # Strat√©gie 3 : balises sp√©cifiques Babepedia
        if not tags:
            raw = tree.xpath('//span[@class="tag"]/text() | //div[@class="tags"]//text()')
            tags = [t.strip() for t in raw if t.strip() and len(t.strip()) > 2]

        data["tags"] = list(dict.fromkeys(tags))[:50]  # d√©dupliquer, limiter √† 50

        # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["urls"]["babepedia"] = url

        # Extraire liens vers d'autres databases
        links = tree.xpath('//a[contains(@href, "http")]/@href')
        for link in links:
            link = link.strip()
            if "iafd.com" in link:
                data["urls"]["iafd"] = link
            elif "freeones.com" in link:
                data["urls"]["freeones"] = link
            elif "thenude" in link:
                data["urls"]["thenude"] = link
            elif "twitter.com" in link or "x.com" in link:
                data["urls"]["twitter"] = link
            elif "instagram.com" in link:
                data["urls"]["instagram"] = link

        return data

    def _get_stat(self, tree, label: str) -> str | None:
        """Extraire une stat de Babepedia (format tableau)."""
        xpaths = [
            f'//td[contains(text(), "{label}")]/following-sibling::td[1]',
            f'//span[contains(text(), "{label}")]/following-sibling::span[1]',
            f'//li[contains(., "{label}")]',
            f'//div[contains(@class,"stat")]//*[contains(text(), "{label}")]/..',
        ]
        for xpath in xpaths:
            val = self._get_text(tree, xpath)
            if val:
                # Nettoyer le label du texte si pr√©sent
                val = re.sub(rf'^{label}\s*[:]\s*', '', val, flags=re.I).strip()
                if val:
                    return val
        return None


============================================================
[55/124] Legacy\services\extractors\base.py
------------------------------------------------------------
import re
import subprocess
from abc import ABC, abstractmethod

from lxml import html as lxml_html

from services.scrape_cache import ScrapeCache


class BaseExtractor(ABC):
    """
    Base commune √† tous les extracteurs de donn√©es performer.
    Chaque sous-classe impl√©mente extract_from_url() qui retourne
    un dict unifi√© Phase 2.
    """

    SOURCE_NAME: str = "base"
    HEADERS = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept-Language": "en-US,en;q=0.9",
    }
    TIMEOUT = 15

    # ‚îÄ‚îÄ M√©thode principale ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    @abstractmethod
    def extract_from_url(self, url: str) -> dict:
        """
        Extraire les donn√©es Phase 1 + Phase 2 depuis une URL.
        
        Retourne un dict unifi√© avec champs Phase 1 (m√©tadonn√©es)
        et Phase 2 (awards, trivia, details, tattoos, piercings, tags, urls).
        """
        ...

    def build_url(self, performer_name: str) -> str | None:
        """
        Construire l'URL d'un performer √† partir de son nom.
        √Ä surcharger dans les sous-classes.
        Retourne None si pas de logique de construction disponible.
        """
        return None

    # ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _empty_result(self, url: str) -> dict:
        """Retourner un r√©sultat vide avec les m√©tadonn√©es source."""
        return {
            "_source": self.SOURCE_NAME,
            "_url": url,
            # Phase 1 ‚Äî m√©tadonn√©es
            "name": None,
            "aliases": [],
            "birthdate": None,
            "death_date": None,
            "country": None,
            "ethnicity": None,
            "hair_color": None,
            "eye_color": None,
            "height": None,
            "weight": None,
            "measurements": None,
            "fake_tits": None,
            "career_length": None,
            # Phase 2 ‚Äî champs avanc√©s
            "awards": [],
            "trivia": None,
            "details": None,
            "tattoos": [],
            "piercings": [],
            "tags": [],
            "urls": {},
        }

    def _fetch_tree(self, url: str) -> lxml_html.HtmlElement | None:
        """
        T√©l√©charger une page HTML et retourner l'arbre lxml.
        Utilise le ScrapeCache si disponible.
        """
        try:
            command = ['curl.exe', '-L', '-s', '-A', self.HEADERS['User-Agent'], url]
            # Ex√©cuter curl et capturer la sortie binaire brute
            result = subprocess.run(command, capture_output=True, check=True, timeout=self.TIMEOUT)
            
            # Laisser lxml analyser le contenu binaire, il auto-d√©tectera l'encodage
            return lxml_html.fromstring(result.stdout)
        except subprocess.CalledProcessError as e:
            # D√©coder stderr pour l'impression, avec un fallback s√ªr
            error_message = e.stderr.decode('utf-8', errors='replace') if e.stderr else ''
            print(f"[{self.SOURCE_NAME}] Erreur fetch {url}: {error_message}")
            return None
        except Exception as e:
            print(f"[{self.SOURCE_NAME}] Erreur fetch {url}: {e}")
            return None

    def _get_text(self, tree: lxml_html.HtmlElement, xpath: str) -> str | None:
        """Extraire le texte d'un n≈ìud via XPath."""
        nodes = tree.xpath(xpath)
        if nodes:
            if isinstance(nodes[0], str):
                return nodes[0].strip() or None
            text = nodes[0].text_content().strip()
            return text if text else None
        return None

    def _get_texts(self, tree: lxml_html.HtmlElement, xpath: str) -> list[str]:
        """Extraire une liste de textes via XPath."""
        nodes = tree.xpath(xpath)
        result = []
        for n in nodes:
            if isinstance(n, str):
                t = n.strip()
            else:
                t = n.text_content().strip()
            if t:
                result.append(t)
        return result

    @staticmethod
    def _normalize_name(name: str) -> str:
        """Normaliser un nom pour construction d'URL (espaces ‚Üí tirets, lowercase)."""
        name = name.strip().lower()
        name = re.sub(r'[^a-z0-9\s-]', '', name)
        name = re.sub(r'\s+', '-', name)
        return name

    @staticmethod
    def _normalize_name_plus(name: str) -> str:
        """Normaliser un nom pour URL FreeOnes (espaces ‚Üí +)."""
        return name.strip().replace(' ', '+')

    @staticmethod
    def _normalize_name_underscore(name: str) -> str:
        """Normaliser un nom pour URL (espaces ‚Üí _)."""
        return name.strip().replace(' ', '_')


============================================================
[56/124] Legacy\services\extractors\dvd\__init__.py
------------------------------------------------------------


============================================================
[57/124] Legacy\services\extractors\dvd\adultempire_dvd.py
------------------------------------------------------------
"""
AdultEmpireDVDExtractor ‚Äî Source secondaire (P2).
Extrait aussi les URLs de clips/sc√®nes pour la Phase 2.
"""
import re
from services.extractors.dvd.base_dvd import BaseExtractorDVD


class AdultEmpireDVDExtractor(BaseExtractorDVD):
    SOURCE_NAME = "adultdvdempire"

    # Cookie requis pour acc√®s
    COOKIES = "ageConfirmed=true"

    def _fetch_tree(self, url: str):
        """Override pour injecter le cookie age."""
        cached = self.cache.get(url)
        if cached:
            from lxml import html as lxml_html
            return lxml_html.fromstring(cached)
        try:
            import subprocess
            from lxml import html as lxml_html
            result = subprocess.run(
                ["curl", "-sL", "--max-time", "15",
                 "-H", f"Cookie: {self.COOKIES}", url],
                capture_output=True, timeout=20
            )
            html = result.stdout.decode("utf-8", errors="replace")
            if html and len(html) > 500:
                self.cache.set(url, html)
                return lxml_html.fromstring(html)
        except Exception as e:
            print(f"[adultdvdempire] Fetch error: {e}")
        return None

    def search_urls(self, title: str, studio: str = "") -> list[str]:
        slug = re.sub(r'[^a-z0-9]+', '-', title.lower()).strip('-')
        return [
            f"https://www.adultdvdempire.com/{slug}.html",
            f"https://www.adultdvdempire.com/dvd/{slug}/",
        ]

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)
        tree = self._fetch_tree(url)
        if tree is None:
            return data

        data["title"] = (self._get_text(tree, '//h1[@class="title"]/text()') or 
                        self._get_text(tree, '//h1/text()'))

        data["studio"] = self._get_text(tree, '//a[contains(@href,"/studio/")]/text()')

        # Date ‚Äî format "Sep 22 2008"
        data["date"] = (self._get_text(tree, '//li[contains(text(),"Released")]/text()') or 
                       self._get_text(tree, '//*[contains(@class,"release-date")]/text()'))

        # Duration ‚Äî format "1 hrs. 55 mins."
        data["duration_raw"] = (self._get_text(tree, '//li[contains(text(),"Run Time")]/text()') or 
                                self._get_text(tree, '//*[contains(@class,"run-time")]/text()'))

        data["director"] = self._get_text(tree, '//a[contains(@href,"/director/")]/text()')

        # Synopsis ‚Äî strip "Show More"
        desc_nodes = tree.xpath('//*[contains(@class,"synopsis")]//text()')
        if desc_nodes:
            desc = " ".join(t.strip() for t in desc_nodes if t.strip()
                            and "show more" not in t.lower())
            data["description"] = desc[:2000] or None

        # Covers
        data["front_cover_url"] = (self._get_text(tree, '//img[contains(@class,"cover-front")]/@src') or 
                                   self._get_text(tree, '//div[@id="front-cover"]//img/@src'))
        data["back_cover_url"]  = (self._get_text(tree, '//img[contains(@class,"cover-back")]/@src') or 
                                   self._get_text(tree, '//div[@id="back-cover"]//img/@src'))

        # ‚îÄ‚îÄ PHASE 2 : extraction URLs sc√®nes via liens /clip/ ‚îÄ‚îÄ‚îÄ
        clip_links = tree.xpath('//a[contains(@href, "/clip/")]/@href')
        seen = set()
        for i, href in enumerate(clip_links):
            if href in seen:
                continue
            seen.add(href)
            scene_url = href if href.startswith("http") else f"https://www.adultdvdempire.com{href}"
            data["scenes"].append({
                "index": i + 1,
                "title": None,
                "url_adultdvdempire": scene_url,
            })

        return data


============================================================
[58/124] Legacy\services\extractors\dvd\base_dvd.py
------------------------------------------------------------
"""
BaseExtractorDVD ‚Äî Base commune pour tous les extracteurs de groupes/DVD.
Similaire √† BaseExtractor (performer) mais adapt√© aux m√©tadonn√©es group.
"""
from abc import ABC, abstractmethod
import re
import subprocess
from lxml import html as lxml_html
from services.scrape_cache import ScrapeCache


class BaseExtractorDVD(ABC):
    SOURCE_NAME = "unknown"

    def __init__(self):
        self.cache = ScrapeCache()

    def _fetch_tree(self, url: str):
        """Fetch HTML avec cache. Retourne lxml tree ou None."""
        cached = self.cache.get(url)
        if cached:
            return lxml_html.fromstring(cached)
        try:
            result = subprocess.run(
                ["curl", "-sL", "--max-time", "15", url],
                capture_output=True, timeout=20
            )
            html = result.stdout.decode("utf-8", errors="replace")
            if html and len(html) > 500:
                self.cache.set(url, html)
                return lxml_html.fromstring(html)
        except Exception as e:
            print(f"[{self.SOURCE_NAME}] Fetch error: {e}")
        return None

    def _get_text(self, tree, xpath: str) -> str | None:
        nodes = tree.xpath(xpath)
        if nodes:
            text = nodes[0] if isinstance(nodes[0], str) else nodes[0].text_content()
            return text.strip() or None
        return None

    def _empty_result(self, url: str) -> dict:
        """Retourne un dict vide standard pour un group."""
        return {
            "_source": self.SOURCE_NAME,
            "_url": url,
            # M√©tadonn√©es group (Phase 1)
            "title": None,
            "aliases": None,
            "date": None,
            "studio": None,
            "director": None,
            "duration_raw": None,   # cha√Æne brute ‚Äî conversion dans merger
            "description": None,
            "tags": [],
            "front_cover_url": None,
            "back_cover_url": None,
            # Sc√®nes extraites du DVD (Phase 2)
            "scenes": [],   # list[dict] avec cl√©s : index, title, url_source
        }

    @abstractmethod
    def extract_from_url(self, url: str) -> dict:
        """Scraper une URL de group et retourner le dict unifi√©."""
        ...

    def search_urls(self, title: str, studio: str = "") -> list[str]:
        """
        Recherche de l'URL du group sur la source.
        √Ä impl√©menter dans chaque sous-classe.
        Retourne une liste d'URLs candidates (max 5).
        """
        return []


============================================================
[59/124] Legacy\services\extractors\dvd\data18_dvd.py
------------------------------------------------------------
"""
Data18DVDExtractor ‚Äî Source prioritaire (P1) pour les m√©tadonn√©es group.
Extrait aussi les URLs de sc√®nes individuelles pour la Phase 2.
"""
import re
from services.extractors.dvd.base_dvd import BaseExtractorDVD


class Data18DVDExtractor(BaseExtractorDVD):
    SOURCE_NAME = "data18"

    def search_urls(self, title: str, studio: str = "") -> list[str]:
        """G√©n√®re des URLs candidates data18 pour un titre DVD."""
        slug = re.sub(r'[^a-z0-9]+', '-', title.lower()).strip('-')
        candidates = [
            f"https://www.data18.com/movies/{slug}",
            f"https://www.data18.com/content/{slug}",
            f"https://www.data18.com/movies/{slug}-dvd",
        ]
        
        # RESTAURATION DE LA RECHERCHE ACTIVE
        # Le site data18.com a un endpoint de recherche qui peut retourner une liste de r√©sultats
        try:
            search_query = title.strip().replace(' ', '+')
            search_url = f"https://www.data18.com/search?k={search_query}"
            
            tree = self._fetch_tree(search_url)
            if tree is not None:
                # R√©cup√©rer tous les liens qui ressemblent √† des films/DVDs
                # Pattern: /movies/ID-titre ou /content/ID-titre
                links = tree.xpath('//a[contains(@href, "/movies/")]/@href')
                links += tree.xpath('//a[contains(@href, "/content/")]/@href')
                
                # Aussi chercher dans les blocs gen11/gen12 typiques de data18
                links += tree.xpath('//div[contains(@class,"gen")]//a/@href')

                for link in links:
                    # Filtrer les faux positifs (scenes, tags, sites, etc.)
                    if any(x in link for x in ["/scenes/", "/tags/", "/sites/", "/pornstars/", "/studios/"]):
                        continue
                        
                    full_link = link if link.startswith("http") else f"https://www.data18.com{link}"
                    
                    # √âviter les doublons
                    if full_link not in candidates:
                        # Priorit√© : ins√©rer au d√©but si le titre est tr√®s proche (TODO)
                        # Pour l'instant on ajoute √† la fin
                        candidates.append(full_link)
        except Exception as e:
            print(f"[Data18] Search error: {e}")

        return candidates

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)
        tree = self._fetch_tree(url)
        if tree is None:
            return data

        # Titre
        data["title"] = self._get_text(tree, '//h1[@itemprop="name"]/text()') or \
                        self._get_text(tree, '//h1/text()')

        # Date ‚Äî plusieurs fallbacks
        date_raw = self._get_text(tree, '//span[contains(text(),"Release")]/following-sibling::text()[1]')
        if not date_raw:
            date_raw = self._get_text(tree, '//div[@class="gen-wrap"]//span[contains(@class,"date")]/text()')
        data["date"] = date_raw

        # Studio
        data["studio"] = self._get_text(tree, '//a[@itemprop="publisher"]/text()') or \
                         self._get_text(tree, '//span[@itemprop="publisher"]/text()')

        # Director
        data["director"] = self._get_text(tree, '//span[@itemprop="director"]/text()')

        # Duration ‚Äî format [HH:MM:SS]
        dur = self._get_text(tree, '//span[@itemprop="duration"]/text()') or \
              self._get_text(tree, '//*[contains(text(),"Run Time")]/following-sibling::text()[1]')
        data["duration_raw"] = dur

        # Description / Synopsis
        desc_nodes = tree.xpath('//div[@class="boxdesc"]//text()')
        if desc_nodes:
            desc = " ".join(t.strip() for t in desc_nodes if t.strip())
            # Couper avant <ul> ou <br><br>
            data["description"] = desc[:2000] if desc else None

        # Covers
        data["front_cover_url"] = self._get_text(tree, '//img[@id="frontbox"]/@src') or \
                                   self._get_text(tree, '//img[contains(@alt,"front")]/@src')
        data["back_cover_url"]  = self._get_text(tree, '//img[@id="backbox"]/@src') or \
                                   self._get_text(tree, '//img[contains(@alt,"back")]/@src')

        # Tags ‚Äî 3 listes : categories, themes, genres
        tags = []
        for xpath in [
            '//a[contains(@href,"/categories/")]/text()',
            '//a[contains(@href,"/themes/")]/text()',
            '//a[contains(@href,"/genres/")]/text()',
        ]:
            tags += [t.strip() for t in tree.xpath(xpath) if t.strip()]
        data["tags"] = list(dict.fromkeys(tags))  # d√©dupliqu√©, ordre pr√©serv√©

        # ‚îÄ‚îÄ PHASE 2 : extraction URLs sc√®nes individuelles ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # data18 liste les sc√®nes avec des liens /content/SCENE_ID
        scene_links = tree.xpath('//div[contains(@class,"scene")]//a[contains(@href,"/content/")]/@href')
        if not scene_links:
            scene_links = tree.xpath('//a[contains(@href,"/content/") and contains(@href,"/scene")]/@href')

        for i, href in enumerate(scene_links):
            scene_url = href if href.startswith("http") else f"https://www.data18.com{href}"
            data["scenes"].append({
                "index": i + 1,
                "title": None,      # sera enrichi si besoin
                "url_data18": scene_url,
            })

        return data


============================================================
[60/124] Legacy\services\extractors\dvd\iafd_dvd.py
------------------------------------------------------------
"""
IafdDVDExtractor ‚Äî Extracteur IAFD pour les titres DVDs/Groups.
Source P2 (prioritaire pour DVDs classiques US, cast fiable, dates pr√©cises).
"""
import re
from services.extractors.dvd.base_dvd import BaseExtractorDVD
from utils.duration import parse_duration_to_seconds


class IafdDVDExtractor(BaseExtractorDVD):
    SOURCE_NAME = "iafd_dvd"
    BASE_URL = "https://www.iafd.com/title.rme/title="

    def build_url_from_title(self, title: str, year: str | None = None) -> str:
        """Construire l'URL de recherche IAFD depuis un titre."""
        slug = re.sub(r'[^a-z0-9]+', '-', title.lower()).strip('-')
        url = f"{self.BASE_URL}{slug}"
        if year:
            url += f"/year={year}"
        return url

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)

        tree = self._fetch_tree(url)
        if tree is None:
            return data

        # ‚îÄ‚îÄ Titre ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["title"] = self._get_text(tree, '//h1[@itemprop="name"]/text()')
        if not data["title"]:
            data["title"] = self._get_text(tree, '//h1/text()')

        # ‚îÄ‚îÄ Date / Ann√©e ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["date"] = (self._get_stat(tree, "Release Date") or 
                       self._get_stat(tree, "Year"))

        # ‚îÄ‚îÄ Studio ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["studio"] = (self._get_text(
            tree, '//p[@class="subheading"]/a[1]/text()'
        ) or self._get_stat(tree, "Studio"))

        # ‚îÄ‚îÄ R√©alisateur ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["director"] = (self._get_text(
            tree, '//p[b[contains(text(),"Director")]]/a/text()'
        ) or self._get_stat(tree, "Director"))

        # ‚îÄ‚îÄ Dur√©e ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        raw_duration = (self._get_stat(tree, "Running Time") or 
                       self._get_stat(tree, "Duration"))
        if raw_duration:
            data["duration"] = raw_duration
            secs = parse_duration_to_seconds(raw_duration)
            if secs:
                data["_duration_seconds"] = secs

        # ‚îÄ‚îÄ Description / Synopsis ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["description"] = (self._get_text(
            tree, '//div[@class="synopsis"]//text()'
        ) or self._get_text(
            tree, '//div[contains(@class,"description")]//text()'
        ))

        # ‚îÄ‚îÄ Aliases / Titres alternatifs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        aliases_text = (self._get_stat(tree, "Alternate Titles") or 
                       self._get_stat(tree, "AKA"))
        if aliases_text:
            data["aliases"] = [a.strip() for a in aliases_text.split(',') if a.strip()]

        # ‚îÄ‚îÄ Tags / Cat√©gories ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tags = tree.xpath('//div[@id="genres"]//a/text()')
        if not tags:
            tags = tree.xpath('//a[contains(@href,"/genre/")]/text()')
        data["tags"] = [t.strip() for t in tags if t.strip()]

        # ‚îÄ‚îÄ Sc√®nes (liste index√©e) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["scenes"] = self._extract_scenes(tree)

        # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["urls"] = {"iafd_dvd": url}

        return data

    def _extract_scenes(self, tree) -> list[dict]:
        """
        Extraire la liste des sc√®nes du DVD depuis la page IAFD.
        Retourne : [{"index": int, "title": str | None, "performers": [str]}]
        """
        scenes = []

        # IAFD liste les sc√®nes dans un tableau ou une liste ordonn√©e
        # Structure typique : <div id="sceneinfo"> ou <table class="w100">
        scene_rows = tree.xpath(
            '//div[contains(@id,"scene")] | //tr[contains(@class,"scene")]'
        )

        for i, row in enumerate(scene_rows, start=1):
            scene = {"index": i, "title": None, "performers": [], "url": None}

            # Titre de sc√®ne
            title_nodes = row.xpath('.//b/text() | .//strong/text()')
            if title_nodes:
                scene["title"] = title_nodes[0].strip()

            # Performers
            perf_links = row.xpath('.//a[contains(@href,"/person.rme/")]/text()')
            scene["performers"] = [p.strip() for p in perf_links if p.strip()]

            # URL directe de la sc√®ne (si disponible)
            scene_link = row.xpath('.//a[contains(@href,"/scene.rme/")]/@href')
            if scene_link:
                scene["url"] = "https://www.iafd.com" + scene_link[0]

            if scene["title"] or scene["performers"]:
                scenes.append(scene)

        return scenes

    def _get_stat(self, tree, label: str) -> str | None:
        """Extraire une valeur depuis les tableaux info IAFD."""
        xpaths = [
            f'//td[contains(text(),"{label}")]/following-sibling::td[1]',
            f'//b[contains(text(),"{label}")]/parent::*/following-sibling::*[1]',
            f'//p[contains(text(),"{label}")]/following-sibling::p[1]',
            f'//span[normalize-space(text())="{label}"]/following-sibling::span[1]',
        ]
        for xpath in xpaths:
            val = self._get_text(tree, xpath)
            if val:
                return val.strip()
        return None


============================================================
[61/124] Legacy\services\extractors\dvd\jeedoo_dvd.py
------------------------------------------------------------
"""
JedeeDVDExtractor ‚Äî Extracteur Jeedoo pour productions europ√©ennes.
Source P4 (priorit√© basse ‚Äî sp√©cialis√© europe).
"""
from services.extractors.dvd.base_dvd import BaseExtractorDVD


class JedeeDVDExtractor(BaseExtractorDVD):
    SOURCE_NAME = "jeedoo"
    BASE_URL = "https://www.jeedoo.com/"

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)
        tree = self._fetch_tree(url)
        if tree is None:
            return data

        data["title"] = self._get_text(tree, '//h1/text()')
        data["date"] = self._get_text(tree,
            '//span[contains(@class,"date")]/text() | '
            '//p[contains(text(),"Ann√©e")]/following-sibling::p[1]/text()')
        data["studio"] = self._get_text(tree,
            '//a[contains(@href,"/studio/")]/text()')
        data["director"] = self._get_text(tree,
            '//p[contains(text(),"R√©alisateur")]/following-sibling::p[1]/text()')
        data["duration"] = self._get_text(tree,
            '//p[contains(text(),"Dur√©e")]/following-sibling::p[1]/text()')
        data["description"] = self._get_text(tree,
            '//div[contains(@class,"description")]//text()')

        tags = tree.xpath('//a[contains(@href,"/categorie/")]/text()')
        data["tags"] = [t.strip() for t in tags if t.strip()]

        data["scenes"] = []  # Jeedoo ne liste pas les sc√®nes individuellement
        data["urls"] = {"jeedoo": url}

        return data


============================================================
[62/124] Legacy\services\extractors\freeones.py
------------------------------------------------------------
"""
Extracteur FreeOnes ‚Äî source primaire pour trivia et bio.
S√©pare details / trivia / awards contrairement au V1.
"""
import re
import html as html_std

from services.extractors.base import BaseExtractor
from utils.body_art_parser import parse_body_art


# Regex multi-ceremonies pour extraire les awards du texte
AWARD_PATTERN = re.compile(
    r'((?:\d{4}\s+)?(?:AVN|XBIZ|XRCO|NightMoves|XCritic|AEBN|AdultFilmDatabase'
    r'|Fans of Adult|TEA|GayVN|Grabby|Urban X|CAVR|Inked)[^\n.;]*)',
    re.I
)


class FreeonesExtractor(BaseExtractor):
    SOURCE_NAME = "freeones"

    def build_url(self, performer_name: str) -> str | None:
        """Construire l'URL FreeOnes depuis le nom."""
        slug = self._normalize_name(performer_name)
        return f"https://www.freeones.com/{slug}/bio"

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)

        tree = self._fetch_tree(url)
        if tree is None:
            return data

        # ‚îÄ‚îÄ Phase 1 ‚Äî M√©tadonn√©es ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["name"] = self._get_text(tree, '//h1/text()') or self._get_text(tree, '//h1//text()')
        
        aliases_el = self._get_stat(tree, "Aliases") or self._get_stat(tree, "Also Known As")
        if aliases_el:
            data["aliases"] = [a.strip() for a in aliases_el.split(',') if a.strip()]

        data["birthdate"] = self._get_stat(tree, "Date of Birth") or self._get_stat(tree, "Birthday")
        data["country"] = self._get_stat(tree, "Place of Birth") or self._get_stat(tree, "Birthplace")
        data["ethnicity"] = self._get_stat(tree, "Ethnicity")
        data["hair_color"] = self._get_stat(tree, "Hair Color") or self._get_stat(tree, "Hair")
        data["eye_color"] = self._get_stat(tree, "Eye Color") or self._get_stat(tree, "Eyes")
        data["height"] = self._get_stat(tree, "Height")
        data["weight"] = self._get_stat(tree, "Weight")

        # Ethnie : Essayer de r√©cup√©rer via les liens si le texte simple √©choue
        if not data["ethnicity"]:
            eth_links = tree.xpath('//a[contains(@href, "/ethnicity/")]/text()')
            if eth_links:
                data["ethnicity"] = eth_links[0].strip()

        data["measurements"] = self._get_stat(tree, "Measurements")
        
        # Si pas de measurements, essayer de construire depuis Bust/Waist/Hip
        if not data["measurements"]:
            bust = self._get_stat(tree, "Bust")
            waist = self._get_stat(tree, "Waist")
            hip = self._get_stat(tree, "Hip")
            if bust and waist and hip:
                data["measurements"] = f"{bust}-{waist}-{hip}"
        
        # Normalisation Mensurations (Conversion CM -> Pouces si n√©cessaire)
        if data["measurements"]:
            try:
                # Si les valeurs semblent √™tre en cm (toutes > 50), on convertit
                parts = re.findall(r'\d+', data["measurements"])
                if len(parts) == 3 and all(int(x) > 50 for x in parts):
                    imperial = [str(round(int(x) / 2.54)) for x in parts]
                    data["measurements"] = "-".join(imperial)
            except Exception:
                pass

        data["fake_tits"] = self._get_stat(tree, "Boobs") or self._get_stat(tree, "Enhanced")
        
        # Career Length : Combiner Start/End si Years Active est vide
        data["career_length"] = self._get_stat(tree, "Career Start") or self._get_stat(tree, "Years Active")
        if not data["career_length"]:
            start = self._get_stat(tree, "Career Start") or self._get_stat(tree, "Started")
            end = self._get_stat(tree, "Career End") or self._get_stat(tree, "Ended") or "Present"
            if start:
                data["career_length"] = f"{start}-{end}"

        # ‚îÄ‚îÄ Bio / Details ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        bio_xpaths = [
            '//div[@data-test="biography"]//text()[normalize-space()]',
            '//div[contains(@class,"biography")]//text()[normalize-space()]',
            '//div[@class="bio"]//text()[normalize-space()]',
        ]
        for xpath in bio_xpaths:
            bio_texts = tree.xpath(xpath)
            if bio_texts:
                bio = " ".join(t.strip() for t in bio_texts if t.strip())
                if len(bio) > 20:
                    data["details"] = bio
                    break

        # ‚îÄ‚îÄ Trivia / Additional Information ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        trivia_xpaths = [
            (
                "//p[normalize-space(text())='Additional Information']"
                "/following-sibling::div[contains(@class,'hide-on-edit')]"
                "//text()[normalize-space()]"
            ),
            (
                "//h3[contains(text(),'Additional')]"
                "/following-sibling::div//text()[normalize-space()]"
            ),
            (
                "//div[contains(@class,'additional')]"
                "//text()[normalize-space()]"
            ),
        ]
        for xpath in trivia_xpaths:
            trivia_texts = tree.xpath(xpath)
            if trivia_texts:
                raw = " ".join(t.strip() for t in trivia_texts if t.strip())
                if len(raw) > 10:
                    data["trivia"] = html_std.unescape(raw)
                    break

        # ‚îÄ‚îÄ Awards (regex depuis texte bio + trivia) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        combined = (data.get("details") or "") + " " + (data.get("trivia") or "")
        if combined.strip():
            matches = AWARD_PATTERN.findall(combined)
            data["awards"] = [m.strip() for m in matches if m.strip()]

        # ‚îÄ‚îÄ Tattoos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tattoo_text = self._get_stat(tree, "Tattoos")
        data["tattoos"] = parse_body_art(tattoo_text) if tattoo_text else []

        # ‚îÄ‚îÄ Piercings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        piercing_text = self._get_stat(tree, "Piercings")
        data["piercings"] = parse_body_art(piercing_text) if piercing_text else []

        # ‚îÄ‚îÄ Tags (cat√©gories) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tag_xpaths = [
            "//a[contains(@href,'/category/')]//text()",
            "//a[contains(@href,'/tag/')]//text()",
            "//div[contains(@class,'tag')]//a/text()",
        ]
        for xpath in tag_xpaths:
            tags_raw = self._get_texts(tree, xpath)
            if tags_raw:
                data["tags"] = [t.strip() for t in tags_raw if t.strip() and len(t.strip()) > 1]
                break

        # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["urls"]["freeones"] = url

        # Extraire les liens vers databases ext√©rieures
        links = tree.xpath('//a[contains(@class,"link") or contains(@class,"database")]/@href')
        for link in links:
            link = link.strip()
            if "iafd.com" in link:
                data["urls"]["iafd"] = link
            elif "babepedia.com" in link:
                data["urls"]["babepedia"] = link
            elif "thenude" in link:
                data["urls"]["thenude"] = link
            elif "twitter.com" in link or "x.com" in link:
                data["urls"]["twitter"] = link
            elif "instagram.com" in link:
                data["urls"]["instagram"] = link

        return data

    def _get_stat(self, tree, label: str) -> str | None:
        """Extraire une stat depuis la page FreeOnes."""
        label_slug = label.lower().replace(" ", "-")
        xpaths = [
            f'//span[contains(text(), "{label}")]/following-sibling::span[1]',
            f'//p[contains(@data-test, "{label.lower()}")]',
            f'//p[contains(@data-test, "{label_slug}")]',
            f'//div[contains(@data-test, "{label.lower()}")]',
            f'//div[contains(@data-test, "{label_slug}")]',
            f'//a[contains(@data-test, "{label_slug}")]',
            f'//td[contains(text(), "{label}")]/following-sibling::td[1]',
            f'//*[contains(text(), "{label}")]/following-sibling::*[1]',
        ]
        for xpath in xpaths:
            val = self._get_text(tree, xpath)
            if val:
                return val
        return None


============================================================
[63/124] Legacy\services\extractors\iafd.py
------------------------------------------------------------
"""
Extracteur IAFD ‚Äî source primaire pour les awards.
"""
import re

from services.extractors.base import BaseExtractor
from utils.body_art_parser import parse_body_art


# Lignes parasites √† ignorer dans les awards
AWARD_SKIP = re.compile(r'^([-‚Äì‚Äî]+|nomin[√©e]e?d?|winner|won|year|award)$', re.I)

# Regex √©tendue multi-ceremonies pour fallback texte
AWARD_PATTERN = re.compile(
    r'(\d{4}\s+)?(AVN|XBIZ|XRCO|NightMoves|XCritic|AEBN|AdultFilmDatabase'
    r'|Fans of Adult Media|TEA|GayVN|Grabby|Urban X|CAVR|Inked)[^.\n]*',
    re.I
)


class IafdExtractor(BaseExtractor):
    SOURCE_NAME = "iafd"

    def build_url(self, performer_name: str) -> str | None:
        """Construire l'URL IAFD depuis le nom."""
        slug = self._normalize_name(performer_name)
        return f"https://www.iafd.com/person.rme/perfid={slug}/gender=f/{slug}.htm"

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)

        tree = self._fetch_tree(url)
        if tree is None:
            return data

        # ‚îÄ‚îÄ Phase 1 ‚Äî M√©tadonn√©es ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["name"] = self._get_text(tree, '//h1/text()')
        
        aliases_text = self._get_stat(tree, "Aliases") or self._get_stat(tree, "AKA")
        if aliases_text:
            data["aliases"] = [a.strip() for a in aliases_text.split(',') if a.strip()]

        data["birthdate"] = self._get_stat(tree, "Birthday") or self._get_stat(tree, "Date of Birth")
        data["death_date"] = self._get_stat(tree, "Death")
        data["ethnicity"] = self._get_stat(tree, "Ethnicity") or self._get_stat(tree, "Race")
        data["country"] = self._get_stat(tree, "Birthplace") or self._get_stat(tree, "Nationality")
        data["hair_color"] = self._get_stat(tree, "Hair Color")
        data["eye_color"] = self._get_stat(tree, "Eye Color")
        
        height_text = self._get_stat(tree, "Height")
        if height_text:
            data["height"] = height_text
        
        weight_text = self._get_stat(tree, "Weight")
        if weight_text:
            data["weight"] = weight_text
            
        data["measurements"] = self._get_stat(tree, "Measurements")
        data["fake_tits"] = self._get_stat(tree, "Boobs") or self._get_stat(tree, "Breast")
        
        career_years = self._get_stat(tree, "Years Active")
        if career_years:
            data["career_length"] = career_years
        else:
            start = self._get_stat(tree, "Start") or self._get_stat(tree, "Career Start")
            end = self._get_stat(tree, "End") or self._get_stat(tree, "Career End") or "Present"
            if start:
                data["career_length"] = f"{start} - {end}"

        # ‚îÄ‚îÄ Phase 2 ‚Äî Awards ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["awards"] = self._extract_awards(tree)

        # ‚îÄ‚îÄ Bio / Details ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        bio_text = self._get_text(tree, '//div[@id="bio"]')
        if not bio_text:
            bio_text = self._get_text(tree, '//div[contains(@class,"biodata")]')
        data["details"] = bio_text

        # ‚îÄ‚îÄ Tattoos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tattoo_text = self._get_stat(tree, "Tattoos")
        data["tattoos"] = parse_body_art(tattoo_text) if tattoo_text else []

        # ‚îÄ‚îÄ Piercings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        piercing_text = self._get_stat(tree, "Piercings")
        data["piercings"] = parse_body_art(piercing_text) if piercing_text else []

        # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["urls"]["iafd"] = url

        # Extraire les liens vers d'autres databases
        db_links = tree.xpath('//a[contains(@href, "http")]/@href')
        for link in db_links:
            link = link.strip()
            if "freeones.com" in link:
                data["urls"]["freeones"] = link
            elif "babepedia.com" in link:
                data["urls"]["babepedia"] = link
            elif "thenude.com" in link or "thenude.eu" in link:
                data["urls"]["thenude"] = link

        return data

    def _extract_awards(self, tree) -> list[str]:
        """Parser les awards depuis div#awards de IAFD."""
        awards_divs = tree.xpath("//div[@id='awards']")
        if not awards_divs:
            # Essayer un s√©lecteur alternatif
            awards_divs = tree.xpath("//div[contains(@class,'award')]")
        
        if not awards_divs:
            return []

        div = awards_divs[0]
        # Ins√©rer des sauts de ligne avant chaque <br>
        for br in div.xpath(".//br"):
            br.tail = "\n" + (br.tail or "")

        awards_text = div.text_content()
        raw_lines = [line.strip() for line in awards_text.splitlines() if line.strip()]
        
        # Filtrer les lignes parasites
        result = []
        current_year = ""

        for line in raw_lines:
            # D√©tection de l'ann√©e (ex: 2012) pour l'associer √† l'award
            if re.match(r'^\d{4}$', line):
                current_year = line
                continue

            if AWARD_SKIP.match(line):
                continue
            if len(line) < 4:
                continue
            
            # Ajouter l'ann√©e devant la ligne si elle n'y est pas d√©j√†
            if current_year and not line.startswith(current_year):
                result.append(f"{current_year} {line}")
            else:
                result.append(line)

        return result

    def _get_stat(self, tree, label: str) -> str | None:
        """Extraire une stat depuis le tableau IAFD (label ‚Üí valeur)."""
        # Essayer plusieurs formats de tableau bio IAFD
        xpaths = [
            f'//p[contains(text(), "{label}")]/following-sibling::p[1]',
            f'//td[contains(text(), "{label}")]/following-sibling::td[1]',
            f'//b[contains(text(), "{label}")]/parent::*/following-sibling::*[1]',
            f'//span[contains(text(), "{label}")]/following-sibling::span[1]',
        ]
        for xpath in xpaths:
            val = self._get_text(tree, xpath)
            if val:
                return val
        return None


============================================================
[64/124] Legacy\services\extractors\thenude.py
------------------------------------------------------------
"""
Extracteur TheNude ‚Äî source pour bios studio contextualis√©es.
"""
import re

from services.extractors.base import BaseExtractor
from utils.body_art_parser import parse_body_art


class ThenudeExtractor(BaseExtractor):
    SOURCE_NAME = "thenude"

    def build_url(self, performer_name: str) -> str | None:
        """Construire l'URL TheNude depuis le nom."""
        slug = self._normalize_name_underscore(performer_name)
        return f"https://www.thenude.com/models/{slug}.htm"

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)

        tree = self._fetch_tree(url)
        if tree is None:
            return data

        # ‚îÄ‚îÄ Phase 1 ‚Äî M√©tadonn√©es ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["name"] = self._get_text(tree, '//h1/text()')
        
        aliases_text = self._get_stat(tree, "Aliases") or self._get_stat(tree, "AKA")
        if aliases_text:
            data["aliases"] = [a.strip() for a in aliases_text.split(',') if a.strip()]

        data["birthdate"] = self._get_stat(tree, "Born") or self._get_stat(tree, "Date of Birth")
        data["country"] = self._get_stat(tree, "Birthplace") or self._get_stat(tree, "Country")
        data["ethnicity"] = self._get_stat(tree, "Ethnicity")
        data["hair_color"] = self._get_stat(tree, "Hair")
        data["eye_color"] = self._get_stat(tree, "Eyes")
        data["height"] = self._get_stat(tree, "Height")
        data["weight"] = self._get_stat(tree, "Weight")
        data["measurements"] = self._get_stat(tree, "Measurements")
        data["fake_tits"] = self._get_stat(tree, "Boobs")

        # ‚îÄ‚îÄ Bio principale ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        bio_xpaths = [
            '//p[@class="description"]/text()',
            '//div[@class="description"]//text()',
            '//div[contains(@class,"bio")]//text()',
        ]
        for xpath in bio_xpaths:
            texts = tree.xpath(xpath)
            if texts:
                bio = " ".join(t.strip() for t in texts if t.strip())
                if len(bio) > 20:
                    data["details"] = bio
                    break

        # ‚îÄ‚îÄ Bios studio (trivia) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        studio_bios = self._extract_studio_bios(tree)
        if studio_bios:
            data["trivia"] = "\n\n".join(studio_bios)

        # ‚îÄ‚îÄ Tattoos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tattoo_text = self._get_stat(tree, "Tattoos")
        data["tattoos"] = parse_body_art(tattoo_text) if tattoo_text else []

        # ‚îÄ‚îÄ Piercings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        piercing_text = self._get_stat(tree, "Piercings")
        data["piercings"] = parse_body_art(piercing_text) if piercing_text else []

        # ‚îÄ‚îÄ Tags (keywords) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tag_xpaths = [
            '//div[@class="keywords"]//a/text()',
            '//meta[@name="keywords"]/@content',
            '//div[contains(@class,"tag")]//a/text()',
        ]
        for xpath in tag_xpaths:
            tags_raw = tree.xpath(xpath)
            if tags_raw:
                # Si c'est un meta keywords, splitter par virgule
                if len(tags_raw) == 1 and ',' in tags_raw[0]:
                    tags_raw = [t.strip() for t in tags_raw[0].split(',')]
                data["tags"] = [t.strip() for t in tags_raw 
                                if t.strip() and len(t.strip()) > 1]
                break

        # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["urls"]["thenude"] = url

        # Extraire les liens vers databases
        links = tree.xpath('//a[contains(@href, "http")]/@href')
        for link in links:
            link = link.strip()
            if "iafd.com" in link:
                data["urls"]["iafd"] = link
            elif "freeones.com" in link:
                data["urls"]["freeones"] = link
            elif "babepedia.com" in link:
                data["urls"]["babepedia"] = link

        return data

    def _extract_studio_bios(self, tree) -> list[str]:
        """Extraire les bios par studio (ex: 'BRAZZERS biography')."""
        studio_bios = []
        
        # Chercher les headers contenant "biography"
        bio_headers = tree.xpath(
            '//*[contains(translate(text(),"BIOGRAPHY","biography"), "biography")]'
        )
        
        for h in bio_headers:
            title = h.text_content().strip()
            if title.lower() == "biography":
                continue

            # Le contenu suit le header
            content_node = h.getnext()
            if content_node is not None:
                content = content_node.text_content().strip()
                if content and len(content) > 20:
                    studio_bios.append(f"[{title}]\n{content}")

        return studio_bios

    def _get_stat(self, tree, label: str) -> str | None:
        """Extraire une stat depuis la page TheNude."""
        xpaths = [
            f'//span[contains(text(), "{label}")]/following-sibling::span[1]',
            f'//td[contains(text(), "{label}")]/following-sibling::td[1]',
            f'//li[contains(., "{label}")]',
            f'//div[contains(@class,"stat")]//*[contains(text(), "{label}")]/..',
        ]
        for xpath in xpaths:
            val = self._get_text(tree, xpath)
            if val:
                # Nettoyer le label du texte si pr√©sent
                val = re.sub(rf'^{label}\s*[:]\s*', '', val, flags=re.I).strip()
                if val:
                    return val
        return None


============================================================
[65/124] Legacy\services\group_phase1_merger.py
------------------------------------------------------------
"""
GroupPhase1Merger ‚Äî Fusionne les m√©tadonn√©es DVD/Group scrap√©es avec la DB.
"""

GROUP_FIELDS = {
    "Title": "name",
    "Aliases": "aliases",
    "Date": "date",
    "Studio": "studio",
    "Director": "director",
    "Duration": "duration",
    "Description": "description",
    "Tags": "tags",
    "URLs": "urls",
}

class GroupPhase1Merger:
    def merge(self, db_data: dict, scraped_results: list[dict], checked_fields: list[str]) -> dict:
        result = {}
        for field in checked_fields:
db_key="***MASKED***"
            if not db_key: continue
            
            db_val = db_data.get(db_key)
            if field == "Studio" and db_data.get("studio_name"):
                db_val = db_data["studio_name"]

            scraped_vals = {}
            for r in scraped_results:
                val = r.get(db_key.replace("name", "title") if db_key == "name" else db_key)
                if val:
                    scraped_vals[r["_source"]] = val
            
            if not scraped_vals:
                status = "empty"
                suggestion = db_val
            elif not db_val:
                status = "new"
                suggestion = self._pick_best(scraped_vals)
            else:
                # Priorit√© Data18 : si data18 est pr√©sent et diff√®re de DB, c'est un conflit (m√™me si d'autres sources matchent)
                data18_val = scraped_vals.get("data18")
                if data18_val and str(data18_val).lower().strip() != str(db_val).lower().strip():
                    status = "conflict"
                    suggestion = data18_val
                elif self._matches(db_val, scraped_vals):
                    status = "confirmed"
                    suggestion = db_val
                else:
                    status = "conflict"
                    suggestion = self._pick_best(scraped_vals)

            result[field] = {
                "status": status,
                "db_value": db_val,
                "scraped_values": scraped_vals,
                "suggestion": suggestion
            }
        return result

    def _matches(self, db_val, scraped_vals):
        db_s = str(db_val).lower().strip()
        for v in scraped_vals.values():
            if str(v).lower().strip() == db_s:
                return True
        return False

    def _pick_best(self, scraped_vals):
        # Priorit√© simple : data18 > iafd_dvd > adultdvdempire
        for src in ["data18", "iafd_dvd", "adultdvdempire"]:
            if src in scraped_vals:
                return scraped_vals[src]
        return list(scraped_vals.values())[0]


============================================================
[66/124] Legacy\services\group_phase1_scraper.py
------------------------------------------------------------
"""
GroupPhase1ScraperService ‚Äî Orchestre le scraping des m√©tadonn√©es DVD/Group.
"""
from services.extractors.dvd.data18_dvd import Data18DVDExtractor
from services.extractors.dvd.adultempire_dvd import AdultEmpireDVDExtractor
from services.extractors.dvd.iafd_dvd import IafdDVDExtractor
from services.extractors.dvd.jeedoo_dvd import JedeeDVDExtractor

class GroupPhase1ScraperService:
    def __init__(self):
        self.extractors = {
            "data18": Data18DVDExtractor(),
            "adultdvdempire": AdultEmpireDVDExtractor(),
            "iafd_dvd": IafdDVDExtractor(),
            "jeedoo": JedeeDVDExtractor(),
        }

    def scrape(self, title: str, year: str = None, known_urls: list = None, progress_callback=None) -> list[dict]:
        results = []
        known_urls = known_urls or []

        # 1. Utiliser les URLs connues
        for url in known_urls:
            extractor = self._find_extractor_for_url(url)
            if extractor:
                if progress_callback:
                    progress_callback(extractor.SOURCE_NAME, "Fetching from known URL...")
                res = extractor.extract_from_url(url)
                if res and res.get("title"):
                    results.append(res)

        # 2. Si Data18 n'est pas trouv√© dans les URLs connues, essayer de deviner l'URL
        has_data18 = any(r.get("_source") == "data18" for r in results)
        if not has_data18:
            data18 = self.extractors["data18"]
            candidates = data18.search_urls(title)
            # Pas de boucle ici si search_urls ne fait que des guesses simples qui ont peu de chance de marcher
            # Mais on essaie quand m√™me les guesses simples
            for url in candidates:
                if progress_callback:
                    progress_callback("data18", f"Trying candidate: {url}...")
                res = data18.extract_from_url(url)
                if res and res.get("title"):
                    results.append(res)
                    break # Found one valid Data18 page

        # 3. Si pas de r√©sultat ou pour compl√©ter, essayer IAFD par titre
        has_iafd = any(r.get("_source") == "iafd_dvd" for r in results)
        if not has_iafd:
            # Essayer IAFD par titre
            iafd = self.extractors["iafd_dvd"]
            url = iafd.build_url_from_title(title, year)
            if progress_callback:
                progress_callback("iafd_dvd", f"Searching for {title}...")
            res = iafd.extract_from_url(url)
            if res and res.get("title"):
                results.append(res)

        return results

    def _find_extractor_for_url(self, url: str):
        if "data18.com" in url: return self.extractors["data18"]
        if "adultdvdempire.com" in url: return self.extractors["adultdvdempire"]
        if "iafd.com" in url: return self.extractors["iafd_dvd"]
        if "jeedoo.com" in url: return self.extractors["jeedoo"]
        return None


============================================================
[67/124] Legacy\services\group_phase2_merger.py
------------------------------------------------------------
"""
GroupPhase2Merger ‚Äî Associe les URLs scrap√©es aux sc√®nes Stash par index.

Strat√©gie de matching :
  1. Exact sur scene_index (num√©ro de sc√®ne dans le DVD)
  2. Fallback : comparaison de titre normalis√©
"""


class GroupPhase2Merger:

    def merge(
        self,
        stash_scenes: list[dict],
        scraped_scene_urls: dict[int, dict[str, str]]
    ) -> list[dict]:
        """
        Associe les URLs scrap√©es √† chaque sc√®ne Stash.

        Args:
            stash_scenes:       liste de sc√®nes DB (scene_id, scene_index, scene_title, existing_urls)
            scraped_scene_urls: {scene_index: {"data18": url, "adultdvdempire": url}}

        Returns:
            Liste de dicts enrichis avec "new_urls" et "status"
        """
        result = []

        for scene in stash_scenes:
            scene_id    = scene["scene_id"]
            scene_index = scene.get("scene_index")
            scene_title = scene.get("scene_title", f"Sc√®ne {scene_id}")
            existing    = scene.get("existing_urls", [])

            # Chercher par index exact
            new_urls = scraped_scene_urls.get(scene_index, {})

            # Filtrer les URLs d√©j√† pr√©sentes
            truly_new = {
                src: url for src, url in new_urls.items()
                if url and url not in existing
            }

            # D√©terminer le statut
            if not new_urls:
                status = "no_match"
            elif not truly_new:
                status = "already_present"
            elif existing:
                status = "partial"
            else:
                status = "new"

            result.append({
                "scene_id":      scene_id,
                "scene_index":   scene_index,
                "scene_title":   scene_title,
                "existing_urls": existing,
                "new_urls":      truly_new,
                "status":        status,
            })

        return result


============================================================
[68/124] Legacy\services\group_phase2_scraper.py
------------------------------------------------------------
"""
GroupPhase2ScraperService ‚Äî Scraping des URLs de sc√®nes individuelles pour un Group.
"""
from services.extractors.dvd.data18_dvd import Data18DVDExtractor
from services.extractors.dvd.adultempire_dvd import AdultEmpireDVDExtractor

class GroupPhase2ScraperService:
    def __init__(self):
        self.extractors = {
            "data18": Data18DVDExtractor(),
            "adultdvdempire": AdultEmpireDVDExtractor(),
        }

    def scrape(self, group_data: dict, progress_callback=None) -> dict[int, dict[str, str]]:
        """
        Retourne : { scene_index: { "source": "url" } }
        """
        all_scene_urls = {} # {index: {source: url}}

        urls = group_data.get("urls", [])
        for url in urls:
            extractor = None
            if "data18.com" in url: extractor = self.extractors["data18"]
            elif "adultdvdempire.com" in url: extractor = self.extractors["adultdvdempire"]
            
            if extractor:
                if progress_callback:
                    progress_callback(extractor.SOURCE_NAME, f"Scraping scene URLs...")
                
                res = extractor.extract_from_url(url)
                scenes = res.get("scenes", [])
                for s in scenes:
                    idx = s.get("index")
                    s_url = s.get("url")
                    if idx and s_url:
                        if idx not in all_scene_urls:
                            all_scene_urls[idx] = {}
                        all_scene_urls[idx][extractor.SOURCE_NAME] = s_url

        return all_scene_urls


============================================================
[69/124] Legacy\services\phase1_merger.py
------------------------------------------------------------
"""
Phase1Merger ‚Äî Compare les donn√©es scrap√©es avec la DB pour les champs Phase 1.
Identifie les confirmations et les conflits.
G√®re la fusion et la normalisation des ALIAS.
"""

# Ordre de priorit√© des sources pour les champs simples
SOURCE_PRIORITY = ["iafd", "freeones", "babepedia", "thenude"]

# Champs Phase 1 avec leur cl√© DB
PHASE1_FIELDS = {
    "Name": "name",
    "Aliases": "aliases",
    "Birthdate": "birthdate",
    "Deathdate": "death_date",
    "Country": "country",
    "Ethnicity": "ethnicity",
    "Hair Color": "hair_color",
    "Eye Color": "eye_color",
    "Height": "height",
    "Weight": "weight",
    "Measurements": "measurements",
    "Fake Tits": "fake_tits",
    "Career Length": "career_length",
}


class Phase1Merger:
    def merge(self, db_data: dict, scraped_results: list[dict], 
              checked_fields: list[str]) -> dict:
        """
        Fusionner les donn√©es pour les champs coch√©s.
        Logique sp√©ciale pour les ALIASES.
        """
        result = {}
        
        for field_name in checked_fields:
db_key="***MASKED***"
            if not db_key:
                continue

            # --- Logique sp√©ciale pour les ALIASES ---
            if db_key == 'aliases':
                result[field_name] = self._merge_aliases(db_data, scraped_results)
                continue

            # --- Logique g√©n√©rale pour les autres champs ---
            db_value = self._normalize_value(db_data.get(db_key))
            
            scraped_values = {}
            for r in scraped_results:
                source = r.get("_source", "?")
                val = self._normalize_value(r.get(db_key))
                if val:
                    scraped_values[source] = val
            
            if not scraped_values:
                result[field_name] = self._build_result("empty", db_value, scraped_values, db_value)
            elif not db_value:
                suggestion = self._pick_best(scraped_values)
                result[field_name] = self._build_result("new", None, scraped_values, suggestion)
            elif self._matches_any(db_value, scraped_values):
                result[field_name] = self._build_result("confirmed", db_value, scraped_values, db_value)
            else:
                suggestion = self._pick_best(scraped_values)
                result[field_name] = self._build_result("conflict", db_value, scraped_values, suggestion)
        
        return result

    def _normalize_alias(self, alias: str) -> str | None:
        """Normalise une cha√Æne d'alias (lowercase, strip)."""
        if not isinstance(alias, str):
            return None
        normalized = alias.lower().strip()
        return normalized if normalized else None

    def _merge_aliases(self, db_data: dict, scraped_results: list[dict]) -> dict:
        """Fusionne, normalise et d√©doublonne les alias de toutes les sources."""
db_key="***MASKED***"
        db_aliases = db_data.get(db_key) or []
        
        final_aliases = set()
        
        # 1. Ajouter les alias de la DB
        for alias in db_aliases:
            norm_alias = self._normalize_alias(alias)
            if norm_alias:
                final_aliases.add(norm_alias)
        
        # 2. Ajouter les alias des sources scrap√©es
        scraped_values_dict = {}
        for r in scraped_results:
            source_name = r.get('_source', '?')
            source_aliases = r.get(db_key)
            if source_aliases:
                scraped_values_dict[source_name] = source_aliases
                for alias in source_aliases:
                    norm_alias = self._normalize_alias(alias)
                    if norm_alias:
                        final_aliases.add(norm_alias)
        
        # 3. D√©terminer le statut de la fusion
        db_set = {self._normalize_alias(a) for a in db_aliases if self._normalize_alias(a)}
        status = "empty"
        if final_aliases:
            if not db_set:
                status = "new"
            elif db_set == final_aliases:
                status = "confirmed"
            else:
                status = "conflict"  # 'conflict' indique qu'il y a des changements/ajouts

        # 4. Construire le r√©sultat final pour l'UI
        return self._build_result(
            status,
            db_value=", ".join(sorted(list(db_set))),
            scraped_values={k: ", ".join(v) for k, v in scraped_values_dict.items()},
            suggestion=", ".join(sorted(list(final_aliases)))
        )

    def _build_result(self, status, db_value, scraped_values, suggestion):
        """Helper pour construire le dictionnaire de r√©sultat."""
        return {
            "status": status,
            "db_value": db_value,
            "scraped_values": scraped_values,
            "suggestion": suggestion,
        }

    def _normalize_value(self, val) -> str | None:
        """Normaliser une valeur simple pour comparaison."""
        if val is None:
            return None
        # Ne g√®re plus les listes ici, elles sont trait√©es par _merge_aliases
        val = str(val).strip()
        return val if val else None

    def _matches_any(self, db_value: str, scraped_values: dict) -> bool:
        """V√©rifier si la valeur DB correspond √† au moins une source."""
        db_lower = db_value.lower().strip()
        for val in scraped_values.values():
            if val and val.lower().strip() == db_lower:
                return True
        return False

    def _pick_best(self, scraped_values: dict) -> str:
        """Choisir la meilleure valeur selon la priorit√© des sources."""
        for source in SOURCE_PRIORITY:
            if source in scraped_values and scraped_values[source]:
                return scraped_values[source]
        # Fallback : premi√®re valeur non-None
        for val in scraped_values.values():
            if val:
                return val
        return ""


============================================================
[70/124] Legacy\services\phase2_merger.py
------------------------------------------------------------
"""
Phase2Merger ‚Äî fusionne les r√©sultats de scraping Phase 2
en donn√©es pr√™tes pour l'interface de r√©solution.
"""


class Phase2Merger:
    """
    Fusionne les r√©sultats de scraping Phase 2 en donn√©es pr√™tes
    pour l'interface Phase2MergeDialog.
    
    Chaque champ a sa propre strat√©gie de fusion.
    """

    def merge(self, db_data: dict, scraped_results: list[dict]) -> dict:
        """
        Fusionner les donn√©es DB + scraping pour tous les champs Phase 2.
        
        Args:
            db_data: Donn√©es actuelles du performer depuis Stash DB
            scraped_results: Liste de dicts retourn√©s par les extracteurs
            
        Returns:
            Dict avec les r√©sultats fusionn√©s par champ
        """
        return {
            "awards":    self._merge_awards(db_data, scraped_results),
            "trivia":    self._merge_trivia(db_data, scraped_results),
            "details":   self._merge_details(db_data, scraped_results),
            "tattoos":   self._merge_body_art("tattoos", db_data, scraped_results),
            "piercings": self._merge_body_art("piercings", db_data, scraped_results),
            "tags":      self._merge_tags(db_data, scraped_results),
            "urls":      self._merge_urls(db_data, scraped_results),
        }

    # ‚îÄ‚îÄ AWARDS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _merge_awards(self, db, scraped):
        """
        IAFD = source primaire (liste structur√©e).
        Autres sources = regex fallback depuis bio/trivia.
        R√©sultat : union d√©dupliqu√©e, IAFD en premier.
        """
        seen = set()
        result = []

        # 1. IAFD en premier (plus fiable)
        for r in scraped:
            if r.get("_source") == "iafd":
                for award in (r.get("awards") or []):
key="***MASKED***"
                    if key not in seen:
                        seen.add(key)
                        result.append(award)

        # 2. Autres sources en d√©duplication
        for r in scraped:
            if r.get("_source") != "iafd":
                for award in (r.get("awards") or []):
key="***MASKED***"
                    if key not in seen:
                        seen.add(key)
                        result.append(award)

        return {
            "db_value":  db.get("awards", []),
            "merged":    result,
            "sources":   {r["_source"]: r.get("awards", []) for r in scraped},
            "strategy":  "union_iafd_first"
        }

    # ‚îÄ‚îÄ TRIVIA ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _merge_trivia(self, db, scraped):
        """
        FreeOnes "Additional Information" est la meilleure source.
        TheNude bios studio = contexte suppl√©mentaire.
        Babepedia peut avoir quelques lignes.
        ‚Üí Proposer les 3 s√©par√©ment pour que l'utilisateur choisisse/combine.
        """
        by_source = {}
        for r in scraped:
            trivia = r.get("trivia")
            if trivia:
                by_source[r["_source"]] = trivia

        # S√©lection automatique par priorit√©
        best = None
        TRIVIA_PRIORITY = ["freeones", "thenude", "babepedia"]
        for src in TRIVIA_PRIORITY:
            if src in by_source:
                best = by_source[src]
                break

        return {
            "db_value":   db.get("trivia"),
            "by_source":  by_source,
            "suggestion": best,
            "strategy":   "best_source_priority"
        }

    # ‚îÄ‚îÄ BIO / DETAILS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _merge_details(self, db, scraped):
        """
        FreeOnes = bio narrative la plus compl√®te.
        TheNude = bios studio (contexte professionnel).
        Babepedia = court texte.
        ‚Üí Option 1 : FreeOnes seule
        ‚Üí Option 2 : Toutes sources concat√©n√©es (s√©parateurs clairs)
        ‚Üí Option 3 : DB actuelle
        """
        by_source = {}
        for r in scraped:
            detail = r.get("details")
            if detail and len(detail) > 50:
                by_source[r["_source"]] = detail

        # Construire option "fusion" ordonn√©e
        DETAIL_ORDER = ["freeones", "babepedia", "thenude", "boobpedia"]
        fused_parts = []
        for src in DETAIL_ORDER:
            if src in by_source:
                fused_parts.append(f"[Source: {src.upper()}]\n{by_source[src]}")

        fused = "\n\n---\n\n".join(fused_parts) if fused_parts else None

        return {
            "db_value":  db.get("details"),
            "by_source": by_source,
            "fused":     fused,
            "strategy":  "user_choice"
        }

    # ‚îÄ‚îÄ TATTOOS / PIERCINGS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _merge_body_art(self, field: str, db, scraped):
        """
        Hi√©rarchie : IAFD/FreeOnes (structur√©) > Babepedia/TheNude (flat).
        Union d√©dupliqu√©e par (position, description).
        Les entr√©es position="multiple" ne sont gard√©es que si aucune
        entr√©e structur√©e n'existe pour ce champ.
        """
        structured = []
        flat = []
        seen = set()

        QUALITY_ORDER = ["iafd", "freeones", "thenude", "babepedia"]
        for source in QUALITY_ORDER:
            for r in scraped:
                if r.get("_source") != source:
                    continue
                for item in (r.get(field) or []):
                    pos  = (item.get("position") or "").lower().strip()
                    desc = (item.get("description") or "").lower().strip()
key="***MASKED***"
                    if key in seen:
                        continue
                    seen.add(key)
                    if pos == "multiple":
                        flat.append(item)
                    else:
                        structured.append(item)

        # Utiliser flat uniquement si aucune entr√©e structur√©e
        merged = structured if structured else flat

        return {
            "db_value":  db.get(field, ""),
            "merged":    merged,
            "sources":   {r["_source"]: r.get(field, []) for r in scraped},
            "strategy":  "structured_priority"
        }

    # ‚îÄ‚îÄ TAGS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _merge_tags(self, db, scraped):
        """
        Union de toutes les sources, d√©duplification insensible √† la casse.
        """
        seen = set()
        merged = []
        for r in scraped:
            for tag in (r.get("tags") or []):
key="***MASKED***"
                if key and key not in seen:
                    seen.add(key)
                    merged.append(tag.strip().title())
        merged.sort()
        return {
            "db_value": db.get("tags", []),
            "merged":   merged,
            "sources":  {r["_source"]: r.get("tags", []) for r in scraped},
            "strategy": "union_all"
        }

    # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _merge_urls(self, db, scraped):
        """
        Agr√©ger databases + social_media de toutes les sources.
        Priorit√© : freeones > iafd > babepedia > thenude (pour ordre d'affichage).
        """
        URL_PRIORITY = ["freeones", "iafd", "babepedia", "thenude", "boobpedia"]
        merged = {}
        for source in reversed(URL_PRIORITY):
            for r in scraped:
                if r.get("_source") != source:
                    continue
                for key, url in (r.get("urls") or {}).items():
                    if url:
                        merged[key] = url
        return {
            "db_value": db.get("urls", []),
            "merged":   merged,
            "strategy": "priority_merge"
        }


============================================================
[71/124] Legacy\services\phase2_scraper.py
------------------------------------------------------------
"""
Phase2ScraperService ‚Äî orchestre les 4 extracteurs et g√®re le cache.
"""
from services.scrape_cache import ScrapeCache
from services.extractors.iafd import IafdExtractor
from services.extractors.freeones import FreeonesExtractor
from services.extractors.thenude import ThenudeExtractor
from services.extractors.babepedia import BabepediaExtractor


class Phase2ScraperService:
    """
    Lance le scraping Phase 2 sur toutes les sources disponibles.
    
    Utilise le ScrapeCache pour √©viter le double scraping.
    Construit les URLs automatiquement si non disponibles.
    """

    def __init__(self):
        self.extractors = [
            IafdExtractor(),
            FreeonesExtractor(),
            ThenudeExtractor(),
            BabepediaExtractor(),
        ]

    def scrape(
        self,
        performer_name: str,
        known_urls: list[str] | None = None,
        progress_callback=None,
    ) -> list[dict]:
        """
        Scraper toutes les sources pour un performer.
        
        Args:
            performer_name: Nom du performer
            known_urls: URLs d√©j√† connues (depuis DB Stash)
            progress_callback: Callback(source_name, status) pour le progr√®s
            
        Returns:
            Liste de dicts Phase 2 (un par source r√©ussie)
        """
        results = []
        known_urls = known_urls or []

        # Mapper les URLs connues par source
        url_map = self._map_urls_to_sources(known_urls)

        for i, extractor in enumerate(self.extractors):
            source = extractor.SOURCE_NAME
            
            if progress_callback:
                progress_callback(source, f"Scraping {source}...")

            # D√©terminer l'URL √† utiliser
            url = url_map.get(source)
            if not url:
                url = extractor.build_url(performer_name)
            
            if not url:
                print(f"[Phase2Scraper] Pas d'URL pour {source}, skip")
                continue

            try:
                # V√©rifier le cache d'abord
                cached = ScrapeCache.get(url)
                if cached:
                    print(f"[Phase2Scraper] Cache hit pour {source}: {url}")
                    results.append(cached)
                    continue

                # Scraper
                print(f"[Phase2Scraper] Scraping {source}: {url}")
                data = extractor.extract_from_url(url)
                
                if data:
                    # Stocker en cache
                    ScrapeCache.set(url, data)
                    results.append(data)
                    print(f"[Phase2Scraper] {source} OK ‚Äî "
                          f"awards:{len(data.get('awards',[]))}, "
                          f"tags:{len(data.get('tags',[]))}, "
                          f"tattoos:{len(data.get('tattoos',[]))}")
                          
            except Exception as e:
                print(f"[Phase2Scraper] Erreur {source}: {e}")
                if progress_callback:
                    progress_callback(source, f"Erreur: {e}")

        if progress_callback:
            progress_callback("done", f"Scraping termin√© ‚Äî {len(results)} sources")

        return results

    def _map_urls_to_sources(self, urls: list[str]) -> dict[str, str]:
        """Mapper les URLs connues aux noms de source."""
        url_map = {}
        for url in urls:
            url_lower = url.lower()
            if "iafd.com" in url_lower:
                url_map["iafd"] = url
            elif "freeones.com" in url_lower:
                url_map["freeones"] = url
            elif "thenude.com" in url_lower or "thenude.eu" in url_lower:
                url_map["thenude"] = url
            elif "babepedia.com" in url_lower:
                url_map["babepedia"] = url
        return url_map


============================================================
[72/124] Legacy\services\scrape_cache.py
------------------------------------------------------------
"""
Cache en m√©moire des r√©sultats de scraping pour √©viter le double
appel r√©seau entre Phase 1 et Phase 2.
"""


class ScrapeCache:
    """
    Cache class-level partag√© entre toutes les instances.
    Stocke les r√©sultats de scraping par URL.
    """
    _data: dict[str, dict] = {}

    @classmethod
    def set(cls, url: str, data: dict):
        """Stocker le r√©sultat de scraping pour une URL."""
        cls._data[url] = data

    @classmethod
    def get(cls, url: str) -> dict | None:
        """R√©cup√©rer le r√©sultat de scraping pour une URL, ou None."""
        return cls._data.get(url)

    @classmethod
    def has(cls, url: str) -> bool:
        """V√©rifier si une URL est en cache."""
        return url in cls._data

    @classmethod
    def clear(cls):
        """Vider tout le cache."""
        cls._data.clear()

    @classmethod
    def size(cls) -> int:
        """Nombre d'entr√©es en cache."""
        return len(cls._data)


============================================================
[73/124] Legacy\start.bat
------------------------------------------------------------
@echo off
REM Optimized launcher: checks venv, dependencies, installs if missing, generates a report, and then runs main.py

REM Set venv directory name
set VENV_DIR=.venv

REM Check if venv exists and is valid
if not exist %VENV_DIR%\Scripts\activate.bat (
    echo Creating virtual environment...
    python -m venv %VENV_DIR%
    if %errorlevel% neq 0 (
        echo ERROR: Failed to create virtual environment. Please ensure Python is installed and in your PATH.
        pause
        exit /b 1
    )
)

REM Activate venv
call %VENV_DIR%\Scripts\activate.bat

REM Check and install dependencies if needed
echo Upgrading pip and installing requirements from requirements.txt...
python -m pip install --upgrade pip >nul
pip install -r requirements.txt

REM --- System Hardware and AI Report ---

REM Verify for NVIDIA GPU and Ollama, and generate a report
echo Generating system report...
for /f %%i in ('powershell -Command "Get-Date -format 'yyyyMMdd-HHmmss'"') do set TIMESTAMP=%%i
set REPORT_FILE=rapport_Ollama_%TIMESTAMP%.txt

(
    echo Report generated on %DATE% at %TIME%
    echo.
) > %REPORT_FILE%

REM Check for NVIDIA GPU
nvidia-smi >nul 2>&1
if %errorlevel% neq 0 (
    echo WARNING: NVIDIA GPU not found or nvidia-smi is not in your PATH.
    (
        echo === NVIDIA GPU Status ===
        echo NVIDIA GPU not found or nvidia-smi command failed.
    ) >> %REPORT_FILE%
) else (
    echo NVIDIA GPU detected.
    (
        echo === NVIDIA GPU Status ===
        nvidia-smi
    ) >> %REPORT_FILE%
)

REM Check for Ollama
where ollama >nul 2>&1
if %errorlevel% neq 0 (
    echo WARNING: Ollama not found. Please ensure it is installed and in your PATH.
    (
        echo.
        echo === Ollama Status ===
        echo Ollama not found.
    ) >> %REPORT_FILE%
) else (
    echo Ollama detected.
    (
        echo.
        echo === Ollama Models List ===
        ollama list
    ) >> %REPORT_FILE%
)

echo Report saved to %REPORT_FILE%
echo.

REM --- Launching Application ---
echo Starting the main application...
python main.py

pause


============================================================
[74/124] Legacy\tests\__init__.py
------------------------------------------------------------


============================================================
[75/124] Legacy\tests\test_db.py
------------------------------------------------------------
import unittest
from services.db import PerformerDB

class TestDB(unittest.TestCase):
    def setUp(self):
        self.db = PerformerDB()

    def tearDown(self):
        self.db.close()

    def test_get_known_performers(self):
        performers = self.db.get_known_performers()
        self.assertIsInstance(performers, list)
        if performers:
            self.assertIsInstance(performers[0], str)

if __name__ == '__main__':
    unittest.main()


============================================================
[76/124] Legacy\tests\test_performer_fields.py
------------------------------------------------------------
import unittest
from gui.performer_frame import PerformerFrame
import tkinter as tk

class TestPerformerFields(unittest.TestCase):
    def setUp(self):
        self.root = tk.Tk()
        # On passe un stash_id de test
        self.frame = PerformerFrame(self.root, "1")

    def tearDown(self):
        self.root.destroy()

    def test_phase1_fields(self):
        # Le PerformerFrame d√©marre en Phase 1
        phase1_frame = self.frame.current_frame
        
        # V√©rifier que les champs de la Phase 1 sont bien cr√©√©s
        for field in phase1_frame.fields_list:
            self.assertIn(field, phase1_frame.fields)
            self.assertIsNotNone(phase1_frame.fields[field])

    def test_phase2_fields(self):
        # Naviguer vers la Phase 2
        self.frame.goto_phase2()
        phase2_frame = self.frame.current_frame

        # V√©rifier que les champs de la Phase 2 sont bien cr√©√©s
        for field in phase2_frame.fields_list:
            self.assertIn(field, phase2_frame.fields)
            self.assertIsNotNone(phase2_frame.fields[field])

if __name__ == "__main__":
    unittest.main()


============================================================
[77/124] Legacy\utils\__init__.py
------------------------------------------------------------


============================================================
[78/124] Legacy\utils\audit_markers.csv
------------------------------------------------------------
Ôªøid,title,duration,markers,points,tags,overlaps,short,long,exact_dupes,penalty
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",966.67,6,0,2,0,0,0,0,0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",800.35,6,0,2,0,0,1,0,1
179,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc5",915.47,5,0,2,0,0,0,0,0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",1138.83,8,0,2,0,0,0,0,0
181,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc1",955.18,2,0,1,0,0,0,0,0
182,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc5",336.28,1,0,1,0,0,1,0,1
183,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc3,986.29,3,0,1,0,0,0,0,0
186,"Sai Tai Tiger in Frauen Knast, Teufelsbrut Hinter Gittern! Sc2",1004.31,1,0,1,0,0,0,0,0
187,Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc1,669.9,1,0,1,0,0,1,0,1
188,Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc4,523.91,2,0,1,0,0,0,0,0
190,"Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc7",444.5,3,0,2,0,0,1,0,1
191,Aderes Quin in StepMom Gets Double Dick,2797.73,11,0,2,0,0,0,0,0
192,Alejandra Rico in Que Rico!,1346.01,4,0,2,0,0,0,0,0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",1367.91,5,0,2,0,0,0,0,0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",2422.35,7,0,2,0,0,0,0,0
195,"Athenea Rose in Safe Cracked, Holes Filled",2387.01,6,0,4,0,0,1,0,1
196,Ava Devine in Milf Asian Cummouth Facial,1971.46,8,0,2,0,0,0,0,0
197,"Barbie Sins in Anal Domination, 6on1 DAP",2676.75,11,0,4,0,0,0,0,0
198,Barbie Sins in DP Bandits! Sc4,2951.67,8,0,1,0,0,0,0,0
199,Blanche Bradburry in 10 Guy Anal Showdown,5188.42,16,0,3,0,0,0,0,0
200,Blanche Bradburry in Gangbang Anal Blitz,2098.1,6,0,1,0,0,0,0,0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,4003.86,10,0,4,0,0,0,0,0
202,Blanche Bradburry in Rough DAP Gangbang,3178.32,6,0,2,0,0,0,0,0
203,Blanche Bradburry in Triple Penetration Madness,3097.07,8,0,2,0,0,0,0,0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3392.67,5,0,1,0,0,0,0,0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3460.38,7,0,2,0,0,0,0,0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3478.7,7,0,2,0,0,0,0,0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",4584.67,6,0,3,0,0,0,0,0
208,Cherry Kiss in DP Bandits! Sc2,2380.63,5,0,1,0,0,0,0,0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3294.08,17,0,6,0,0,0,0,0
210,Destiny Mira in Put My Back Into It,1176.67,2,0,1,0,0,0,0,0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",6115.7,16,0,4,0,0,0,0,0
212,Jolee Love in Craving For Cocks,3458.3,7,0,3,0,0,0,0,0
213,Jolee Love in DP Bandits! 2 Sc3,3020.64,8,0,3,0,0,0,0,0
214,Jolee Love in Hardcore DAP Creampie,2874.07,5,0,2,0,0,0,0,0
215,,2020.04,3,0,1,0,0,0,0,0
216,,1843.83,2,0,1,0,0,0,0,0
217,,2426.39,4,0,2,0,0,0,0,0
218,,2378.98,2,0,1,0,0,0,0,0
219,,2209.99,12,0,3,0,0,0,0,0
220,,2484.12,3,0,1,0,0,0,0,0
221,,1027.97,6,0,3,0,0,0,0,0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",3097.41,18,0,3,0,0,0,0,0
223,,2142.37,14,0,4,0,0,0,0,0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2381.19,10,0,3,0,0,0,0,0
225,,2856.41,9,0,2,0,0,0,0,0
226,,2213.08,14,0,4,0,0,0,0,0
227,Adriana Chechik in Horny Housewives 6 Sc2,2073.71,8,0,2,0,0,0,0,0
228,Adira Allure in Airtight Diva Sc4,1848.07,4,0,2,0,0,0,0,0
229,Alexis Tae in Gangbang Sluts Sc1,2470.0,15,0,3,0,0,0,0,0
230,Amirah Adara in DP Bandits! Sc1,3162.37,8,0,1,0,0,0,0,0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,1838.44,11,0,4,0,0,0,0,0
232,Anai Loves in My stepmom,1847.97,4,0,2,0,0,0,0,0
233,Angela White in Angela's Airtight DP,2710.14,7,0,3,0,0,1,0,1
234,Anissa Kate in Love Everything About Her,2003.57,4,0,2,0,0,0,0,0
235,Anissa Kate in Sizziling Double Penetration Delight,1963.0,7,0,3,0,0,0,0,0
236,"Anissa Kate, Olivia Del Rio in Personal Guide Chapter 1",1572.27,1,0,1,0,0,0,0,0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",2276.54,6,0,2,0,0,0,0,0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,2911.74,11,0,3,0,0,0,0,0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",1808.53,10,0,2,0,0,0,0,0
240,Assh Lee in All Over That Cock,1501.5,2,0,1,0,0,0,0,0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",1811.95,5,0,1,0,0,0,0,0
242,Baby Gemini in Ricky's Room Blowbang,930.94,7,0,1,0,0,0,0,0
243,Barbie Sins in DAP with Creampie,2322.6,5,0,1,0,0,0,0,0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4231.12,21,0,2,0,0,0,0,0
245,Belinha Baracho in Intense 5on1 Gangbang,3285.92,14,0,3,0,0,0,0,0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3969.33,8,0,1,0,0,0,0,0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",4620.67,6,0,3,0,0,0,0,0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",3686.05,6,0,3,0,0,0,0,0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",2988.67,8,0,3,0,0,0,0,0
250,,7096.8,15,0,3,0,0,0,0,0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",3115.78,7,0,1,0,0,0,0,0
252,Bonny Bon in Sexual Rage 2 Sc3,2024.6,4,0,1,0,0,1,0,1
253,Cali Caliente in The Gangbang Part IV,2273.1,12,0,3,0,0,0,0,0
254,Carla Morelli in Gangbang with 4 Cocks,2008.38,6,0,2,0,0,1,0,1
255,Carla Morelli in Hot for Teacher,1324.61,3,0,2,0,0,0,0,0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4683.35,16,0,4,0,0,0,0,0
257,Chloe Amour in Mon Amour Sc1,2140.95,14,0,3,0,0,0,0,0
258,Chloe Amour in Mon Amour Sc2,1705.6,4,0,2,0,0,0,0,0
259,Chloe Amour in Mon Amour Sc3,2999.48,12,0,4,0,0,0,0,0
260,"Chloe Amour, Jennifer White in Mon Amour Sc4",2322.44,4,0,2,0,0,0,0,0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,4135.34,17,0,4,0,0,0,0,0
262,"Cindy Starfall, Gaia in Swappers Sc1",1883.39,5,0,3,0,0,0,0,0
263,Cookie Cream in Asian 1st BBG Threesome & DP,1959.48,6,0,2,0,0,0,0,0
265,"Danielle Renee, MarsFoxxx in Group Bang",2928.73,16,0,4,0,0,0,0,0
266,Daria Glower in Die Haremsw√§chterin des √ñl Scheichs Sc6,872.11,2,0,1,0,0,1,0,1
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,3879.57,12,0,3,0,0,0,0,0
268,Anissa Kate in Les Retrouvailles Sc5,1655.17,10,0,2,0,0,0,0,0
269,Emmanuelle Noire in Busty Ebony Beauty,2046.87,10,0,3,0,0,0,0,0
270,Francesca Le in Lewood Gangbang Battle of the MILFs Sc1,313.95,1,0,1,0,0,0,0,0
271,Francesca Le in Lewood Gangbang Battle of the MILFs Sc2,322.69,2,0,1,0,0,0,0,0
272,Francesca Le in Lewood Gangbang Battle of the MILFs Sc3,351.55,1,0,1,0,0,1,0,1
273,Francesca Le in Lewood Gangbang Battle of the MILFs Sc4,353.15,2,0,1,0,0,0,0,0
274,Francesca Le in Lewood Gangbang Battle of the MILFs Sc5,372.77,2,0,2,0,0,0,0,0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,2935.1,18,0,3,0,0,0,0,0
276,"Gaia in Screw My Wife, Please 76 Sc2",970.13,1,0,1,0,0,0,0,0
277,Gaia in Throated 39 Sc5,1687.53,9,0,3,0,0,0,0,0
278,Hannah Jo in Thick Dick Threesome,2248.52,10,0,1,0,0,0,0,0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,4474.28,23,0,3,0,0,0,0,0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",1356.44,8,0,3,0,0,0,0,0
281,Jada Fire in Assault That Ass 8 Sc3,1772.08,6,0,3,0,0,0,0,0
282,Jada Fire in Throat Yogurt 2 Sc1,825.17,2,0,2,0,0,1,0,1
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,1741.89,5,0,1,0,0,0,0,0
284,Jasminy Villar in The Stepfather And His Four Friends,3601.9,16,0,4,0,0,0,0,0
285,Jena LaRose in Blacks On Blondes,2051.49,6,0,1,0,0,0,0,0
286,Jennifer White in Jennifer White Overload Sc1,3271.9,19,0,3,0,0,0,0,0
287,Jennifer White in Jennifer White Overload Sc2,3398.01,14,0,4,0,0,1,0,1
288,Jennifer White in Jennifer White Overload Sc3,2525.56,17,0,4,0,0,0,0,0
289,Jenny Pretinha in Fucked by 3 Huge Cocks,1525.5,2,0,2,0,0,0,0,0
290,Juelz Ventura in POV BBC Airtight Gangbang,2889.31,15,0,2,0,0,0,0,0
291,"Jureka Del Mar, Maylee Fun in Asian Hotties Work to Cure",927.16,1,0,1,0,0,0,0,0
292,Katalina Kyle in Ass Worship 18 Sc3,2497.1,12,0,3,0,0,0,0,0
293,Katalina Kyle in Takes Every Inch Of Manuel,1619.48,7,0,2,0,0,0,0,0
294,Katia Belinii in Swallowing 5 Big Loads,1602.46,8,0,3,0,0,0,0,0
295,Kayla Carera in Bride Bangers Sc1,1466.64,1,0,1,0,0,0,0,0
296,Kayla Carrera in Anal Integrity Sc1,2701.5,3,0,3,0,0,0,0,0
297,Kaylani Lei in Asian Fuck Machines Sc5,2666.77,5,0,3,0,0,0,0,0
298,Kazumi Squirts in BBC Orgy Room,4253.97,18,0,3,0,0,0,0,0
299,Kazumi Squirts in Gangbang With Piss and DP,3392.33,15,0,5,0,0,0,0,0
301,Kelly Oliveira in Assfucked 4on1 with DP,2982.93,6,0,2,0,0,0,0,0
302,Kelly Oliveira in First DP for Brazilian Teen,1941.68,4,0,2,0,0,0,0,0
303,Kelly Oliveira in Sexy Latina DAP 3on1,2731.42,5,0,3,0,0,0,0,0
304,"Kelly Oliveira, Veronica Leal in 5on2 orgy with DP",3354.17,5,0,3,0,0,0,0,0
305,Keri Sable in Cum Filled Asshole Overload 2 Sc1,2796.03,3,0,2,0,0,1,0,1
306,Kim XXX in Manga Total Vollgespritzt Sc1,1921.96,7,0,3,0,0,0,0,0
307,Kim XXX in Tiny Whore Kim XXX Stuffed,2071.48,5,0,2,0,0,0,0,0
308,Kira Thorn in Balls Deep 5on2,2984.69,7,0,2,0,0,0,0,0
309,Kitana Montana in Birthday Threeway,1767.27,6,0,2,0,0,0,0,0
310,Kitana Montana in Post,2356.53,10,0,3,0,0,0,0,0
311,Laura Fiorentino in 6on1 Swallow,3877.5,14,0,6,0,0,0,0,0
312,Lela Star in Assparade 54 Sc2,719.83,2,0,2,0,0,0,0,0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,2180.24,5,0,3,0,0,0,0,0
314,Lolly Ink in True Gonzo Sc5,1580.37,4,0,3,0,0,1,0,1
315,Luna Star in Double Stuffed,2003.93,8,0,2,0,0,0,0,0
316,Luna Star in Why She's A Pornstar,2068.02,7,0,3,0,0,0,0,0
317,Marilyn Johnson in Airtight Diva Sc1,1617.07,2,0,1,0,0,0,0,0
318,"Mariska, Tekohas in Cum Cum Gangbang Orgy",996.07,6,0,2,0,0,0,0,0
319,Maxine X in I'm Here for the Gang Bang! Sc1,2883.66,9,0,4,0,0,0,0,0
320,Maxine X in I'm Here for the Gang Bang! Sc2,3520.19,11,0,2,0,0,0,0,0
321,Megan Rain in 10 Cock Blowbang!,1505.33,2,0,2,0,0,1,0,1
322,Melissa Hot in Fucked by 4 Big Cocks,2871.21,8,0,3,0,0,0,0,0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",3110.92,7,0,1,0,0,0,0,0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1472.38,5,0,2,0,0,0,0,0
325,Mercedes Carrera in MILF Cumsluts Sc4,2974.8,10,0,3,0,0,0,0,0
326,Mia Trejsi in 100% Hell,3432.54,14,0,3,0,0,0,0,0
327,"Mia Trejsi, Ria Sunn in Angels Of Hardcore 3 Sc1",368.17,2,0,2,0,0,0,0,0
328,Mih Ninfetinha in 4on1 with DP,2697.69,12,0,3,0,0,0,0,0
329,Miss Teela in First Time 10 Gangbang,1702.4,9,0,2,0,0,0,0,0
330,Monika Fox in DP Fantasies 11 Sc3,2549.07,6,0,3,0,0,1,0,1
331,Natasha Teen in Pussy DAPTAP,2914.04,6,0,2,0,0,0,0,0
332,Nia Nacci in Cum Bang 15 Sc3,2274.57,10,0,3,0,0,1,0,1
333,Nia Nacci in We Fuck Black Girls 14 Sc5,1953.01,2,0,2,0,0,0,0,0
334,Nia Nacci in White Out 9 Sc2,3971.75,17,0,4,0,0,0,0,0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",2217.92,11,0,2,0,0,0,0,0
336,Nina Elle in Big Wet Milf Asses Sc2,1890.05,3,0,1,0,0,0,0,0
337,Nina Elle in Gang Bang Addiction Sc4,2890.12,7,0,2,0,0,0,0,0
338,Nina Elle in MILF Cumsluts Sc3,2316.06,2,0,2,0,0,1,0,1
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1654.47,5,0,4,0,0,0,0,0
340,Phoenix Marie in Ass Worship 13 Sc4,2550.74,8,0,1,0,0,0,0,0
341,Rachele Richey in Gangbang Audition,2679.13,8,0,2,0,0,0,0,0
342,Rose Lynn in Airtight Diva Sc3,1585.07,5,0,3,0,0,0,0,0
343,Sadie Summers in Gangbang Sluts Sc2,3098.0,19,0,4,0,0,0,0,0
344,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc2,692.67,4,0,1,0,0,0,0,0
345,"Sai Tai Tiger, Daria Glower in Die Haremsw√§chterin des √ñl Scheichs Sc5",912.0,1,0,1,0,0,0,0,0
346,"Sai Tai Tiger, Daria Glower, Valerie Hilton in Die Haremsw√§chterin des √ñl Scheichs Sc1",1574.35,1,0,1,0,0,1,0,1
347,Sandra Parker in 1st at GB Junkies,1725.71,8,0,2,0,0,0,0,0
348,Sandra Parker in Anal Driller 9 Sc3,1411.03,6,0,1,0,0,0,0,0
349,Sandra Parker in Analizator Sc4,1873.37,2,0,2,0,0,1,0,1
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1852.02,4,0,1,0,0,0,0,0
351,Sandra Parker in Double Stuffed 8 Sc1,1689.02,5,0,1,0,0,0,0,0
352,Sara Retali in BBC Piss Gangbang,2231.14,13,0,2,0,0,0,0,0
353,Sara Retali in Slut Cant Get Enough Gangbang,1841.15,9,0,4,0,0,0,0,0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1207.62,6,0,2,0,0,0,0,0
355,"Sara Retali, Sapphire Astrea in Spa Day Gone Wild",930.08,2,0,2,0,0,0,0,0
356,Sarai Minx in Big Tit Slut Milks Cock,1562.59,5,0,3,0,0,0,0,0
357,"Sheila Ortega, Sara Retali, Anny Star in 3 Latinas by the Pool",2443.6,5,0,3,0,0,0,0,0
358,Shyla Stylez in Anal Integrity Sc2,2097.2,4,0,3,0,0,0,0,0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,3149.11,14,0,2,0,0,0,0,0
360,Skin Diamond in Rump Raiders Sc3,1820.97,3,0,1,0,0,0,0,0
361,,18562.53,50,0,4,0,0,0,0,0
362,Summer Day in America Bukkake Live,1495.04,2,0,2,0,0,1,0,1
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1893.47,11,0,3,0,0,0,0,0
364,Summer Vixen in Gangbang Sluts Sc3,2318.0,14,0,3,0,0,0,0,0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,3808.68,15,0,4,0,0,0,0,0
366,Tekohas in Ass & BigTits,1987.33,6,0,3,0,0,0,0,0
367,Tekohas in Bareback Party in Stuttgart,3021.03,15,0,3,0,0,0,0,0
368,Thai Suzy in WeLoveBukkake 4,993.54,2,0,2,0,0,1,0,1
369,Tia Maria in Cum On Melon Tits,2922.37,8,0,2,0,0,0,0,0
370,Tia Maria in DPd By Two BWCs,1677.93,4,0,2,0,0,0,0,0
371,Tyra Ride in First BBC  DP Gangbang,2411.49,12,0,4,0,0,0,0,0
372,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc10,363.0,1,0,1,0,0,0,0,0
373,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc11,377.98,1,0,1,0,0,0,0,0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,3230.76,17,0,4,0,0,0,0,0
376,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc8,287.95,1,0,1,0,0,0,0,0
378,Veronica Leal in Domination Gangbang,3676.9,18,0,4,0,0,0,0,0
379,Vittoria Devine in DP Pee 5on1,4787.33,17,0,3,0,0,0,0,0
380,Vittoria Devine in Domination Gangbang,2075.61,8,0,4,0,0,0,0,0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,2631.01,4,0,1,0,0,0,0,0
382,Vit√≥ria Beatriz in Edjunior VideoGuru,892.0,2,0,1,0,0,0,0,0
383,Willow Ryder in I Love Anal 3 Sc3,2177.1,11,0,2,0,0,1,0,1
384,Yasmina Khan in Birthday Gangbang,1583.33,6,0,3,0,0,0,0,0
385,Yasmina Khan in Play with 4 Cocks at Once!,2459.47,11,0,1,0,0,0,0,0
386,AJ Applegate in Gangbang Me Sc1,3184.69,4,0,2,0,0,0,0,0
388,Adira Allure in Interracial Blowbang 24 Sc1,2018.72,8,0,4,0,0,1,0,1
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",1200.63,6,0,2,0,0,0,0,0
392,"Adriana Chechik, Gaia in Grease XXX A Parody Sc5",1245.2,2,0,1,0,0,1,0,1
394,Adrianna Luna in Praise The Load 7 Sc1,1590.98,6,0,4,0,0,1,0,1
395,Adrianna Luna in Slut Puppies 5 Sc5,1963.99,7,0,1,0,0,0,0,0
396,Aidra Fox in Gangbanged 7 Sc1,3703.02,7,0,3,0,0,0,0,0
397,Alena Croft in Blacks on Cougars 17 Sc1,1905.91,4,0,2,0,0,1,0,1
398,Alena Croft in Feeding Frenzy 12 Sc2,1668.84,4,0,3,0,0,1,0,1
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2074.42,8,0,3,0,0,0,0,0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",3823.94,9,0,3,0,0,0,0,0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2108.36,9,0,2,0,0,1,0,1
402,Alexis Ford in Gang Bang Addiction Sc3,3074.11,17,0,3,0,0,0,0,0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2406.19,8,0,3,0,0,2,0,2
404,Alexis Monroe in Gang Bang Addiction Sc5,3167.07,7,0,4,0,0,0,0,0
405,Alexis Texas in Gang Bang Addiction Sc1,1871.1,9,0,2,0,0,0,0,0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2099.5,12,0,2,0,0,0,0,0
407,Alina in Annegret Zugekleistert Sc4,1086.13,4,0,2,0,0,0,0,0
409,Alina Lopez in No Going Back Sc1,720.81,1,0,1,0,0,0,0,0
410,Alina Lopez in Perfectly Natural 19 Sc4,1335.45,2,0,1,0,0,0,0,0
411,Alina Lopez in Pussy is The Best Medicine 9 Sc5,643.23,1,0,1,0,0,0,0,0
413,Alina Lopez in Wet Food 9 Sc1,3173.84,17,0,2,0,0,1,0,1
415,"Alyssa Divine, Kiki Minaj, Victoria Pure, Victoria Summers in Backstage Bangers Sc1",1698.25,5,0,2,0,0,0,0,0
416,Amara Romani in Gangbang Auditions 31 Sc3,3535.03,12,0,4,0,0,0,0,0
418,Amber Deen in Dogfart Invades Europe! 4 Sc1,2030.69,4,0,2,0,0,1,0,1
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2825.64,11,0,3,0,0,0,0,0
420,Amelia Sadaat in White Dicks in Black Chics 3 Sc4,631.71,1,0,1,0,0,0,0,0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2221.59,9,0,2,0,0,1,0,1
422,Andi Anderson in Young Harlots Gang Bang Sc2,4059.46,23,0,4,0,0,0,0,0
423,"Angel Eyes, Jada Fire in Freak Nasty Sc1",681.83,3,0,2,0,0,0,0,0
424,Angela White in Going All Out with a Gangbang 2 Sc4,2827.82,8,0,4,0,0,0,0,0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3376.44,15,0,3,0,0,0,0,0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",1079.8,6,0,3,0,0,1,0,1
428,Alejandra Rico in Intense Latin Gangbang,1350.14,5,0,2,0,0,0,0,0
429,Alejandra Rico in Tons of Cum,1581.57,5,0,2,0,0,1,0,1
430,Alex Grey in A Dirty Submissive Slut For Cock,1393.96,3,0,1,0,0,1,0,1
431,Alexa Nova in GangBang Creampie 246,2005.99,7,0,1,0,0,0,0,0
432,Alexa Payne in Stepmom Seduction on the Kitchen Counter,2150.67,5,0,2,0,0,0,0,0
433,"Alexia Vendome, Sothy Hiko, Yasmine Lafitte in No Limits Anal and Oral Orgy",1317.99,4,0,1,0,0,0,0,0
434,Alexis Kay in GangBang Creampie 417,782.58,4,0,3,0,0,0,0,0
435,Alicia Ribeiro in Anal and Oral with 3 Big Black Cocks,2701.5,4,0,2,0,0,0,0,0
436,Alicia Trece in Rough Gangbang and Pee Play,2822.9,8,0,3,0,0,0,0,0
437,Aliyah Taylor in Gang Bang All Her Holes,1805.91,7,0,3,0,0,0,0,0
438,Allatra Hot in MILF Craving Hardcore Attention,1370.7,4,0,1,0,0,0,0,0
439,Alura Jenson in GangBang Creampie 240,2719.32,10,0,3,0,0,0,0,0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",3930.19,11,0,3,0,0,0,0,0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2893.35,5,0,3,0,0,0,0,0
442,Amari Anne in You Wanna Cheat Again,2416.03,6,0,2,0,0,0,0,0
443,Amirah Adara in Rough Gangbang Session,3034.87,2,0,1,0,0,0,0,0
444,Amy Reid in AllOut Blowbang Session,1026.51,2,0,2,0,0,1,0,1
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,3231.2,7,0,3,0,0,0,0,0
446,Ana J√∫lia in DP com a Mulata Cavala,2450.12,7,0,3,0,0,0,0,0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",1562.35,3,0,2,0,0,0,0,0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",5115.56,4,0,2,0,0,0,0,0
449,Angel Lima in Big Butt Airtight Show,2837.54,5,0,2,0,0,0,0,0
450,Angel Lima in Hardcore Brazilian Double Anal,3478.13,7,0,3,0,0,0,0,0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3114.73,8,0,2,0,0,0,0,0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",1476.81,6,0,2,0,0,0,0,0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3114.77,11,0,2,0,0,0,0,0
454,Angel Sins in Short Stuff Big Stuff,2863.71,2,0,1,0,0,0,0,0
455,"Angel Smalls, Anna De Ville, Barbie Sins, Jureka Del Mar, May Thai, Nathaly Cherie, Selvaggia in Messy Facial Compilation",479.91,1,0,1,0,0,1,0,1
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",4638.91,13,0,4,0,0,0,0,0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,2835.8,8,0,1,0,0,0,0,0
459,Ania Kinski in Kinky DP Session At The Clinic,2079.32,1,0,1,0,0,1,0,1
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,2198.79,8,0,2,0,0,0,0,0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",4163.26,11,0,3,0,0,0,0,0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",1977.47,3,0,2,0,0,0,0,0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",2405.01,4,0,1,0,0,0,0,0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3407.16,7,0,2,0,0,0,0,0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",1761.96,8,0,2,0,0,0,0,0
468,Anissa Kate in A Hot Surfer Threesome,1927.33,6,0,2,0,0,0,0,0
469,Anissa Kate in Hardcore Business Meeting,2364.29,6,0,3,0,0,0,0,0
470,Anissa Kate in Poolside DP with BBC and Cumshot Finish,3007.15,6,0,2,0,0,0,0,0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3045.74,8,0,3,0,0,0,0,0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3016.48,7,0,2,0,0,0,0,0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",2188.8,8,0,3,0,0,0,0,0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",2800.07,7,0,3,0,0,1,0,1
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",913.45,7,0,2,0,0,0,0,0
477,Anni Star in Lingerie Pleasure Premi√®re,841.89,2,0,2,0,0,0,0,0
479,April Snow in GangBang Creampie 232,2387.31,11,0,3,0,0,0,0,0
480,"Ariel Pure Magic, Zoey Reyes in Dominican Oil Twins",997.95,1,0,1,0,0,0,0,0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,2230.09,4,0,1,0,0,0,0,0
482,"Ashby Winter in Vogue 2, Part 5",2819.84,6,0,3,0,0,0,0,0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,945.82,5,0,2,0,0,0,0,0
484,Ashley Cumstar in Gangbang Party,685.52,4,0,2,0,0,0,0,0
485,Athenea Rose in 5on1 Hardcore Gangbang,3371.07,15,0,3,0,0,0,0,0
486,Athenea Rose in 7on1 DAP Gangbang,3957.23,12,0,5,0,0,0,0,0
487,Athenea Rose in Airtight 6on1 Destruction,3391.51,16,0,3,0,0,0,0,0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,3491.85,11,0,3,0,0,0,0,0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5211.6,22,0,5,0,0,0,0,0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,4281.95,14,0,5,0,0,0,0,0
491,Athenea Rose in Hardcore Interracial DAP,3057.12,2,0,1,0,0,0,0,0
492,Athenea Rose in Hecho en Medelln,3900.67,6,0,3,0,0,0,0,0
493,Athenea Rose in Intense Anal Destruction,1839.23,2,0,1,0,0,0,0,0
494,Athenea Rose in Loves Public Anal,2607.95,2,0,1,0,0,0,0,0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,1953.07,1,0,1,0,0,0,0,0
496,Athenea Rose in Playing with 3 BBC,2441.44,5,0,1,0,0,0,0,0
497,Athenea Rose in PremiumBukkake #1,1263.3,5,0,2,0,0,1,0,1
498,Athenea Rose in PremiumBukkake #2,1064.0,2,0,2,0,0,2,0,2
499,Athenea Rose in PremiumBukkake #3,2329.59,4,0,2,0,0,1,0,1
500,Athenea Rose in Sex Crazed Slut 4on1,2765.94,8,0,4,0,0,0,0,0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,3828.96,15,0,5,0,0,0,0,0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",3745.63,3,0,1,0,0,0,0,0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",4161.3,10,0,3,0,0,0,0,0
504,Aubrey Black in GangBang Creampie 225,1779.39,8,0,3,0,0,0,0,0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",4394.87,12,0,4,0,0,0,0,0
506,Avery Jane in Brutal 7on1 DAP Birthday,3513.58,10,0,5,0,0,0,0,0
507,Avery Jane in Milking Mike Adriano,2301.68,4,0,3,0,0,0,0,0
508,Avery Jane in Piss Soaked Backdoor Debut,4138.75,16,0,5,0,0,0,0,0
509,Avi Love in GangBang Creampie 216,2673.71,11,0,4,0,0,0,0,0
511,Baby Gemini in All About The Booty,1607.66,1,0,1,0,0,0,0,0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,2648.0,10,0,3,0,0,1,0,1
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",3220.68,6,0,2,0,0,0,0,0
514,Barbie Esm in Rough 4on1 DAP Fantasy,3077.02,12,0,3,0,0,0,0,0
515,Barbie Sins in Barbie Gets wet with 2 BBC,2173.76,6,0,2,0,0,0,0,0
517,"Barbie Sins in DAP, Piss and Power Play",3215.13,8,0,3,0,0,0,0,0
518,Barbie Sins in No Holes Barred Gonzo Assault,3180.83,14,0,4,0,0,0,0,0
519,Barbie Sins in Rough DAP & Swallow Madness,2896.44,12,0,4,0,0,0,0,0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",1603.11,4,0,2,0,0,0,0,0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",3022.73,9,0,1,0,0,0,0,0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",2763.36,12,0,2,0,0,0,0,0
931,"Sarai Minx, Savanah Storm in Messy Manicure Threesome",1798.08,6,0,2,0,0,0,0,0
1008,"Sai Tai Tiger, Salma De Nora in Die Haremsw√§chterin des √ñl Scheichs Sc4",1003.34,3,0,1,0,0,0,0,0
1020,,2733.12,8,0,1,0,0,0,0,0
1023,Sai Tai Tiger in Die Haremsw√§chterin die Verbannung Sc3,568.01,1,0,1,0,0,0,0,0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",1826.8,7,0,2,0,0,0,0,0
1030,Sai Tai Tiger in Die Haremsw√§chterin die Verbannung Sc2,973.26,4,0,1,0,0,0,0,0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",2183.45,6,0,1,0,0,0,0,0
1038,Alina Li in Asian Fuck Faces 3 Sc6,1208.25,2,0,2,0,0,2,0,2
1043,"Anni Star, CJ Miles in Glamorous Double Penetration",1859.5,4,0,2,0,0,0,0,0
1044,"Ania Kinski, Anissa Kate in Real Estate Gets Real Dirty",1707.04,4,0,2,0,0,1,0,1
1047,Ania Kinski in Home Alone Double Penetration,2581.06,10,0,3,0,0,0,0,0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",2046.91,7,0,3,0,0,0,0,0
1062,Six Bodies In Motion,4574.0,8,0,2,0,0,0,0,0
1063,,1618.58,5,0,2,0,0,0,0,0
1065,,2630.13,8,0,4,0,0,0,0,0
1068,,1027.87,5,0,3,0,0,0,0,0


============================================================
[79/124] Legacy\utils\body_art_parser.py
------------------------------------------------------------
"""
Utilitaire partag√© pour parser les tattoos/piercings depuis n'importe quelle source.
G√®re les formats structur√©s ("wrist (tribal)") et flat ("multiple tattoos").
"""
import re


def parse_body_art(raw_text: str) -> list[dict]:
    """
    Parse un texte brut de tattoos/piercings en liste structur√©e.
    
    Retourne: [{"position": str, "description": str | None}]
    
    Exemples d'entr√©e:
        "left wrist (tribal); right arm (sleeve)" ‚Üí 2 entr√©es structur√©es
        "Yes - multiple tattoos" ‚Üí 1 entr√©e flat
        "None" ‚Üí []
    """
    items = []
    if not raw_text or raw_text.strip().lower() in ('unknown', 'no', 'n/a', 'none', ''):
        return items

    # Retirer le pr√©fixe "Yes - " ou "Yes," courant
    cleaned = re.sub(r'^Yes\s*[-,]?\s*', '', raw_text, flags=re.I).strip()
    if not cleaned:
        return items

    # S√©parer par point-virgule d'abord (plus fiable), sinon virgule
    if ';' in cleaned:
        parts = cleaned.split(';')
    else:
        parts = cleaned.split(',')

    for part in parts:
        part = part.strip()
        if not part or part.lower() in ('unknown', 'no', 'n/a', 'none'):
            continue

        # Tenter de parser "position (description)"
        m = re.match(r'(.+?)\s*\((.+?)\)\s*$', part)
        if m:
            items.append({
                "position": m.group(1).strip(),
                "description": m.group(2).strip()
            })
        else:
            # Tenter "position - description" ou "position : description"
            m2 = re.match(r'(.+?)\s*[-‚Äì:]\s+(.+)', part)
            if m2 and len(m2.group(1)) < 30:
                items.append({
                    "position": m2.group(1).strip(),
                    "description": m2.group(2).strip()
                })
            else:
                items.append({
                    "position": part,
                    "description": None
                })

    return items


============================================================
[80/124] Legacy\utils\cleanup_all.py
------------------------------------------------------------
from __future__ import annotations
import sqlite3
from collections import defaultdict, OrderedDict
from typing import Dict, List, Set, Tuple, Optional, Any

DB_PATH = r"H:\Stash\stash-go.sqlite"

# ============================================================
# CONFIGURATION DU NETTOYAGE
# ============================================================

# Dur√©e minimale globale : tout marqueur <= cette valeur (s) sera supprim√©
# Mettre √† 0.0 pour d√©sactiver.
GLOBAL_MIN_DURATION: float = 60.0

# R√®gles cibl√©es par tag (ind√©pendantes de la r√®gle globale)
# Format : (tag, dur√©e_max_ou_None_pour_tous)
TARGETED_DELETE_RULES: List[Tuple[str, Optional[float]]] = [
    ("69", None),  # Supprimer TOUS les marqueurs "69" (quelle que soit la dur√©e)
]

# ============================================================
conn = sqlite3.connect(DB_PATH)
cur = conn.cursor()

print("=== M√âNAGE COMPLET DES MARQUEURS ===\n")

# ---- 1. Chargement de tous les marqueurs ----
cur.execute("""
    SELECT m.id, m.seconds, m.end_seconds, GROUP_CONCAT(t.name), m.title
    FROM scene_markers m
    LEFT JOIN scene_markers_tags mt ON m.id = mt.scene_marker_id
    LEFT JOIN tags t ON mt.tag_id = t.id
    GROUP BY m.id
    ORDER BY m.scene_id, m.seconds
""")
all_markers: List[Any] = cur.fetchall()

# Construire un dict id -> marker data
marker_data: Dict[int, Dict[str, Any]] = {}
for row in all_markers:
    mid: int = int(row[0])
    start: float = float(row[1])
    end: float = float(row[2]) if row[2] else 0.0
    tags_str: str = str(row[3]) if row[3] else ""
    title: str = str(row[4]) if row[4] else ""
    tags: Set[str] = set(tags_str.split(",")) if tags_str else set()
    if title:
        tags.add(title)
    marker_data[mid] = {"start": start, "end": end, "tags": tags}

# ---- 2. Grouper les IDs par sc√®ne (ordonn√©s par seconds) ----
cur.execute("SELECT scene_id, id FROM scene_markers ORDER BY scene_id, seconds")
scene_groups: Dict[int, List[int]] = OrderedDict()
for row in cur.fetchall():
    sid: int = int(row[0])
    mid: int = int(row[1])
    if sid not in scene_groups:
        scene_groups[sid] = []
    scene_groups[sid].append(mid)

# ---- 3. R√®gles FUSION + CONTENANCE ----
markers_to_delete: Set[int] = set()
markers_to_update: Dict[int, float] = {}
fusions_count: int = 0
contained_count: int = 0

for sid, mids in scene_groups.items():
    for i in range(len(mids)):
        id1: int = mids[i]
        if id1 not in marker_data:
            continue
        d1 = marker_data[id1]
        start1: float = d1["start"]
        end1: float = d1["end"]
        tags1: Set[str] = d1["tags"]

        for j in range(i + 1, len(mids)):
            id2: int = mids[j]
            if id2 not in marker_data or id2 in markers_to_delete:
                continue
            d2 = marker_data[id2]
            start2: float = d2["start"]
            end2: float = d2["end"]
            tags2: Set[str] = d2["tags"]

            # Doit avoir le m√™me tag unique
            if tags1 != tags2 or len(tags1) != 1:
                if start2 >= end1:
                    break
                continue

            gap: float = start2 - end1

            # R√®gle FUSION : gap < 10s
            if gap < 10:
                new_end: float = max(end1, end2 if end2 else start2)
                markers_to_update[id1] = new_end
                markers_to_delete.add(id2)
                end1 = new_end
                d1["end"] = new_end
                fusions_count += 1
                continue

            # Arr√™t si hors zone
            if start2 >= end1:
                break

            # R√®gle CONTENANCE
            if start1 < end2 and start2 < end1:
                if start1 <= start2 and end1 >= end2 and end2 > 0:
                    markers_to_delete.add(id2)
                    contained_count += 1
                elif start2 <= start1 and end2 >= end1 and end1 > 0:
                    markers_to_delete.add(id1)
                    contained_count += 1

# ---- 4. R√®gle GLOBALE : dur√©e minimale ----
global_short_count: int = 0
if GLOBAL_MIN_DURATION > 0:
    for mid, d in marker_data.items():
        if mid in markers_to_delete:
            continue
        m_dur: float = d["end"] - d["start"] if d["end"] > d["start"] else 0.0
        if m_dur <= GLOBAL_MIN_DURATION:
            markers_to_delete.add(mid)
            global_short_count += 1
            # Annuler la fusion si ce marqueur √©tait source d'une mise √† jour
            if mid in markers_to_update:
                del markers_to_update[mid]

# ---- 5. R√®gles CIBL√âES par tag ----
targeted_deletes: Dict[str, int] = {}
for mid, d in marker_data.items():
    if mid in markers_to_delete:
        continue
    m_dur = d["end"] - d["start"] if d["end"] > d["start"] else 0.0
    m_tags: Set[str] = d["tags"]
    for tag_rule, max_dur in TARGETED_DELETE_RULES:
        if tag_rule in m_tags:
            if max_dur is None or m_dur <= max_dur:
                markers_to_delete.add(mid)
                rule_key: str = f"{tag_rule} (<= {max_dur}s)" if max_dur else tag_rule
                targeted_deletes[rule_key] = targeted_deletes.get(rule_key, 0) + 1
                break

# ---- 6. Bilan ----
total_updates: int = len(markers_to_update)
total_deletes: int = len(markers_to_delete)

print(f"Bilan du nettoyage identifi√© :\n")
print(f"  [FUSION]     {fusions_count} marqueurs fusionn√©s")
print(f"  [CONTENANCE] {contained_count} marqueurs contenus dans un autre")
if GLOBAL_MIN_DURATION > 0:
    print(f"  [DUR√âE ‚â§{int(GLOBAL_MIN_DURATION)}s] {global_short_count} marqueurs trop courts (toutes cat√©gories)")
for rule, count in targeted_deletes.items():
    print(f"  [CIBL√â]      {count} marqueurs '{rule}'")
print(f"\n  TOTAL : {total_updates} UPDATE(s), {total_deletes} DELETE(s)\n")

if not markers_to_delete and not markers_to_update:
    print("[OK] Aucune action n√©cessaire. Base d√©j√† propre !")
    conn.close()
    exit()

confirm: str = input("Voulez-vous appliquer tous ces changements ? (OUI/non) : ")

if confirm == "OUI":
    try:
        for mid, new_end in markers_to_update.items():
            cur.execute("UPDATE scene_markers SET end_seconds = ? WHERE id = ?", (new_end, mid))

        ids_to_del: List[Tuple[int]] = [(mid,) for mid in markers_to_delete]
        cur.executemany("DELETE FROM scene_markers_tags WHERE scene_marker_id = ?", ids_to_del)
        cur.executemany("DELETE FROM scene_markers WHERE id = ?", ids_to_del)

        conn.commit()
        print(f"\n[OK] {total_updates} mise(s) √† jour et {total_deletes} suppression(s) appliqu√©es avec succ√®s.")
    except Exception as e:
        print(f"\n[ERREUR] √âchec : {e}")
        conn.rollback()
else:
    print("[INFO] Nettoyage annul√©.")

conn.close()


============================================================
[81/124] Legacy\utils\cleanup_specific.py
------------------------------------------------------------
import sqlite3

DB_PATH = r"H:\Stash\stash-go.sqlite"

def main():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()

    print("=== NETTOYAGE CIBL√â DES MARQUEURS ===\n")

    # 1. Identifier les marqueurs "69" (via Tag ou Titre)
    cur.execute("""
        SELECT DISTINCT m.id 
        FROM scene_markers m
        LEFT JOIN scene_markers_tags mt ON m.id = mt.scene_marker_id
        LEFT JOIN tags t ON mt.tag_id = t.id
        WHERE m.title = '69' OR t.name = '69'
    """)
    ids_69 = [r[0] for r in cur.fetchall()]

    # 2. Identifier les marqueurs "Anal" <= 60s (via Tag ou Titre)
    cur.execute("""
        SELECT DISTINCT m.id 
        FROM scene_markers m
        LEFT JOIN scene_markers_tags mt ON m.id = mt.scene_marker_id
        LEFT JOIN tags t ON mt.tag_id = t.id
        WHERE (m.title = 'Anal' OR t.name = 'Anal')
          AND ((m.end_seconds - m.seconds) <= 60 
               OR m.end_seconds IS NULL 
               OR m.end_seconds = 0)
    """)
    ids_anal_short = [r[0] for r in cur.fetchall()]

    total_ids = set(ids_69) | set(ids_anal_short)

    print(f"Bilan avant suppression :")
    print(f"  - Marqueurs '69' trouv√©s : {len(ids_69)}")
    print(f"  - Marqueurs 'Anal' (<= 60s) trouv√©s : {len(ids_anal_short)}")
    print(f"  - Total unique √† supprimer : {len(total_ids)}")

    if not total_ids:
        print("\n[!] Aucun marqueur ne correspond aux crit√®res. Fin du script.")
        conn.close()
        return

    confirm = input("\n√ätes-vous s√ªr de vouloir SUPPRIMER ces marqueurs d√©finitivement ? (OUI/non) : ")
    
    if confirm == "OUI":
        try:
            ids_tuple = [(int(mid),) for mid in total_ids]
            
            # Supprimer les liens tags
            cur.executemany("DELETE FROM scene_markers_tags WHERE scene_marker_id = ?", ids_tuple)
            # Supprimer les marqueurs
            cur.executemany("DELETE FROM scene_markers WHERE id = ?", ids_tuple)
            
            conn.commit()
            print(f"\n[OK] {len(total_ids)} marqueurs ont √©t√© supprim√©s avec succ√®s.")
        except Exception as e:
            print(f"\n[ERREUR] √âchec de la suppression : {e}")
            conn.rollback()
    else:
        print("\n[INFO] Op√©ration annul√©e.")

    conn.close()

if __name__ == "__main__":
    main()


============================================================
[82/124] Legacy\utils\customfield_utils.py
------------------------------------------------------------
# Utilitaire pour injecter des customfields performer

def inject_customfields(db, performer_id, customfields):
    """
    Injecte une liste de customfields pour un performer.
    customfields = [
        {"type": "award", "value": "..."},
        {"type": "trivia", "value": "..."},
        {"type": "tattoo", "value": "..."},
        {"type": "piercing", "value": "..."},
    ]
    """
    cur = db.conn.cursor()
    for cf in customfields:
        cur.execute(
            "INSERT INTO performer_customfields (performer_id, type, value) VALUES (?, ?, ?)",
            (performer_id, cf["type"], cf["value"])
        )
    db.conn.commit()


============================================================
[83/124] Legacy\utils\duration.py
------------------------------------------------------------
"""
Utilitaire de conversion de dur√©es pour les DVDs.
G√®re tous les formats rencontr√©s dans les sources DVD.
"""
import re
from typing import Optional


def parse_duration_to_seconds(raw: str) -> Optional[int]:
    """
    Convertit n'importe quelle repr√©sentation de dur√©e en secondes (INTEGER).

    Formats support√©s :
        '[01:55:32]'  ‚Üí data18
        '01:55:00'    ‚Üí adultdvdempire (apr√®s conversion interne)
        '115'         ‚Üí iafd (minutes brutes)
        '240 minutes' ‚Üí jeedoo
        '1 hrs. 55 mins.' ‚Üí adultdvdempire brut
    """
    if not raw:
        return None
    raw = str(raw).strip().strip("[]")

    # HH:MM:SS
    m = re.match(r'^(\d+):(\d{2}):(\d{2})$', raw)
    if m:
        return int(m.group(1)) * 3600 + int(m.group(2)) * 60 + int(m.group(3))

    # MM:SS
    m = re.match(r'^(\d+):(\d{2})$', raw)
    if m:
        return int(m.group(1)) * 60 + int(m.group(2))

    # "1 hrs. 55 mins." ou "1 hr 55 min"
    m = re.search(r'(\d+)\s*hr', raw, re.I)
    if m:
        hours = int(m.group(1))
        mins_m = re.search(r'(\d+)\s*min', raw, re.I)
        mins = int(mins_m.group(1)) if mins_m else 0
        return hours * 3600 + mins * 60

    # "240 minutes" ou "240 mins"
    m = re.search(r'(\d+)\s*min', raw, re.I)
    if m:
        return int(m.group(1)) * 60

    # Nombre seul ‚Üí consid√©r√© comme minutes (iafd)
    m = re.match(r'^(\d+)$', raw)
    if m:
        return int(m.group(1)) * 60

    return None


def format_duration(seconds: int) -> str:
    """Formate des secondes en HH:MM:SS pour affichage."""
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    return f"{h:02d}:{m:02d}:{s:02d}"


============================================================
[84/124] Legacy\utils\list_short_markers.py
------------------------------------------------------------
import sqlite3
import csv

DB_PATH = r"H:\Stash\stash-go.sqlite"
OUTPUT_CSV = "short_markers.csv"

def main():
    print(f"Extraction des marqueurs <= 60s...")
    
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    
    query = """
    SELECT 
        s.id as scene_id, 
        s.title as scene_title, 
        m.id as marker_id, 
        m.title as marker_title, 
        m.seconds as start, 
        m.end_seconds as end
    FROM scene_markers m
    JOIN scenes s ON m.scene_id = s.id
    WHERE (m.end_seconds - m.seconds) <= 60 
       OR m.end_seconds IS NULL 
       OR m.end_seconds = 0
    ORDER BY s.id, m.seconds
    """
    
    cur.execute(query)
    rows = cur.fetchall()
    
    results = []
    for r in rows:
        scene_id, scene_title, m_id, m_title, start, end = r
        duration = (end - start) if (end and end > start) else 0
        results.append({
            "scene_id": scene_id,
            "scene_title": scene_title,
            "marker_id": m_id,
            "marker_title": m_title,
            "start": round(start, 2),
            "end": round(end, 2) if end else 0,
            "duration": round(duration, 2)
        })
    
    if results:
        with open(OUTPUT_CSV, mode='w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=results[0].keys())
            writer.writeheader()
            writer.writerows(results)
        print(f"[OK] {len(results)} marqueurs extraits dans {OUTPUT_CSV}")
    else:
        print("[!] Aucun marqueur de moins de 60s trouv√©.")
        
    conn.close()

if __name__ == "__main__":
    main()


============================================================
[85/124] Legacy\utils\marker.py
------------------------------------------------------------
from __future__ import annotations
import sqlite3
import csv
from collections import defaultdict
from typing import Dict, List, Set, Tuple, Optional, Any

DB_PATH = r"H:\Stash\stash-go.sqlite"
CSV_REPORT = "audit_markers.csv"

conn = sqlite3.connect(DB_PATH)
cur = conn.cursor()

print("=== AUDIT AVANCE DES MARKERS ===\n")

# R√©cup√©rer toutes sc√®nes
cur.execute("""
    SELECT s.id, s.title, v.duration 
    FROM scenes s
    LEFT JOIN scenes_files sf ON s.id = sf.scene_id
    LEFT JOIN video_files v ON sf.file_id = v.file_id
    WHERE sf."primary" = 1 OR sf."primary" IS NULL
    GROUP BY s.id
""")
scenes: List[Any] = cur.fetchall()

tag_stats: Dict[str, Dict[str, int]] = defaultdict(lambda: {
    "total": 0,
    "overlaps": 0,
    "short": 0,
    "long": 0,
    "exact_dupes": 0
})

scene_results: List[Dict[str, Any]] = []
scene_scores: List[Tuple[Optional[str], int]] = []
markers_to_delete: Set[int] = set()
markers_to_update: Dict[int, float] = {}

for scene_row in scenes:
    scene_id: int = int(scene_row[0])
    title: Optional[str] = str(scene_row[1]) if scene_row[1] else None
    duration: float = float(scene_row[2]) if scene_row[2] else 0.0

    cur.execute("""
    SELECT m.id, m.title, m.seconds, m.end_seconds,
           GROUP_CONCAT(t.name)
    FROM scene_markers m
    LEFT JOIN scene_markers_tags mt ON m.id = mt.scene_marker_id
    LEFT JOIN tags t ON mt.tag_id = t.id
    WHERE m.scene_id = ?
    GROUP BY m.id
    """, (scene_id,))

    markers: List[Any] = cur.fetchall()
    if not markers:
        continue

    # Stats par sc√®ne
    markers_count: int = len(markers)
    unique_tags: Set[str] = set()
    scene_overlaps: int = 0
    scene_short: int = 0
    scene_long: int = 0
    scene_exact_dupes: int = 0
    scene_points: int = 0

    overlaps_list: List[Any] = []
    seen_ranges: Dict[Tuple[float, float], List[Set[str]]] = {}

    markers_sorted: List[Any] = sorted(markers, key=lambda x: x[2])

    for i in range(len(markers_sorted)):
        _row1 = markers_sorted[i]
        id1: int = int(_row1[0])
        m_title1: str = str(_row1[1]) if _row1[1] else ""
        start1: float = float(_row1[2])
        end1: float = float(_row1[3]) if _row1[3] else 0.0
        tags1_str: str = str(_row1[4]) if _row1[4] else ""

        tags1: Set[str] = set(tags1_str.split(",")) if tags1_str else set()
        if m_title1:
            tags1.add(m_title1)

        unique_tags.update(tags1)
        duration_marker: float = end1 - start1 if end1 else 0.0

        # Detection points sans dur√©e
        if not end1 or end1 <= start1:
            scene_points += 1

        # Detection doublons exacts (m√™me plage)
        time_range: Tuple[float, float] = (round(start1, 2), round(end1, 2) if end1 else 0.0)
        if time_range in seen_ranges:
            for prev_tags in seen_ranges[time_range]:
                common: Set[str] = tags1.intersection(prev_tags)
                if common:
                    scene_exact_dupes += len(common)
                    for t in common:
                        tag_stats[t]["exact_dupes"] += 1

        if time_range not in seen_ranges:
            seen_ranges[time_range] = []
        seen_ranges[time_range].append(tags1)

        # Stats tags
        for tag in tags1:
            tag_stats[tag]["total"] += 1
            if duration_marker > 0 and duration_marker < 3:
                tag_stats[tag]["short"] += 1
                scene_short += 1
            if duration and duration_marker > duration * 0.2:
                tag_stats[tag]["long"] += 1
                scene_long += 1

        # Check overlaps / Fusion / Redondance
        for j in range(i + 1, len(markers_sorted)):
            _row2 = markers_sorted[j]
            id2: int = int(_row2[0])
            m_title2: str = str(_row2[1]) if _row2[1] else ""
            start2: float = float(_row2[2])
            end2: float = float(_row2[3]) if _row2[3] else 0.0
            tags2_str: str = str(_row2[4]) if _row2[4] else ""

            tags2: Set[str] = set(tags2_str.split(",")) if tags2_str else set()
            if m_title2:
                tags2.add(m_title2)

            # --- LOGIQUE DE FUSION (GAP < 10s) ---
            if tags1 == tags2 and len(tags1) == 1:
                gap: float = start2 - (end1 if end1 else start1)
                if gap < 10:
                    new_end: float = max(end1 if end1 else start1, end2 if end2 else start2)
                    markers_to_update[id1] = new_end
                    markers_to_delete.add(id2)
                    end1 = new_end

            # Arr√™t de la boucle j si on d√©passe la zone de collision
            if start2 >= (end1 if end1 else start1):
                break

            # --- LOGIQUE DE CHEVAUCHEMENT / OVERLAP ---
            if start1 < (end2 if end2 else start2) and start2 < (end1 if end1 else start1):
                common2: Set[str] = tags1.intersection(tags2)
                if common2:
                    scene_overlaps += len(common2)
                    overlaps_list.append((id1, id2, start1, end1, start2, end2, list(common2)))
                    for tag in common2:
                        tag_stats[tag]["overlaps"] += 1

                    # --- LOGIQUE DE SUPPRESSION (CONTENU DANS) ---
                    if tags1 == tags2 and len(tags1) == 1:
                        # B est dans A
                        if start1 <= start2 and (end1 >= end2 if end1 and end2 else False):
                            markers_to_delete.add(id2)
                        # A est dans B
                        elif start2 <= start1 and (end2 >= end1 if end1 and end2 else False):
                            markers_to_delete.add(id1)

    scene_penalty: int = (scene_overlaps * 2) + scene_short + scene_long + (scene_exact_dupes * 5) + scene_points

    res: Dict[str, Any] = {
        "id": scene_id,
        "title": title,
        "duration": duration,
        "markers": markers_count,
        "points": scene_points,
        "tags": len(unique_tags),
        "overlaps": scene_overlaps,
        "short": scene_short,
        "long": scene_long,
        "exact_dupes": scene_exact_dupes,
        "penalty": scene_penalty
    }
    scene_results.append(res)
    scene_scores.append((title, scene_penalty))

    if scene_penalty > 0:
        display_title: str = title[:60] if title else "Sans titre"
        print(f"SCENE: {display_title}...")
        print(f"  - Dur√©e: {duration}s | Marqueurs: {markers_count} (Points: {scene_points}) | Tags: {len(unique_tags)}")
        print(f"  - Probl√®mes: Overlaps={scene_overlaps}, Courts={scene_short}, Longs={scene_long}, Doublons Plage={scene_exact_dupes}")
        print(f"  - Score P√©nalit√©: {scene_penalty}")
        print("-" * 30)

# Export CSV
with open(CSV_REPORT, mode='w', newline='', encoding='utf-8-sig') as f:
    if scene_results:
        writer = csv.DictWriter(f, fieldnames=list(scene_results[0].keys()))
        writer.writeheader()
        writer.writerows(scene_results)

print(f"\n[OK] Rapport d√©taill√© export√© dans: {CSV_REPORT}")

# Classement sc√®nes
print("\n=== TOP 20 SCENES PROBLEMATIQUES ===\n")
scene_scores.sort(key=lambda x: x[1], reverse=True)
top20: List[Tuple[Optional[str], int]] = scene_scores[:20]
for s in top20:
    if s[1] > 0:
        print(f"{s[1]:>4} pts | {s[0]}")

# Audit Tags
print("\n=== SCORE DE COHERENCE PAR TAG ===\n")
tag_audit: List[Tuple[str, int, Dict[str, int]]] = []
for tag, data in tag_stats.items():
    score: int = max(0, 100 - (data["overlaps"] * 3) - data["short"] - data["long"] - (data["exact_dupes"] * 10))
    tag_audit.append((tag, score, data))

tag_audit.sort(key=lambda x: x[1])
top_tags: List[Tuple[str, int, Dict[str, int]]] = tag_audit[:15]
for tag, score, data in top_tags:
    print(f"{tag[:20]:<20} | Score: {score:>3}/100 | {data['total']} total, {data['overlaps']} over, {data['exact_dupes']} dupes")

# --- SUPPRESSION / MISE A JOUR DES DOUBLONS ET FUSIONS ---
if markers_to_delete or markers_to_update:
    print(f"\n[!] ALERT: Travail de nettoyage identifi√© :")
    if markers_to_delete:
        print(f"  - {len(markers_to_delete)} marqueurs √† SUPPRIMER (doublons/fusions)")
    if markers_to_update:
        print(f"  - {len(markers_to_update)} marqueurs √† METTRE √Ä JOUR (fusions de dur√©e)")

    confirm: str = input(f"\nVoulez-vous appliquer ces changements √† la base de donn√©es ? (oui/non) : ")

    if confirm.lower() in ['oui', 'y', 'yes', 'o']:
        try:
            # 1. Mises √† jour (Fusions)
            for mid, new_end in markers_to_update.items():
                cur.execute("UPDATE scene_markers SET end_seconds = ? WHERE id = ?", (new_end, mid))

            # 2. Suppressions
            ids_to_del: List[Tuple[int]] = [(mid,) for mid in markers_to_delete]
            cur.executemany("DELETE FROM scene_markers WHERE id = ?", ids_to_del)
            cur.executemany("DELETE FROM scene_markers_tags WHERE scene_marker_id = ?", ids_to_del)

            conn.commit()
            print(f"[OK] Modifications appliqu√©es avec succ√®s.")
        except Exception as e:
            print(f"[ERREUR] √âchec des modifications : {e}")
            conn.rollback()
    else:
        print("[INFO] Nettoyage annul√©.")

conn.close()

============================================================
[86/124] Legacy\utils\meta_tag_utils.py
------------------------------------------------------------
# Utilitaires pour la normalisation et la propagation des tags de m√©tadonn√©es

COLORED_HAIR_TAGS = {
    "Blue": "BlueHair",
    "Green": "GreenHair",
    "Grey": "GreyHair",
    "Pink": "PinkHair",
    "Purple": "PurpleHair",
    "Red": "RedHair",
    "White": "WhiteHair"
}
NATURAL_HAIR_TAGS = {
    "Black": "BlackHair",
    "Blond": "BlondHair",
    "Brown": "BrownHair",
    "Redhead": "RedHead"
}
HAIR_TAGS = {**COLORED_HAIR_TAGS, **NATURAL_HAIR_TAGS}

NATIONALITY_TAGS = {
    "Cubaine": "Cuban",
    "Dominicaine": "Dominican",
    "Colombienne": "Colombian",
    "Thai": "Thai",
    "V√©n√©zu√©lienne": "Venezuelan",
    "Brasilian": "Brazilian",
    "Mexicaine": "Mexican"
}

ETHNY_TAGS = {
    "Asian": "Asian",
    "Ebony": "Ebony",
    "Latina": "Latina"
}


def normalize_tag(value):
    """Normalise une valeur de m√©tadonn√©e pour correspondre √† un tag."""
    v = value.strip().capitalize()
    return v


def meta_to_tags(meta):
    """Transforme les m√©tadonn√©es en tags selon les r√®gles."""
    tags = []
    # Couleur de cheveux
    hair = meta.get("Hair Color", "")
    for color in [c.strip() for c in hair.split(",") if c.strip()]:
        tag = HAIR_TAGS.get(normalize_tag(color))
        if tag:
            tags.append(tag)
    # Nationalit√©
    nat = meta.get("Country", "")
    tag = NATIONALITY_TAGS.get(normalize_tag(nat))
    if tag:
        tags.append(tag)
    # Ethnie
    ethny = meta.get("Ethnicity", "")
    tag = ETHNY_TAGS.get(normalize_tag(ethny))
    if tag:
        tags.append(tag)
    # MILF
    import datetime
    birthdate = meta.get("Birthdate", "")
    if birthdate:
        try:
            birth = datetime.datetime.strptime(birthdate, "%Y-%m-%d")
            age = (datetime.datetime.now() - birth).days // 365
            if age >= 35:
                tags.append("MILF")
        except Exception:
            pass
    # BigTits
    measurements = meta.get("Measurements", "")
    bust = None
    hips = None
    if measurements:
        # Format attendu: 36-24-38
        parts = [p.strip() for p in measurements.split("-")]
        if len(parts) == 3:
            try:
                bust = int(parts[0])
            except Exception:
                pass
            try:
                hips = int(parts[2])
            except Exception:
                pass
    # BigTits
    if bust is not None and bust >= 36:
        tags.append("BigTits")
    # BigButt
    if hips is not None and hips >= 38:
        tags.append("BigButt")
    # Bimbo
    if "BigTits" in tags and "BigButt" in tags:
        tags.append("Bimbo")
    return tags


def propagate_tags_to_scenes(db, performer_id, tags):
    """Ajoute/supprime les tags de couleur de cheveux, nationalit√©, ethnie sur toutes les sc√®nes de l'artiste."""
    # R√©cup√©rer toutes les sc√®nes de l'artiste
    cur = db.conn.cursor()
    cur.execute("SELECT scene_id FROM performers_scenes WHERE performer_id=?", (performer_id,))
    scene_ids = [r[0] for r in cur.fetchall()]
    for scene_id in scene_ids:
        # R√©cup√©rer les tags actuels
        cur.execute("SELECT t.name FROM tags t JOIN scenes_tags st ON st.tag_id = t.id WHERE st.scene_id=?", (scene_id,))
        scene_tags = [r[0] for r in cur.fetchall()]
        # Ajouter les tags manquants
        for tag in tags:
            if tag not in scene_tags:
                # Ajout du tag √† la sc√®ne
                cur.execute("SELECT id FROM tags WHERE name=?", (tag,))
                row = cur.fetchone()
                if row:
                    tag_id = row[0]
                    cur.execute("INSERT INTO scenes_tags (scene_id, tag_id) VALUES (?,?)", (scene_id, tag_id))
        # Supprimer les tags erron√©s
        valid_tags = set(HAIR_TAGS.values()) | set(NATIONALITY_TAGS.values()) | set(ETHNY_TAGS.values()) | {"MILF", "BigTits", "BigButt", "Bimbo"}
        for tag in scene_tags:
            if tag in valid_tags and tag not in tags:
                cur.execute("SELECT id FROM tags WHERE name=?", (tag,))
                row = cur.fetchone()
                if row:
                    tag_id = row[0]
                    cur.execute("DELETE FROM scenes_tags WHERE scene_id=? AND tag_id=?", (scene_id, tag_id))
    db.conn.commit()


============================================================
[87/124] Legacy\utils\short_markers.csv
------------------------------------------------------------
Ôªøscene_id,scene_title,marker_id,marker_title,start,end,duration
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",23,BlowJob,16.0,28.0,12.0
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",24,BlowJob,246.0,266.0,20.0
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",28,Cumshot,382.0,402.0,20.0
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",30,Cumshot,590.0,612.0,22.0
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",31,Cumshot,718.0,750.0,32.0
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",26,BlowJob,816.0,838.0,22.0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",42,Cumshot,134.0,170.0,36.0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",36,BlowJob,276.0,286.0,10.0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",37,BlowJob,338.0,372.0,34.0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",40,BlowJob,476.0,526.0,50.0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",39,BlowJob,720.0,732.0,12.0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",44,Cumshot,778.0,796.0,18.0
179,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc5",68,BlowJob,176.0,218.0,42.0
179,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc5",69,BlowJob,358.0,370.0,12.0
179,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc5",70,BlowJob,656.0,666.0,10.0
179,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc5",71,BlowJob,718.0,744.0,26.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",52,BlowJob,184.0,188.0,4.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",47,BlowJob,416.0,444.0,28.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",48,BlowJob,504.0,540.0,36.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",54,BlowJob,582.0,590.0,8.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",60,Cumshot,622.0,634.0,12.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",61,Cumshot,658.0,664.0,6.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",49,BlowJob,814.0,832.0,18.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",50,BlowJob,878.0,902.0,24.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",58,Cumshot,888.0,910.0,22.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",51,BlowJob,972.0,1002.0,30.0
181,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc1",64,BlowJob,104.0,124.0,20.0
181,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc1",62,BlowJob,144.0,156.0,12.0
181,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc1",63,BlowJob,206.0,236.0,30.0
181,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc1",65,BlowJob,876.0,922.0,46.0
181,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc1",66,Cumshot,896.0,932.0,36.0
182,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc5",77,Cumshot,280.0,306.0,26.0
183,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc3,82,BlowJob,92.0,142.0,50.0
183,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc3,83,BlowJob,196.0,208.0,12.0
183,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc3,86,Anal,696.0,724.0,28.0
183,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc3,84,BlowJob,848.0,860.0,12.0
186,"Sai Tai Tiger in Frauen Knast, Teufelsbrut Hinter Gittern! Sc2",80,Anal,346.0,370.0,24.0
186,"Sai Tai Tiger in Frauen Knast, Teufelsbrut Hinter Gittern! Sc2",81,Anal,608.0,620.0,12.0
188,Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc4,93,Cumshot,140.0,150.0,10.0
188,Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc4,94,Cumshot,208.0,234.0,26.0
190,"Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc7",103,Cumshot,160.0,194.0,34.0
190,"Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc7",104,Cumshot,246.0,258.0,12.0
191,Aderes Quin in StepMom Gets Double Dick,110,BlowJob,744.0,756.0,12.0
191,Aderes Quin in StepMom Gets Double Dick,112,BlowJob,1092.0,1102.0,10.0
191,Aderes Quin in StepMom Gets Double Dick,113,BlowJob,1144.0,1168.0,24.0
191,Aderes Quin in StepMom Gets Double Dick,114,BlowJob,1212.0,1264.0,52.0
191,Aderes Quin in StepMom Gets Double Dick,117,BlowJob,1878.0,1918.0,40.0
191,Aderes Quin in StepMom Gets Double Dick,122,Grabbing Boobs,2572.0,2592.0,20.0
191,Aderes Quin in StepMom Gets Double Dick,123,Grabbing Boobs,2692.0,2746.0,54.0
192,Alejandra Rico in Que Rico!,1950,BlowJob,154.0,210.0,56.0
192,Alejandra Rico in Que Rico!,1952,BlowJob,836.0,876.0,40.0
192,Alejandra Rico in Que Rico!,1957,Cumshot,1310.0,1346.0,36.0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",2434,Gangbang,146.0,160.0,14.0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",2427,BlowJob,550.0,564.0,14.0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",2428,BlowJob,1118.0,1158.0,40.0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",2429,BlowJob,1228.0,1258.0,30.0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",2436,Grabbing Boobs,1310.0,1328.0,18.0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",2433,BlowJob,1340.0,1354.0,14.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1869,Grabbing Boobs,418.0,444.0,26.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1870,Grabbing Boobs,488.0,524.0,36.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1871,Grabbing Boobs,972.0,992.0,20.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1878,Anal,1480.0,1506.0,26.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1879,Anal,1556.0,1570.0,14.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1880,Anal,1636.0,1692.0,56.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1881,Anal,1980.0,2010.0,30.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1875,BlowJob,2046.0,2068.0,22.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1882,Anal,2230.0,2268.0,38.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1883,Cumshot,2358.0,2398.0,40.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1877,BlowJob,2360.0,2402.0,42.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1884,Grabbing Boobs,464.0,502.0,38.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1885,Grabbing Boobs,542.0,570.0,28.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1886,Grabbing Boobs,740.0,774.0,34.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1892,Gangbang,968.0,984.0,16.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1887,Grabbing Boobs,1706.0,1752.0,46.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1894,Anal,1832.0,1842.0,10.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1896,Anal,2248.0,2280.0,32.0
196,Ava Devine in Milf Asian Cummouth Facial,3663,Grabbing Boobs,6.0,26.0,20.0
196,Ava Devine in Milf Asian Cummouth Facial,3665,BlowJob,418.0,466.0,48.0
196,Ava Devine in Milf Asian Cummouth Facial,3670,BlowJob,508.0,520.0,12.0
196,Ava Devine in Milf Asian Cummouth Facial,3666,BlowJob,810.0,834.0,24.0
196,Ava Devine in Milf Asian Cummouth Facial,3667,BlowJob,1030.0,1054.0,24.0
196,Ava Devine in Milf Asian Cummouth Facial,3668,BlowJob,1222.0,1242.0,20.0
196,Ava Devine in Milf Asian Cummouth Facial,3671,Anal,1374.0,1414.0,40.0
196,Ava Devine in Milf Asian Cummouth Facial,3669,BlowJob,1424.0,1436.0,12.0
196,Ava Devine in Milf Asian Cummouth Facial,3672,Anal,1612.0,1666.0,54.0
196,Ava Devine in Milf Asian Cummouth Facial,3673,Anal,1764.0,1806.0,42.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3644,BlowJob,204.0,246.0,42.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3649,Pissing,318.0,324.0,6.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3657,Gangbang,700.0,726.0,26.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3646,BlowJob,794.0,820.0,26.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3658,Gangbang,1004.0,1022.0,18.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3659,Gangbang,1176.0,1186.0,10.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3647,BlowJob,1370.0,1400.0,30.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3660,Gangbang,1508.0,1546.0,38.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3652,Anal,1818.0,1858.0,40.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3653,Anal,1938.0,1952.0,14.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3654,Anal,2032.0,2054.0,22.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3655,Anal,2110.0,2134.0,24.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3656,Anal,2218.0,2268.0,50.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3661,Cumshot,2470.0,2528.0,58.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3662,Cumshot,2604.0,2620.0,16.0
198,Barbie Sins in DP Bandits! Sc4,3701,BlowJob,1008.0,1016.0,8.0
198,Barbie Sins in DP Bandits! Sc4,3705,Anal,1554.0,1564.0,10.0
198,Barbie Sins in DP Bandits! Sc4,3697,BlowJob,1964.0,1994.0,30.0
198,Barbie Sins in DP Bandits! Sc4,3707,Anal,2146.0,2164.0,18.0
198,Barbie Sins in DP Bandits! Sc4,3698,BlowJob,2612.0,2632.0,20.0
198,Barbie Sins in DP Bandits! Sc4,3709,Cumshot,2798.0,2822.0,24.0
198,Barbie Sins in DP Bandits! Sc4,3700,BlowJob,2816.0,2852.0,36.0
198,Barbie Sins in DP Bandits! Sc4,3710,Cumshot,2892.0,2924.0,32.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3621,BlowJob,584.0,630.0,46.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3600,Anal,680.0,698.0,18.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3601,Anal,742.0,780.0,38.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3629,BlowJob,816.0,862.0,46.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3603,Anal,1258.0,1278.0,20.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3630,Cumshot,1344.0,1378.0,34.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3605,Anal,1722.0,1744.0,22.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3606,Anal,1800.0,1858.0,58.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3607,Anal,1924.0,1944.0,20.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3609,Anal,2728.0,2780.0,52.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3610,Anal,2862.0,2900.0,38.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3635,Cumshot,2912.0,2922.0,10.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3611,Anal,2940.0,2974.0,34.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3612,Anal,3030.0,3064.0,34.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3624,BlowJob,3158.0,3172.0,14.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3636,Cumshot,3168.0,3174.0,6.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3625,BlowJob,3454.0,3476.0,22.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3613,Anal,3496.0,3548.0,52.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3639,Gangbang,3530.0,3570.0,40.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3626,BlowJob,3544.0,3558.0,14.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3614,Anal,3604.0,3618.0,14.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3640,Gangbang,3678.0,3692.0,14.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3627,BlowJob,3682.0,3694.0,12.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3615,Anal,3764.0,3796.0,32.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3616,Anal,4008.0,4024.0,16.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3617,Anal,4168.0,4224.0,56.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3643,Gangbang,4306.0,4336.0,30.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3618,Anal,4332.0,4344.0,12.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3619,Anal,4382.0,4400.0,18.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3628,BlowJob,4560.0,4602.0,42.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3632,Cumshot,4784.0,4796.0,12.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3637,Cumshot,4816.0,4822.0,6.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3633,Cumshot,4888.0,4896.0,8.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3620,Anal,4922.0,4948.0,26.0
200,Blanche Bradburry in Gangbang Anal Blitz,3588,BlowJob,126.0,154.0,28.0
200,Blanche Bradburry in Gangbang Anal Blitz,3595,69,274.0,300.0,26.0
200,Blanche Bradburry in Gangbang Anal Blitz,3589,BlowJob,346.0,372.0,26.0
200,Blanche Bradburry in Gangbang Anal Blitz,3590,BlowJob,468.0,494.0,26.0
200,Blanche Bradburry in Gangbang Anal Blitz,3596,Anal,742.0,758.0,16.0
200,Blanche Bradburry in Gangbang Anal Blitz,3591,BlowJob,780.0,792.0,12.0
200,Blanche Bradburry in Gangbang Anal Blitz,3597,Anal,792.0,812.0,20.0
200,Blanche Bradburry in Gangbang Anal Blitz,3592,BlowJob,994.0,1010.0,16.0
200,Blanche Bradburry in Gangbang Anal Blitz,3593,BlowJob,1868.0,1882.0,14.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3714,Anal,1066.0,1080.0,14.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3715,Anal,1158.0,1176.0,18.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3725,BlowJob,1282.0,1296.0,14.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3726,BlowJob,1340.0,1368.0,28.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3727,BlowJob,1514.0,1530.0,16.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3729,BlowJob,1782.0,1792.0,10.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3731,Gangbang,2082.0,2098.0,16.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3717,Anal,2116.0,2154.0,38.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3732,Gangbang,2144.0,2160.0,16.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3718,Anal,2206.0,2246.0,40.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3733,Gangbang,2214.0,2226.0,12.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3719,Anal,2282.0,2292.0,10.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3720,Anal,2450.0,2480.0,30.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3721,Anal,2828.0,2860.0,32.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3730,BlowJob,2982.0,3012.0,30.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3723,Anal,3106.0,3122.0,16.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3734,Cumshot,3832.0,3854.0,22.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3735,Cumshot,3890.0,3928.0,38.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3738,Cumshot,3958.0,3992.0,34.0
202,Blanche Bradburry in Rough DAP Gangbang,3800,Grabbing Boobs,4.0,34.0,30.0
202,Blanche Bradburry in Rough DAP Gangbang,3802,BlowJob,1230.0,1246.0,16.0
202,Blanche Bradburry in Rough DAP Gangbang,3806,Anal,1296.0,1308.0,12.0
202,Blanche Bradburry in Rough DAP Gangbang,3807,Anal,1340.0,1362.0,22.0
202,Blanche Bradburry in Rough DAP Gangbang,3803,BlowJob,1376.0,1418.0,42.0
202,Blanche Bradburry in Rough DAP Gangbang,3808,Anal,1514.0,1562.0,48.0
202,Blanche Bradburry in Rough DAP Gangbang,3805,BlowJob,3020.0,3034.0,14.0
202,Blanche Bradburry in Rough DAP Gangbang,3812,Anal,3122.0,3150.0,28.0
203,Blanche Bradburry in Triple Penetration Madness,3739,Grabbing Boobs,130.0,190.0,60.0
203,Blanche Bradburry in Triple Penetration Madness,3740,Grabbing Boobs,276.0,298.0,22.0
203,Blanche Bradburry in Triple Penetration Madness,3741,Grabbing Boobs,384.0,404.0,20.0
203,Blanche Bradburry in Triple Penetration Madness,3742,Grabbing Boobs,602.0,622.0,20.0
203,Blanche Bradburry in Triple Penetration Madness,3745,Anal,922.0,956.0,34.0
203,Blanche Bradburry in Triple Penetration Madness,3751,BlowJob,1486.0,1516.0,30.0
203,Blanche Bradburry in Triple Penetration Madness,3752,BlowJob,2220.0,2250.0,30.0
203,Blanche Bradburry in Triple Penetration Madness,3748,Anal,2804.0,2818.0,14.0
203,Blanche Bradburry in Triple Penetration Madness,3743,Grabbing Boobs,2830.0,2848.0,18.0
203,Blanche Bradburry in Triple Penetration Madness,3749,Anal,2888.0,2912.0,24.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3674,Anal,354.0,370.0,16.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3675,Anal,448.0,460.0,12.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3676,Anal,582.0,602.0,20.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3677,Anal,662.0,674.0,12.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3688,BlowJob,1382.0,1442.0,60.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3679,Anal,1546.0,1558.0,12.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3690,BlowJob,1640.0,1666.0,26.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3678,Anal,1692.0,1740.0,48.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3691,BlowJob,1836.0,1870.0,34.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3692,BlowJob,2214.0,2230.0,16.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3681,Anal,2618.0,2642.0,24.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3693,BlowJob,2672.0,2706.0,34.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3682,Anal,2686.0,2710.0,24.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3683,Anal,2798.0,2812.0,14.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3684,Anal,2858.0,2904.0,46.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3685,Anal,2938.0,2960.0,22.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3686,Anal,3208.0,3226.0,18.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3687,Anal,3284.0,3310.0,26.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3753,Grabbing Boobs,422.0,482.0,60.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3754,Anal,1222.0,1244.0,22.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3755,Anal,1286.0,1346.0,60.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3757,Anal,1356.0,1390.0,34.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3758,Anal,1482.0,1498.0,16.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3765,BlowJob,1822.0,1838.0,16.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3762,BlowJob,2180.0,2192.0,12.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3763,BlowJob,2368.0,2386.0,18.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3764,BlowJob,3150.0,3160.0,10.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3813,Grabbing Boobs,16.0,50.0,34.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3814,Anal,382.0,392.0,10.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3824,Anal,970.0,980.0,10.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3825,Anal,1196.0,1206.0,10.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3826,BlowJob,1412.0,1422.0,10.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3827,BlowJob,1500.0,1554.0,54.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3828,BlowJob,1988.0,2006.0,18.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3815,Anal,2186.0,2204.0,18.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3829,BlowJob,2304.0,2314.0,10.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3816,Anal,2384.0,2416.0,32.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3817,Anal,2520.0,2552.0,32.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3818,Anal,2622.0,2682.0,60.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3819,Anal,2716.0,2730.0,14.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3830,BlowJob,2744.0,2760.0,16.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3820,Anal,2798.0,2826.0,28.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3821,Anal,2924.0,2938.0,14.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3822,Anal,2972.0,3018.0,46.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3823,Anal,3166.0,3178.0,12.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3831,BlowJob,3418.0,3442.0,24.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3878,Grabbing Boobs,98.0,136.0,38.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3879,Anal,164.0,188.0,24.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3880,Anal,646.0,676.0,30.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3881,Anal,708.0,720.0,12.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3884,BlowJob,1926.0,1964.0,38.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3885,BlowJob,4178.0,4192.0,14.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3886,BlowJob,4296.0,4324.0,28.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3887,Cumshot,4512.0,4526.0,14.0
208,Cherry Kiss in DP Bandits! Sc2,3867,BlowJob,518.0,568.0,50.0
208,Cherry Kiss in DP Bandits! Sc2,3873,Anal,696.0,750.0,54.0
208,Cherry Kiss in DP Bandits! Sc2,3868,BlowJob,698.0,732.0,34.0
208,Cherry Kiss in DP Bandits! Sc2,3869,BlowJob,798.0,812.0,14.0
208,Cherry Kiss in DP Bandits! Sc2,3870,BlowJob,852.0,874.0,22.0
208,Cherry Kiss in DP Bandits! Sc2,3875,Anal,1120.0,1142.0,22.0
208,Cherry Kiss in DP Bandits! Sc2,3871,BlowJob,1936.0,1968.0,32.0
208,Cherry Kiss in DP Bandits! Sc2,3872,BlowJob,2342.0,2358.0,16.0
208,Cherry Kiss in DP Bandits! Sc2,3877,Cumshot,2346.0,2362.0,16.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3768,Grabbing Boobs,6.0,56.0,50.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3772,Gangbang,156.0,168.0,12.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3780,Anal,172.0,228.0,56.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3790,BlowJob,256.0,286.0,30.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3781,Anal,276.0,302.0,26.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3791,BlowJob,494.0,506.0,12.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3792,BlowJob,626.0,658.0,32.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3774,Gangbang,782.0,816.0,34.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3782,Anal,882.0,900.0,18.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3775,Gangbang,888.0,916.0,28.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3776,Gangbang,956.0,978.0,22.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3777,Gangbang,1178.0,1192.0,14.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3793,BlowJob,1232.0,1274.0,42.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3784,Anal,1276.0,1292.0,16.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3796,Pissing,1332.0,1370.0,38.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3794,BlowJob,1394.0,1406.0,12.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3778,Gangbang,1406.0,1424.0,18.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3769,Grabbing Boobs,1574.0,1602.0,28.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3798,Titjob,1590.0,1606.0,16.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3785,Anal,1718.0,1734.0,16.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3779,Gangbang,1762.0,1804.0,42.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3787,Anal,2226.0,2242.0,16.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3797,Pissing,2296.0,2334.0,38.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3770,Grabbing Boobs,2442.0,2478.0,36.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3771,Grabbing Boobs,3176.0,3214.0,38.0
210,Destiny Mira in Put My Back Into It,1899,BlowJob,288.0,312.0,24.0
210,Destiny Mira in Put My Back Into It,1900,BlowJob,612.0,662.0,50.0
210,Destiny Mira in Put My Back Into It,1901,Anal,960.0,1004.0,44.0
210,Destiny Mira in Put My Back Into It,1902,Anal,1048.0,1058.0,10.0
210,Destiny Mira in Put My Back Into It,1903,Anal,1114.0,1146.0,32.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3832,BlowJob,1516.0,1572.0,56.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3834,BlowJob,1826.0,1856.0,30.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3843,Gangbang,1978.0,1992.0,14.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3836,BlowJob,2228.0,2264.0,36.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3837,BlowJob,2522.0,2540.0,18.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3849,Pissing,2804.0,2816.0,12.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3838,BlowJob,2834.0,2844.0,10.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3852,Anal,3442.0,3452.0,10.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3839,BlowJob,3472.0,3484.0,12.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3853,Anal,3600.0,3618.0,18.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3854,Anal,3660.0,3710.0,50.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3855,Anal,3746.0,3762.0,16.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3857,Anal,4076.0,4088.0,12.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3850,Pissing,4250.0,4268.0,18.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3847,Gangbang,4724.0,4744.0,20.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3848,Gangbang,4776.0,4816.0,40.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3859,Anal,4820.0,4842.0,22.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3840,BlowJob,4828.0,4876.0,48.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3862,Cumshot,5800.0,5814.0,14.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3863,Cumshot,5856.0,5906.0,50.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3864,Cumshot,5950.0,5994.0,44.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3865,Cumshot,6046.0,6076.0,30.0
212,Jolee Love in Craving For Cocks,1905,BlowJob,972.0,1000.0,28.0
212,Jolee Love in Craving For Cocks,1906,BlowJob,1134.0,1144.0,10.0
212,Jolee Love in Craving For Cocks,1907,BlowJob,1206.0,1230.0,24.0
212,Jolee Love in Craving For Cocks,1912,Anal,1804.0,1820.0,16.0
212,Jolee Love in Craving For Cocks,1908,BlowJob,2194.0,2212.0,18.0
212,Jolee Love in Craving For Cocks,1915,Anal,2540.0,2590.0,50.0
212,Jolee Love in Craving For Cocks,1909,BlowJob,2746.0,2798.0,52.0
212,Jolee Love in Craving For Cocks,1917,DP,3182.0,3186.0,4.0
213,Jolee Love in DP Bandits! 2 Sc3,1918,Grabbing Boobs,1030.0,1050.0,20.0
213,Jolee Love in DP Bandits! 2 Sc3,1929,Anal,1470.0,1508.0,38.0
213,Jolee Love in DP Bandits! 2 Sc3,1920,BlowJob,1526.0,1544.0,18.0
213,Jolee Love in DP Bandits! 2 Sc3,1930,Anal,1606.0,1640.0,34.0
213,Jolee Love in DP Bandits! 2 Sc3,1921,BlowJob,1820.0,1848.0,28.0
213,Jolee Love in DP Bandits! 2 Sc3,1926,BlowJob,1914.0,1948.0,34.0
213,Jolee Love in DP Bandits! 2 Sc3,1935,Cumshot,1952.0,1958.0,6.0
213,Jolee Love in DP Bandits! 2 Sc3,1923,BlowJob,2360.0,2416.0,56.0
213,Jolee Love in DP Bandits! 2 Sc3,1924,BlowJob,2452.0,2468.0,16.0
213,Jolee Love in DP Bandits! 2 Sc3,1933,Anal,2654.0,2690.0,36.0
213,Jolee Love in DP Bandits! 2 Sc3,1934,Anal,2758.0,2788.0,30.0
213,Jolee Love in DP Bandits! 2 Sc3,1925,BlowJob,2890.0,2928.0,38.0
214,Jolee Love in Hardcore DAP Creampie,1941,BlowJob,370.0,382.0,12.0
214,Jolee Love in Hardcore DAP Creampie,1944,BlowJob,1780.0,1794.0,14.0
214,Jolee Love in Hardcore DAP Creampie,1945,BlowJob,2056.0,2078.0,22.0
214,Jolee Love in Hardcore DAP Creampie,1947,Pissing,2628.0,2656.0,28.0
215,,2140,Anal,188.0,208.0,20.0
215,,2141,Anal,722.0,742.0,20.0
215,,2144,BlowJob,818.0,834.0,16.0
215,,2145,BlowJob,894.0,932.0,38.0
215,,2143,Anal,1452.0,1464.0,12.0
215,,2146,BlowJob,1640.0,1652.0,12.0
216,,2150,Anal,162.0,188.0,26.0
216,,2151,Anal,222.0,260.0,38.0
216,,2152,Anal,388.0,408.0,20.0
216,,2153,Anal,602.0,620.0,18.0
216,,2154,Cumshot,1830.0,1844.0,14.0
217,,2159,BlowJob,692.0,740.0,48.0
217,,2157,Anal,1220.0,1238.0,18.0
217,,2160,BlowJob,1800.0,1852.0,52.0
217,,2161,BlowJob,2262.0,2278.0,16.0
218,,2162,BlowJob,486.0,504.0,18.0
218,,2165,Anal,788.0,842.0,54.0
218,,2166,Anal,1494.0,1536.0,42.0
218,,2167,Anal,2144.0,2170.0,26.0
218,,2168,Anal,2206.0,2236.0,30.0
218,,2163,BlowJob,2268.0,2280.0,12.0
218,,2169,Cumshot,2300.0,2330.0,30.0
218,,2164,BlowJob,2306.0,2312.0,6.0
219,,2171,BlowJob,66.0,86.0,20.0
219,,2181,BlowJob,444.0,468.0,24.0
219,,2186,Cumshot,450.0,496.0,46.0
219,,2189,Cumshot,528.0,534.0,6.0
219,,2172,BlowJob,584.0,638.0,54.0
219,,2173,BlowJob,682.0,706.0,24.0
219,,2182,BlowJob,1032.0,1038.0,6.0
219,,2190,Cumshot,1074.0,1112.0,38.0
219,,2174,BlowJob,1086.0,1120.0,34.0
219,,2191,Cumshot,1312.0,1324.0,12.0
219,,2176,BlowJob,1382.0,1428.0,46.0
219,,2194,Gangbang,1408.0,1422.0,14.0
219,,2177,BlowJob,1512.0,1532.0,20.0
219,,2178,BlowJob,1634.0,1690.0,56.0
219,,2179,BlowJob,1726.0,1742.0,16.0
219,,2183,BlowJob,1778.0,1820.0,42.0
219,,2192,Cumshot,1820.0,1832.0,12.0
219,,2180,BlowJob,2050.0,2090.0,40.0
219,,2184,BlowJob,2140.0,2172.0,32.0
219,,2193,Cumshot,2176.0,2186.0,10.0
220,,2196,BlowJob,662.0,678.0,16.0
220,,2198,BlowJob,688.0,698.0,10.0
220,,2197,BlowJob,716.0,726.0,10.0
220,,2199,BlowJob,1334.0,1356.0,22.0
220,,2201,Cumshot,2468.0,2478.0,10.0
221,,2202,BlowJob,114.0,158.0,44.0
221,,2203,BlowJob,332.0,382.0,50.0
221,,2205,Gangbang,388.0,398.0,10.0
221,,2206,Cumshot,670.0,696.0,26.0
221,,2204,BlowJob,732.0,772.0,40.0
221,,2210,Cumshot,806.0,834.0,28.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2017,Gangbang,218.0,274.0,56.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2058,BlowJob,256.0,272.0,16.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2018,Gangbang,358.0,376.0,18.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2020,Gangbang,576.0,634.0,58.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2063,Anal,586.0,600.0,14.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2059,BlowJob,592.0,602.0,10.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2064,Anal,746.0,772.0,26.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2065,Anal,822.0,860.0,38.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2021,Gangbang,982.0,994.0,12.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2030,Anal,1046.0,1080.0,34.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2052,Gangbang,1062.0,1116.0,54.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2031,Anal,1140.0,1150.0,10.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2067,Anal,1230.0,1242.0,12.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2023,Gangbang,1506.0,1542.0,36.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2024,Gangbang,1630.0,1678.0,48.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2069,Cumshot,1806.0,1834.0,28.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2034,BlowJob,1808.0,1838.0,30.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2032,Anal,2002.0,2016.0,14.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2055,Gangbang,2076.0,2086.0,10.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2057,Gangbang,2504.0,2534.0,30.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2073,Cumshot,2662.0,2704.0,42.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2074,Cumshot,2770.0,2780.0,10.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2036,BlowJob,2940.0,2988.0,48.0
223,,2212,Cumshot,112.0,122.0,10.0
223,,2215,BlowJob,208.0,218.0,10.0
223,,2218,BlowJob,258.0,274.0,16.0
223,,2216,BlowJob,352.0,406.0,54.0
223,,2220,BlowJob,466.0,488.0,22.0
223,,2231,Gangbang,800.0,812.0,12.0
223,,2222,BlowJob,868.0,884.0,16.0
223,,2228,Grabbing Boobs,1150.0,1178.0,28.0
223,,2223,BlowJob,1164.0,1174.0,10.0
223,,2229,Grabbing Boobs,1250.0,1298.0,48.0
223,,2232,Gangbang,1400.0,1436.0,36.0
223,,2233,Anal,1488.0,1504.0,16.0
223,,2225,BlowJob,1606.0,1616.0,10.0
223,,2230,Grabbing Boobs,2106.0,2130.0,24.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2350,BlowJob,106.0,112.0,6.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2357,BlowJob,310.0,332.0,22.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2235,BlowJob,408.0,444.0,36.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2236,BlowJob,770.0,818.0,48.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2241,BlowJob,886.0,898.0,12.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2244,Cumshot,888.0,894.0,6.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2363,Anal,1358.0,1394.0,36.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2245,Anal,1426.0,1438.0,12.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2353,BlowJob,1610.0,1618.0,8.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2246,Anal,1696.0,1716.0,20.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2243,Grabbing Boobs,1880.0,1928.0,48.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2242,BlowJob,2224.0,2232.0,8.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2355,BlowJob,2332.0,2340.0,8.0
225,,2247,Grabbing Boobs,26.0,56.0,30.0
225,,2250,BlowJob,746.0,758.0,12.0
225,,2251,BlowJob,806.0,822.0,16.0
225,,2256,Anal,1120.0,1132.0,12.0
225,,2257,Anal,1202.0,1216.0,14.0
225,,2258,Anal,1256.0,1284.0,28.0
225,,2252,BlowJob,1336.0,1356.0,20.0
225,,2259,Anal,1416.0,1430.0,14.0
225,,2260,Anal,1522.0,1544.0,22.0
225,,2249,Grabbing Boobs,1618.0,1646.0,28.0
225,,2253,BlowJob,1754.0,1796.0,42.0
225,,2261,Anal,1918.0,1950.0,32.0
225,,2263,Anal,2238.0,2268.0,30.0
225,,2254,BlowJob,2382.0,2406.0,24.0
225,,2264,Anal,2630.0,2656.0,26.0
225,,2265,Anal,2706.0,2756.0,50.0
225,,2255,BlowJob,2766.0,2814.0,48.0
226,,2266,Grabbing Boobs,26.0,44.0,18.0
226,,2267,Grabbing Boobs,236.0,264.0,28.0
226,,2268,Grabbing Boobs,300.0,320.0,20.0
226,,2281,Titjob,308.0,318.0,10.0
226,,2272,BlowJob,788.0,798.0,10.0
226,,2269,Grabbing Boobs,912.0,940.0,28.0
226,,2273,BlowJob,1074.0,1108.0,34.0
226,,2274,BlowJob,1448.0,1458.0,10.0
226,,2288,Anal,1476.0,1496.0,20.0
226,,2275,BlowJob,1502.0,1542.0,40.0
226,,2283,Cumshot,1600.0,1632.0,32.0
226,,2276,BlowJob,1636.0,1660.0,24.0
226,,2289,Anal,1694.0,1734.0,40.0
226,,2277,BlowJob,1736.0,1778.0,42.0
226,,2278,BlowJob,1878.0,1906.0,28.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2295,Gangbang,638.0,652.0,14.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2377,BlowJob,996.0,1010.0,14.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2298,Anal,1048.0,1064.0,16.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2388,Cumshot,1112.0,1118.0,6.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2382,Anal,1142.0,1160.0,18.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2378,BlowJob,1154.0,1174.0,20.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2379,BlowJob,1220.0,1256.0,36.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2299,Anal,1232.0,1278.0,46.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2385,Anal,1808.0,1818.0,10.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2293,BlowJob,1830.0,1844.0,14.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2386,Anal,1924.0,1936.0,12.0
228,Adira Allure in Airtight Diva Sc4,2332,Grabbing Boobs,178.0,218.0,40.0
228,Adira Allure in Airtight Diva Sc4,2339,Cumshot,798.0,802.0,4.0
228,Adira Allure in Airtight Diva Sc4,2340,Cumshot,854.0,880.0,26.0
228,Adira Allure in Airtight Diva Sc4,2334,BlowJob,872.0,924.0,52.0
228,Adira Allure in Airtight Diva Sc4,2335,BlowJob,1108.0,1148.0,40.0
228,Adira Allure in Airtight Diva Sc4,2336,BlowJob,1184.0,1210.0,26.0
228,Adira Allure in Airtight Diva Sc4,2342,Cumshot,1808.0,1832.0,24.0
228,Adira Allure in Airtight Diva Sc4,2338,BlowJob,1824.0,1834.0,10.0
229,Alexis Tae in Gangbang Sluts Sc1,2533,BlowJob,796.0,828.0,32.0
229,Alexis Tae in Gangbang Sluts Sc1,2534,BlowJob,888.0,898.0,10.0
229,Alexis Tae in Gangbang Sluts Sc1,2524,Gangbang,912.0,936.0,24.0
229,Alexis Tae in Gangbang Sluts Sc1,2535,BlowJob,936.0,972.0,36.0
229,Alexis Tae in Gangbang Sluts Sc1,2536,BlowJob,1050.0,1102.0,52.0
229,Alexis Tae in Gangbang Sluts Sc1,2526,Gangbang,1266.0,1290.0,24.0
229,Alexis Tae in Gangbang Sluts Sc1,2527,Gangbang,1344.0,1384.0,40.0
229,Alexis Tae in Gangbang Sluts Sc1,2528,Gangbang,1538.0,1548.0,10.0
229,Alexis Tae in Gangbang Sluts Sc1,2529,Gangbang,1600.0,1656.0,56.0
229,Alexis Tae in Gangbang Sluts Sc1,2537,BlowJob,1696.0,1708.0,12.0
229,Alexis Tae in Gangbang Sluts Sc1,2530,Gangbang,2018.0,2072.0,54.0
229,Alexis Tae in Gangbang Sluts Sc1,2531,Gangbang,2134.0,2170.0,36.0
229,Alexis Tae in Gangbang Sluts Sc1,2539,Cumshot,2218.0,2256.0,38.0
230,Amirah Adara in DP Bandits! Sc1,2814,BlowJob,1308.0,1354.0,46.0
230,Amirah Adara in DP Bandits! Sc1,2823,Anal,1510.0,1528.0,18.0
230,Amirah Adara in DP Bandits! Sc1,2815,BlowJob,1786.0,1844.0,58.0
230,Amirah Adara in DP Bandits! Sc1,2825,Anal,1856.0,1870.0,14.0
230,Amirah Adara in DP Bandits! Sc1,2816,BlowJob,1914.0,1946.0,32.0
230,Amirah Adara in DP Bandits! Sc1,2817,BlowJob,2078.0,2104.0,26.0
230,Amirah Adara in DP Bandits! Sc1,5061,Anal,2086.0,2132.0,46.0
230,Amirah Adara in DP Bandits! Sc1,5062,Anal,2230.0,2284.0,54.0
230,Amirah Adara in DP Bandits! Sc1,2818,BlowJob,2560.0,2590.0,30.0
230,Amirah Adara in DP Bandits! Sc1,5065,Anal,2592.0,2606.0,14.0
230,Amirah Adara in DP Bandits! Sc1,2819,BlowJob,2840.0,2896.0,56.0
230,Amirah Adara in DP Bandits! Sc1,2827,Anal,2932.0,2948.0,16.0
230,Amirah Adara in DP Bandits! Sc1,2822,BlowJob,3002.0,3022.0,20.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2892,Grabbing Boobs,86.0,110.0,24.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2900,Gangbang,230.0,254.0,24.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2895,BlowJob,744.0,774.0,30.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2901,Gangbang,834.0,878.0,44.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2896,BlowJob,904.0,930.0,26.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2902,Gangbang,1280.0,1332.0,52.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2898,BlowJob,1282.0,1334.0,52.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2893,Grabbing Boobs,1370.0,1410.0,40.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2899,BlowJob,1474.0,1486.0,12.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2904,Cumshot,1630.0,1634.0,4.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2905,Cumshot,1728.0,1752.0,24.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2906,Cumshot,1822.0,1832.0,10.0
232,Anai Loves in My stepmom,2908,Grabbing Boobs,668.0,710.0,42.0
232,Anai Loves in My stepmom,2916,Titjob,710.0,736.0,26.0
232,Anai Loves in My stepmom,2914,BlowJob,726.0,780.0,54.0
232,Anai Loves in My stepmom,2915,BlowJob,796.0,802.0,6.0
232,Anai Loves in My stepmom,2917,Anal,850.0,908.0,58.0
232,Anai Loves in My stepmom,2918,Anal,1060.0,1098.0,38.0
232,Anai Loves in My stepmom,2910,Grabbing Boobs,1208.0,1240.0,32.0
232,Anai Loves in My stepmom,2919,Anal,1446.0,1460.0,14.0
233,Angela White in Angela's Airtight DP,3062,Anal,488.0,516.0,28.0
233,Angela White in Angela's Airtight DP,3063,Anal,648.0,702.0,54.0
233,Angela White in Angela's Airtight DP,3069,Grabbing Boobs,660.0,686.0,26.0
233,Angela White in Angela's Airtight DP,3064,Anal,750.0,770.0,20.0
233,Angela White in Angela's Airtight DP,3065,Anal,1052.0,1094.0,42.0
233,Angela White in Angela's Airtight DP,3060,BlowJob,1104.0,1140.0,36.0
233,Angela White in Angela's Airtight DP,3070,Grabbing Boobs,1494.0,1516.0,22.0
233,Angela White in Angela's Airtight DP,3067,Anal,2158.0,2170.0,12.0
233,Angela White in Angela's Airtight DP,3068,Anal,2244.0,2264.0,20.0
233,Angela White in Angela's Airtight DP,3071,Cumshot,2480.0,2508.0,28.0
233,Angela White in Angela's Airtight DP,3072,Cumshot,2578.0,2612.0,34.0
233,Angela White in Angela's Airtight DP,3073,Cumshot,2656.0,2674.0,18.0
234,Anissa Kate in Love Everything About Her,3314,BlowJob,192.0,226.0,34.0
234,Anissa Kate in Love Everything About Her,3318,BlowJob,322.0,346.0,24.0
234,Anissa Kate in Love Everything About Her,3316,BlowJob,942.0,954.0,12.0
234,Anissa Kate in Love Everything About Her,3317,BlowJob,1358.0,1368.0,10.0
234,Anissa Kate in Love Everything About Her,3322,Anal,1676.0,1720.0,44.0
235,Anissa Kate in Sizziling Double Penetration Delight,3334,Grabbing Boobs,170.0,212.0,42.0
235,Anissa Kate in Sizziling Double Penetration Delight,3336,BlowJob,330.0,382.0,52.0
235,Anissa Kate in Sizziling Double Penetration Delight,3335,Grabbing Boobs,466.0,518.0,52.0
235,Anissa Kate in Sizziling Double Penetration Delight,3339,BlowJob,470.0,516.0,46.0
235,Anissa Kate in Sizziling Double Penetration Delight,3343,Anal,1102.0,1138.0,36.0
235,Anissa Kate in Sizziling Double Penetration Delight,3341,BlowJob,1192.0,1226.0,34.0
235,Anissa Kate in Sizziling Double Penetration Delight,3342,BlowJob,1536.0,1546.0,10.0
235,Anissa Kate in Sizziling Double Penetration Delight,3344,Anal,1564.0,1596.0,32.0
235,Anissa Kate in Sizziling Double Penetration Delight,3345,Anal,1658.0,1690.0,32.0
235,Anissa Kate in Sizziling Double Penetration Delight,3346,Cumshot,1876.0,1916.0,40.0
236,"Anissa Kate, Olivia Del Rio in Personal Guide Chapter 1",3406,Grabbing Boobs,328.0,386.0,58.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3432,BlowJob,468.0,518.0,50.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3433,BlowJob,784.0,798.0,14.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3438,Anal,884.0,928.0,44.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3439,Anal,964.0,988.0,24.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3440,Anal,1254.0,1274.0,20.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3441,Anal,1540.0,1550.0,10.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3442,Anal,1594.0,1606.0,12.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3436,BlowJob,1632.0,1644.0,12.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3437,BlowJob,2168.0,2194.0,26.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3434,BlowJob,2204.0,2256.0,52.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3444,Cumshot,2264.0,2274.0,10.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3888,Gangbang,64.0,98.0,34.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3891,BlowJob,556.0,588.0,32.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3892,BlowJob,814.0,858.0,44.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3893,BlowJob,906.0,956.0,50.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3894,BlowJob,1034.0,1052.0,18.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3895,BlowJob,1104.0,1152.0,48.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3896,BlowJob,1354.0,1374.0,20.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3899,Anal,2380.0,2414.0,34.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3900,Anal,2458.0,2476.0,18.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3901,Anal,2578.0,2610.0,32.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3902,Cumshot,2780.0,2812.0,32.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3903,Cumshot,2864.0,2898.0,34.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3905,BlowJob,114.0,132.0,18.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3906,BlowJob,368.0,412.0,44.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3912,Cumshot,376.0,402.0,26.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3908,BlowJob,620.0,654.0,34.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3909,BlowJob,734.0,754.0,20.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3910,BlowJob,870.0,906.0,36.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3911,BlowJob,1108.0,1116.0,8.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3914,Cumshot,1128.0,1136.0,8.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3917,Cumshot,1460.0,1492.0,32.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3918,Cumshot,1574.0,1598.0,24.0
240,Assh Lee in All Over That Cock,3964,Anal,114.0,130.0,16.0
240,Assh Lee in All Over That Cock,3966,BlowJob,790.0,830.0,40.0
240,Assh Lee in All Over That Cock,3969,Cumshot,1436.0,1440.0,4.0
240,Assh Lee in All Over That Cock,3967,BlowJob,1470.0,1480.0,10.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3922,BlowJob,276.0,286.0,10.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3924,BlowJob,364.0,422.0,58.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3928,Anal,570.0,582.0,12.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3925,BlowJob,1124.0,1154.0,30.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3926,BlowJob,1298.0,1308.0,10.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3932,Anal,1314.0,1330.0,16.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3929,Anal,1372.0,1424.0,52.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3927,BlowJob,1434.0,1458.0,24.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3930,Anal,1568.0,1584.0,16.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3934,Anal,1610.0,1626.0,16.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3935,Anal,1706.0,1746.0,40.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3936,Cumshot,1768.0,1810.0,42.0
242,Baby Gemini in Ricky's Room Blowbang,3937,Gangbang,62.0,84.0,22.0
242,Baby Gemini in Ricky's Room Blowbang,3938,Gangbang,124.0,140.0,16.0
242,Baby Gemini in Ricky's Room Blowbang,3939,Gangbang,238.0,254.0,16.0
242,Baby Gemini in Ricky's Room Blowbang,3940,Gangbang,300.0,350.0,50.0
242,Baby Gemini in Ricky's Room Blowbang,3941,Gangbang,594.0,638.0,44.0
242,Baby Gemini in Ricky's Room Blowbang,3942,Gangbang,694.0,722.0,28.0
242,Baby Gemini in Ricky's Room Blowbang,3943,Gangbang,764.0,806.0,42.0
242,Baby Gemini in Ricky's Room Blowbang,3944,Gangbang,864.0,898.0,34.0
243,Barbie Sins in DAP with Creampie,3977,Anal,652.0,672.0,20.0
243,Barbie Sins in DAP with Creampie,3978,Anal,704.0,722.0,18.0
243,Barbie Sins in DAP with Creampie,3971,BlowJob,770.0,782.0,12.0
243,Barbie Sins in DAP with Creampie,3972,BlowJob,946.0,980.0,34.0
243,Barbie Sins in DAP with Creampie,3973,BlowJob,1390.0,1440.0,50.0
243,Barbie Sins in DAP with Creampie,3981,Anal,1498.0,1542.0,44.0
243,Barbie Sins in DAP with Creampie,3982,Anal,1624.0,1648.0,24.0
243,Barbie Sins in DAP with Creampie,3984,Anal,1888.0,1926.0,38.0
243,Barbie Sins in DAP with Creampie,3985,Anal,1968.0,2002.0,34.0
243,Barbie Sins in DAP with Creampie,3974,BlowJob,1970.0,2012.0,42.0
243,Barbie Sins in DAP with Creampie,3986,Anal,2172.0,2216.0,44.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4014,Gangbang,180.0,220.0,40.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4015,Gangbang,374.0,398.0,24.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4024,Anal,532.0,572.0,40.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4025,Anal,666.0,676.0,10.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4003,BlowJob,882.0,896.0,14.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4027,Anal,1158.0,1214.0,56.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4004,BlowJob,1216.0,1226.0,10.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4038,Cumshot,1312.0,1320.0,8.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4040,Cumshot,1334.0,1344.0,10.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4005,BlowJob,1360.0,1394.0,34.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4017,Gangbang,1488.0,1502.0,14.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4028,Anal,1642.0,1676.0,34.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4007,BlowJob,1678.0,1692.0,14.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4018,Gangbang,1792.0,1802.0,10.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4019,Gangbang,1878.0,1904.0,26.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4030,Anal,2288.0,2334.0,46.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4008,BlowJob,2304.0,2330.0,26.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4031,Anal,2514.0,2536.0,22.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4021,Gangbang,2638.0,2666.0,28.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4009,BlowJob,2642.0,2684.0,42.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4032,Anal,2646.0,2686.0,40.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4022,Gangbang,2812.0,2828.0,16.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4034,Anal,2848.0,2862.0,14.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4013,BlowJob,3046.0,3056.0,10.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4035,Anal,3554.0,3590.0,36.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4036,Anal,3672.0,3700.0,28.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4037,Anal,3978.0,3990.0,12.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4012,BlowJob,4124.0,4138.0,14.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4039,Cumshot,4200.0,4204.0,4.0
245,Belinha Baracho in Intense 5on1 Gangbang,4041,Grabbing Boobs,6.0,28.0,22.0
245,Belinha Baracho in Intense 5on1 Gangbang,4042,Grabbing Boobs,142.0,176.0,34.0
245,Belinha Baracho in Intense 5on1 Gangbang,4050,Anal,482.0,520.0,38.0
245,Belinha Baracho in Intense 5on1 Gangbang,4057,Gangbang,608.0,632.0,24.0
245,Belinha Baracho in Intense 5on1 Gangbang,4051,Anal,610.0,662.0,52.0
245,Belinha Baracho in Intense 5on1 Gangbang,4044,BlowJob,624.0,656.0,32.0
245,Belinha Baracho in Intense 5on1 Gangbang,4058,Gangbang,720.0,758.0,38.0
245,Belinha Baracho in Intense 5on1 Gangbang,4045,BlowJob,746.0,772.0,26.0
245,Belinha Baracho in Intense 5on1 Gangbang,4052,Anal,758.0,796.0,38.0
245,Belinha Baracho in Intense 5on1 Gangbang,4059,Gangbang,932.0,984.0,52.0
245,Belinha Baracho in Intense 5on1 Gangbang,4060,Gangbang,1212.0,1232.0,20.0
245,Belinha Baracho in Intense 5on1 Gangbang,4061,Gangbang,1362.0,1380.0,18.0
245,Belinha Baracho in Intense 5on1 Gangbang,4046,BlowJob,1686.0,1722.0,36.0
245,Belinha Baracho in Intense 5on1 Gangbang,4048,BlowJob,1992.0,2004.0,12.0
245,Belinha Baracho in Intense 5on1 Gangbang,4049,BlowJob,2044.0,2058.0,14.0
245,Belinha Baracho in Intense 5on1 Gangbang,4063,Gangbang,2282.0,2304.0,22.0
245,Belinha Baracho in Intense 5on1 Gangbang,4064,Gangbang,2350.0,2372.0,22.0
245,Belinha Baracho in Intense 5on1 Gangbang,4054,Anal,2858.0,2878.0,20.0
245,Belinha Baracho in Intense 5on1 Gangbang,4055,Anal,2994.0,3028.0,34.0
245,Belinha Baracho in Intense 5on1 Gangbang,4056,Anal,3184.0,3194.0,10.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3946,Anal,1076.0,1128.0,52.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3956,BlowJob,1256.0,1286.0,30.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3947,Anal,1294.0,1336.0,42.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3949,Anal,1996.0,2038.0,42.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3951,Anal,2370.0,2428.0,58.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3952,Anal,2478.0,2508.0,30.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3957,BlowJob,2996.0,3028.0,32.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3953,Anal,3056.0,3080.0,24.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3958,BlowJob,3092.0,3112.0,20.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3954,Anal,3256.0,3274.0,18.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3959,BlowJob,3312.0,3352.0,40.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3960,BlowJob,3394.0,3418.0,24.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3961,BlowJob,3638.0,3656.0,18.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3962,BlowJob,3816.0,3858.0,42.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3987,Cumshot,1354.0,1374.0,20.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3988,Gangbang,1708.0,1720.0,12.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3991,BlowJob,1926.0,1958.0,32.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3994,Anal,2090.0,2120.0,30.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3995,Anal,2276.0,2292.0,16.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3996,Anal,2328.0,2376.0,48.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3989,Gangbang,2706.0,2720.0,14.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3998,Anal,3830.0,3846.0,16.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3992,BlowJob,3834.0,3848.0,14.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3999,Anal,3878.0,3916.0,38.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3990,Gangbang,3894.0,3908.0,14.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",4000,Anal,4006.0,4020.0,14.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",4001,Anal,4136.0,4146.0,10.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4065,Grabbing Boobs,26.0,44.0,18.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4066,Anal,232.0,264.0,32.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4067,Anal,478.0,502.0,24.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4068,Anal,966.0,1006.0,40.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4078,BlowJob,1160.0,1216.0,56.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4082,Cumshot,1596.0,1618.0,22.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4070,Anal,2034.0,2072.0,38.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4071,Anal,2138.0,2170.0,32.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4072,Anal,2210.0,2266.0,56.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4079,BlowJob,2484.0,2542.0,58.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4080,BlowJob,2690.0,2706.0,16.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4081,BlowJob,2864.0,2874.0,10.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4074,Anal,3228.0,3262.0,34.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4075,Anal,3312.0,3348.0,36.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4076,Anal,3396.0,3416.0,20.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4077,Anal,3456.0,3484.0,28.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4084,BlowJob,696.0,738.0,42.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4083,Grabbing Boobs,886.0,924.0,38.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4085,BlowJob,1008.0,1018.0,10.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4086,BlowJob,1050.0,1060.0,10.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4087,BlowJob,1206.0,1218.0,12.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4089,BlowJob,1932.0,1952.0,20.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4090,BlowJob,2688.0,2708.0,20.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4091,Cumshot,2928.0,2978.0,50.0
250,,2302,Gangbang,368.0,380.0,12.0
250,,2303,Gangbang,426.0,456.0,30.0
250,,2304,Gangbang,588.0,620.0,32.0
250,,2306,Gangbang,1028.0,1058.0,30.0
250,,2324,Grabbing Boobs,1182.0,1200.0,18.0
250,,2326,Anal,1224.0,1256.0,32.0
250,,2307,Gangbang,1230.0,1252.0,22.0
250,,2308,Gangbang,1308.0,1354.0,46.0
250,,2309,Gangbang,1602.0,1614.0,12.0
250,,2310,Gangbang,1646.0,1696.0,50.0
250,,2311,Gangbang,1736.0,1760.0,24.0
250,,2327,Anal,1802.0,1836.0,34.0
250,,2312,Gangbang,1906.0,1922.0,16.0
250,,2328,Anal,2064.0,2084.0,20.0
250,,2313,Gangbang,2158.0,2188.0,30.0
250,,2329,Anal,3432.0,3450.0,18.0
250,,2330,Anal,3514.0,3532.0,18.0
250,,2319,BlowJob,5992.0,6014.0,22.0
250,,2320,BlowJob,6062.0,6098.0,36.0
250,,2325,Grabbing Boobs,6140.0,6184.0,44.0
250,,2321,BlowJob,6160.0,6186.0,26.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4094,BlowJob,976.0,996.0,20.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4101,Anal,1360.0,1398.0,38.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4102,Anal,1552.0,1596.0,44.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4095,BlowJob,1756.0,1778.0,22.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4103,Anal,1782.0,1830.0,48.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4104,Anal,1934.0,1980.0,46.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4096,BlowJob,2416.0,2436.0,20.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4097,BlowJob,2642.0,2680.0,38.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4098,BlowJob,2714.0,2726.0,12.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4107,Anal,2748.0,2764.0,16.0
252,Bonny Bon in Sexual Rage 2 Sc3,4109,Anal,570.0,610.0,40.0
252,Bonny Bon in Sexual Rage 2 Sc3,4110,Anal,724.0,758.0,34.0
252,Bonny Bon in Sexual Rage 2 Sc3,4111,Anal,930.0,952.0,22.0
252,Bonny Bon in Sexual Rage 2 Sc3,4112,Anal,986.0,1018.0,32.0
252,Bonny Bon in Sexual Rage 2 Sc3,4113,Anal,1112.0,1152.0,40.0
252,Bonny Bon in Sexual Rage 2 Sc3,4117,BlowJob,1120.0,1156.0,36.0
252,Bonny Bon in Sexual Rage 2 Sc3,4114,Anal,1450.0,1474.0,24.0
252,Bonny Bon in Sexual Rage 2 Sc3,4120,Cumshot,1952.0,1960.0,8.0
252,Bonny Bon in Sexual Rage 2 Sc3,4119,BlowJob,1962.0,1994.0,32.0
253,Cali Caliente in The Gangbang Part IV,4121,BlowJob,172.0,214.0,42.0
253,Cali Caliente in The Gangbang Part IV,4129,Gangbang,832.0,878.0,46.0
253,Cali Caliente in The Gangbang Part IV,4133,Anal,1014.0,1036.0,22.0
253,Cali Caliente in The Gangbang Part IV,4124,BlowJob,1214.0,1246.0,32.0
253,Cali Caliente in The Gangbang Part IV,4134,Anal,1342.0,1376.0,34.0
253,Cali Caliente in The Gangbang Part IV,4135,Anal,1460.0,1502.0,42.0
253,Cali Caliente in The Gangbang Part IV,4136,Anal,1562.0,1602.0,40.0
253,Cali Caliente in The Gangbang Part IV,4137,Anal,1668.0,1690.0,22.0
253,Cali Caliente in The Gangbang Part IV,4131,Gangbang,1756.0,1788.0,32.0
253,Cali Caliente in The Gangbang Part IV,4126,BlowJob,1844.0,1886.0,42.0
253,Cali Caliente in The Gangbang Part IV,4138,Anal,1922.0,1934.0,12.0
253,Cali Caliente in The Gangbang Part IV,4127,BlowJob,1956.0,1970.0,14.0
253,Cali Caliente in The Gangbang Part IV,4128,BlowJob,2048.0,2060.0,12.0
253,Cali Caliente in The Gangbang Part IV,4139,Cumshot,2118.0,2128.0,10.0
253,Cali Caliente in The Gangbang Part IV,4140,Cumshot,2206.0,2222.0,16.0
254,Carla Morelli in Gangbang with 4 Cocks,4142,BlowJob,862.0,902.0,40.0
254,Carla Morelli in Gangbang with 4 Cocks,4145,BlowJob,1556.0,1600.0,44.0
254,Carla Morelli in Gangbang with 4 Cocks,4143,BlowJob,1646.0,1668.0,22.0
254,Carla Morelli in Gangbang with 4 Cocks,4144,BlowJob,1722.0,1734.0,12.0
254,Carla Morelli in Gangbang with 4 Cocks,4146,Cumshot,1760.0,1796.0,36.0
255,Carla Morelli in Hot for Teacher,1936,Grabbing Boobs,98.0,140.0,42.0
255,Carla Morelli in Hot for Teacher,1937,BlowJob,226.0,274.0,48.0
255,Carla Morelli in Hot for Teacher,1939,Cumshot,1294.0,1308.0,14.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4152,Grabbing Boobs,552.0,594.0,42.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4154,BlowJob,752.0,766.0,14.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4164,Anal,1074.0,1104.0,30.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4156,BlowJob,1244.0,1258.0,14.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4165,Anal,1448.0,1470.0,22.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4166,Anal,1754.0,1782.0,28.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4157,BlowJob,1822.0,1880.0,58.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4167,Anal,2000.0,2022.0,22.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4158,BlowJob,2108.0,2150.0,42.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4173,Gangbang,2260.0,2274.0,14.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4160,BlowJob,2406.0,2454.0,48.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4169,Anal,3170.0,3206.0,36.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4174,Gangbang,3178.0,3188.0,10.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4153,Grabbing Boobs,3536.0,3568.0,32.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4155,BlowJob,3668.0,3686.0,18.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4162,BlowJob,3772.0,3812.0,40.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4171,Anal,3908.0,3922.0,14.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4163,BlowJob,3952.0,3962.0,10.0
257,Chloe Amour in Mon Amour Sc1,4181,BlowJob,292.0,302.0,10.0
257,Chloe Amour in Mon Amour Sc1,4182,BlowJob,504.0,546.0,42.0
257,Chloe Amour in Mon Amour Sc1,4183,BlowJob,618.0,650.0,32.0
257,Chloe Amour in Mon Amour Sc1,4189,BlowJob,906.0,920.0,14.0
257,Chloe Amour in Mon Amour Sc1,4185,BlowJob,1020.0,1054.0,34.0
257,Chloe Amour in Mon Amour Sc1,4190,DP,1100.0,1110.0,10.0
257,Chloe Amour in Mon Amour Sc1,4186,BlowJob,1110.0,1122.0,12.0
257,Chloe Amour in Mon Amour Sc1,4191,DP,1220.0,1248.0,28.0
257,Chloe Amour in Mon Amour Sc1,4192,DP,1292.0,1310.0,18.0
257,Chloe Amour in Mon Amour Sc1,4188,BlowJob,1568.0,1608.0,40.0
257,Chloe Amour in Mon Amour Sc1,4193,DP,1848.0,1868.0,20.0
257,Chloe Amour in Mon Amour Sc1,4179,Anal,1958.0,1974.0,16.0
257,Chloe Amour in Mon Amour Sc1,4198,Gangbang,2004.0,2028.0,24.0
257,Chloe Amour in Mon Amour Sc1,4196,Cumshot,2132.0,2140.0,8.0
258,Chloe Amour in Mon Amour Sc2,4200,BlowJob,372.0,382.0,10.0
258,Chloe Amour in Mon Amour Sc2,4201,BlowJob,614.0,644.0,30.0
258,Chloe Amour in Mon Amour Sc2,4202,BlowJob,738.0,794.0,56.0
259,Chloe Amour in Mon Amour Sc3,4205,Gangbang,164.0,190.0,26.0
259,Chloe Amour in Mon Amour Sc3,4206,Gangbang,242.0,260.0,18.0
259,Chloe Amour in Mon Amour Sc3,4207,Gangbang,442.0,456.0,14.0
259,Chloe Amour in Mon Amour Sc3,4208,Gangbang,548.0,578.0,30.0
259,Chloe Amour in Mon Amour Sc3,4215,BlowJob,918.0,974.0,56.0
259,Chloe Amour in Mon Amour Sc3,4209,Gangbang,944.0,980.0,36.0
259,Chloe Amour in Mon Amour Sc3,4210,Gangbang,1352.0,1372.0,20.0
259,Chloe Amour in Mon Amour Sc3,4228,DP,1392.0,1420.0,28.0
259,Chloe Amour in Mon Amour Sc3,4211,Gangbang,2184.0,2202.0,18.0
259,Chloe Amour in Mon Amour Sc3,4212,Gangbang,2468.0,2492.0,24.0
259,Chloe Amour in Mon Amour Sc3,4213,Gangbang,2804.0,2828.0,24.0
259,Chloe Amour in Mon Amour Sc3,4226,Cumshot,2960.0,2988.0,28.0
260,"Chloe Amour, Jennifer White in Mon Amour Sc4",151,BlowJob,230.0,244.0,14.0
260,"Chloe Amour, Jennifer White in Mon Amour Sc4",153,BlowJob,412.0,432.0,20.0
260,"Chloe Amour, Jennifer White in Mon Amour Sc4",157,Cumshot,2310.0,2324.0,14.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,159,Grabbing Boobs,308.0,348.0,40.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,161,BlowJob,784.0,820.0,36.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,172,BlowJob,872.0,908.0,36.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,174,Gangbang,994.0,1008.0,14.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,164,BlowJob,1610.0,1646.0,36.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,165,BlowJob,2016.0,2060.0,44.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,166,BlowJob,2132.0,2144.0,12.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,168,BlowJob,2498.0,2514.0,16.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,178,Anal,2508.0,2556.0,48.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,173,BlowJob,2654.0,2660.0,6.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,176,Gangbang,3258.0,3308.0,50.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,169,BlowJob,3414.0,3426.0,12.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,170,BlowJob,3466.0,3500.0,34.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,180,Anal,3720.0,3730.0,10.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,181,Anal,3814.0,3864.0,50.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,171,BlowJob,3856.0,3914.0,58.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,182,Cumshot,3944.0,3990.0,46.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,184,Cumshot,4024.0,4072.0,48.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,185,Cumshot,4110.0,4132.0,22.0
262,"Cindy Starfall, Gaia in Swappers Sc1",213,Grabbing Boobs,200.0,242.0,42.0
262,"Cindy Starfall, Gaia in Swappers Sc1",215,BlowJob,842.0,852.0,10.0
262,"Cindy Starfall, Gaia in Swappers Sc1",218,69,876.0,906.0,30.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,186,Grabbing Boobs,116.0,146.0,30.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,188,BlowJob,214.0,252.0,38.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,189,BlowJob,332.0,348.0,16.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,195,Anal,1414.0,1466.0,52.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,194,BlowJob,1860.0,1868.0,8.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,187,Grabbing Boobs,1868.0,1914.0,46.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,196,Cumshot,1950.0,1956.0,6.0
265,"Danielle Renee, MarsFoxxx in Group Bang",235,Gangbang,230.0,262.0,32.0
265,"Danielle Renee, MarsFoxxx in Group Bang",224,BlowJob,380.0,392.0,12.0
265,"Danielle Renee, MarsFoxxx in Group Bang",236,Gangbang,396.0,428.0,32.0
265,"Danielle Renee, MarsFoxxx in Group Bang",243,Anal,416.0,426.0,10.0
265,"Danielle Renee, MarsFoxxx in Group Bang",226,BlowJob,648.0,700.0,52.0
265,"Danielle Renee, MarsFoxxx in Group Bang",238,Gangbang,684.0,706.0,22.0
265,"Danielle Renee, MarsFoxxx in Group Bang",244,Anal,814.0,842.0,28.0
265,"Danielle Renee, MarsFoxxx in Group Bang",245,Anal,920.0,930.0,10.0
265,"Danielle Renee, MarsFoxxx in Group Bang",228,BlowJob,942.0,968.0,26.0
265,"Danielle Renee, MarsFoxxx in Group Bang",246,Cumshot,1072.0,1092.0,20.0
265,"Danielle Renee, MarsFoxxx in Group Bang",229,BlowJob,1160.0,1206.0,46.0
265,"Danielle Renee, MarsFoxxx in Group Bang",241,Grabbing Boobs,1252.0,1282.0,30.0
265,"Danielle Renee, MarsFoxxx in Group Bang",230,BlowJob,1326.0,1354.0,28.0
265,"Danielle Renee, MarsFoxxx in Group Bang",239,Gangbang,1330.0,1350.0,20.0
265,"Danielle Renee, MarsFoxxx in Group Bang",247,Cumshot,1494.0,1504.0,10.0
265,"Danielle Renee, MarsFoxxx in Group Bang",248,Cumshot,1588.0,1594.0,6.0
265,"Danielle Renee, MarsFoxxx in Group Bang",231,BlowJob,1662.0,1704.0,42.0
265,"Danielle Renee, MarsFoxxx in Group Bang",232,BlowJob,1874.0,1916.0,42.0
265,"Danielle Renee, MarsFoxxx in Group Bang",242,Grabbing Boobs,1908.0,1942.0,34.0
265,"Danielle Renee, MarsFoxxx in Group Bang",233,BlowJob,2442.0,2464.0,22.0
265,"Danielle Renee, MarsFoxxx in Group Bang",240,Gangbang,2482.0,2508.0,26.0
266,Daria Glower in Die Haremsw√§chterin des √ñl Scheichs Sc6,250,BlowJob,782.0,804.0,22.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,251,Grabbing Boobs,84.0,114.0,30.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,254,BlowJob,590.0,614.0,24.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,265,Cumshot,1372.0,1376.0,4.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,257,BlowJob,1412.0,1428.0,16.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,252,Grabbing Boobs,1458.0,1512.0,54.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,269,Anal,2194.0,2208.0,14.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,260,BlowJob,2660.0,2676.0,16.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,261,BlowJob,2716.0,2730.0,14.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,271,Anal,2746.0,2798.0,52.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,262,BlowJob,3350.0,3404.0,54.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,263,BlowJob,3520.0,3530.0,10.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,264,BlowJob,3658.0,3694.0,36.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,266,Cumshot,3696.0,3718.0,22.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1858,BlowJob,176.0,214.0,38.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1859,BlowJob,382.0,410.0,28.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1860,BlowJob,460.0,476.0,16.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1861,BlowJob,560.0,592.0,32.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1856,Grabbing Boobs,638.0,666.0,28.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1862,BlowJob,650.0,680.0,30.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1863,BlowJob,890.0,902.0,12.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1857,Grabbing Boobs,1016.0,1074.0,58.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1864,BlowJob,1064.0,1094.0,30.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1866,BlowJob,1310.0,1316.0,6.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1868,Cumshot,1572.0,1592.0,20.0
269,Emmanuelle Noire in Busty Ebony Beauty,132,Grabbing Boobs,214.0,236.0,22.0
269,Emmanuelle Noire in Busty Ebony Beauty,140,BlowJob,342.0,354.0,12.0
269,Emmanuelle Noire in Busty Ebony Beauty,141,BlowJob,482.0,486.0,4.0
269,Emmanuelle Noire in Busty Ebony Beauty,134,BlowJob,546.0,564.0,18.0
269,Emmanuelle Noire in Busty Ebony Beauty,133,Grabbing Boobs,574.0,608.0,34.0
269,Emmanuelle Noire in Busty Ebony Beauty,135,BlowJob,924.0,936.0,12.0
269,Emmanuelle Noire in Busty Ebony Beauty,143,Anal,1124.0,1184.0,60.0
269,Emmanuelle Noire in Busty Ebony Beauty,136,BlowJob,1172.0,1204.0,32.0
269,Emmanuelle Noire in Busty Ebony Beauty,144,Anal,1216.0,1234.0,18.0
269,Emmanuelle Noire in Busty Ebony Beauty,137,BlowJob,1242.0,1260.0,18.0
269,Emmanuelle Noire in Busty Ebony Beauty,145,Anal,1420.0,1430.0,10.0
269,Emmanuelle Noire in Busty Ebony Beauty,138,BlowJob,1614.0,1636.0,22.0
269,Emmanuelle Noire in Busty Ebony Beauty,139,BlowJob,1688.0,1700.0,12.0
269,Emmanuelle Noire in Busty Ebony Beauty,142,BlowJob,1726.0,1730.0,4.0
269,Emmanuelle Noire in Busty Ebony Beauty,146,Cumshot,1976.0,2002.0,26.0
269,Emmanuelle Noire in Busty Ebony Beauty,148,Cumshot,2040.0,2048.0,8.0
270,Francesca Le in Lewood Gangbang Battle of the MILFs Sc1,149,BlowJob,162.0,210.0,48.0
271,Francesca Le in Lewood Gangbang Battle of the MILFs Sc2,274,BlowJob,52.0,72.0,20.0
271,Francesca Le in Lewood Gangbang Battle of the MILFs Sc2,275,BlowJob,214.0,228.0,14.0
271,Francesca Le in Lewood Gangbang Battle of the MILFs Sc2,276,Anal,278.0,296.0,18.0
272,Francesca Le in Lewood Gangbang Battle of the MILFs Sc3,278,69,136.0,148.0,12.0
273,Francesca Le in Lewood Gangbang Battle of the MILFs Sc4,279,BlowJob,16.0,48.0,32.0
273,Francesca Le in Lewood Gangbang Battle of the MILFs Sc4,282,Anal,130.0,140.0,10.0
273,Francesca Le in Lewood Gangbang Battle of the MILFs Sc4,280,BlowJob,178.0,202.0,24.0
273,Francesca Le in Lewood Gangbang Battle of the MILFs Sc4,281,BlowJob,330.0,342.0,12.0
274,Francesca Le in Lewood Gangbang Battle of the MILFs Sc5,283,BlowJob,136.0,160.0,24.0
274,Francesca Le in Lewood Gangbang Battle of the MILFs Sc5,284,Grabbing Boobs,224.0,252.0,28.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,300,Gangbang,162.0,190.0,28.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,285,BlowJob,166.0,188.0,22.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,286,BlowJob,222.0,238.0,16.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,312,Cumshot,230.0,236.0,6.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,287,BlowJob,274.0,296.0,22.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,288,BlowJob,342.0,364.0,22.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,302,Gangbang,510.0,562.0,52.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,289,BlowJob,512.0,534.0,22.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,303,Gangbang,660.0,692.0,32.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,291,BlowJob,982.0,994.0,12.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,306,Gangbang,1230.0,1256.0,26.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,316,Anal,1468.0,1508.0,40.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,307,Gangbang,1480.0,1528.0,48.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,292,BlowJob,1514.0,1540.0,26.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,317,Anal,1560.0,1580.0,20.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,293,BlowJob,1582.0,1612.0,30.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,294,BlowJob,1670.0,1684.0,14.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,318,Anal,1684.0,1712.0,28.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,319,Anal,1764.0,1814.0,50.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,308,Gangbang,1814.0,1836.0,22.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,295,BlowJob,1920.0,1944.0,24.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,322,DP,2300.0,2324.0,24.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,310,Gangbang,2318.0,2350.0,32.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,311,Gangbang,2692.0,2706.0,14.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,299,BlowJob,2744.0,2750.0,6.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,298,BlowJob,2852.0,2872.0,20.0
276,"Gaia in Screw My Wife, Please 76 Sc2",323,BlowJob,184.0,230.0,46.0
277,Gaia in Throated 39 Sc5,324,Grabbing Boobs,254.0,298.0,44.0
277,Gaia in Throated 39 Sc5,326,BlowJob,350.0,362.0,12.0
277,Gaia in Throated 39 Sc5,332,BlowJob,378.0,406.0,28.0
277,Gaia in Throated 39 Sc5,335,Cumshot,586.0,596.0,10.0
277,Gaia in Throated 39 Sc5,334,BlowJob,702.0,752.0,50.0
277,Gaia in Throated 39 Sc5,329,BlowJob,1080.0,1140.0,60.0
277,Gaia in Throated 39 Sc5,330,BlowJob,1302.0,1356.0,54.0
277,Gaia in Throated 39 Sc5,325,Grabbing Boobs,1346.0,1382.0,36.0
277,Gaia in Throated 39 Sc5,331,BlowJob,1530.0,1568.0,38.0
278,Hannah Jo in Thick Dick Threesome,340,BlowJob,968.0,978.0,10.0
278,Hannah Jo in Thick Dick Threesome,341,BlowJob,1036.0,1046.0,10.0
278,Hannah Jo in Thick Dick Threesome,342,BlowJob,1140.0,1174.0,34.0
278,Hannah Jo in Thick Dick Threesome,343,BlowJob,1212.0,1230.0,18.0
278,Hannah Jo in Thick Dick Threesome,344,BlowJob,1364.0,1374.0,10.0
278,Hannah Jo in Thick Dick Threesome,345,BlowJob,1892.0,1910.0,18.0
278,Hannah Jo in Thick Dick Threesome,346,BlowJob,2038.0,2094.0,56.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,348,Grabbing Boobs,50.0,80.0,30.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,349,Grabbing Boobs,378.0,402.0,24.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,350,Grabbing Boobs,638.0,658.0,20.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,353,BlowJob,916.0,932.0,16.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,354,BlowJob,1060.0,1074.0,14.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,370,Anal,1084.0,1116.0,32.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,355,BlowJob,1124.0,1166.0,42.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,356,BlowJob,1414.0,1430.0,16.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,357,BlowJob,1514.0,1524.0,10.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,358,BlowJob,1726.0,1780.0,54.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,360,BlowJob,2150.0,2180.0,30.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,376,Gangbang,2574.0,2620.0,46.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,351,Grabbing Boobs,2706.0,2738.0,32.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,371,Anal,2750.0,2760.0,10.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,372,Anal,2814.0,2836.0,22.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,373,Anal,2892.0,2934.0,42.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,352,Grabbing Boobs,2916.0,2954.0,38.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,374,Anal,3048.0,3102.0,54.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,364,BlowJob,3074.0,3114.0,40.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,366,BlowJob,3646.0,3690.0,44.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,369,BlowJob,4148.0,4180.0,32.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,377,Cumshot,4236.0,4294.0,58.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,378,Cumshot,4354.0,4362.0,8.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,379,Cumshot,4424.0,4466.0,42.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",380,BlowJob,266.0,294.0,28.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",385,BlowJob,312.0,318.0,6.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",386,Gangbang,336.0,346.0,10.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",382,BlowJob,442.0,474.0,32.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",387,Gangbang,488.0,514.0,26.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",5011,BlowJob,576.0,586.0,10.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",5012,BlowJob,744.0,792.0,48.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",383,BlowJob,834.0,846.0,12.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",388,Gangbang,864.0,894.0,30.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",5014,BlowJob,900.0,922.0,22.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",393,Cumshot,1224.0,1248.0,24.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",5015,BlowJob,1260.0,1300.0,40.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",400,Cumshot,1280.0,1302.0,22.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",401,Grabbing Boobs,1308.0,1332.0,24.0
281,Jada Fire in Assault That Ass 8 Sc3,404,BlowJob,338.0,372.0,34.0
281,Jada Fire in Assault That Ass 8 Sc3,405,BlowJob,416.0,470.0,54.0
281,Jada Fire in Assault That Ass 8 Sc3,408,Cumshot,472.0,476.0,4.0
281,Jada Fire in Assault That Ass 8 Sc3,406,BlowJob,912.0,942.0,30.0
281,Jada Fire in Assault That Ass 8 Sc3,407,BlowJob,1082.0,1124.0,42.0
281,Jada Fire in Assault That Ass 8 Sc3,409,Cumshot,1650.0,1654.0,4.0
282,Jada Fire in Throat Yogurt 2 Sc1,412,Cumshot,126.0,182.0,56.0
282,Jada Fire in Throat Yogurt 2 Sc1,413,Cumshot,788.0,826.0,38.0
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,417,BlowJob,886.0,928.0,42.0
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,418,BlowJob,980.0,1022.0,42.0
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,419,BlowJob,1116.0,1136.0,20.0
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,420,BlowJob,1350.0,1382.0,32.0
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,421,Anal,1522.0,1538.0,16.0
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,422,Cumshot,1696.0,1738.0,42.0
284,Jasminy Villar in The Stepfather And His Four Friends,423,Anal,194.0,230.0,36.0
284,Jasminy Villar in The Stepfather And His Four Friends,434,BlowJob,386.0,390.0,4.0
284,Jasminy Villar in The Stepfather And His Four Friends,438,Gangbang,690.0,722.0,32.0
284,Jasminy Villar in The Stepfather And His Four Friends,439,Gangbang,756.0,786.0,30.0
284,Jasminy Villar in The Stepfather And His Four Friends,430,BlowJob,1148.0,1166.0,18.0
284,Jasminy Villar in The Stepfather And His Four Friends,425,Anal,1174.0,1196.0,22.0
284,Jasminy Villar in The Stepfather And His Four Friends,440,Gangbang,1350.0,1388.0,38.0
284,Jasminy Villar in The Stepfather And His Four Friends,447,DP,1648.0,1670.0,22.0
284,Jasminy Villar in The Stepfather And His Four Friends,448,DP,1840.0,1876.0,36.0
284,Jasminy Villar in The Stepfather And His Four Friends,441,Gangbang,1910.0,1944.0,34.0
284,Jasminy Villar in The Stepfather And His Four Friends,442,Gangbang,2006.0,2016.0,10.0
284,Jasminy Villar in The Stepfather And His Four Friends,444,Gangbang,2330.0,2382.0,52.0
284,Jasminy Villar in The Stepfather And His Four Friends,427,Anal,2368.0,2380.0,12.0
284,Jasminy Villar in The Stepfather And His Four Friends,431,BlowJob,2520.0,2532.0,12.0
284,Jasminy Villar in The Stepfather And His Four Friends,445,Gangbang,2532.0,2552.0,20.0
284,Jasminy Villar in The Stepfather And His Four Friends,449,Cumshot,3312.0,3366.0,54.0
284,Jasminy Villar in The Stepfather And His Four Friends,452,Cumshot,3384.0,3394.0,10.0
284,Jasminy Villar in The Stepfather And His Four Friends,433,BlowJob,3406.0,3458.0,52.0
284,Jasminy Villar in The Stepfather And His Four Friends,450,Cumshot,3422.0,3432.0,10.0
284,Jasminy Villar in The Stepfather And His Four Friends,451,Cumshot,3468.0,3492.0,24.0
285,Jena LaRose in Blacks On Blondes,453,BlowJob,206.0,246.0,40.0
285,Jena LaRose in Blacks On Blondes,461,Anal,492.0,548.0,56.0
285,Jena LaRose in Blacks On Blondes,462,Anal,602.0,626.0,24.0
285,Jena LaRose in Blacks On Blondes,463,Anal,702.0,716.0,14.0
285,Jena LaRose in Blacks On Blondes,458,BlowJob,750.0,782.0,32.0
285,Jena LaRose in Blacks On Blondes,464,Anal,764.0,800.0,36.0
285,Jena LaRose in Blacks On Blondes,465,Anal,892.0,934.0,42.0
285,Jena LaRose in Blacks On Blondes,454,BlowJob,968.0,990.0,22.0
285,Jena LaRose in Blacks On Blondes,459,BlowJob,1250.0,1276.0,26.0
285,Jena LaRose in Blacks On Blondes,456,BlowJob,1622.0,1660.0,38.0
285,Jena LaRose in Blacks On Blondes,457,BlowJob,1778.0,1804.0,26.0
285,Jena LaRose in Blacks On Blondes,467,Cumshot,2034.0,2052.0,18.0
286,Jennifer White in Jennifer White Overload Sc1,469,Grabbing Boobs,212.0,244.0,32.0
286,Jennifer White in Jennifer White Overload Sc1,470,Gangbang,414.0,446.0,32.0
286,Jennifer White in Jennifer White Overload Sc1,488,BlowJob,598.0,622.0,24.0
286,Jennifer White in Jennifer White Overload Sc1,472,Gangbang,656.0,672.0,16.0
286,Jennifer White in Jennifer White Overload Sc1,473,Gangbang,754.0,772.0,18.0
286,Jennifer White in Jennifer White Overload Sc1,489,BlowJob,816.0,838.0,22.0
286,Jennifer White in Jennifer White Overload Sc1,475,Gangbang,1224.0,1244.0,20.0
286,Jennifer White in Jennifer White Overload Sc1,490,BlowJob,1640.0,1664.0,24.0
286,Jennifer White in Jennifer White Overload Sc1,492,BlowJob,1826.0,1860.0,34.0
286,Jennifer White in Jennifer White Overload Sc1,478,Gangbang,1990.0,2026.0,36.0
286,Jennifer White in Jennifer White Overload Sc1,479,Gangbang,2094.0,2114.0,20.0
286,Jennifer White in Jennifer White Overload Sc1,481,Gangbang,2312.0,2342.0,30.0
286,Jennifer White in Jennifer White Overload Sc1,482,Gangbang,2394.0,2416.0,22.0
286,Jennifer White in Jennifer White Overload Sc1,494,Anal,2550.0,2570.0,20.0
286,Jennifer White in Jennifer White Overload Sc1,495,Anal,2608.0,2620.0,12.0
286,Jennifer White in Jennifer White Overload Sc1,484,Gangbang,2676.0,2696.0,20.0
286,Jennifer White in Jennifer White Overload Sc1,485,Gangbang,2732.0,2746.0,14.0
286,Jennifer White in Jennifer White Overload Sc1,486,Gangbang,2780.0,2798.0,18.0
286,Jennifer White in Jennifer White Overload Sc1,497,Anal,2934.0,2950.0,16.0
287,Jennifer White in Jennifer White Overload Sc2,498,Grabbing Boobs,84.0,114.0,30.0
287,Jennifer White in Jennifer White Overload Sc2,499,Grabbing Boobs,196.0,220.0,24.0
287,Jennifer White in Jennifer White Overload Sc2,508,Gangbang,712.0,752.0,40.0
287,Jennifer White in Jennifer White Overload Sc2,511,Cumshot,846.0,878.0,32.0
287,Jennifer White in Jennifer White Overload Sc2,517,Anal,1004.0,1042.0,38.0
287,Jennifer White in Jennifer White Overload Sc2,518,Anal,1186.0,1242.0,56.0
287,Jennifer White in Jennifer White Overload Sc2,519,Anal,1342.0,1364.0,22.0
287,Jennifer White in Jennifer White Overload Sc2,501,BlowJob,1644.0,1692.0,48.0
287,Jennifer White in Jennifer White Overload Sc2,502,BlowJob,1792.0,1812.0,20.0
287,Jennifer White in Jennifer White Overload Sc2,520,Anal,1970.0,2026.0,56.0
287,Jennifer White in Jennifer White Overload Sc2,521,Anal,2102.0,2162.0,60.0
287,Jennifer White in Jennifer White Overload Sc2,522,Anal,2270.0,2330.0,60.0
287,Jennifer White in Jennifer White Overload Sc2,504,BlowJob,2274.0,2312.0,38.0
287,Jennifer White in Jennifer White Overload Sc2,505,BlowJob,2588.0,2646.0,58.0
287,Jennifer White in Jennifer White Overload Sc2,509,Gangbang,2626.0,2640.0,14.0
287,Jennifer White in Jennifer White Overload Sc2,524,Anal,2722.0,2766.0,44.0
287,Jennifer White in Jennifer White Overload Sc2,506,BlowJob,2876.0,2918.0,42.0
287,Jennifer White in Jennifer White Overload Sc2,510,Gangbang,3104.0,3138.0,34.0
287,Jennifer White in Jennifer White Overload Sc2,514,Cumshot,3106.0,3140.0,34.0
287,Jennifer White in Jennifer White Overload Sc2,515,Cumshot,3196.0,3252.0,56.0
287,Jennifer White in Jennifer White Overload Sc2,513,Cumshot,3282.0,3294.0,12.0
288,Jennifer White in Jennifer White Overload Sc3,525,Grabbing Boobs,58.0,100.0,42.0
288,Jennifer White in Jennifer White Overload Sc3,539,BlowJob,130.0,156.0,26.0
288,Jennifer White in Jennifer White Overload Sc3,540,BlowJob,324.0,358.0,34.0
288,Jennifer White in Jennifer White Overload Sc3,541,BlowJob,400.0,428.0,28.0
288,Jennifer White in Jennifer White Overload Sc3,528,Gangbang,408.0,426.0,18.0
288,Jennifer White in Jennifer White Overload Sc3,542,BlowJob,470.0,484.0,14.0
288,Jennifer White in Jennifer White Overload Sc3,543,BlowJob,568.0,586.0,18.0
288,Jennifer White in Jennifer White Overload Sc3,545,BlowJob,756.0,768.0,12.0
288,Jennifer White in Jennifer White Overload Sc3,526,Grabbing Boobs,784.0,826.0,42.0
288,Jennifer White in Jennifer White Overload Sc3,530,Gangbang,810.0,862.0,52.0
288,Jennifer White in Jennifer White Overload Sc3,531,Gangbang,920.0,944.0,24.0
288,Jennifer White in Jennifer White Overload Sc3,552,Anal,934.0,990.0,56.0
288,Jennifer White in Jennifer White Overload Sc3,551,BlowJob,938.0,950.0,12.0
288,Jennifer White in Jennifer White Overload Sc3,547,BlowJob,1000.0,1010.0,10.0
288,Jennifer White in Jennifer White Overload Sc3,549,BlowJob,1268.0,1282.0,14.0
288,Jennifer White in Jennifer White Overload Sc3,554,Anal,1660.0,1692.0,32.0
288,Jennifer White in Jennifer White Overload Sc3,534,Gangbang,1738.0,1750.0,12.0
288,Jennifer White in Jennifer White Overload Sc3,535,Gangbang,1794.0,1838.0,44.0
288,Jennifer White in Jennifer White Overload Sc3,555,Anal,1840.0,1856.0,16.0
288,Jennifer White in Jennifer White Overload Sc3,550,BlowJob,1952.0,1982.0,30.0
288,Jennifer White in Jennifer White Overload Sc3,536,Gangbang,2042.0,2074.0,32.0
288,Jennifer White in Jennifer White Overload Sc3,557,Cumshot,2368.0,2388.0,20.0
288,Jennifer White in Jennifer White Overload Sc3,558,Cumshot,2438.0,2478.0,40.0
288,Jennifer White in Jennifer White Overload Sc3,538,Gangbang,2440.0,2464.0,24.0
289,Jenny Pretinha in Fucked by 3 Huge Cocks,560,Anal,374.0,408.0,34.0
289,Jenny Pretinha in Fucked by 3 Huge Cocks,559,BlowJob,450.0,466.0,16.0
289,Jenny Pretinha in Fucked by 3 Huge Cocks,561,Anal,696.0,720.0,24.0
289,Jenny Pretinha in Fucked by 3 Huge Cocks,562,Anal,858.0,868.0,10.0
289,Jenny Pretinha in Fucked by 3 Huge Cocks,563,Anal,1088.0,1100.0,12.0
290,Juelz Ventura in POV BBC Airtight Gangbang,565,BlowJob,2.0,18.0,16.0
290,Juelz Ventura in POV BBC Airtight Gangbang,581,Anal,52.0,70.0,18.0
290,Juelz Ventura in POV BBC Airtight Gangbang,582,Anal,192.0,202.0,10.0
290,Juelz Ventura in POV BBC Airtight Gangbang,583,Anal,238.0,282.0,44.0
290,Juelz Ventura in POV BBC Airtight Gangbang,567,BlowJob,376.0,410.0,34.0
290,Juelz Ventura in POV BBC Airtight Gangbang,568,BlowJob,472.0,514.0,42.0
290,Juelz Ventura in POV BBC Airtight Gangbang,569,BlowJob,628.0,642.0,14.0
290,Juelz Ventura in POV BBC Airtight Gangbang,570,BlowJob,674.0,692.0,18.0
290,Juelz Ventura in POV BBC Airtight Gangbang,572,BlowJob,1130.0,1148.0,18.0
290,Juelz Ventura in POV BBC Airtight Gangbang,580,BlowJob,1166.0,1190.0,24.0
290,Juelz Ventura in POV BBC Airtight Gangbang,588,Cumshot,1222.0,1226.0,4.0
290,Juelz Ventura in POV BBC Airtight Gangbang,589,Cumshot,1342.0,1354.0,12.0
290,Juelz Ventura in POV BBC Airtight Gangbang,573,BlowJob,1344.0,1358.0,14.0
290,Juelz Ventura in POV BBC Airtight Gangbang,585,Anal,1402.0,1416.0,14.0
290,Juelz Ventura in POV BBC Airtight Gangbang,590,Cumshot,1512.0,1522.0,10.0
290,Juelz Ventura in POV BBC Airtight Gangbang,594,Cumshot,1628.0,1640.0,12.0
290,Juelz Ventura in POV BBC Airtight Gangbang,575,BlowJob,1702.0,1738.0,36.0
290,Juelz Ventura in POV BBC Airtight Gangbang,576,BlowJob,1774.0,1790.0,16.0
290,Juelz Ventura in POV BBC Airtight Gangbang,586,Anal,1812.0,1846.0,34.0
290,Juelz Ventura in POV BBC Airtight Gangbang,587,Anal,1882.0,1912.0,30.0
290,Juelz Ventura in POV BBC Airtight Gangbang,577,BlowJob,2074.0,2088.0,14.0
290,Juelz Ventura in POV BBC Airtight Gangbang,592,Cumshot,2430.0,2468.0,38.0
290,Juelz Ventura in POV BBC Airtight Gangbang,578,BlowJob,2446.0,2478.0,32.0
290,Juelz Ventura in POV BBC Airtight Gangbang,595,Cumshot,2850.0,2884.0,34.0
290,Juelz Ventura in POV BBC Airtight Gangbang,579,BlowJob,2854.0,2868.0,14.0
291,"Jureka Del Mar, Maylee Fun in Asian Hotties Work to Cure",596,BlowJob,112.0,118.0,6.0
291,"Jureka Del Mar, Maylee Fun in Asian Hotties Work to Cure",597,BlowJob,130.0,186.0,56.0
292,Katalina Kyle in Ass Worship 18 Sc3,603,BlowJob,608.0,620.0,12.0
292,Katalina Kyle in Ass Worship 18 Sc3,604,BlowJob,674.0,684.0,10.0
292,Katalina Kyle in Ass Worship 18 Sc3,605,BlowJob,766.0,776.0,10.0
292,Katalina Kyle in Ass Worship 18 Sc3,606,BlowJob,824.0,834.0,10.0
292,Katalina Kyle in Ass Worship 18 Sc3,600,Grabbing Boobs,942.0,980.0,38.0
292,Katalina Kyle in Ass Worship 18 Sc3,607,BlowJob,1264.0,1300.0,36.0
292,Katalina Kyle in Ass Worship 18 Sc3,614,Anal,1310.0,1342.0,32.0
292,Katalina Kyle in Ass Worship 18 Sc3,615,Anal,1464.0,1480.0,16.0
292,Katalina Kyle in Ass Worship 18 Sc3,619,DP,1712.0,1740.0,28.0
292,Katalina Kyle in Ass Worship 18 Sc3,601,Grabbing Boobs,1898.0,1938.0,40.0
292,Katalina Kyle in Ass Worship 18 Sc3,620,DP,2018.0,2030.0,12.0
292,Katalina Kyle in Ass Worship 18 Sc3,613,BlowJob,2064.0,2112.0,48.0
292,Katalina Kyle in Ass Worship 18 Sc3,609,BlowJob,2174.0,2186.0,12.0
292,Katalina Kyle in Ass Worship 18 Sc3,610,BlowJob,2252.0,2276.0,24.0
292,Katalina Kyle in Ass Worship 18 Sc3,602,Grabbing Boobs,2328.0,2346.0,18.0
292,Katalina Kyle in Ass Worship 18 Sc3,611,BlowJob,2354.0,2396.0,42.0
292,Katalina Kyle in Ass Worship 18 Sc3,612,BlowJob,2436.0,2474.0,38.0
293,Katalina Kyle in Takes Every Inch Of Manuel,630,BlowJob,440.0,468.0,28.0
293,Katalina Kyle in Takes Every Inch Of Manuel,625,Grabbing Boobs,506.0,562.0,56.0
293,Katalina Kyle in Takes Every Inch Of Manuel,626,Grabbing Boobs,746.0,790.0,44.0
293,Katalina Kyle in Takes Every Inch Of Manuel,631,BlowJob,908.0,936.0,28.0
293,Katalina Kyle in Takes Every Inch Of Manuel,635,Anal,1028.0,1040.0,12.0
293,Katalina Kyle in Takes Every Inch Of Manuel,632,BlowJob,1124.0,1140.0,16.0
293,Katalina Kyle in Takes Every Inch Of Manuel,627,Grabbing Boobs,1156.0,1176.0,20.0
293,Katalina Kyle in Takes Every Inch Of Manuel,628,Grabbing Boobs,1484.0,1520.0,36.0
293,Katalina Kyle in Takes Every Inch Of Manuel,634,BlowJob,1544.0,1570.0,26.0
293,Katalina Kyle in Takes Every Inch Of Manuel,636,Cumshot,1550.0,1594.0,44.0
293,Katalina Kyle in Takes Every Inch Of Manuel,629,Grabbing Boobs,1578.0,1614.0,36.0
294,Katia Belinii in Swallowing 5 Big Loads,637,Grabbing Boobs,18.0,48.0,30.0
294,Katia Belinii in Swallowing 5 Big Loads,638,Grabbing Boobs,338.0,368.0,30.0
294,Katia Belinii in Swallowing 5 Big Loads,645,Anal,482.0,514.0,32.0
294,Katia Belinii in Swallowing 5 Big Loads,641,BlowJob,574.0,588.0,14.0
294,Katia Belinii in Swallowing 5 Big Loads,639,Grabbing Boobs,728.0,746.0,18.0
294,Katia Belinii in Swallowing 5 Big Loads,648,Cumshot,740.0,750.0,10.0
294,Katia Belinii in Swallowing 5 Big Loads,642,BlowJob,752.0,784.0,32.0
294,Katia Belinii in Swallowing 5 Big Loads,647,Anal,1100.0,1114.0,14.0
294,Katia Belinii in Swallowing 5 Big Loads,643,BlowJob,1132.0,1172.0,40.0
294,Katia Belinii in Swallowing 5 Big Loads,649,Cumshot,1208.0,1238.0,30.0
294,Katia Belinii in Swallowing 5 Big Loads,644,BlowJob,1262.0,1298.0,36.0
294,Katia Belinii in Swallowing 5 Big Loads,650,Cumshot,1520.0,1538.0,18.0
296,Kayla Carrera in Anal Integrity Sc1,653,Grabbing Boobs,108.0,154.0,46.0
296,Kayla Carrera in Anal Integrity Sc1,654,BlowJob,528.0,570.0,42.0
296,Kayla Carrera in Anal Integrity Sc1,657,Anal,1716.0,1770.0,54.0
296,Kayla Carrera in Anal Integrity Sc1,658,Anal,1848.0,1882.0,34.0
296,Kayla Carrera in Anal Integrity Sc1,660,Anal,2394.0,2404.0,10.0
296,Kayla Carrera in Anal Integrity Sc1,661,Anal,2516.0,2560.0,44.0
297,Kaylani Lei in Asian Fuck Machines Sc5,664,DP,912.0,918.0,6.0
297,Kaylani Lei in Asian Fuck Machines Sc5,666,BlowJob,1622.0,1650.0,28.0
297,Kaylani Lei in Asian Fuck Machines Sc5,667,BlowJob,1696.0,1728.0,32.0
297,Kaylani Lei in Asian Fuck Machines Sc5,665,DP,2096.0,2132.0,36.0
298,Kazumi Squirts in BBC Orgy Room,684,Cumshot,174.0,220.0,46.0
298,Kazumi Squirts in BBC Orgy Room,692,Gangbang,184.0,208.0,24.0
298,Kazumi Squirts in BBC Orgy Room,693,Gangbang,240.0,268.0,28.0
298,Kazumi Squirts in BBC Orgy Room,685,Cumshot,398.0,428.0,30.0
298,Kazumi Squirts in BBC Orgy Room,671,BlowJob,1230.0,1252.0,22.0
298,Kazumi Squirts in BBC Orgy Room,672,BlowJob,1380.0,1408.0,28.0
298,Kazumi Squirts in BBC Orgy Room,695,Gangbang,1398.0,1424.0,26.0
298,Kazumi Squirts in BBC Orgy Room,673,BlowJob,1526.0,1540.0,14.0
298,Kazumi Squirts in BBC Orgy Room,696,Gangbang,2264.0,2292.0,28.0
298,Kazumi Squirts in BBC Orgy Room,674,BlowJob,2386.0,2432.0,46.0
298,Kazumi Squirts in BBC Orgy Room,683,BlowJob,2838.0,2842.0,4.0
298,Kazumi Squirts in BBC Orgy Room,676,BlowJob,3054.0,3064.0,10.0
298,Kazumi Squirts in BBC Orgy Room,677,BlowJob,3234.0,3244.0,10.0
298,Kazumi Squirts in BBC Orgy Room,678,BlowJob,3346.0,3356.0,10.0
298,Kazumi Squirts in BBC Orgy Room,688,Cumshot,3490.0,3510.0,20.0
298,Kazumi Squirts in BBC Orgy Room,689,Cumshot,3560.0,3612.0,52.0
298,Kazumi Squirts in BBC Orgy Room,686,Cumshot,3644.0,3652.0,8.0
298,Kazumi Squirts in BBC Orgy Room,690,Cumshot,3696.0,3706.0,10.0
298,Kazumi Squirts in BBC Orgy Room,687,Cumshot,3830.0,3836.0,6.0
298,Kazumi Squirts in BBC Orgy Room,679,BlowJob,4166.0,4204.0,38.0
298,Kazumi Squirts in BBC Orgy Room,691,Cumshot,4204.0,4228.0,24.0
299,Kazumi Squirts in Gangbang With Piss and DP,704,Pissing,26.0,36.0,10.0
299,Kazumi Squirts in Gangbang With Piss and DP,708,BlowJob,208.0,246.0,38.0
299,Kazumi Squirts in Gangbang With Piss and DP,716,Cumshot,494.0,516.0,22.0
299,Kazumi Squirts in Gangbang With Piss and DP,717,Cumshot,914.0,918.0,4.0
299,Kazumi Squirts in Gangbang With Piss and DP,705,Pissing,940.0,990.0,50.0
299,Kazumi Squirts in Gangbang With Piss and DP,725,Anal,1512.0,1534.0,22.0
299,Kazumi Squirts in Gangbang With Piss and DP,700,Gangbang,1608.0,1620.0,12.0
299,Kazumi Squirts in Gangbang With Piss and DP,726,Anal,1622.0,1636.0,14.0
299,Kazumi Squirts in Gangbang With Piss and DP,701,Gangbang,1808.0,1818.0,10.0
299,Kazumi Squirts in Gangbang With Piss and DP,702,Gangbang,1866.0,1886.0,20.0
299,Kazumi Squirts in Gangbang With Piss and DP,714,BlowJob,1876.0,1894.0,18.0
299,Kazumi Squirts in Gangbang With Piss and DP,711,BlowJob,1962.0,1986.0,24.0
299,Kazumi Squirts in Gangbang With Piss and DP,727,Anal,2016.0,2046.0,30.0
299,Kazumi Squirts in Gangbang With Piss and DP,729,DP,2028.0,2048.0,20.0
299,Kazumi Squirts in Gangbang With Piss and DP,728,Anal,2218.0,2252.0,34.0
299,Kazumi Squirts in Gangbang With Piss and DP,703,Gangbang,2386.0,2402.0,16.0
299,Kazumi Squirts in Gangbang With Piss and DP,712,BlowJob,2418.0,2470.0,52.0
299,Kazumi Squirts in Gangbang With Piss and DP,715,BlowJob,2596.0,2614.0,18.0
299,Kazumi Squirts in Gangbang With Piss and DP,706,Pissing,2934.0,2968.0,34.0
299,Kazumi Squirts in Gangbang With Piss and DP,713,BlowJob,3258.0,3268.0,10.0
301,Kelly Oliveira in Assfucked 4on1 with DP,748,BlowJob,560.0,578.0,18.0
301,Kelly Oliveira in Assfucked 4on1 with DP,749,BlowJob,682.0,710.0,28.0
301,Kelly Oliveira in Assfucked 4on1 with DP,750,BlowJob,758.0,778.0,20.0
301,Kelly Oliveira in Assfucked 4on1 with DP,753,Titjob,760.0,776.0,16.0
301,Kelly Oliveira in Assfucked 4on1 with DP,756,Anal,1420.0,1450.0,30.0
301,Kelly Oliveira in Assfucked 4on1 with DP,757,Anal,1608.0,1636.0,28.0
301,Kelly Oliveira in Assfucked 4on1 with DP,752,BlowJob,1722.0,1754.0,32.0
301,Kelly Oliveira in Assfucked 4on1 with DP,759,Anal,2014.0,2040.0,26.0
301,Kelly Oliveira in Assfucked 4on1 with DP,760,Anal,2174.0,2184.0,10.0
301,Kelly Oliveira in Assfucked 4on1 with DP,761,Anal,2236.0,2292.0,56.0
301,Kelly Oliveira in Assfucked 4on1 with DP,763,Cumshot,2918.0,2922.0,4.0
301,Kelly Oliveira in Assfucked 4on1 with DP,764,Cumshot,2934.0,2970.0,36.0
301,Kelly Oliveira in Assfucked 4on1 with DP,745,Grabbing Boobs,2956.0,2976.0,20.0
302,Kelly Oliveira in First DP for Brazilian Teen,765,BlowJob,160.0,190.0,30.0
302,Kelly Oliveira in First DP for Brazilian Teen,766,BlowJob,270.0,312.0,42.0
302,Kelly Oliveira in First DP for Brazilian Teen,772,Anal,610.0,668.0,58.0
302,Kelly Oliveira in First DP for Brazilian Teen,773,Anal,702.0,724.0,22.0
302,Kelly Oliveira in First DP for Brazilian Teen,774,Anal,796.0,810.0,14.0
302,Kelly Oliveira in First DP for Brazilian Teen,775,Anal,978.0,990.0,12.0
302,Kelly Oliveira in First DP for Brazilian Teen,778,Grabbing Boobs,1762.0,1788.0,26.0
302,Kelly Oliveira in First DP for Brazilian Teen,770,BlowJob,1874.0,1900.0,26.0
302,Kelly Oliveira in First DP for Brazilian Teen,779,Cumshot,1902.0,1912.0,10.0
303,Kelly Oliveira in Sexy Latina DAP 3on1,782,Anal,766.0,812.0,46.0
303,Kelly Oliveira in Sexy Latina DAP 3on1,786,BlowJob,1590.0,1620.0,30.0
303,Kelly Oliveira in Sexy Latina DAP 3on1,781,Grabbing Boobs,2224.0,2250.0,26.0
303,Kelly Oliveira in Sexy Latina DAP 3on1,784,Anal,2538.0,2596.0,58.0
303,Kelly Oliveira in Sexy Latina DAP 3on1,787,Cumshot,2638.0,2698.0,60.0
304,"Kelly Oliveira, Veronica Leal in 5on2 orgy with DP",791,Anal,322.0,332.0,10.0
304,"Kelly Oliveira, Veronica Leal in 5on2 orgy with DP",793,BlowJob,960.0,990.0,30.0
304,"Kelly Oliveira, Veronica Leal in 5on2 orgy with DP",796,Cumshot,3154.0,3196.0,42.0
304,"Kelly Oliveira, Veronica Leal in 5on2 orgy with DP",797,Cumshot,3234.0,3274.0,40.0
305,Keri Sable in Cum Filled Asshole Overload 2 Sc1,806,Grabbing Boobs,2128.0,2154.0,26.0
305,Keri Sable in Cum Filled Asshole Overload 2 Sc1,804,Anal,2324.0,2384.0,60.0
305,Keri Sable in Cum Filled Asshole Overload 2 Sc1,802,BlowJob,2412.0,2428.0,16.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,807,Cumshot,236.0,252.0,16.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,809,Cumshot,266.0,270.0,4.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,813,BlowJob,670.0,680.0,10.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,814,BlowJob,848.0,874.0,26.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,816,Anal,1000.0,1044.0,44.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,817,Anal,1124.0,1134.0,10.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,818,Pissing,1224.0,1238.0,14.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,812,Grabbing Boobs,1616.0,1668.0,52.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,815,BlowJob,1646.0,1700.0,54.0
307,Kim XXX in Tiny Whore Kim XXX Stuffed,819,Grabbing Boobs,144.0,164.0,20.0
307,Kim XXX in Tiny Whore Kim XXX Stuffed,820,BlowJob,184.0,238.0,54.0
307,Kim XXX in Tiny Whore Kim XXX Stuffed,821,BlowJob,316.0,332.0,16.0
307,Kim XXX in Tiny Whore Kim XXX Stuffed,822,BlowJob,672.0,686.0,14.0
307,Kim XXX in Tiny Whore Kim XXX Stuffed,823,BlowJob,1044.0,1096.0,52.0
308,Kira Thorn in Balls Deep 5on2,830,BlowJob,200.0,226.0,26.0
308,Kira Thorn in Balls Deep 5on2,831,BlowJob,508.0,526.0,18.0
308,Kira Thorn in Balls Deep 5on2,839,DP,790.0,794.0,4.0
308,Kira Thorn in Balls Deep 5on2,840,DP,958.0,976.0,18.0
308,Kira Thorn in Balls Deep 5on2,832,BlowJob,1032.0,1088.0,56.0
308,Kira Thorn in Balls Deep 5on2,841,Gangbang,1126.0,1142.0,16.0
308,Kira Thorn in Balls Deep 5on2,833,BlowJob,1128.0,1140.0,12.0
308,Kira Thorn in Balls Deep 5on2,834,BlowJob,1192.0,1206.0,14.0
308,Kira Thorn in Balls Deep 5on2,838,Anal,2728.0,2774.0,46.0
309,Kitana Montana in Birthday Threeway,849,Cumshot,116.0,126.0,10.0
309,Kitana Montana in Birthday Threeway,850,Cumshot,218.0,248.0,30.0
309,Kitana Montana in Birthday Threeway,843,BlowJob,260.0,272.0,12.0
309,Kitana Montana in Birthday Threeway,853,Anal,530.0,570.0,40.0
309,Kitana Montana in Birthday Threeway,854,Anal,916.0,930.0,14.0
309,Kitana Montana in Birthday Threeway,855,Anal,984.0,1004.0,20.0
309,Kitana Montana in Birthday Threeway,845,BlowJob,1032.0,1060.0,28.0
309,Kitana Montana in Birthday Threeway,856,Anal,1202.0,1226.0,24.0
309,Kitana Montana in Birthday Threeway,851,Cumshot,1672.0,1688.0,16.0
309,Kitana Montana in Birthday Threeway,848,BlowJob,1694.0,1706.0,12.0
310,Kitana Montana in Post,1836,BlowJob,176.0,198.0,22.0
310,Kitana Montana in Post,1842,Cumshot,290.0,294.0,4.0
310,Kitana Montana in Post,1838,BlowJob,630.0,640.0,10.0
310,Kitana Montana in Post,1852,DP,1036.0,1040.0,4.0
310,Kitana Montana in Post,1845,Anal,1058.0,1086.0,28.0
310,Kitana Montana in Post,1839,BlowJob,1212.0,1234.0,22.0
310,Kitana Montana in Post,1846,Anal,1422.0,1452.0,30.0
310,Kitana Montana in Post,1853,DP,1740.0,1772.0,32.0
310,Kitana Montana in Post,1848,Anal,1858.0,1904.0,46.0
310,Kitana Montana in Post,1854,DP,1886.0,1892.0,6.0
310,Kitana Montana in Post,1855,DP,1946.0,2006.0,60.0
310,Kitana Montana in Post,1849,Anal,1950.0,1960.0,10.0
310,Kitana Montana in Post,1850,Anal,2000.0,2028.0,28.0
310,Kitana Montana in Post,1851,Anal,2124.0,2162.0,38.0
310,Kitana Montana in Post,1841,BlowJob,2270.0,2274.0,4.0
310,Kitana Montana in Post,1843,Cumshot,2306.0,2350.0,44.0
311,Laura Fiorentino in 6on1 Swallow,861,BlowJob,290.0,320.0,30.0
311,Laura Fiorentino in 6on1 Swallow,870,Gangbang,704.0,748.0,44.0
311,Laura Fiorentino in 6on1 Swallow,874,Cumshot,968.0,972.0,4.0
311,Laura Fiorentino in 6on1 Swallow,880,Pissing,974.0,990.0,16.0
311,Laura Fiorentino in 6on1 Swallow,875,Cumshot,1042.0,1088.0,46.0
311,Laura Fiorentino in 6on1 Swallow,862,BlowJob,1084.0,1120.0,36.0
311,Laura Fiorentino in 6on1 Swallow,883,DP,2126.0,2130.0,4.0
311,Laura Fiorentino in 6on1 Swallow,873,Gangbang,2244.0,2270.0,26.0
311,Laura Fiorentino in 6on1 Swallow,881,Pissing,2322.0,2360.0,38.0
311,Laura Fiorentino in 6on1 Swallow,882,Pissing,2414.0,2442.0,28.0
311,Laura Fiorentino in 6on1 Swallow,876,Cumshot,2474.0,2496.0,22.0
311,Laura Fiorentino in 6on1 Swallow,864,Anal,2772.0,2794.0,22.0
311,Laura Fiorentino in 6on1 Swallow,866,Anal,2974.0,3004.0,30.0
311,Laura Fiorentino in 6on1 Swallow,867,Anal,3048.0,3106.0,58.0
311,Laura Fiorentino in 6on1 Swallow,868,Anal,3194.0,3254.0,60.0
311,Laura Fiorentino in 6on1 Swallow,869,Anal,3360.0,3406.0,46.0
311,Laura Fiorentino in 6on1 Swallow,858,Grabbing Boobs,3558.0,3582.0,24.0
311,Laura Fiorentino in 6on1 Swallow,877,Cumshot,3622.0,3632.0,10.0
311,Laura Fiorentino in 6on1 Swallow,879,Cumshot,3852.0,3856.0,4.0
312,Lela Star in Assparade 54 Sc2,884,Anal,94.0,112.0,18.0
312,Lela Star in Assparade 54 Sc2,885,BlowJob,166.0,178.0,12.0
312,Lela Star in Assparade 54 Sc2,887,Titjob,182.0,216.0,34.0
312,Lela Star in Assparade 54 Sc2,886,BlowJob,368.0,404.0,36.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,890,BlowJob,88.0,148.0,60.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,894,Grabbing Boobs,156.0,180.0,24.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,891,BlowJob,238.0,256.0,18.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,895,DP,980.0,992.0,12.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,892,BlowJob,1030.0,1040.0,10.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,896,Anal,1102.0,1144.0,42.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,897,Anal,1356.0,1396.0,40.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,898,Anal,1438.0,1450.0,12.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,899,Cumshot,2064.0,2094.0,30.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,893,BlowJob,2112.0,2166.0,54.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,900,Cumshot,2136.0,2182.0,46.0
314,Lolly Ink in True Gonzo Sc5,907,Anal,994.0,1014.0,20.0
314,Lolly Ink in True Gonzo Sc5,903,Grabbing Boobs,1180.0,1200.0,20.0
314,Lolly Ink in True Gonzo Sc5,908,Cumshot,1486.0,1504.0,18.0
315,Luna Star in Double Stuffed,909,Grabbing Boobs,18.0,72.0,54.0
315,Luna Star in Double Stuffed,910,BlowJob,156.0,186.0,30.0
315,Luna Star in Double Stuffed,913,BlowJob,986.0,1010.0,24.0
315,Luna Star in Double Stuffed,917,Anal,1338.0,1356.0,18.0
315,Luna Star in Double Stuffed,914,BlowJob,1512.0,1534.0,22.0
315,Luna Star in Double Stuffed,919,Anal,1558.0,1574.0,16.0
315,Luna Star in Double Stuffed,915,BlowJob,1632.0,1684.0,52.0
316,Luna Star in Why She's A Pornstar,920,Grabbing Boobs,212.0,234.0,22.0
316,Luna Star in Why She's A Pornstar,921,Grabbing Boobs,314.0,340.0,26.0
316,Luna Star in Why She's A Pornstar,922,BlowJob,450.0,462.0,12.0
316,Luna Star in Why She's A Pornstar,923,BlowJob,502.0,536.0,34.0
316,Luna Star in Why She's A Pornstar,924,BlowJob,778.0,798.0,20.0
316,Luna Star in Why She's A Pornstar,925,BlowJob,1172.0,1186.0,14.0
316,Luna Star in Why She's A Pornstar,926,BlowJob,1686.0,1700.0,14.0
316,Luna Star in Why She's A Pornstar,927,BlowJob,1956.0,1976.0,20.0
317,Marilyn Johnson in Airtight Diva Sc1,930,BlowJob,578.0,590.0,12.0
317,Marilyn Johnson in Airtight Diva Sc1,935,BlowJob,616.0,632.0,16.0
317,Marilyn Johnson in Airtight Diva Sc1,931,BlowJob,668.0,682.0,14.0
317,Marilyn Johnson in Airtight Diva Sc1,932,BlowJob,878.0,890.0,12.0
317,Marilyn Johnson in Airtight Diva Sc1,933,BlowJob,1446.0,1456.0,10.0
317,Marilyn Johnson in Airtight Diva Sc1,937,Cumshot,1470.0,1474.0,4.0
317,Marilyn Johnson in Airtight Diva Sc1,939,Cumshot,1578.0,1602.0,24.0
318,"Mariska, Tekohas in Cum Cum Gangbang Orgy",940,BlowJob,172.0,218.0,46.0
318,"Mariska, Tekohas in Cum Cum Gangbang Orgy",946,Cumshot,502.0,510.0,8.0
318,"Mariska, Tekohas in Cum Cum Gangbang Orgy",942,BlowJob,592.0,610.0,18.0
318,"Mariska, Tekohas in Cum Cum Gangbang Orgy",943,BlowJob,856.0,866.0,10.0
318,"Mariska, Tekohas in Cum Cum Gangbang Orgy",944,BlowJob,904.0,960.0,56.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,950,BlowJob,164.0,182.0,18.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,953,BlowJob,2002.0,2038.0,36.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,955,Cumshot,2194.0,2236.0,42.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,949,Grabbing Boobs,2216.0,2234.0,18.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,956,Cumshot,2588.0,2598.0,10.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,958,Cumshot,2642.0,2648.0,6.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,959,Cumshot,2704.0,2730.0,26.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,960,Cumshot,2804.0,2844.0,40.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,961,Cumshot,2876.0,2882.0,6.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,962,BlowJob,208.0,240.0,32.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,963,BlowJob,470.0,480.0,10.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,973,Anal,566.0,608.0,42.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,975,Anal,884.0,898.0,14.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,976,Anal,938.0,964.0,26.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,977,Anal,1006.0,1038.0,32.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,964,BlowJob,1064.0,1078.0,14.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,978,Anal,1314.0,1336.0,22.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,965,BlowJob,1392.0,1414.0,22.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,979,Anal,1566.0,1602.0,36.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,980,Anal,1648.0,1678.0,30.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,966,BlowJob,1810.0,1828.0,18.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,981,Anal,1860.0,1902.0,42.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,967,BlowJob,1990.0,2010.0,20.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,970,BlowJob,2090.0,2106.0,16.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,971,BlowJob,2168.0,2190.0,22.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,972,BlowJob,2262.0,2272.0,10.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,968,BlowJob,2898.0,2912.0,14.0
321,Megan Rain in 10 Cock Blowbang!,986,Gangbang,548.0,578.0,30.0
321,Megan Rain in 10 Cock Blowbang!,987,Cumshot,1490.0,1506.0,16.0
322,Melissa Hot in Fucked by 4 Big Cocks,991,BlowJob,76.0,86.0,10.0
322,Melissa Hot in Fucked by 4 Big Cocks,998,Gangbang,94.0,126.0,32.0
322,Melissa Hot in Fucked by 4 Big Cocks,989,Anal,736.0,756.0,20.0
322,Melissa Hot in Fucked by 4 Big Cocks,992,BlowJob,954.0,1000.0,46.0
322,Melissa Hot in Fucked by 4 Big Cocks,993,BlowJob,1032.0,1042.0,10.0
322,Melissa Hot in Fucked by 4 Big Cocks,999,Gangbang,1726.0,1746.0,20.0
322,Melissa Hot in Fucked by 4 Big Cocks,1000,Gangbang,2142.0,2168.0,26.0
322,Melissa Hot in Fucked by 4 Big Cocks,1001,Gangbang,2204.0,2222.0,18.0
322,Melissa Hot in Fucked by 4 Big Cocks,1002,Gangbang,2254.0,2274.0,20.0
322,Melissa Hot in Fucked by 4 Big Cocks,1003,Gangbang,2350.0,2364.0,14.0
322,Melissa Hot in Fucked by 4 Big Cocks,997,BlowJob,2414.0,2432.0,18.0
322,Melissa Hot in Fucked by 4 Big Cocks,1004,Cumshot,2436.0,2448.0,12.0
322,Melissa Hot in Fucked by 4 Big Cocks,1005,Cumshot,2526.0,2538.0,12.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1015,Anal,782.0,792.0,10.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1014,BlowJob,1052.0,1090.0,38.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1016,Anal,1154.0,1176.0,22.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1006,BlowJob,1580.0,1596.0,16.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1007,BlowJob,1684.0,1696.0,12.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1008,BlowJob,1944.0,1954.0,10.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1009,BlowJob,2166.0,2190.0,24.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1010,BlowJob,2484.0,2514.0,30.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1019,Anal,2540.0,2568.0,28.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1011,BlowJob,2630.0,2662.0,32.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1012,BlowJob,2982.0,2998.0,16.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1013,BlowJob,3040.0,3072.0,32.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1021,Cumshot,3064.0,3108.0,44.0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1028,Anal,350.0,380.0,30.0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1024,BlowJob,862.0,904.0,42.0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1029,Grabbing Boobs,1058.0,1084.0,26.0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1030,Grabbing Boobs,1256.0,1284.0,28.0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1025,BlowJob,1374.0,1410.0,36.0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1027,BlowJob,1438.0,1446.0,8.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1031,Grabbing Boobs,182.0,218.0,36.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1041,Cumshot,684.0,698.0,14.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1033,BlowJob,962.0,976.0,14.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1034,BlowJob,1360.0,1392.0,32.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1036,BlowJob,2384.0,2412.0,28.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1037,BlowJob,2464.0,2476.0,12.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1038,BlowJob,2512.0,2558.0,46.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1039,BlowJob,2614.0,2632.0,18.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1042,Cumshot,2882.0,2892.0,10.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1044,Cumshot,2918.0,2960.0,42.0
326,Mia Trejsi in 100% Hell,1050,Gangbang,94.0,126.0,32.0
326,Mia Trejsi in 100% Hell,1059,BlowJob,464.0,480.0,16.0
326,Mia Trejsi in 100% Hell,1051,Gangbang,674.0,688.0,14.0
326,Mia Trejsi in 100% Hell,1061,BlowJob,1318.0,1340.0,22.0
326,Mia Trejsi in 100% Hell,1047,Grabbing Boobs,1324.0,1370.0,46.0
326,Mia Trejsi in 100% Hell,1052,Gangbang,1332.0,1372.0,40.0
326,Mia Trejsi in 100% Hell,1053,Gangbang,1420.0,1434.0,14.0
326,Mia Trejsi in 100% Hell,1054,Gangbang,1626.0,1652.0,26.0
326,Mia Trejsi in 100% Hell,1062,BlowJob,1880.0,1892.0,12.0
326,Mia Trejsi in 100% Hell,1063,BlowJob,2308.0,2326.0,18.0
326,Mia Trejsi in 100% Hell,1055,Gangbang,2496.0,2510.0,14.0
326,Mia Trejsi in 100% Hell,1056,Gangbang,2762.0,2788.0,26.0
326,Mia Trejsi in 100% Hell,1057,Gangbang,2926.0,2938.0,12.0
326,Mia Trejsi in 100% Hell,1058,Gangbang,2972.0,2992.0,20.0
326,Mia Trejsi in 100% Hell,1048,Grabbing Boobs,3280.0,3338.0,58.0
326,Mia Trejsi in 100% Hell,1064,BlowJob,3318.0,3324.0,6.0
326,Mia Trejsi in 100% Hell,1049,Grabbing Boobs,3384.0,3426.0,42.0
326,Mia Trejsi in 100% Hell,1066,Cumshot,3416.0,3424.0,8.0
327,"Mia Trejsi, Ria Sunn in Angels Of Hardcore 3 Sc1",1067,Grabbing Boobs,0.0,20.0,20.0
327,"Mia Trejsi, Ria Sunn in Angels Of Hardcore 3 Sc1",1068,Gangbang,60.0,94.0,34.0
327,"Mia Trejsi, Ria Sunn in Angels Of Hardcore 3 Sc1",1070,Anal,192.0,230.0,38.0
327,"Mia Trejsi, Ria Sunn in Angels Of Hardcore 3 Sc1",1069,Gangbang,314.0,362.0,48.0
328,Mih Ninfetinha in 4on1 with DP,1074,BlowJob,946.0,988.0,42.0
328,Mih Ninfetinha in 4on1 with DP,1079,BlowJob,1024.0,1054.0,30.0
328,Mih Ninfetinha in 4on1 with DP,1080,BlowJob,1542.0,1550.0,8.0
328,Mih Ninfetinha in 4on1 with DP,1076,BlowJob,1848.0,1880.0,32.0
328,Mih Ninfetinha in 4on1 with DP,1084,Gangbang,1852.0,1912.0,60.0
328,Mih Ninfetinha in 4on1 with DP,1081,BlowJob,2004.0,2008.0,4.0
328,Mih Ninfetinha in 4on1 with DP,1077,BlowJob,2288.0,2332.0,44.0
328,Mih Ninfetinha in 4on1 with DP,1085,Cumshot,2516.0,2524.0,8.0
329,Miss Teela in First Time 10 Gangbang,1086,Gangbang,124.0,168.0,44.0
329,Miss Teela in First Time 10 Gangbang,1092,BlowJob,884.0,914.0,30.0
329,Miss Teela in First Time 10 Gangbang,1093,BlowJob,954.0,994.0,40.0
329,Miss Teela in First Time 10 Gangbang,1089,Gangbang,1180.0,1208.0,28.0
329,Miss Teela in First Time 10 Gangbang,1094,BlowJob,1214.0,1266.0,52.0
329,Miss Teela in First Time 10 Gangbang,1095,BlowJob,1314.0,1342.0,28.0
329,Miss Teela in First Time 10 Gangbang,1096,BlowJob,1422.0,1448.0,26.0
329,Miss Teela in First Time 10 Gangbang,1098,Cumshot,1608.0,1636.0,28.0
329,Miss Teela in First Time 10 Gangbang,1099,Cumshot,1658.0,1702.0,44.0
330,Monika Fox in DP Fantasies 11 Sc3,1101,Grabbing Boobs,172.0,204.0,32.0
330,Monika Fox in DP Fantasies 11 Sc3,1102,Grabbing Boobs,430.0,458.0,28.0
330,Monika Fox in DP Fantasies 11 Sc3,1103,Grabbing Boobs,952.0,978.0,26.0
330,Monika Fox in DP Fantasies 11 Sc3,1108,Cumshot,1428.0,1438.0,10.0
330,Monika Fox in DP Fantasies 11 Sc3,1110,Anal,2074.0,2104.0,30.0
330,Monika Fox in DP Fantasies 11 Sc3,1105,BlowJob,2330.0,2348.0,18.0
330,Monika Fox in DP Fantasies 11 Sc3,1112,Anal,2474.0,2490.0,16.0
330,Monika Fox in DP Fantasies 11 Sc3,1109,Cumshot,2520.0,2538.0,18.0
331,Natasha Teen in Pussy DAPTAP,1115,BlowJob,606.0,620.0,14.0
331,Natasha Teen in Pussy DAPTAP,1116,BlowJob,1064.0,1112.0,48.0
331,Natasha Teen in Pussy DAPTAP,1119,Gangbang,1176.0,1200.0,24.0
331,Natasha Teen in Pussy DAPTAP,1117,BlowJob,1178.0,1210.0,32.0
331,Natasha Teen in Pussy DAPTAP,1120,DP,1366.0,1396.0,30.0
331,Natasha Teen in Pussy DAPTAP,1118,BlowJob,2744.0,2772.0,28.0
332,Nia Nacci in Cum Bang 15 Sc3,1132,Cumshot,1060.0,1082.0,22.0
332,Nia Nacci in Cum Bang 15 Sc3,1123,BlowJob,1106.0,1152.0,46.0
332,Nia Nacci in Cum Bang 15 Sc3,1124,BlowJob,1234.0,1284.0,50.0
332,Nia Nacci in Cum Bang 15 Sc3,1126,BlowJob,1532.0,1566.0,34.0
332,Nia Nacci in Cum Bang 15 Sc3,1127,BlowJob,1598.0,1614.0,16.0
332,Nia Nacci in Cum Bang 15 Sc3,1128,BlowJob,1666.0,1696.0,30.0
332,Nia Nacci in Cum Bang 15 Sc3,1129,BlowJob,1742.0,1778.0,36.0
332,Nia Nacci in Cum Bang 15 Sc3,1130,BlowJob,1920.0,1930.0,10.0
333,Nia Nacci in We Fuck Black Girls 14 Sc5,1137,Grabbing Boobs,640.0,680.0,40.0
333,Nia Nacci in We Fuck Black Girls 14 Sc5,1138,Anal,1474.0,1532.0,58.0
333,Nia Nacci in We Fuck Black Girls 14 Sc5,1139,Anal,1606.0,1632.0,26.0
333,Nia Nacci in We Fuck Black Girls 14 Sc5,1140,Cumshot,1924.0,1952.0,28.0
334,Nia Nacci in White Out 9 Sc2,1142,BlowJob,84.0,134.0,50.0
334,Nia Nacci in White Out 9 Sc2,1155,Grabbing Boobs,1052.0,1104.0,52.0
334,Nia Nacci in White Out 9 Sc2,1144,BlowJob,1302.0,1354.0,52.0
334,Nia Nacci in White Out 9 Sc2,1145,BlowJob,1386.0,1402.0,16.0
334,Nia Nacci in White Out 9 Sc2,1161,Anal,1504.0,1530.0,26.0
334,Nia Nacci in White Out 9 Sc2,1156,Grabbing Boobs,1940.0,1978.0,38.0
334,Nia Nacci in White Out 9 Sc2,1152,Gangbang,1954.0,1976.0,22.0
334,Nia Nacci in White Out 9 Sc2,1158,Grabbing Boobs,2526.0,2558.0,32.0
334,Nia Nacci in White Out 9 Sc2,1147,BlowJob,2690.0,2718.0,28.0
334,Nia Nacci in White Out 9 Sc2,1148,BlowJob,2922.0,2938.0,16.0
334,Nia Nacci in White Out 9 Sc2,1159,Grabbing Boobs,3076.0,3118.0,42.0
334,Nia Nacci in White Out 9 Sc2,1160,Grabbing Boobs,3200.0,3224.0,24.0
334,Nia Nacci in White Out 9 Sc2,1149,BlowJob,3330.0,3358.0,28.0
334,Nia Nacci in White Out 9 Sc2,1162,Anal,3578.0,3594.0,16.0
334,Nia Nacci in White Out 9 Sc2,1153,Gangbang,3582.0,3620.0,38.0
334,Nia Nacci in White Out 9 Sc2,1163,Anal,3662.0,3680.0,18.0
334,Nia Nacci in White Out 9 Sc2,1151,BlowJob,3784.0,3792.0,8.0
334,Nia Nacci in White Out 9 Sc2,1164,Cumshot,3850.0,3860.0,10.0
334,Nia Nacci in White Out 9 Sc2,1165,Cumshot,3878.0,3890.0,12.0
334,Nia Nacci in White Out 9 Sc2,1166,Cumshot,3944.0,3970.0,26.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1167,BlowJob,264.0,314.0,50.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1177,Cumshot,334.0,370.0,36.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1168,BlowJob,468.0,488.0,20.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1175,BlowJob,686.0,696.0,10.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1169,BlowJob,742.0,764.0,22.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1170,BlowJob,810.0,846.0,36.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1180,Cumshot,876.0,892.0,16.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1171,BlowJob,934.0,988.0,54.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1181,Cumshot,978.0,996.0,18.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1187,Cumshot,1382.0,1388.0,6.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1173,BlowJob,1434.0,1444.0,10.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1182,Cumshot,1436.0,1446.0,10.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1174,BlowJob,2022.0,2062.0,40.0
336,Nina Elle in Big Wet Milf Asses Sc2,1194,Anal,612.0,656.0,44.0
336,Nina Elle in Big Wet Milf Asses Sc2,1191,BlowJob,676.0,696.0,20.0
336,Nina Elle in Big Wet Milf Asses Sc2,1192,BlowJob,1496.0,1526.0,30.0
336,Nina Elle in Big Wet Milf Asses Sc2,1196,Cumshot,1842.0,1846.0,4.0
336,Nina Elle in Big Wet Milf Asses Sc2,1197,Cumshot,1862.0,1878.0,16.0
336,Nina Elle in Big Wet Milf Asses Sc2,1193,Grabbing Boobs,1870.0,1890.0,20.0
337,Nina Elle in Gang Bang Addiction Sc4,1199,BlowJob,762.0,790.0,28.0
337,Nina Elle in Gang Bang Addiction Sc4,1200,BlowJob,888.0,908.0,20.0
337,Nina Elle in Gang Bang Addiction Sc4,1206,Anal,908.0,938.0,30.0
337,Nina Elle in Gang Bang Addiction Sc4,1209,Gangbang,1070.0,1100.0,30.0
337,Nina Elle in Gang Bang Addiction Sc4,1201,BlowJob,1356.0,1370.0,14.0
337,Nina Elle in Gang Bang Addiction Sc4,1203,BlowJob,2248.0,2266.0,18.0
337,Nina Elle in Gang Bang Addiction Sc4,1208,Anal,2266.0,2284.0,18.0
337,Nina Elle in Gang Bang Addiction Sc4,1204,BlowJob,2300.0,2322.0,22.0
337,Nina Elle in Gang Bang Addiction Sc4,1205,BlowJob,2844.0,2866.0,22.0
338,Nina Elle in MILF Cumsluts Sc3,1212,Anal,494.0,510.0,16.0
338,Nina Elle in MILF Cumsluts Sc3,1213,Anal,730.0,752.0,22.0
338,Nina Elle in MILF Cumsluts Sc3,1214,Anal,990.0,1002.0,12.0
338,Nina Elle in MILF Cumsluts Sc3,1211,Grabbing Boobs,1058.0,1080.0,22.0
338,Nina Elle in MILF Cumsluts Sc3,1222,Titjob,1076.0,1100.0,24.0
338,Nina Elle in MILF Cumsluts Sc3,1223,Cumshot,1090.0,1110.0,20.0
338,Nina Elle in MILF Cumsluts Sc3,1226,Cumshot,1146.0,1154.0,8.0
338,Nina Elle in MILF Cumsluts Sc3,1215,Anal,1286.0,1312.0,26.0
338,Nina Elle in MILF Cumsluts Sc3,1219,Anal,1988.0,2022.0,34.0
338,Nina Elle in MILF Cumsluts Sc3,1224,Cumshot,2232.0,2240.0,8.0
338,Nina Elle in MILF Cumsluts Sc3,1228,Cumshot,2262.0,2314.0,52.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1232,BlowJob,170.0,184.0,14.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1229,Grabbing Boobs,254.0,286.0,32.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1235,BlowJob,644.0,666.0,22.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1241,Gangbang,662.0,682.0,20.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1230,Grabbing Boobs,1016.0,1038.0,22.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1239,Anal,1070.0,1094.0,24.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1240,Anal,1238.0,1266.0,28.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1236,BlowJob,1530.0,1558.0,28.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1237,BlowJob,1602.0,1624.0,22.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1231,Grabbing Boobs,1610.0,1644.0,34.0
340,Phoenix Marie in Ass Worship 13 Sc4,1244,Anal,552.0,604.0,52.0
340,Phoenix Marie in Ass Worship 13 Sc4,1254,BlowJob,1034.0,1074.0,40.0
340,Phoenix Marie in Ass Worship 13 Sc4,1245,Anal,1036.0,1054.0,18.0
340,Phoenix Marie in Ass Worship 13 Sc4,1255,BlowJob,1142.0,1158.0,16.0
340,Phoenix Marie in Ass Worship 13 Sc4,1246,Anal,1332.0,1380.0,48.0
340,Phoenix Marie in Ass Worship 13 Sc4,1247,Anal,1466.0,1514.0,48.0
340,Phoenix Marie in Ass Worship 13 Sc4,1248,Anal,1556.0,1594.0,38.0
340,Phoenix Marie in Ass Worship 13 Sc4,1258,BlowJob,1674.0,1684.0,10.0
340,Phoenix Marie in Ass Worship 13 Sc4,1259,BlowJob,1748.0,1764.0,16.0
340,Phoenix Marie in Ass Worship 13 Sc4,1250,Anal,1894.0,1914.0,20.0
340,Phoenix Marie in Ass Worship 13 Sc4,1251,Anal,1952.0,1990.0,38.0
340,Phoenix Marie in Ass Worship 13 Sc4,1261,BlowJob,2038.0,2080.0,42.0
341,Rachele Richey in Gangbang Audition,1262,Grabbing Boobs,162.0,192.0,30.0
341,Rachele Richey in Gangbang Audition,1263,BlowJob,666.0,690.0,24.0
341,Rachele Richey in Gangbang Audition,1266,BlowJob,700.0,760.0,60.0
341,Rachele Richey in Gangbang Audition,1265,BlowJob,798.0,848.0,50.0
341,Rachele Richey in Gangbang Audition,1268,BlowJob,904.0,918.0,14.0
341,Rachele Richey in Gangbang Audition,1274,Anal,1064.0,1082.0,18.0
341,Rachele Richey in Gangbang Audition,1269,BlowJob,1110.0,1128.0,18.0
341,Rachele Richey in Gangbang Audition,1270,BlowJob,1290.0,1322.0,32.0
341,Rachele Richey in Gangbang Audition,1276,Anal,1428.0,1462.0,34.0
341,Rachele Richey in Gangbang Audition,1277,Anal,1508.0,1540.0,32.0
341,Rachele Richey in Gangbang Audition,1278,Anal,1584.0,1596.0,12.0
341,Rachele Richey in Gangbang Audition,1279,Anal,1654.0,1708.0,54.0
341,Rachele Richey in Gangbang Audition,1271,BlowJob,1700.0,1722.0,22.0
341,Rachele Richey in Gangbang Audition,1272,BlowJob,2534.0,2544.0,10.0
341,Rachele Richey in Gangbang Audition,1273,BlowJob,2584.0,2604.0,20.0
341,Rachele Richey in Gangbang Audition,1282,Cumshot,2666.0,2676.0,10.0
342,Rose Lynn in Airtight Diva Sc3,1283,Grabbing Boobs,212.0,268.0,56.0
342,Rose Lynn in Airtight Diva Sc3,1284,BlowJob,362.0,390.0,28.0
342,Rose Lynn in Airtight Diva Sc3,1285,BlowJob,428.0,464.0,36.0
342,Rose Lynn in Airtight Diva Sc3,1286,BlowJob,948.0,988.0,40.0
343,Sadie Summers in Gangbang Sluts Sc2,1309,Cumshot,542.0,586.0,44.0
343,Sadie Summers in Gangbang Sluts Sc2,1295,Gangbang,1166.0,1206.0,40.0
343,Sadie Summers in Gangbang Sluts Sc2,1296,Gangbang,1250.0,1292.0,42.0
343,Sadie Summers in Gangbang Sluts Sc2,1290,Grabbing Boobs,1608.0,1628.0,20.0
343,Sadie Summers in Gangbang Sluts Sc2,1297,Gangbang,1610.0,1644.0,34.0
343,Sadie Summers in Gangbang Sluts Sc2,1307,BlowJob,1688.0,1702.0,14.0
343,Sadie Summers in Gangbang Sluts Sc2,1298,Gangbang,1692.0,1712.0,20.0
343,Sadie Summers in Gangbang Sluts Sc2,1299,Gangbang,1780.0,1800.0,20.0
343,Sadie Summers in Gangbang Sluts Sc2,1300,Gangbang,1848.0,1900.0,52.0
343,Sadie Summers in Gangbang Sluts Sc2,1306,BlowJob,2132.0,2180.0,48.0
343,Sadie Summers in Gangbang Sluts Sc2,1302,Gangbang,2318.0,2338.0,20.0
343,Sadie Summers in Gangbang Sluts Sc2,1303,Gangbang,2378.0,2392.0,14.0
343,Sadie Summers in Gangbang Sluts Sc2,1292,Grabbing Boobs,2560.0,2602.0,42.0
343,Sadie Summers in Gangbang Sluts Sc2,1311,Cumshot,2782.0,2786.0,4.0
343,Sadie Summers in Gangbang Sluts Sc2,1312,Cumshot,2838.0,2842.0,4.0
343,Sadie Summers in Gangbang Sluts Sc2,1316,Cumshot,3022.0,3056.0,34.0
343,Sadie Summers in Gangbang Sluts Sc2,1317,Cumshot,3088.0,3098.0,10.0
344,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc2,1319,BlowJob,220.0,230.0,10.0
344,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc2,1320,BlowJob,302.0,316.0,14.0
344,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc2,1321,BlowJob,370.0,398.0,28.0
344,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc2,1323,BlowJob,636.0,662.0,26.0
344,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc2,1324,Grabbing Boobs,642.0,676.0,34.0
345,"Sai Tai Tiger, Daria Glower in Die Haremsw√§chterin des √ñl Scheichs Sc5",1325,69,46.0,64.0,18.0
345,"Sai Tai Tiger, Daria Glower in Die Haremsw√§chterin des √ñl Scheichs Sc5",1326,69,292.0,318.0,26.0
346,"Sai Tai Tiger, Daria Glower, Valerie Hilton in Die Haremsw√§chterin des √ñl Scheichs Sc1",1332,Cumshot,1534.0,1568.0,34.0
347,Sandra Parker in 1st at GB Junkies,1335,BlowJob,402.0,414.0,12.0
347,Sandra Parker in 1st at GB Junkies,1336,BlowJob,656.0,668.0,12.0
347,Sandra Parker in 1st at GB Junkies,1342,Anal,674.0,722.0,48.0
347,Sandra Parker in 1st at GB Junkies,1344,Anal,960.0,1006.0,46.0
347,Sandra Parker in 1st at GB Junkies,1338,BlowJob,1040.0,1056.0,16.0
347,Sandra Parker in 1st at GB Junkies,1339,BlowJob,1316.0,1330.0,14.0
347,Sandra Parker in 1st at GB Junkies,1346,Anal,1374.0,1430.0,56.0
347,Sandra Parker in 1st at GB Junkies,1340,BlowJob,1518.0,1528.0,10.0
347,Sandra Parker in 1st at GB Junkies,1347,Anal,1536.0,1554.0,18.0
347,Sandra Parker in 1st at GB Junkies,1348,Cumshot,1650.0,1658.0,8.0
348,Sandra Parker in Anal Driller 9 Sc3,1351,BlowJob,830.0,852.0,22.0
348,Sandra Parker in Anal Driller 9 Sc3,1352,BlowJob,942.0,998.0,56.0
348,Sandra Parker in Anal Driller 9 Sc3,1353,BlowJob,1040.0,1070.0,30.0
348,Sandra Parker in Anal Driller 9 Sc3,1354,BlowJob,1140.0,1198.0,58.0
348,Sandra Parker in Anal Driller 9 Sc3,1355,BlowJob,1320.0,1348.0,28.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1363,Anal,276.0,298.0,22.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1372,BlowJob,598.0,610.0,12.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1364,Anal,650.0,694.0,44.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1375,BlowJob,664.0,674.0,10.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1365,Anal,812.0,838.0,26.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1373,BlowJob,850.0,876.0,26.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1366,Anal,976.0,998.0,22.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1367,Anal,1034.0,1068.0,34.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1368,Anal,1192.0,1236.0,44.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1369,Anal,1306.0,1330.0,24.0
351,Sandra Parker in Double Stuffed 8 Sc1,1377,BlowJob,632.0,662.0,30.0
351,Sandra Parker in Double Stuffed 8 Sc1,1383,Anal,692.0,702.0,10.0
351,Sandra Parker in Double Stuffed 8 Sc1,1384,Anal,764.0,796.0,32.0
351,Sandra Parker in Double Stuffed 8 Sc1,1378,BlowJob,888.0,916.0,28.0
351,Sandra Parker in Double Stuffed 8 Sc1,1385,Anal,958.0,980.0,22.0
351,Sandra Parker in Double Stuffed 8 Sc1,1379,BlowJob,974.0,996.0,22.0
351,Sandra Parker in Double Stuffed 8 Sc1,1380,BlowJob,1240.0,1290.0,50.0
351,Sandra Parker in Double Stuffed 8 Sc1,1387,Anal,1314.0,1346.0,32.0
351,Sandra Parker in Double Stuffed 8 Sc1,1381,BlowJob,1600.0,1658.0,58.0
351,Sandra Parker in Double Stuffed 8 Sc1,1389,Cumshot,1608.0,1612.0,4.0
351,Sandra Parker in Double Stuffed 8 Sc1,1390,Cumshot,1634.0,1686.0,52.0
352,Sara Retali in BBC Piss Gangbang,1391,Gangbang,40.0,92.0,52.0
352,Sara Retali in BBC Piss Gangbang,1398,BlowJob,110.0,136.0,26.0
352,Sara Retali in BBC Piss Gangbang,1392,Gangbang,192.0,204.0,12.0
352,Sara Retali in BBC Piss Gangbang,1399,BlowJob,194.0,214.0,20.0
352,Sara Retali in BBC Piss Gangbang,1400,BlowJob,360.0,406.0,46.0
352,Sara Retali in BBC Piss Gangbang,1393,Gangbang,610.0,656.0,46.0
352,Sara Retali in BBC Piss Gangbang,1409,Anal,748.0,792.0,44.0
352,Sara Retali in BBC Piss Gangbang,1394,Gangbang,792.0,818.0,26.0
352,Sara Retali in BBC Piss Gangbang,1402,BlowJob,830.0,868.0,38.0
352,Sara Retali in BBC Piss Gangbang,1404,BlowJob,1192.0,1216.0,24.0
352,Sara Retali in BBC Piss Gangbang,1395,Gangbang,1222.0,1258.0,36.0
352,Sara Retali in BBC Piss Gangbang,1396,Gangbang,1290.0,1318.0,28.0
352,Sara Retali in BBC Piss Gangbang,1405,BlowJob,1488.0,1500.0,12.0
352,Sara Retali in BBC Piss Gangbang,1410,Anal,1684.0,1718.0,34.0
352,Sara Retali in BBC Piss Gangbang,1397,Gangbang,1686.0,1732.0,46.0
352,Sara Retali in BBC Piss Gangbang,1406,BlowJob,1858.0,1910.0,52.0
352,Sara Retali in BBC Piss Gangbang,1411,Cumshot,2130.0,2142.0,12.0
352,Sara Retali in BBC Piss Gangbang,1413,Cumshot,2152.0,2180.0,28.0
352,Sara Retali in BBC Piss Gangbang,1412,Cumshot,2190.0,2196.0,6.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1414,BlowJob,122.0,158.0,36.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1418,Gangbang,136.0,174.0,38.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1424,Grabbing Boobs,300.0,326.0,26.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1419,Gangbang,420.0,432.0,12.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1420,Gangbang,804.0,838.0,34.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1425,Anal,1124.0,1178.0,54.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1421,Gangbang,1222.0,1248.0,26.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1426,Anal,1262.0,1302.0,40.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1416,BlowJob,1308.0,1360.0,52.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1422,Gangbang,1336.0,1350.0,14.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1427,Anal,1340.0,1366.0,26.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1423,Gangbang,1548.0,1598.0,50.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1428,Anal,1566.0,1582.0,16.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1429,Cumshot,1754.0,1766.0,12.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1430,Gangbang,48.0,62.0,14.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1431,Gangbang,206.0,226.0,20.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1436,BlowJob,284.0,308.0,24.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1433,Gangbang,400.0,414.0,14.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1437,BlowJob,402.0,426.0,24.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1439,Anal,416.0,428.0,12.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1434,Gangbang,474.0,492.0,18.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1440,Anal,626.0,652.0,26.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1441,Anal,710.0,736.0,26.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1438,BlowJob,964.0,986.0,22.0
355,"Sara Retali, Sapphire Astrea in Spa Day Gone Wild",1442,Grabbing Boobs,252.0,280.0,28.0
355,"Sara Retali, Sapphire Astrea in Spa Day Gone Wild",1443,BlowJob,778.0,802.0,24.0
356,Sarai Minx in Big Tit Slut Milks Cock,1445,Grabbing Boobs,278.0,334.0,56.0
356,Sarai Minx in Big Tit Slut Milks Cock,1447,BlowJob,508.0,564.0,56.0
356,Sarai Minx in Big Tit Slut Milks Cock,1449,BlowJob,584.0,626.0,42.0
356,Sarai Minx in Big Tit Slut Milks Cock,1446,Grabbing Boobs,652.0,684.0,32.0
356,Sarai Minx in Big Tit Slut Milks Cock,1451,Titjob,1472.0,1496.0,24.0
356,Sarai Minx in Big Tit Slut Milks Cock,1452,Cumshot,1476.0,1490.0,14.0
356,Sarai Minx in Big Tit Slut Milks Cock,1453,Cumshot,1504.0,1546.0,42.0
357,"Sheila Ortega, Sara Retali, Anny Star in 3 Latinas by the Pool",1460,Titjob,904.0,916.0,12.0
357,"Sheila Ortega, Sara Retali, Anny Star in 3 Latinas by the Pool",1461,Anal,1180.0,1236.0,56.0
357,"Sheila Ortega, Sara Retali, Anny Star in 3 Latinas by the Pool",1458,BlowJob,2230.0,2234.0,4.0
357,"Sheila Ortega, Sara Retali, Anny Star in 3 Latinas by the Pool",1459,BlowJob,2322.0,2330.0,8.0
358,Shyla Stylez in Anal Integrity Sc2,1470,Anal,1116.0,1158.0,42.0
358,Shyla Stylez in Anal Integrity Sc2,1466,BlowJob,1230.0,1266.0,36.0
358,Shyla Stylez in Anal Integrity Sc2,1467,Grabbing Boobs,1272.0,1294.0,22.0
358,Shyla Stylez in Anal Integrity Sc2,1471,Titjob,1284.0,1300.0,16.0
358,Shyla Stylez in Anal Integrity Sc2,1464,BlowJob,1470.0,1480.0,10.0
358,Shyla Stylez in Anal Integrity Sc2,1468,Grabbing Boobs,1502.0,1548.0,46.0
358,Shyla Stylez in Anal Integrity Sc2,1465,BlowJob,2014.0,2032.0,18.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1473,BlowJob,190.0,212.0,22.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1474,BlowJob,246.0,302.0,56.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1475,BlowJob,480.0,492.0,12.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1476,BlowJob,540.0,578.0,38.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1477,BlowJob,892.0,922.0,30.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1478,BlowJob,1258.0,1286.0,28.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1480,BlowJob,1788.0,1804.0,16.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1487,Anal,1840.0,1854.0,14.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1481,BlowJob,1912.0,1940.0,28.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1488,Anal,1954.0,1988.0,34.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1482,BlowJob,2140.0,2172.0,32.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1484,BlowJob,2488.0,2520.0,32.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1489,Anal,2542.0,2562.0,20.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1485,BlowJob,2642.0,2680.0,38.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1486,BlowJob,2800.0,2820.0,20.0
360,Skin Diamond in Rump Raiders Sc3,1492,BlowJob,774.0,786.0,12.0
360,Skin Diamond in Rump Raiders Sc3,1493,BlowJob,826.0,868.0,42.0
360,Skin Diamond in Rump Raiders Sc3,1494,Anal,906.0,958.0,52.0
360,Skin Diamond in Rump Raiders Sc3,1495,Anal,1014.0,1062.0,48.0
361,,3538,Cumshot,1358.0,1362.0,4.0
361,,3564,Cumshot,1764.0,1778.0,14.0
361,,3453,Grabbing Boobs,2804.0,2838.0,34.0
361,,3540,Cumshot,2820.0,2852.0,32.0
361,,3541,Cumshot,3052.0,3104.0,52.0
361,,3454,Grabbing Boobs,3754.0,3810.0,56.0
361,,3518,BlowJob,3892.0,3902.0,10.0
361,,3455,Grabbing Boobs,4208.0,4230.0,22.0
361,,3456,Grabbing Boobs,4730.0,4772.0,42.0
361,,3457,Grabbing Boobs,4926.0,4974.0,48.0
361,,3582,Titjob,4930.0,4966.0,36.0
361,,3583,Titjob,5410.0,5424.0,14.0
361,,3570,Cumshot,5454.0,5510.0,56.0
361,,3459,Grabbing Boobs,5666.0,5714.0,48.0
361,,3571,Cumshot,6348.0,6366.0,18.0
361,,3547,Cumshot,6568.0,6576.0,8.0
361,,3573,Cumshot,7966.0,8012.0,46.0
361,,3460,Grabbing Boobs,8028.0,8078.0,50.0
361,,3584,Titjob,9490.0,9522.0,32.0
361,,3550,Cumshot,10168.0,10172.0,4.0
361,,3577,Cumshot,11414.0,11420.0,6.0
361,,3463,Grabbing Boobs,11842.0,11884.0,42.0
361,,3585,Titjob,11878.0,11898.0,20.0
361,,3523,BlowJob,11900.0,11912.0,12.0
361,,3464,Grabbing Boobs,11920.0,11942.0,22.0
361,,3586,Titjob,11936.0,11952.0,16.0
361,,3508,BlowJob,12116.0,12130.0,14.0
361,,3525,BlowJob,12148.0,12194.0,46.0
361,,3465,Grabbing Boobs,12158.0,12178.0,20.0
361,,3526,BlowJob,12236.0,12274.0,38.0
361,,3527,BlowJob,12324.0,12372.0,48.0
361,,3578,Cumshot,12354.0,12396.0,42.0
361,,3466,Grabbing Boobs,12384.0,12424.0,40.0
361,,3467,Grabbing Boobs,12642.0,12688.0,46.0
361,,3529,BlowJob,12704.0,12718.0,14.0
361,,3468,Grabbing Boobs,13206.0,13226.0,20.0
361,,3469,Grabbing Boobs,13326.0,13346.0,20.0
361,,3554,Cumshot,14380.0,14384.0,4.0
361,,3556,Cumshot,14534.0,14556.0,22.0
361,,3557,Cumshot,14758.0,14770.0,12.0
361,,3471,Grabbing Boobs,15488.0,15526.0,38.0
361,,3558,Cumshot,15776.0,15824.0,48.0
361,,3559,Cumshot,16422.0,16442.0,20.0
361,,3563,Cumshot,18522.0,18532.0,10.0
362,Summer Day in America Bukkake Live,1497,Gangbang,914.0,938.0,24.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1508,Gangbang,294.0,308.0,14.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1509,Gangbang,382.0,392.0,10.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1514,Cumshot,384.0,388.0,4.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1501,BlowJob,520.0,532.0,12.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1502,BlowJob,572.0,606.0,34.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1510,Gangbang,708.0,730.0,22.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1503,BlowJob,762.0,798.0,36.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1511,Gangbang,848.0,864.0,16.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1518,Anal,850.0,862.0,12.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1519,Anal,906.0,958.0,52.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1512,Gangbang,1014.0,1030.0,16.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1504,BlowJob,1106.0,1122.0,16.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1505,BlowJob,1310.0,1352.0,42.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1521,Anal,1388.0,1404.0,16.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1507,BlowJob,1592.0,1638.0,46.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1513,Gangbang,1646.0,1682.0,36.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1517,Cumshot,1698.0,1716.0,18.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1515,Cumshot,1750.0,1766.0,16.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1516,Cumshot,1854.0,1886.0,32.0
364,Summer Vixen in Gangbang Sluts Sc3,1522,Gangbang,86.0,116.0,30.0
364,Summer Vixen in Gangbang Sluts Sc3,1523,Gangbang,158.0,198.0,40.0
364,Summer Vixen in Gangbang Sluts Sc3,1535,BlowJob,222.0,242.0,20.0
364,Summer Vixen in Gangbang Sluts Sc3,1538,Anal,230.0,252.0,22.0
364,Summer Vixen in Gangbang Sluts Sc3,1524,Gangbang,364.0,374.0,10.0
364,Summer Vixen in Gangbang Sluts Sc3,1536,BlowJob,368.0,424.0,56.0
364,Summer Vixen in Gangbang Sluts Sc3,1539,Anal,398.0,444.0,46.0
364,Summer Vixen in Gangbang Sluts Sc3,1525,Gangbang,424.0,448.0,24.0
364,Summer Vixen in Gangbang Sluts Sc3,1540,Anal,602.0,612.0,10.0
364,Summer Vixen in Gangbang Sluts Sc3,1526,Gangbang,688.0,706.0,18.0
364,Summer Vixen in Gangbang Sluts Sc3,1541,Anal,710.0,724.0,14.0
364,Summer Vixen in Gangbang Sluts Sc3,1542,Anal,858.0,900.0,42.0
364,Summer Vixen in Gangbang Sluts Sc3,1528,Gangbang,956.0,1014.0,58.0
364,Summer Vixen in Gangbang Sluts Sc3,1537,BlowJob,1206.0,1218.0,12.0
364,Summer Vixen in Gangbang Sluts Sc3,1529,Gangbang,1288.0,1338.0,50.0
364,Summer Vixen in Gangbang Sluts Sc3,1544,Anal,1378.0,1396.0,18.0
364,Summer Vixen in Gangbang Sluts Sc3,1530,Gangbang,1386.0,1400.0,14.0
364,Summer Vixen in Gangbang Sluts Sc3,1531,Gangbang,1462.0,1498.0,36.0
364,Summer Vixen in Gangbang Sluts Sc3,1548,DP,1484.0,1500.0,16.0
364,Summer Vixen in Gangbang Sluts Sc3,1545,Anal,1486.0,1504.0,18.0
364,Summer Vixen in Gangbang Sluts Sc3,1532,Gangbang,1692.0,1722.0,30.0
364,Summer Vixen in Gangbang Sluts Sc3,1533,Gangbang,1798.0,1826.0,28.0
364,Summer Vixen in Gangbang Sluts Sc3,1534,Gangbang,1874.0,1886.0,12.0
364,Summer Vixen in Gangbang Sluts Sc3,1549,Cumshot,2290.0,2318.0,28.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1550,Grabbing Boobs,266.0,290.0,24.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1551,Grabbing Boobs,462.0,500.0,38.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1554,BlowJob,996.0,1008.0,12.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1560,BlowJob,1086.0,1098.0,12.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1562,BlowJob,1182.0,1200.0,18.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1563,Gangbang,1402.0,1416.0,14.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1555,BlowJob,1468.0,1478.0,10.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1556,BlowJob,1700.0,1748.0,48.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1557,BlowJob,1908.0,1944.0,36.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1565,Anal,2236.0,2270.0,34.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1558,BlowJob,2356.0,2376.0,20.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1566,Anal,2420.0,2442.0,22.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1552,Grabbing Boobs,2472.0,2518.0,46.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1567,Anal,2738.0,2792.0,54.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1564,Gangbang,2874.0,2884.0,10.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1568,Anal,3006.0,3042.0,36.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1569,Anal,3112.0,3140.0,28.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1570,Anal,3284.0,3344.0,60.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1571,Anal,3388.0,3422.0,34.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1561,BlowJob,3620.0,3628.0,8.0
366,Tekohas in Ass & BigTits,1573,BlowJob,206.0,234.0,28.0
366,Tekohas in Ass & BigTits,1574,BlowJob,344.0,404.0,60.0
366,Tekohas in Ass & BigTits,1576,Grabbing Boobs,432.0,452.0,20.0
366,Tekohas in Ass & BigTits,1577,Anal,658.0,690.0,32.0
366,Tekohas in Ass & BigTits,1575,BlowJob,972.0,982.0,10.0
366,Tekohas in Ass & BigTits,1578,Cumshot,1070.0,1080.0,10.0
367,Tekohas in Bareback Party in Stuttgart,1585,BlowJob,434.0,458.0,24.0
367,Tekohas in Bareback Party in Stuttgart,1581,Grabbing Boobs,850.0,876.0,26.0
367,Tekohas in Bareback Party in Stuttgart,1586,BlowJob,898.0,910.0,12.0
367,Tekohas in Bareback Party in Stuttgart,1587,BlowJob,1028.0,1040.0,12.0
367,Tekohas in Bareback Party in Stuttgart,1588,BlowJob,1090.0,1104.0,14.0
367,Tekohas in Bareback Party in Stuttgart,1582,Grabbing Boobs,1092.0,1114.0,22.0
367,Tekohas in Bareback Party in Stuttgart,1589,BlowJob,1184.0,1244.0,60.0
367,Tekohas in Bareback Party in Stuttgart,1596,Cumshot,1234.0,1240.0,6.0
367,Tekohas in Bareback Party in Stuttgart,1600,Cumshot,1250.0,1300.0,50.0
367,Tekohas in Bareback Party in Stuttgart,1590,BlowJob,1338.0,1348.0,10.0
367,Tekohas in Bareback Party in Stuttgart,1607,Anal,1460.0,1476.0,16.0
367,Tekohas in Bareback Party in Stuttgart,1591,BlowJob,2142.0,2162.0,20.0
367,Tekohas in Bareback Party in Stuttgart,1601,Cumshot,2232.0,2266.0,34.0
367,Tekohas in Bareback Party in Stuttgart,1602,Cumshot,2324.0,2360.0,36.0
367,Tekohas in Bareback Party in Stuttgart,1603,Cumshot,2450.0,2466.0,16.0
367,Tekohas in Bareback Party in Stuttgart,1592,BlowJob,2530.0,2548.0,18.0
367,Tekohas in Bareback Party in Stuttgart,1593,BlowJob,2656.0,2668.0,12.0
367,Tekohas in Bareback Party in Stuttgart,1595,BlowJob,2766.0,2792.0,26.0
367,Tekohas in Bareback Party in Stuttgart,1598,Cumshot,2780.0,2806.0,26.0
367,Tekohas in Bareback Party in Stuttgart,1605,Cumshot,2868.0,2872.0,4.0
368,Thai Suzy in WeLoveBukkake 4,1611,Cumshot,90.0,110.0,20.0
368,Thai Suzy in WeLoveBukkake 4,1612,Cumshot,974.0,994.0,20.0
369,Tia Maria in Cum On Melon Tits,1613,BlowJob,244.0,270.0,26.0
369,Tia Maria in Cum On Melon Tits,1614,BlowJob,338.0,348.0,10.0
369,Tia Maria in Cum On Melon Tits,1620,Grabbing Boobs,900.0,932.0,32.0
369,Tia Maria in Cum On Melon Tits,1615,BlowJob,1152.0,1186.0,34.0
369,Tia Maria in Cum On Melon Tits,1616,BlowJob,1248.0,1272.0,24.0
369,Tia Maria in Cum On Melon Tits,1621,Grabbing Boobs,1294.0,1320.0,26.0
369,Tia Maria in Cum On Melon Tits,1622,Grabbing Boobs,1702.0,1740.0,38.0
369,Tia Maria in Cum On Melon Tits,1623,Grabbing Boobs,2648.0,2708.0,60.0
369,Tia Maria in Cum On Melon Tits,1624,Cumshot,2896.0,2918.0,22.0
370,Tia Maria in DPd By Two BWCs,1626,BlowJob,236.0,254.0,18.0
370,Tia Maria in DPd By Two BWCs,1630,Cumshot,660.0,680.0,20.0
370,Tia Maria in DPd By Two BWCs,1633,Anal,920.0,962.0,42.0
370,Tia Maria in DPd By Two BWCs,1634,Anal,1034.0,1076.0,42.0
370,Tia Maria in DPd By Two BWCs,1628,BlowJob,1336.0,1350.0,14.0
370,Tia Maria in DPd By Two BWCs,1629,BlowJob,1384.0,1432.0,48.0
370,Tia Maria in DPd By Two BWCs,1636,Anal,1482.0,1500.0,18.0
370,Tia Maria in DPd By Two BWCs,1631,Cumshot,1628.0,1650.0,22.0
370,Tia Maria in DPd By Two BWCs,1632,Cumshot,1660.0,1668.0,8.0
371,Tyra Ride in First BBC  DP Gangbang,1651,Pissing,366.0,370.0,4.0
371,Tyra Ride in First BBC  DP Gangbang,1644,BlowJob,378.0,414.0,36.0
371,Tyra Ride in First BBC  DP Gangbang,1645,BlowJob,468.0,478.0,10.0
371,Tyra Ride in First BBC  DP Gangbang,1646,BlowJob,538.0,580.0,42.0
371,Tyra Ride in First BBC  DP Gangbang,1638,Gangbang,584.0,598.0,14.0
371,Tyra Ride in First BBC  DP Gangbang,1647,BlowJob,674.0,724.0,50.0
371,Tyra Ride in First BBC  DP Gangbang,1639,Gangbang,682.0,696.0,14.0
371,Tyra Ride in First BBC  DP Gangbang,1648,BlowJob,944.0,970.0,26.0
371,Tyra Ride in First BBC  DP Gangbang,1640,Gangbang,1012.0,1040.0,28.0
371,Tyra Ride in First BBC  DP Gangbang,1652,Pissing,1200.0,1204.0,4.0
371,Tyra Ride in First BBC  DP Gangbang,1654,Anal,1344.0,1356.0,12.0
371,Tyra Ride in First BBC  DP Gangbang,1655,Anal,1390.0,1412.0,22.0
371,Tyra Ride in First BBC  DP Gangbang,1657,Anal,1626.0,1662.0,36.0
371,Tyra Ride in First BBC  DP Gangbang,1649,BlowJob,1748.0,1766.0,18.0
371,Tyra Ride in First BBC  DP Gangbang,1658,Anal,1750.0,1782.0,32.0
371,Tyra Ride in First BBC  DP Gangbang,1642,Gangbang,1908.0,1936.0,28.0
371,Tyra Ride in First BBC  DP Gangbang,1659,Anal,1970.0,2016.0,46.0
371,Tyra Ride in First BBC  DP Gangbang,1660,Cumshot,2274.0,2320.0,46.0
371,Tyra Ride in First BBC  DP Gangbang,1661,Cumshot,2348.0,2372.0,24.0
371,Tyra Ride in First BBC  DP Gangbang,1650,BlowJob,2354.0,2364.0,10.0
372,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc10,1662,BlowJob,104.0,140.0,36.0
373,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc11,1663,BlowJob,40.0,54.0,14.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1678,Gangbang,192.0,202.0,10.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1679,Gangbang,434.0,474.0,40.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1665,BlowJob,552.0,582.0,30.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1680,Gangbang,560.0,576.0,16.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1666,BlowJob,732.0,752.0,20.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1681,Gangbang,884.0,914.0,30.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1667,BlowJob,888.0,898.0,10.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1682,Gangbang,1042.0,1052.0,10.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1668,BlowJob,1102.0,1128.0,26.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1683,Grabbing Boobs,1174.0,1202.0,28.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1684,Anal,1308.0,1318.0,10.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1670,BlowJob,1428.0,1462.0,34.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1685,Anal,1474.0,1488.0,14.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1686,Anal,1578.0,1608.0,30.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1687,Anal,1678.0,1694.0,16.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1671,BlowJob,1694.0,1706.0,12.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1688,Anal,1792.0,1802.0,10.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1689,Anal,1852.0,1866.0,14.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1692,DP,1904.0,1922.0,18.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1673,BlowJob,2238.0,2256.0,18.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1675,BlowJob,2756.0,2788.0,32.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1691,Anal,2924.0,2936.0,12.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1676,BlowJob,2932.0,2956.0,24.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1693,Cumshot,2976.0,2984.0,8.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1694,Cumshot,3016.0,3074.0,58.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1695,Cumshot,3166.0,3222.0,56.0
376,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc8,1696,Grabbing Boobs,204.0,232.0,28.0
377,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc9,1697,BlowJob,218.0,234.0,16.0
378,Veronica Leal in Domination Gangbang,1698,BlowJob,76.0,94.0,18.0
378,Veronica Leal in Domination Gangbang,1699,BlowJob,148.0,184.0,36.0
378,Veronica Leal in Domination Gangbang,1709,Gangbang,236.0,252.0,16.0
378,Veronica Leal in Domination Gangbang,1710,Gangbang,292.0,324.0,32.0
378,Veronica Leal in Domination Gangbang,1711,Gangbang,570.0,580.0,10.0
378,Veronica Leal in Domination Gangbang,1712,Gangbang,640.0,652.0,12.0
378,Veronica Leal in Domination Gangbang,1724,Pissing,1232.0,1250.0,18.0
378,Veronica Leal in Domination Gangbang,1701,BlowJob,1280.0,1308.0,28.0
378,Veronica Leal in Domination Gangbang,1713,Gangbang,1330.0,1364.0,34.0
378,Veronica Leal in Domination Gangbang,1702,BlowJob,1342.0,1368.0,26.0
378,Veronica Leal in Domination Gangbang,1714,Gangbang,1430.0,1452.0,22.0
378,Veronica Leal in Domination Gangbang,1715,Gangbang,1498.0,1520.0,22.0
378,Veronica Leal in Domination Gangbang,1716,Gangbang,1964.0,1996.0,32.0
378,Veronica Leal in Domination Gangbang,1717,Gangbang,2178.0,2196.0,18.0
378,Veronica Leal in Domination Gangbang,1718,Gangbang,2254.0,2270.0,16.0
378,Veronica Leal in Domination Gangbang,1703,BlowJob,2398.0,2432.0,34.0
378,Veronica Leal in Domination Gangbang,1725,Pissing,2484.0,2512.0,28.0
378,Veronica Leal in Domination Gangbang,1719,Gangbang,2548.0,2578.0,30.0
378,Veronica Leal in Domination Gangbang,1704,BlowJob,2550.0,2562.0,12.0
378,Veronica Leal in Domination Gangbang,1705,BlowJob,2700.0,2726.0,26.0
378,Veronica Leal in Domination Gangbang,1706,BlowJob,2786.0,2812.0,26.0
378,Veronica Leal in Domination Gangbang,1720,Gangbang,2832.0,2856.0,24.0
378,Veronica Leal in Domination Gangbang,1707,BlowJob,2844.0,2858.0,14.0
378,Veronica Leal in Domination Gangbang,1721,Gangbang,2954.0,2970.0,16.0
378,Veronica Leal in Domination Gangbang,1723,Grabbing Boobs,3206.0,3238.0,32.0
379,Vittoria Devine in DP Pee 5on1,1726,BlowJob,1042.0,1078.0,36.0
379,Vittoria Devine in DP Pee 5on1,1727,BlowJob,1132.0,1190.0,58.0
379,Vittoria Devine in DP Pee 5on1,1741,Gangbang,1380.0,1426.0,46.0
379,Vittoria Devine in DP Pee 5on1,1750,Pissing,1606.0,1648.0,42.0
379,Vittoria Devine in DP Pee 5on1,1743,Gangbang,2144.0,2172.0,28.0
379,Vittoria Devine in DP Pee 5on1,1744,Gangbang,2224.0,2254.0,30.0
379,Vittoria Devine in DP Pee 5on1,1730,BlowJob,2418.0,2450.0,32.0
379,Vittoria Devine in DP Pee 5on1,1745,Gangbang,2622.0,2642.0,20.0
379,Vittoria Devine in DP Pee 5on1,1746,Gangbang,2782.0,2804.0,22.0
379,Vittoria Devine in DP Pee 5on1,1732,BlowJob,2942.0,2968.0,26.0
379,Vittoria Devine in DP Pee 5on1,1734,BlowJob,3510.0,3562.0,52.0
379,Vittoria Devine in DP Pee 5on1,1747,Gangbang,3592.0,3614.0,22.0
379,Vittoria Devine in DP Pee 5on1,1735,BlowJob,3650.0,3680.0,30.0
379,Vittoria Devine in DP Pee 5on1,1748,Gangbang,3760.0,3788.0,28.0
379,Vittoria Devine in DP Pee 5on1,1749,Gangbang,3862.0,3894.0,32.0
379,Vittoria Devine in DP Pee 5on1,1736,BlowJob,3868.0,3910.0,42.0
379,Vittoria Devine in DP Pee 5on1,1752,Cumshot,4692.0,4696.0,4.0
379,Vittoria Devine in DP Pee 5on1,1753,Cumshot,4734.0,4742.0,8.0
380,Vittoria Devine in Domination Gangbang,1757,BlowJob,296.0,346.0,50.0
380,Vittoria Devine in Domination Gangbang,1758,BlowJob,356.0,360.0,4.0
380,Vittoria Devine in Domination Gangbang,1759,BlowJob,394.0,432.0,38.0
380,Vittoria Devine in Domination Gangbang,1760,BlowJob,472.0,482.0,10.0
380,Vittoria Devine in Domination Gangbang,1754,Anal,698.0,716.0,18.0
380,Vittoria Devine in Domination Gangbang,1762,Gangbang,1048.0,1068.0,20.0
380,Vittoria Devine in Domination Gangbang,1764,Cumshot,1190.0,1194.0,4.0
380,Vittoria Devine in Domination Gangbang,1763,Gangbang,1204.0,1230.0,26.0
380,Vittoria Devine in Domination Gangbang,1755,Anal,1286.0,1328.0,42.0
380,Vittoria Devine in Domination Gangbang,1767,Pissing,1496.0,1502.0,6.0
380,Vittoria Devine in Domination Gangbang,1766,Cumshot,2060.0,2066.0,6.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1773,BlowJob,374.0,412.0,38.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1768,Anal,582.0,610.0,28.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1769,Anal,756.0,796.0,40.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1776,BlowJob,1612.0,1654.0,42.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1772,Anal,2292.0,2304.0,12.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1779,Cumshot,2590.0,2612.0,22.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1777,BlowJob,2594.0,2610.0,16.0
382,Vit√≥ria Beatriz in Edjunior VideoGuru,1784,Anal,380.0,396.0,16.0
382,Vit√≥ria Beatriz in Edjunior VideoGuru,1782,BlowJob,412.0,460.0,48.0
382,Vit√≥ria Beatriz in Edjunior VideoGuru,1785,Anal,610.0,620.0,10.0
383,Willow Ryder in I Love Anal 3 Sc3,1798,Cumshot,516.0,544.0,28.0
383,Willow Ryder in I Love Anal 3 Sc3,1795,BlowJob,948.0,952.0,4.0
383,Willow Ryder in I Love Anal 3 Sc3,1787,BlowJob,1052.0,1074.0,22.0
383,Willow Ryder in I Love Anal 3 Sc3,1788,BlowJob,1310.0,1346.0,36.0
383,Willow Ryder in I Love Anal 3 Sc3,1789,BlowJob,1430.0,1440.0,10.0
383,Willow Ryder in I Love Anal 3 Sc3,1790,BlowJob,1526.0,1542.0,16.0
383,Willow Ryder in I Love Anal 3 Sc3,1791,BlowJob,1658.0,1692.0,34.0
383,Willow Ryder in I Love Anal 3 Sc3,1792,BlowJob,1774.0,1788.0,14.0
383,Willow Ryder in I Love Anal 3 Sc3,1797,BlowJob,2142.0,2154.0,12.0
384,Yasmina Khan in Birthday Gangbang,1804,Gangbang,58.0,98.0,40.0
384,Yasmina Khan in Birthday Gangbang,1807,BlowJob,224.0,268.0,44.0
384,Yasmina Khan in Birthday Gangbang,1805,Gangbang,596.0,610.0,14.0
384,Yasmina Khan in Birthday Gangbang,1808,BlowJob,668.0,694.0,26.0
384,Yasmina Khan in Birthday Gangbang,1809,BlowJob,822.0,878.0,56.0
384,Yasmina Khan in Birthday Gangbang,1812,Anal,904.0,926.0,22.0
384,Yasmina Khan in Birthday Gangbang,1813,Anal,960.0,972.0,12.0
384,Yasmina Khan in Birthday Gangbang,1814,Anal,1072.0,1100.0,28.0
384,Yasmina Khan in Birthday Gangbang,1815,Cumshot,1240.0,1254.0,14.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1817,BlowJob,466.0,508.0,42.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1818,BlowJob,586.0,596.0,10.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1829,Anal,648.0,662.0,14.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1835,Gangbang,692.0,706.0,14.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1819,BlowJob,744.0,756.0,12.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1820,BlowJob,806.0,860.0,54.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1821,BlowJob,1070.0,1086.0,16.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1831,Anal,1594.0,1616.0,22.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1822,BlowJob,1708.0,1724.0,16.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1832,Anal,1742.0,1774.0,32.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1823,BlowJob,1814.0,1856.0,42.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1824,BlowJob,1966.0,2004.0,38.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1825,BlowJob,2092.0,2120.0,28.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1834,Anal,2234.0,2248.0,14.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1826,BlowJob,2284.0,2306.0,22.0
386,AJ Applegate in Gangbang Me Sc1,2402,BlowJob,368.0,420.0,52.0
386,AJ Applegate in Gangbang Me Sc1,2406,Anal,984.0,996.0,12.0
386,AJ Applegate in Gangbang Me Sc1,2403,BlowJob,1166.0,1212.0,46.0
386,AJ Applegate in Gangbang Me Sc1,2416,Grabbing Boobs,1624.0,1672.0,48.0
386,AJ Applegate in Gangbang Me Sc1,2408,Anal,1774.0,1810.0,36.0
386,AJ Applegate in Gangbang Me Sc1,2409,Anal,1898.0,1950.0,52.0
386,AJ Applegate in Gangbang Me Sc1,2410,Anal,2090.0,2124.0,34.0
386,AJ Applegate in Gangbang Me Sc1,2411,Anal,2296.0,2338.0,42.0
386,AJ Applegate in Gangbang Me Sc1,2412,Anal,2384.0,2410.0,26.0
386,AJ Applegate in Gangbang Me Sc1,2414,Anal,2690.0,2714.0,24.0
386,AJ Applegate in Gangbang Me Sc1,2417,Cumshot,3090.0,3118.0,28.0
386,AJ Applegate in Gangbang Me Sc1,2419,Cumshot,3140.0,3144.0,4.0
386,AJ Applegate in Gangbang Me Sc1,2418,Cumshot,3178.0,3182.0,4.0
387,"Abigail Mac, Alina Lopez in Blindsided Sc3",3452,Grabbing Boobs,430.0,454.0,24.0
388,Adira Allure in Interracial Blowbang 24 Sc1,1977,Grabbing Boobs,826.0,856.0,30.0
388,Adira Allure in Interracial Blowbang 24 Sc1,1971,Gangbang,1510.0,1530.0,20.0
388,Adira Allure in Interracial Blowbang 24 Sc1,1972,Gangbang,1572.0,1588.0,16.0
388,Adira Allure in Interracial Blowbang 24 Sc1,1973,Gangbang,1784.0,1822.0,38.0
388,Adira Allure in Interracial Blowbang 24 Sc1,1979,Cumshot,1998.0,2018.0,20.0
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",2344,BlowJob,328.0,384.0,56.0
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",2345,BlowJob,680.0,736.0,56.0
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",2346,BlowJob,838.0,884.0,46.0
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",2348,Gangbang,916.0,926.0,10.0
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",2347,BlowJob,984.0,990.0,6.0
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",2349,Gangbang,1126.0,1180.0,54.0
392,"Adriana Chechik, Gaia in Grease XXX A Parody Sc5",2390,BlowJob,1104.0,1138.0,34.0
392,"Adriana Chechik, Gaia in Grease XXX A Parody Sc5",2392,Cumshot,1232.0,1246.0,14.0
394,Adrianna Luna in Praise The Load 7 Sc1,2396,Gangbang,560.0,580.0,20.0
394,Adrianna Luna in Praise The Load 7 Sc1,2397,Gangbang,820.0,848.0,28.0
394,Adrianna Luna in Praise The Load 7 Sc1,2398,Grabbing Boobs,872.0,894.0,22.0
394,Adrianna Luna in Praise The Load 7 Sc1,2400,Titjob,976.0,998.0,22.0
394,Adrianna Luna in Praise The Load 7 Sc1,2399,Grabbing Boobs,1034.0,1056.0,22.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2128,BlowJob,682.0,724.0,42.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2129,BlowJob,1012.0,1036.0,24.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2136,BlowJob,1058.0,1062.0,4.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2130,BlowJob,1276.0,1296.0,20.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2131,BlowJob,1352.0,1368.0,16.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2132,BlowJob,1652.0,1694.0,42.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2137,Grabbing Boobs,1748.0,1788.0,40.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2133,BlowJob,1778.0,1838.0,60.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2134,BlowJob,1870.0,1884.0,14.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2135,BlowJob,1926.0,1946.0,20.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2138,Cumshot,1930.0,1954.0,24.0
396,Aidra Fox in Gangbanged 7 Sc1,1987,BlowJob,1854.0,1898.0,44.0
396,Aidra Fox in Gangbanged 7 Sc1,2000,Gangbang,2018.0,2028.0,10.0
396,Aidra Fox in Gangbanged 7 Sc1,1996,Anal,2776.0,2800.0,24.0
396,Aidra Fox in Gangbanged 7 Sc1,1997,Anal,2842.0,2896.0,54.0
396,Aidra Fox in Gangbanged 7 Sc1,1989,BlowJob,2906.0,2918.0,12.0
396,Aidra Fox in Gangbanged 7 Sc1,1998,Anal,2990.0,3000.0,10.0
396,Aidra Fox in Gangbanged 7 Sc1,1990,BlowJob,3034.0,3046.0,12.0
396,Aidra Fox in Gangbanged 7 Sc1,2002,Cumshot,3420.0,3434.0,14.0
396,Aidra Fox in Gangbanged 7 Sc1,2004,Cumshot,3464.0,3474.0,10.0
397,Alena Croft in Blacks on Cougars 17 Sc1,2423,Anal,1128.0,1162.0,34.0
397,Alena Croft in Blacks on Cougars 17 Sc1,2421,BlowJob,1162.0,1182.0,20.0
397,Alena Croft in Blacks on Cougars 17 Sc1,2425,Cumshot,1702.0,1736.0,34.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2081,Gangbang,326.0,360.0,34.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2082,Gangbang,402.0,416.0,14.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2083,Gangbang,494.0,524.0,30.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2079,BlowJob,946.0,956.0,10.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2080,BlowJob,990.0,1002.0,12.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2084,Gangbang,1580.0,1610.0,30.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2087,Cumshot,2024.0,2028.0,4.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2085,Gangbang,2044.0,2054.0,10.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2076,Grabbing Boobs,2058.0,2076.0,18.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2088,Cumshot,2068.0,2074.0,6.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2090,Grabbing Boobs,86.0,106.0,20.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2092,Grabbing Boobs,644.0,666.0,22.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2096,BlowJob,696.0,750.0,54.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2100,Anal,818.0,852.0,34.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2093,Grabbing Boobs,1058.0,1106.0,48.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2107,Gangbang,1362.0,1374.0,12.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2097,BlowJob,1810.0,1868.0,58.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2101,Anal,2110.0,2140.0,30.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2102,Anal,2600.0,2614.0,14.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2104,Anal,3068.0,3078.0,10.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2108,Gangbang,3228.0,3240.0,12.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2098,BlowJob,3240.0,3292.0,52.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2105,Anal,3500.0,3522.0,22.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2106,Anal,3556.0,3588.0,32.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2118,Cumshot,728.0,760.0,32.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2111,Cumshot,812.0,856.0,44.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2120,Cumshot,912.0,934.0,22.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2112,Cumshot,1096.0,1100.0,4.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2113,Cumshot,1418.0,1440.0,22.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2114,Cumshot,1492.0,1502.0,10.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2115,Cumshot,1900.0,1912.0,12.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2116,Cumshot,2006.0,2030.0,24.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2121,Cumshot,2064.0,2106.0,42.0
402,Alexis Ford in Gang Bang Addiction Sc3,2459,Grabbing Boobs,322.0,352.0,30.0
402,Alexis Ford in Gang Bang Addiction Sc3,2462,BlowJob,360.0,416.0,56.0
402,Alexis Ford in Gang Bang Addiction Sc3,2475,BlowJob,462.0,484.0,22.0
402,Alexis Ford in Gang Bang Addiction Sc3,2482,Anal,1032.0,1050.0,18.0
402,Alexis Ford in Gang Bang Addiction Sc3,2481,Gangbang,1034.0,1094.0,60.0
402,Alexis Ford in Gang Bang Addiction Sc3,2460,Grabbing Boobs,1358.0,1394.0,36.0
402,Alexis Ford in Gang Bang Addiction Sc3,2483,Anal,1398.0,1408.0,10.0
402,Alexis Ford in Gang Bang Addiction Sc3,2466,BlowJob,1534.0,1570.0,36.0
402,Alexis Ford in Gang Bang Addiction Sc3,2467,BlowJob,1618.0,1670.0,52.0
402,Alexis Ford in Gang Bang Addiction Sc3,2484,Anal,1700.0,1726.0,26.0
402,Alexis Ford in Gang Bang Addiction Sc3,2468,BlowJob,1840.0,1860.0,20.0
402,Alexis Ford in Gang Bang Addiction Sc3,2485,Anal,1890.0,1914.0,24.0
402,Alexis Ford in Gang Bang Addiction Sc3,2469,BlowJob,1918.0,1950.0,32.0
402,Alexis Ford in Gang Bang Addiction Sc3,2470,BlowJob,2116.0,2136.0,20.0
402,Alexis Ford in Gang Bang Addiction Sc3,2471,BlowJob,2296.0,2312.0,16.0
402,Alexis Ford in Gang Bang Addiction Sc3,2472,BlowJob,2396.0,2408.0,12.0
402,Alexis Ford in Gang Bang Addiction Sc3,2473,BlowJob,2442.0,2468.0,26.0
402,Alexis Ford in Gang Bang Addiction Sc3,2461,Grabbing Boobs,2842.0,2864.0,22.0
402,Alexis Ford in Gang Bang Addiction Sc3,2479,BlowJob,2960.0,2972.0,12.0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2494,Grabbing Boobs,4.0,62.0,58.0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2500,Cumshot,320.0,326.0,6.0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2501,Cumshot,1094.0,1114.0,20.0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2502,Cumshot,1310.0,1342.0,32.0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2503,Cumshot,1428.0,1458.0,30.0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2495,Grabbing Boobs,2058.0,2082.0,24.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2509,Cumshot,130.0,150.0,20.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2520,Grabbing Boobs,1846.0,1874.0,28.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2512,BlowJob,1882.0,1892.0,10.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2521,Grabbing Boobs,2008.0,2044.0,36.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2513,BlowJob,2044.0,2078.0,34.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2514,BlowJob,2370.0,2382.0,12.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2518,Anal,2746.0,2774.0,28.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2522,Grabbing Boobs,2874.0,2892.0,18.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2510,Cumshot,3138.0,3164.0,26.0
405,Alexis Texas in Gang Bang Addiction Sc1,2542,Gangbang,174.0,214.0,40.0
405,Alexis Texas in Gang Bang Addiction Sc1,2543,BlowJob,270.0,282.0,12.0
405,Alexis Texas in Gang Bang Addiction Sc1,2545,BlowJob,594.0,604.0,10.0
405,Alexis Texas in Gang Bang Addiction Sc1,2547,BlowJob,1164.0,1210.0,46.0
405,Alexis Texas in Gang Bang Addiction Sc1,2549,BlowJob,1392.0,1404.0,12.0
405,Alexis Texas in Gang Bang Addiction Sc1,2550,BlowJob,1448.0,1490.0,42.0
405,Alexis Texas in Gang Bang Addiction Sc1,2551,BlowJob,1542.0,1574.0,32.0
405,Alexis Texas in Gang Bang Addiction Sc1,2554,BlowJob,1738.0,1776.0,38.0
405,Alexis Texas in Gang Bang Addiction Sc1,2555,BlowJob,1826.0,1836.0,10.0
405,Alexis Texas in Gang Bang Addiction Sc1,2556,Cumshot,1848.0,1866.0,18.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2557,BlowJob,186.0,198.0,12.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2567,Cumshot,430.0,452.0,22.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2573,Anal,496.0,522.0,26.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2559,BlowJob,554.0,582.0,28.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2560,BlowJob,618.0,648.0,30.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2561,BlowJob,734.0,756.0,22.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2568,Cumshot,870.0,878.0,8.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2574,Anal,1010.0,1022.0,12.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2571,Cumshot,1220.0,1236.0,16.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2569,Cumshot,1712.0,1716.0,4.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2566,BlowJob,1730.0,1740.0,10.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2572,Cumshot,1734.0,1750.0,16.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2576,Anal,1794.0,1828.0,34.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2564,BlowJob,1962.0,1976.0,14.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2570,Cumshot,1996.0,2014.0,18.0
407,Alina in Annegret Zugekleistert Sc4,2605,Grabbing Boobs,28.0,88.0,60.0
407,Alina in Annegret Zugekleistert Sc4,2610,Anal,474.0,496.0,22.0
407,Alina in Annegret Zugekleistert Sc4,2611,Anal,578.0,590.0,12.0
407,Alina in Annegret Zugekleistert Sc4,2612,Anal,628.0,644.0,16.0
407,Alina in Annegret Zugekleistert Sc4,2606,Grabbing Boobs,820.0,862.0,42.0
407,Alina in Annegret Zugekleistert Sc4,2614,Anal,862.0,874.0,12.0
407,Alina in Annegret Zugekleistert Sc4,2608,BlowJob,898.0,930.0,32.0
409,Alina Lopez in No Going Back Sc1,2623,BlowJob,262.0,276.0,14.0
410,Alina Lopez in Perfectly Natural 19 Sc4,2625,BlowJob,572.0,582.0,10.0
410,Alina Lopez in Perfectly Natural 19 Sc4,2627,Cumshot,1294.0,1302.0,8.0
410,Alina Lopez in Perfectly Natural 19 Sc4,2626,BlowJob,1304.0,1320.0,16.0
411,Alina Lopez in Pussy is The Best Medicine 9 Sc5,2628,Grabbing Boobs,144.0,184.0,40.0
412,Alina Lopez in Sneaky Sex 30 Sc1,2629,BlowJob,1352.0,1366.0,14.0
412,Alina Lopez in Sneaky Sex 30 Sc1,2630,Cumshot,1390.0,1408.0,18.0
413,Alina Lopez in Wet Food 9 Sc1,2631,BlowJob,348.0,392.0,44.0
413,Alina Lopez in Wet Food 9 Sc1,2633,BlowJob,618.0,652.0,34.0
413,Alina Lopez in Wet Food 9 Sc1,2649,Gangbang,706.0,742.0,36.0
413,Alina Lopez in Wet Food 9 Sc1,2653,Cumshot,838.0,858.0,20.0
413,Alina Lopez in Wet Food 9 Sc1,2650,Gangbang,890.0,912.0,22.0
413,Alina Lopez in Wet Food 9 Sc1,2654,Cumshot,892.0,924.0,32.0
413,Alina Lopez in Wet Food 9 Sc1,2635,BlowJob,956.0,990.0,34.0
413,Alina Lopez in Wet Food 9 Sc1,2636,BlowJob,1040.0,1058.0,18.0
413,Alina Lopez in Wet Food 9 Sc1,2655,Cumshot,1092.0,1114.0,22.0
413,Alina Lopez in Wet Food 9 Sc1,2637,BlowJob,1106.0,1126.0,20.0
413,Alina Lopez in Wet Food 9 Sc1,2638,BlowJob,1254.0,1286.0,32.0
413,Alina Lopez in Wet Food 9 Sc1,2656,Cumshot,1286.0,1324.0,38.0
413,Alina Lopez in Wet Food 9 Sc1,2657,Cumshot,1366.0,1386.0,20.0
413,Alina Lopez in Wet Food 9 Sc1,2639,BlowJob,1372.0,1392.0,20.0
413,Alina Lopez in Wet Food 9 Sc1,2644,BlowJob,1444.0,1468.0,24.0
413,Alina Lopez in Wet Food 9 Sc1,2658,Cumshot,1454.0,1498.0,44.0
413,Alina Lopez in Wet Food 9 Sc1,2659,Cumshot,1654.0,1668.0,14.0
413,Alina Lopez in Wet Food 9 Sc1,2660,Cumshot,1710.0,1760.0,50.0
413,Alina Lopez in Wet Food 9 Sc1,2640,BlowJob,1738.0,1774.0,36.0
413,Alina Lopez in Wet Food 9 Sc1,2661,Cumshot,1798.0,1818.0,20.0
413,Alina Lopez in Wet Food 9 Sc1,2662,Cumshot,1882.0,1938.0,56.0
413,Alina Lopez in Wet Food 9 Sc1,2641,BlowJob,1886.0,1920.0,34.0
413,Alina Lopez in Wet Food 9 Sc1,2652,Gangbang,2168.0,2186.0,18.0
413,Alina Lopez in Wet Food 9 Sc1,2664,Cumshot,3008.0,3016.0,8.0
414,"Alina Lopez, Vera King in Mommy's Dream Sc4",2670,Grabbing Boobs,274.0,298.0,24.0
415,"Alyssa Divine, Kiki Minaj, Victoria Pure, Victoria Summers in Backstage Bangers Sc1",2747,BlowJob,324.0,358.0,34.0
415,"Alyssa Divine, Kiki Minaj, Victoria Pure, Victoria Summers in Backstage Bangers Sc1",2748,BlowJob,406.0,416.0,10.0
415,"Alyssa Divine, Kiki Minaj, Victoria Pure, Victoria Summers in Backstage Bangers Sc1",2750,Anal,582.0,594.0,12.0
415,"Alyssa Divine, Kiki Minaj, Victoria Pure, Victoria Summers in Backstage Bangers Sc1",2746,Grabbing Boobs,1082.0,1130.0,48.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2751,Gangbang,414.0,446.0,32.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2754,BlowJob,654.0,678.0,24.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2765,Anal,668.0,702.0,34.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2755,BlowJob,1210.0,1244.0,34.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2756,BlowJob,1466.0,1478.0,12.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2752,Gangbang,1774.0,1808.0,34.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2757,BlowJob,1790.0,1818.0,28.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2758,BlowJob,1944.0,1964.0,20.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2759,BlowJob,2010.0,2054.0,44.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2760,BlowJob,2582.0,2594.0,12.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2766,Anal,2636.0,2692.0,56.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2769,Grabbing Boobs,2852.0,2870.0,18.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2767,Anal,3218.0,3270.0,52.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2762,BlowJob,3302.0,3316.0,14.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2768,Anal,3306.0,3326.0,20.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2770,Cumshot,3344.0,3398.0,54.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2773,Cumshot,3424.0,3468.0,44.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2774,Cumshot,3496.0,3534.0,38.0
417,"Amari Anne, Ana Foxxx, Jenna Foxx, Kira Noir, Maya Farrell in Kira vs Kira Sc2",2784,Anal,1232.0,1244.0,12.0
417,"Amari Anne, Ana Foxxx, Jenna Foxx, Kira Noir, Maya Farrell in Kira vs Kira Sc2",2785,Anal,1378.0,1398.0,20.0
418,Amber Deen in Dogfart Invades Europe! 4 Sc1,2787,BlowJob,1252.0,1272.0,20.0
418,Amber Deen in Dogfart Invades Europe! 4 Sc1,2789,BlowJob,1810.0,1818.0,8.0
418,Amber Deen in Dogfart Invades Europe! 4 Sc1,2790,Cumshot,1884.0,1890.0,6.0
418,Amber Deen in Dogfart Invades Europe! 4 Sc1,2791,Cumshot,1930.0,1934.0,4.0
418,Amber Deen in Dogfart Invades Europe! 4 Sc1,2792,Cumshot,1956.0,1964.0,8.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2793,Grabbing Boobs,544.0,564.0,20.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2799,BlowJob,760.0,770.0,10.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2800,BlowJob,1234.0,1256.0,22.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2808,Anal,1284.0,1296.0,12.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2809,Cumshot,1670.0,1684.0,14.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2810,Cumshot,1920.0,1924.0,4.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2803,BlowJob,2104.0,2124.0,20.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2804,BlowJob,2168.0,2216.0,48.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2806,BlowJob,2692.0,2702.0,10.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2797,BlowJob,2738.0,2766.0,28.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2844,Cumshot,534.0,558.0,24.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2846,Cumshot,630.0,634.0,4.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2847,Cumshot,800.0,806.0,6.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2848,Cumshot,976.0,1034.0,58.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2855,Cumshot,1214.0,1218.0,4.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2856,Cumshot,1258.0,1266.0,8.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2859,Anal,1384.0,1418.0,34.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2857,Cumshot,1510.0,1524.0,14.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2850,Cumshot,1570.0,1606.0,36.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2852,Cumshot,2158.0,2194.0,36.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2853,Cumshot,2212.0,2216.0,4.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2932,Grabbing Boobs,16.0,42.0,26.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2933,Grabbing Boobs,314.0,340.0,26.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2934,Anal,428.0,454.0,26.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2950,BlowJob,702.0,750.0,48.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2935,Anal,788.0,802.0,14.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2936,Anal,878.0,902.0,24.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2951,BlowJob,906.0,956.0,50.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2937,Anal,1018.0,1050.0,32.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2939,Anal,1206.0,1226.0,20.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2940,Anal,1334.0,1364.0,30.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2941,Anal,1418.0,1468.0,50.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2954,BlowJob,1704.0,1716.0,12.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2955,BlowJob,1770.0,1810.0,40.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2956,BlowJob,1854.0,1906.0,52.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2957,BlowJob,1992.0,2032.0,40.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2958,BlowJob,2114.0,2148.0,34.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2959,BlowJob,2302.0,2328.0,26.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2943,Anal,2352.0,2386.0,34.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2944,Anal,2456.0,2470.0,14.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2945,Anal,2514.0,2532.0,18.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2961,BlowJob,2534.0,2570.0,36.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2962,BlowJob,2604.0,2620.0,16.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2964,BlowJob,2870.0,2886.0,16.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2969,Gangbang,3046.0,3102.0,56.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2966,BlowJob,3246.0,3290.0,44.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2946,Anal,3294.0,3344.0,50.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2967,BlowJob,3348.0,3362.0,14.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2947,Anal,3538.0,3552.0,14.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2948,Anal,3748.0,3800.0,52.0
423,"Angel Eyes, Jada Fire in Freak Nasty Sc1",2973,Grabbing Boobs,0.0,26.0,26.0
423,"Angel Eyes, Jada Fire in Freak Nasty Sc1",2974,Grabbing Boobs,258.0,316.0,58.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3074,Grabbing Boobs,242.0,272.0,30.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3082,Gangbang,580.0,594.0,14.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3075,Grabbing Boobs,596.0,640.0,44.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3079,BlowJob,610.0,642.0,32.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3083,Gangbang,632.0,668.0,36.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3081,BlowJob,996.0,1010.0,14.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3076,Grabbing Boobs,2380.0,2404.0,24.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3085,Cumshot,2458.0,2474.0,16.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3086,Cumshot,2624.0,2644.0,20.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3077,Grabbing Boobs,2774.0,2808.0,34.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3087,BlowJob,324.0,342.0,18.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3088,BlowJob,408.0,430.0,22.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3089,BlowJob,580.0,598.0,18.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3094,BlowJob,670.0,684.0,14.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3107,Titjob,900.0,952.0,52.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3091,BlowJob,1334.0,1366.0,32.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3103,Grabbing Boobs,1916.0,1960.0,44.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3104,Grabbing Boobs,2330.0,2380.0,50.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3097,BlowJob,2616.0,2628.0,12.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3105,Grabbing Boobs,2828.0,2862.0,34.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3093,BlowJob,3044.0,3054.0,10.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3098,BlowJob,3122.0,3126.0,4.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3108,Cumshot,3130.0,3148.0,18.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3109,Cumshot,3174.0,3182.0,8.0
426,"Angela White, Jada Stevens in Jada Loves Gonzo Sc1",3110,BlowJob,526.0,544.0,18.0
426,"Angela White, Jada Stevens in Jada Loves Gonzo Sc1",3111,Grabbing Boobs,570.0,602.0,32.0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",2365,BlowJob,34.0,44.0,10.0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",2366,BlowJob,188.0,248.0,60.0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",2369,Cumshot,200.0,228.0,28.0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",2367,BlowJob,432.0,456.0,24.0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",2371,Cumshot,818.0,838.0,20.0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",2372,Cumshot,988.0,998.0,10.0
428,Alejandra Rico in Intense Latin Gangbang,1958,BlowJob,148.0,158.0,10.0
428,Alejandra Rico in Intense Latin Gangbang,1961,Anal,714.0,730.0,16.0
428,Alejandra Rico in Intense Latin Gangbang,1960,BlowJob,904.0,926.0,22.0
428,Alejandra Rico in Intense Latin Gangbang,1962,Cumshot,962.0,1000.0,38.0
428,Alejandra Rico in Intense Latin Gangbang,1965,Cumshot,1106.0,1136.0,30.0
429,Alejandra Rico in Tons of Cum,2013,Cumshot,1084.0,1090.0,6.0
429,Alejandra Rico in Tons of Cum,2008,BlowJob,1122.0,1130.0,8.0
429,Alejandra Rico in Tons of Cum,2009,BlowJob,1244.0,1272.0,28.0
429,Alejandra Rico in Tons of Cum,2010,BlowJob,1348.0,1352.0,4.0
429,Alejandra Rico in Tons of Cum,2015,Cumshot,1358.0,1366.0,8.0
429,Alejandra Rico in Tons of Cum,2011,BlowJob,1436.0,1442.0,6.0
429,Alejandra Rico in Tons of Cum,2012,BlowJob,1540.0,1546.0,6.0
430,Alex Grey in A Dirty Submissive Slut For Cock,1982,BlowJob,736.0,746.0,10.0
430,Alex Grey in A Dirty Submissive Slut For Cock,1985,Cumshot,1342.0,1348.0,6.0
431,Alexa Nova in GangBang Creampie 246,2437,BlowJob,76.0,98.0,22.0
431,Alexa Nova in GangBang Creampie 246,2438,BlowJob,376.0,414.0,38.0
431,Alexa Nova in GangBang Creampie 246,2444,Anal,972.0,982.0,10.0
431,Alexa Nova in GangBang Creampie 246,2440,BlowJob,1128.0,1150.0,22.0
431,Alexa Nova in GangBang Creampie 246,2441,BlowJob,1390.0,1400.0,10.0
431,Alexa Nova in GangBang Creampie 246,2443,BlowJob,1842.0,1876.0,34.0
432,Alexa Payne in Stepmom Seduction on the Kitchen Counter,2445,BlowJob,478.0,536.0,58.0
432,Alexa Payne in Stepmom Seduction on the Kitchen Counter,2446,BlowJob,618.0,646.0,28.0
432,Alexa Payne in Stepmom Seduction on the Kitchen Counter,2448,Grabbing Boobs,1344.0,1382.0,38.0
432,Alexa Payne in Stepmom Seduction on the Kitchen Counter,2449,Grabbing Boobs,1830.0,1882.0,52.0
433,"Alexia Vendome, Sothy Hiko, Yasmine Lafitte in No Limits Anal and Oral Orgy",2452,BlowJob,260.0,276.0,16.0
433,"Alexia Vendome, Sothy Hiko, Yasmine Lafitte in No Limits Anal and Oral Orgy",2453,BlowJob,316.0,354.0,38.0
433,"Alexia Vendome, Sothy Hiko, Yasmine Lafitte in No Limits Anal and Oral Orgy",2456,Anal,1018.0,1062.0,44.0
433,"Alexia Vendome, Sothy Hiko, Yasmine Lafitte in No Limits Anal and Oral Orgy",2455,BlowJob,1098.0,1108.0,10.0
433,"Alexia Vendome, Sothy Hiko, Yasmine Lafitte in No Limits Anal and Oral Orgy",2457,Anal,1258.0,1282.0,24.0
434,Alexis Kay in GangBang Creampie 417,2487,Gangbang,88.0,100.0,12.0
434,Alexis Kay in GangBang Creampie 417,2489,Grabbing Boobs,124.0,150.0,26.0
434,Alexis Kay in GangBang Creampie 417,2490,BlowJob,202.0,230.0,28.0
434,Alexis Kay in GangBang Creampie 417,2491,BlowJob,448.0,482.0,34.0
434,Alexis Kay in GangBang Creampie 417,2492,BlowJob,608.0,656.0,48.0
434,Alexis Kay in GangBang Creampie 417,2488,Gangbang,636.0,654.0,18.0
435,Alicia Ribeiro in Anal and Oral with 3 Big Black Cocks,2581,Gangbang,214.0,236.0,22.0
435,Alicia Ribeiro in Anal and Oral with 3 Big Black Cocks,2578,BlowJob,400.0,414.0,14.0
435,Alicia Ribeiro in Anal and Oral with 3 Big Black Cocks,2579,BlowJob,606.0,646.0,40.0
435,Alicia Ribeiro in Anal and Oral with 3 Big Black Cocks,2583,Cumshot,2666.0,2692.0,26.0
436,Alicia Trece in Rough Gangbang and Pee Play,2584,BlowJob,124.0,140.0,16.0
436,Alicia Trece in Rough Gangbang and Pee Play,2585,BlowJob,188.0,198.0,10.0
436,Alicia Trece in Rough Gangbang and Pee Play,2586,BlowJob,244.0,258.0,14.0
436,Alicia Trece in Rough Gangbang and Pee Play,2590,Anal,286.0,308.0,22.0
436,Alicia Trece in Rough Gangbang and Pee Play,2587,BlowJob,384.0,400.0,16.0
436,Alicia Trece in Rough Gangbang and Pee Play,2588,BlowJob,434.0,460.0,26.0
436,Alicia Trece in Rough Gangbang and Pee Play,2591,Anal,564.0,584.0,20.0
436,Alicia Trece in Rough Gangbang and Pee Play,2592,Anal,746.0,798.0,52.0
436,Alicia Trece in Rough Gangbang and Pee Play,2593,Anal,886.0,922.0,36.0
436,Alicia Trece in Rough Gangbang and Pee Play,2603,Grabbing Boobs,1968.0,1988.0,20.0
436,Alicia Trece in Rough Gangbang and Pee Play,2596,Anal,2004.0,2028.0,24.0
436,Alicia Trece in Rough Gangbang and Pee Play,2601,Gangbang,2030.0,2044.0,14.0
436,Alicia Trece in Rough Gangbang and Pee Play,2602,Gangbang,2180.0,2198.0,18.0
436,Alicia Trece in Rough Gangbang and Pee Play,2597,Anal,2206.0,2216.0,10.0
436,Alicia Trece in Rough Gangbang and Pee Play,2604,Cumshot,2816.0,2822.0,6.0
437,Aliyah Taylor in Gang Bang All Her Holes,2679,Gangbang,366.0,384.0,18.0
437,Aliyah Taylor in Gang Bang All Her Holes,2672,BlowJob,544.0,600.0,56.0
437,Aliyah Taylor in Gang Bang All Her Holes,2680,Anal,1182.0,1218.0,36.0
437,Aliyah Taylor in Gang Bang All Her Holes,2674,BlowJob,1364.0,1376.0,12.0
437,Aliyah Taylor in Gang Bang All Her Holes,2675,BlowJob,1724.0,1734.0,10.0
437,Aliyah Taylor in Gang Bang All Her Holes,2677,BlowJob,1750.0,1784.0,34.0
437,Aliyah Taylor in Gang Bang All Her Holes,2676,BlowJob,1794.0,1804.0,10.0
438,Allatra Hot in MILF Craving Hardcore Attention,2684,BlowJob,404.0,432.0,28.0
438,Allatra Hot in MILF Craving Hardcore Attention,2688,BlowJob,464.0,508.0,44.0
438,Allatra Hot in MILF Craving Hardcore Attention,2686,BlowJob,630.0,688.0,58.0
438,Allatra Hot in MILF Craving Hardcore Attention,2690,Anal,854.0,894.0,40.0
438,Allatra Hot in MILF Craving Hardcore Attention,2691,Anal,940.0,966.0,26.0
438,Allatra Hot in MILF Craving Hardcore Attention,2687,BlowJob,1094.0,1106.0,12.0
438,Allatra Hot in MILF Craving Hardcore Attention,2693,Cumshot,1316.0,1330.0,14.0
439,Alura Jenson in GangBang Creampie 240,2695,BlowJob,164.0,182.0,18.0
439,Alura Jenson in GangBang Creampie 240,2702,Gangbang,188.0,212.0,24.0
439,Alura Jenson in GangBang Creampie 240,2708,Grabbing Boobs,232.0,270.0,38.0
439,Alura Jenson in GangBang Creampie 240,2696,BlowJob,244.0,272.0,28.0
439,Alura Jenson in GangBang Creampie 240,2703,Gangbang,448.0,470.0,22.0
439,Alura Jenson in GangBang Creampie 240,2710,Anal,452.0,482.0,30.0
439,Alura Jenson in GangBang Creampie 240,2704,Gangbang,514.0,526.0,12.0
439,Alura Jenson in GangBang Creampie 240,2709,Grabbing Boobs,868.0,904.0,36.0
439,Alura Jenson in GangBang Creampie 240,2706,Gangbang,1166.0,1184.0,18.0
439,Alura Jenson in GangBang Creampie 240,2700,BlowJob,2186.0,2204.0,18.0
439,Alura Jenson in GangBang Creampie 240,2707,Gangbang,2188.0,2220.0,32.0
439,Alura Jenson in GangBang Creampie 240,2701,BlowJob,2344.0,2394.0,50.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2711,Gangbang,878.0,892.0,14.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2723,Anal,1268.0,1308.0,40.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2727,Grabbing Boobs,1434.0,1460.0,26.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2728,Titjob,1446.0,1462.0,16.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2715,BlowJob,1468.0,1502.0,34.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2722,BlowJob,1606.0,1610.0,4.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2724,Anal,1854.0,1872.0,18.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2725,Anal,1916.0,1966.0,50.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2716,BlowJob,1930.0,1948.0,18.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2717,BlowJob,2092.0,2104.0,12.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2718,BlowJob,2888.0,2914.0,26.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2719,BlowJob,3134.0,3146.0,12.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2712,Gangbang,3276.0,3294.0,18.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2713,Gangbang,3348.0,3362.0,14.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2720,BlowJob,3440.0,3460.0,20.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2721,BlowJob,3640.0,3654.0,14.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2729,Cumshot,3644.0,3650.0,6.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2726,Anal,3712.0,3730.0,18.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2731,Gangbang,534.0,554.0,20.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2732,Anal,692.0,708.0,16.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2740,BlowJob,1122.0,1134.0,12.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2734,Anal,1502.0,1514.0,12.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2736,Anal,1828.0,1872.0,44.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2742,Cumshot,2656.0,2676.0,20.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2730,Grabbing Boobs,2722.0,2748.0,26.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2743,Cumshot,2724.0,2766.0,42.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2741,BlowJob,2836.0,2848.0,12.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2744,Cumshot,2840.0,2874.0,34.0
442,Amari Anne in You Wanna Cheat Again,2778,BlowJob,644.0,654.0,10.0
442,Amari Anne in You Wanna Cheat Again,2779,BlowJob,694.0,716.0,22.0
442,Amari Anne in You Wanna Cheat Again,2780,BlowJob,760.0,784.0,24.0
442,Amari Anne in You Wanna Cheat Again,2781,BlowJob,846.0,884.0,38.0
442,Amari Anne in You Wanna Cheat Again,2782,Anal,1662.0,1680.0,18.0
442,Amari Anne in You Wanna Cheat Again,2777,Grabbing Boobs,2286.0,2326.0,40.0
442,Amari Anne in You Wanna Cheat Again,2783,Cumshot,2380.0,2406.0,26.0
443,Amirah Adara in Rough Gangbang Session,2828,BlowJob,1188.0,1222.0,34.0
443,Amirah Adara in Rough Gangbang Session,2830,Anal,1314.0,1324.0,10.0
443,Amirah Adara in Rough Gangbang Session,2829,BlowJob,2712.0,2732.0,20.0
444,Amy Reid in AllOut Blowbang Session,2834,Gangbang,408.0,442.0,34.0
444,Amy Reid in AllOut Blowbang Session,2835,Cumshot,980.0,1024.0,44.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2860,BlowJob,394.0,410.0,16.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2866,Gangbang,562.0,574.0,12.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2862,BlowJob,1116.0,1130.0,14.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2867,Gangbang,1740.0,1768.0,28.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2869,Anal,2026.0,2042.0,16.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2868,Gangbang,2462.0,2498.0,36.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2863,BlowJob,2468.0,2494.0,26.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2864,BlowJob,2720.0,2734.0,14.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2872,Anal,2870.0,2890.0,20.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2865,BlowJob,3160.0,3178.0,18.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2873,Cumshot,3164.0,3216.0,52.0
446,Ana J√∫lia in DP com a Mulata Cavala,2876,Grabbing Boobs,238.0,264.0,26.0
446,Ana J√∫lia in DP com a Mulata Cavala,2879,BlowJob,516.0,534.0,18.0
446,Ana J√∫lia in DP com a Mulata Cavala,2880,BlowJob,1096.0,1136.0,40.0
446,Ana J√∫lia in DP com a Mulata Cavala,2881,BlowJob,1192.0,1214.0,22.0
446,Ana J√∫lia in DP com a Mulata Cavala,2882,Anal,1674.0,1702.0,28.0
446,Ana J√∫lia in DP com a Mulata Cavala,2884,Cumshot,2372.0,2408.0,36.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2885,BlowJob,38.0,76.0,38.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2887,Anal,168.0,188.0,20.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2888,Anal,286.0,314.0,28.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2889,Anal,346.0,366.0,20.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2886,BlowJob,502.0,528.0,26.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2890,69,662.0,676.0,14.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2891,Cumshot,1492.0,1504.0,12.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2920,Anal,342.0,360.0,18.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2921,Anal,2394.0,2404.0,10.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2928,Grabbing Boobs,2410.0,2468.0,58.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2922,Anal,2438.0,2458.0,20.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2923,Anal,2550.0,2562.0,12.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2929,BlowJob,2622.0,2632.0,10.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2925,Anal,2928.0,2944.0,16.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2926,Anal,2980.0,2998.0,18.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2930,BlowJob,4210.0,4242.0,32.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2931,BlowJob,4882.0,4906.0,24.0
449,Angel Lima in Big Butt Airtight Show,2977,Grabbing Boobs,486.0,516.0,30.0
449,Angel Lima in Big Butt Airtight Show,2978,Grabbing Boobs,588.0,628.0,40.0
449,Angel Lima in Big Butt Airtight Show,2981,Anal,1022.0,1034.0,12.0
449,Angel Lima in Big Butt Airtight Show,2979,Grabbing Boobs,1326.0,1350.0,24.0
449,Angel Lima in Big Butt Airtight Show,2982,Anal,1868.0,1914.0,46.0
449,Angel Lima in Big Butt Airtight Show,2983,Anal,2192.0,2230.0,38.0
449,Angel Lima in Big Butt Airtight Show,2984,Anal,2328.0,2362.0,34.0
449,Angel Lima in Big Butt Airtight Show,2985,Anal,2420.0,2434.0,14.0
449,Angel Lima in Big Butt Airtight Show,2980,Grabbing Boobs,2714.0,2734.0,20.0
450,Angel Lima in Hardcore Brazilian Double Anal,2988,Grabbing Boobs,40.0,100.0,60.0
450,Angel Lima in Hardcore Brazilian Double Anal,2989,Grabbing Boobs,154.0,172.0,18.0
450,Angel Lima in Hardcore Brazilian Double Anal,2995,BlowJob,732.0,766.0,34.0
450,Angel Lima in Hardcore Brazilian Double Anal,2998,Titjob,756.0,808.0,52.0
450,Angel Lima in Hardcore Brazilian Double Anal,2991,Grabbing Boobs,876.0,894.0,18.0
450,Angel Lima in Hardcore Brazilian Double Anal,2992,Grabbing Boobs,972.0,998.0,26.0
450,Angel Lima in Hardcore Brazilian Double Anal,2996,BlowJob,984.0,1000.0,16.0
450,Angel Lima in Hardcore Brazilian Double Anal,2997,BlowJob,1046.0,1062.0,16.0
450,Angel Lima in Hardcore Brazilian Double Anal,2993,Grabbing Boobs,1820.0,1850.0,30.0
450,Angel Lima in Hardcore Brazilian Double Anal,3001,Anal,1964.0,1980.0,16.0
450,Angel Lima in Hardcore Brazilian Double Anal,3004,Anal,2530.0,2542.0,12.0
450,Angel Lima in Hardcore Brazilian Double Anal,3005,Anal,2608.0,2628.0,20.0
450,Angel Lima in Hardcore Brazilian Double Anal,2994,Grabbing Boobs,3346.0,3364.0,18.0
450,Angel Lima in Hardcore Brazilian Double Anal,3008,Cumshot,3392.0,3408.0,16.0
450,Angel Lima in Hardcore Brazilian Double Anal,3009,Cumshot,3442.0,3472.0,30.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3010,Grabbing Boobs,18.0,44.0,26.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3011,Grabbing Boobs,710.0,738.0,28.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3012,Grabbing Boobs,828.0,878.0,50.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3017,BlowJob,1986.0,2002.0,16.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3013,Grabbing Boobs,2194.0,2244.0,50.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3014,Grabbing Boobs,2412.0,2430.0,18.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3021,Anal,2574.0,2630.0,56.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3018,BlowJob,2768.0,2786.0,18.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3019,BlowJob,2926.0,2960.0,34.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3022,Cumshot,3022.0,3078.0,56.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3015,Grabbing Boobs,3064.0,3092.0,28.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3023,Grabbing Boobs,198.0,220.0,22.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3024,Grabbing Boobs,254.0,286.0,32.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3025,Grabbing Boobs,330.0,352.0,22.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3027,BlowJob,414.0,446.0,32.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3029,BlowJob,684.0,744.0,60.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3031,Anal,904.0,924.0,20.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3026,Grabbing Boobs,950.0,970.0,20.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3032,Grabbing Boobs,18.0,44.0,26.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3033,Grabbing Boobs,176.0,206.0,30.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3034,Grabbing Boobs,710.0,738.0,28.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3043,BlowJob,1280.0,1294.0,14.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3040,BlowJob,1312.0,1316.0,4.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3047,Anal,1356.0,1368.0,12.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3035,Grabbing Boobs,1530.0,1564.0,34.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3036,Grabbing Boobs,2220.0,2244.0,24.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3048,Anal,2348.0,2372.0,24.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3045,BlowJob,2734.0,2776.0,42.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3041,BlowJob,2954.0,2960.0,6.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3038,Grabbing Boobs,3072.0,3100.0,28.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3050,Cumshot,3076.0,3080.0,4.0
455,"Angel Smalls, Anna De Ville, Barbie Sins, Jureka Del Mar, May Thai, Nathaly Cherie, Selvaggia in Messy Facial Compilation",3057,Grabbing Boobs,442.0,468.0,26.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3112,Grabbing Boobs,64.0,86.0,22.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3117,BlowJob,730.0,758.0,28.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3118,BlowJob,1152.0,1196.0,44.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3132,Gangbang,1176.0,1188.0,12.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3135,Pissing,1256.0,1262.0,6.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3119,BlowJob,1268.0,1316.0,48.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3113,Grabbing Boobs,1300.0,1340.0,40.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3120,BlowJob,1392.0,1434.0,42.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3121,BlowJob,1486.0,1502.0,16.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3134,Gangbang,2370.0,2420.0,50.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3123,BlowJob,2406.0,2424.0,18.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3137,Cumshot,2468.0,2490.0,22.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3124,BlowJob,2474.0,2486.0,12.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3125,BlowJob,2524.0,2580.0,56.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3138,Cumshot,2584.0,2590.0,6.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3126,BlowJob,3060.0,3076.0,16.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3130,Anal,3820.0,3830.0,10.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3131,Anal,3928.0,3954.0,26.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3115,BlowJob,4124.0,4134.0,10.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3136,Pissing,4630.0,4634.0,4.0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,3158,BlowJob,1248.0,1282.0,34.0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,3160,BlowJob,2064.0,2078.0,14.0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,3161,BlowJob,2360.0,2384.0,24.0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,3162,BlowJob,2616.0,2646.0,30.0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,3163,BlowJob,2696.0,2726.0,30.0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,3164,BlowJob,2766.0,2790.0,24.0
459,Ania Kinski in Kinky DP Session At The Clinic,3169,Anal,1408.0,1446.0,38.0
459,Ania Kinski in Kinky DP Session At The Clinic,3170,Anal,1522.0,1536.0,14.0
459,Ania Kinski in Kinky DP Session At The Clinic,3172,Cumshot,2032.0,2060.0,28.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3173,Grabbing Boobs,710.0,732.0,22.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3175,Cumshot,712.0,726.0,14.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3178,BlowJob,1182.0,1196.0,14.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3179,BlowJob,1258.0,1310.0,52.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3180,BlowJob,1400.0,1420.0,20.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3186,Anal,1422.0,1440.0,18.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3181,BlowJob,1482.0,1500.0,18.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3182,BlowJob,1554.0,1572.0,18.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3187,Anal,1574.0,1612.0,38.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3188,Anal,1708.0,1742.0,34.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3185,BlowJob,2074.0,2094.0,20.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3176,Cumshot,2078.0,2086.0,8.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3174,Grabbing Boobs,2154.0,2182.0,28.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3203,Grabbing Boobs,1352.0,1372.0,20.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3204,Grabbing Boobs,2140.0,2168.0,28.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3210,BlowJob,2266.0,2276.0,10.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3218,Anal,2294.0,2314.0,20.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3211,BlowJob,2382.0,2398.0,16.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3205,Grabbing Boobs,2434.0,2478.0,44.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3212,BlowJob,2534.0,2584.0,50.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3206,Grabbing Boobs,2634.0,2682.0,48.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3219,Anal,2728.0,2782.0,54.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3220,Anal,2828.0,2870.0,42.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3221,Anal,3466.0,3488.0,22.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3222,Anal,3532.0,3560.0,28.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3213,BlowJob,3728.0,3746.0,18.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3216,BlowJob,4028.0,4052.0,24.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3207,Grabbing Boobs,4126.0,4160.0,34.0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",3231,Anal,340.0,350.0,10.0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",3234,DP,954.0,960.0,6.0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",3228,BlowJob,1074.0,1108.0,34.0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",3233,Grabbing Boobs,1128.0,1146.0,18.0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",3229,BlowJob,1168.0,1178.0,10.0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",3235,Cumshot,1924.0,1976.0,52.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3246,BlowJob,678.0,716.0,38.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3250,Anal,822.0,832.0,10.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3253,Anal,1390.0,1412.0,22.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3244,BlowJob,1518.0,1528.0,10.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3249,BlowJob,1606.0,1640.0,34.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3254,Anal,1696.0,1716.0,20.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3255,Anal,1820.0,1830.0,10.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3256,Anal,1938.0,1954.0,16.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3257,Anal,2002.0,2028.0,26.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3258,Anal,2102.0,2134.0,32.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3247,BlowJob,2306.0,2318.0,12.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3242,Grabbing Boobs,2356.0,2404.0,48.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3259,BlowJob,896.0,924.0,28.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3260,BlowJob,1216.0,1238.0,22.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3272,Grabbing Boobs,1746.0,1770.0,24.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3266,Anal,1748.0,1774.0,26.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3267,Anal,1978.0,1998.0,20.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3268,Anal,2038.0,2054.0,16.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3262,BlowJob,2082.0,2094.0,12.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3269,Anal,2122.0,2180.0,58.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3263,BlowJob,2562.0,2598.0,36.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3273,69,2614.0,2636.0,22.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3271,Anal,2898.0,2908.0,10.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3264,BlowJob,3312.0,3344.0,32.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3274,Grabbing Boobs,82.0,102.0,20.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3279,BlowJob,352.0,362.0,10.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3280,BlowJob,408.0,420.0,12.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3275,Grabbing Boobs,666.0,708.0,42.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3276,Grabbing Boobs,1070.0,1102.0,32.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3277,Grabbing Boobs,1316.0,1352.0,36.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3278,Grabbing Boobs,1430.0,1480.0,50.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3281,BlowJob,1506.0,1536.0,30.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3282,BlowJob,1576.0,1620.0,44.0
468,Anissa Kate in A Hot Surfer Threesome,3290,BlowJob,170.0,184.0,14.0
468,Anissa Kate in A Hot Surfer Threesome,3296,BlowJob,298.0,312.0,14.0
468,Anissa Kate in A Hot Surfer Threesome,3292,BlowJob,370.0,388.0,18.0
468,Anissa Kate in A Hot Surfer Threesome,3293,BlowJob,468.0,484.0,16.0
468,Anissa Kate in A Hot Surfer Threesome,3283,Anal,740.0,796.0,56.0
468,Anissa Kate in A Hot Surfer Threesome,3285,Anal,1042.0,1064.0,22.0
468,Anissa Kate in A Hot Surfer Threesome,3294,BlowJob,1138.0,1178.0,40.0
468,Anissa Kate in A Hot Surfer Threesome,3286,Anal,1414.0,1426.0,12.0
468,Anissa Kate in A Hot Surfer Threesome,3287,Anal,1466.0,1476.0,10.0
468,Anissa Kate in A Hot Surfer Threesome,3288,Anal,1520.0,1578.0,58.0
468,Anissa Kate in A Hot Surfer Threesome,3289,Anal,1662.0,1688.0,26.0
468,Anissa Kate in A Hot Surfer Threesome,3298,Grabbing Boobs,1730.0,1754.0,24.0
468,Anissa Kate in A Hot Surfer Threesome,3300,Cumshot,1750.0,1766.0,16.0
468,Anissa Kate in A Hot Surfer Threesome,3297,BlowJob,1766.0,1790.0,24.0
468,Anissa Kate in A Hot Surfer Threesome,3301,Cumshot,1790.0,1794.0,4.0
468,Anissa Kate in A Hot Surfer Threesome,3299,Grabbing Boobs,1906.0,1924.0,18.0
469,Anissa Kate in Hardcore Business Meeting,3310,Titjob,662.0,674.0,12.0
469,Anissa Kate in Hardcore Business Meeting,3306,BlowJob,754.0,764.0,10.0
469,Anissa Kate in Hardcore Business Meeting,3307,BlowJob,1356.0,1376.0,20.0
469,Anissa Kate in Hardcore Business Meeting,3308,BlowJob,1488.0,1514.0,26.0
469,Anissa Kate in Hardcore Business Meeting,3312,Cumshot,2226.0,2282.0,56.0
469,Anissa Kate in Hardcore Business Meeting,3313,Cumshot,2332.0,2364.0,32.0
469,Anissa Kate in Hardcore Business Meeting,3304,Grabbing Boobs,2340.0,2358.0,18.0
470,Anissa Kate in Poolside DP with BBC and Cumshot Finish,3326,BlowJob,1768.0,1826.0,58.0
470,Anissa Kate in Poolside DP with BBC and Cumshot Finish,3327,BlowJob,2204.0,2248.0,44.0
470,Anissa Kate in Poolside DP with BBC and Cumshot Finish,3330,Anal,2542.0,2554.0,12.0
470,Anissa Kate in Poolside DP with BBC and Cumshot Finish,3331,Anal,2672.0,2698.0,26.0
470,Anissa Kate in Poolside DP with BBC and Cumshot Finish,3328,BlowJob,2916.0,2922.0,6.0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3356,Grabbing Boobs,790.0,814.0,24.0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3357,Grabbing Boobs,864.0,888.0,24.0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3349,BlowJob,1520.0,1576.0,56.0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3351,BlowJob,2034.0,2056.0,22.0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3352,BlowJob,2478.0,2488.0,10.0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3355,BlowJob,2960.0,2966.0,6.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3362,Grabbing Boobs,1214.0,1246.0,32.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3363,BlowJob,1500.0,1528.0,28.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3367,BlowJob,1564.0,1606.0,42.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3368,BlowJob,1710.0,1732.0,22.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3369,BlowJob,1906.0,1940.0,34.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3371,Anal,2126.0,2140.0,14.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3372,Anal,2236.0,2254.0,18.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3373,Anal,2404.0,2440.0,36.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3370,BlowJob,2422.0,2436.0,14.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3374,Anal,2502.0,2516.0,14.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3375,Anal,2550.0,2562.0,12.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3376,Anal,2664.0,2696.0,32.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3377,Anal,2786.0,2800.0,14.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3365,BlowJob,2936.0,2984.0,48.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3378,Cumshot,2964.0,2994.0,30.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3401,BlowJob,644.0,654.0,10.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3396,BlowJob,836.0,848.0,12.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3402,BlowJob,866.0,870.0,4.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3397,BlowJob,1440.0,1452.0,12.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3398,BlowJob,1654.0,1692.0,38.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3404,BlowJob,1904.0,1926.0,22.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3399,BlowJob,2028.0,2040.0,12.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3405,Titjob,2048.0,2064.0,16.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3394,Grabbing Boobs,2152.0,2170.0,18.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3407,Grabbing Boobs,100.0,160.0,60.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3416,Anal,450.0,472.0,22.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3409,Grabbing Boobs,986.0,1018.0,32.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3410,Grabbing Boobs,1100.0,1144.0,44.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3412,BlowJob,1884.0,1930.0,46.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3418,Anal,2346.0,2380.0,34.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3419,Anal,2440.0,2464.0,24.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3420,Anal,2542.0,2570.0,28.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3421,Anal,2612.0,2658.0,46.0
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",3423,BlowJob,102.0,150.0,48.0
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",3427,Cumshot,184.0,220.0,36.0
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",3428,Cumshot,304.0,340.0,36.0
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",3424,BlowJob,372.0,416.0,44.0
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",3425,BlowJob,456.0,490.0,34.0
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",3426,BlowJob,524.0,558.0,34.0
477,Anni Star in Lingerie Pleasure Premi√®re,3445,BlowJob,66.0,118.0,52.0
477,Anni Star in Lingerie Pleasure Premi√®re,3446,BlowJob,156.0,166.0,10.0
477,Anni Star in Lingerie Pleasure Premi√®re,3447,Grabbing Boobs,188.0,228.0,40.0
477,Anni Star in Lingerie Pleasure Premi√®re,3448,Titjob,204.0,234.0,30.0
477,Anni Star in Lingerie Pleasure Premi√®re,3449,Anal,350.0,374.0,24.0
477,Anni Star in Lingerie Pleasure Premi√®re,3451,Anal,732.0,756.0,24.0
479,April Snow in GangBang Creampie 232,5205,Gangbang,10.0,40.0,30.0
479,April Snow in GangBang Creampie 232,5206,BlowJob,82.0,104.0,22.0
479,April Snow in GangBang Creampie 232,5216,Grabbing Boobs,268.0,288.0,20.0
479,April Snow in GangBang Creampie 232,5208,BlowJob,866.0,910.0,44.0
479,April Snow in GangBang Creampie 232,5218,Anal,984.0,1030.0,46.0
479,April Snow in GangBang Creampie 232,5209,BlowJob,1160.0,1174.0,14.0
479,April Snow in GangBang Creampie 232,5210,BlowJob,1232.0,1280.0,48.0
479,April Snow in GangBang Creampie 232,5217,Grabbing Boobs,1302.0,1322.0,20.0
479,April Snow in GangBang Creampie 232,5211,BlowJob,1336.0,1358.0,22.0
479,April Snow in GangBang Creampie 232,5213,BlowJob,1750.0,1768.0,18.0
479,April Snow in GangBang Creampie 232,5214,BlowJob,1802.0,1822.0,20.0
479,April Snow in GangBang Creampie 232,5215,BlowJob,1924.0,1952.0,28.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5221,BlowJob,302.0,310.0,8.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5223,BlowJob,406.0,442.0,36.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5227,Anal,690.0,718.0,28.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5228,Anal,1100.0,1114.0,14.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5229,Anal,1168.0,1200.0,32.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5230,Anal,1258.0,1292.0,34.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5224,BlowJob,1616.0,1660.0,44.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5225,BlowJob,1702.0,1718.0,16.0
482,"Ashby Winter in Vogue 2, Part 5",5232,BlowJob,1246.0,1266.0,20.0
482,"Ashby Winter in Vogue 2, Part 5",5237,DP,2152.0,2180.0,28.0
482,"Ashby Winter in Vogue 2, Part 5",5234,BlowJob,2490.0,2508.0,18.0
482,"Ashby Winter in Vogue 2, Part 5",5235,BlowJob,2548.0,2584.0,36.0
482,"Ashby Winter in Vogue 2, Part 5",5238,Cumshot,2636.0,2694.0,58.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5239,BlowJob,8.0,18.0,10.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5245,BlowJob,36.0,86.0,50.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5241,BlowJob,146.0,168.0,22.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5242,BlowJob,218.0,242.0,24.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5243,BlowJob,340.0,372.0,32.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5244,BlowJob,786.0,836.0,50.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5247,Cumshot,794.0,810.0,16.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5249,Cumshot,926.0,946.0,20.0
484,Ashley Cumstar in Gangbang Party,5250,Grabbing Boobs,14.0,32.0,18.0
484,Ashley Cumstar in Gangbang Party,5252,BlowJob,230.0,250.0,20.0
484,Ashley Cumstar in Gangbang Party,5256,Cumshot,302.0,314.0,12.0
484,Ashley Cumstar in Gangbang Party,5253,BlowJob,346.0,398.0,52.0
484,Ashley Cumstar in Gangbang Party,5255,BlowJob,432.0,442.0,10.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5257,Anal,160.0,206.0,46.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5272,BlowJob,298.0,358.0,60.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5259,Anal,448.0,494.0,46.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5274,Gangbang,604.0,616.0,12.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5260,Anal,654.0,712.0,58.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5261,Anal,856.0,892.0,36.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5276,Gangbang,1068.0,1080.0,12.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5277,Gangbang,1162.0,1172.0,10.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5278,Gangbang,1332.0,1348.0,16.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5263,Anal,1380.0,1406.0,26.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5264,Anal,1468.0,1504.0,36.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5279,Gangbang,1470.0,1484.0,14.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5280,Gangbang,1646.0,1678.0,32.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5281,Gangbang,1740.0,1752.0,12.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5266,Anal,1796.0,1856.0,60.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5267,Anal,1950.0,1974.0,24.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5282,Gangbang,2024.0,2052.0,28.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5283,Gangbang,2108.0,2120.0,12.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5284,Gangbang,2258.0,2312.0,54.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5268,Anal,2282.0,2302.0,20.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5286,Pissing,2354.0,2358.0,4.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5285,Gangbang,2540.0,2594.0,54.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5270,Anal,2864.0,2908.0,44.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5273,BlowJob,2910.0,2926.0,16.0
486,Athenea Rose in 7on1 DAP Gangbang,5287,Gangbang,28.0,50.0,22.0
486,Athenea Rose in 7on1 DAP Gangbang,5294,Grabbing Boobs,30.0,54.0,24.0
486,Athenea Rose in 7on1 DAP Gangbang,5297,Anal,68.0,94.0,26.0
486,Athenea Rose in 7on1 DAP Gangbang,5298,Anal,170.0,198.0,28.0
486,Athenea Rose in 7on1 DAP Gangbang,5299,Anal,372.0,410.0,38.0
486,Athenea Rose in 7on1 DAP Gangbang,5304,BlowJob,562.0,592.0,30.0
486,Athenea Rose in 7on1 DAP Gangbang,5300,Anal,572.0,628.0,56.0
486,Athenea Rose in 7on1 DAP Gangbang,5288,Gangbang,586.0,618.0,32.0
486,Athenea Rose in 7on1 DAP Gangbang,5301,Anal,704.0,714.0,10.0
486,Athenea Rose in 7on1 DAP Gangbang,5295,Grabbing Boobs,844.0,870.0,26.0
486,Athenea Rose in 7on1 DAP Gangbang,5306,BlowJob,976.0,992.0,16.0
486,Athenea Rose in 7on1 DAP Gangbang,5289,Gangbang,1024.0,1062.0,38.0
486,Athenea Rose in 7on1 DAP Gangbang,5307,BlowJob,1026.0,1066.0,40.0
486,Athenea Rose in 7on1 DAP Gangbang,5290,Gangbang,1210.0,1226.0,16.0
486,Athenea Rose in 7on1 DAP Gangbang,5291,Gangbang,1426.0,1466.0,40.0
486,Athenea Rose in 7on1 DAP Gangbang,5292,Gangbang,1662.0,1694.0,32.0
486,Athenea Rose in 7on1 DAP Gangbang,5310,Pissing,3466.0,3478.0,12.0
486,Athenea Rose in 7on1 DAP Gangbang,5296,Grabbing Boobs,3646.0,3694.0,48.0
486,Athenea Rose in 7on1 DAP Gangbang,5309,BlowJob,3702.0,3708.0,6.0
487,Athenea Rose in Airtight 6on1 Destruction,5317,Gangbang,356.0,378.0,22.0
487,Athenea Rose in Airtight 6on1 Destruction,5318,Gangbang,424.0,440.0,16.0
487,Athenea Rose in Airtight 6on1 Destruction,5319,Gangbang,514.0,528.0,14.0
487,Athenea Rose in Airtight 6on1 Destruction,5331,BlowJob,704.0,716.0,12.0
487,Athenea Rose in Airtight 6on1 Destruction,5320,Gangbang,716.0,752.0,36.0
487,Athenea Rose in Airtight 6on1 Destruction,5332,BlowJob,904.0,918.0,14.0
487,Athenea Rose in Airtight 6on1 Destruction,5322,Gangbang,1144.0,1160.0,16.0
487,Athenea Rose in Airtight 6on1 Destruction,5334,BlowJob,1534.0,1548.0,14.0
487,Athenea Rose in Airtight 6on1 Destruction,5323,Gangbang,1570.0,1586.0,16.0
487,Athenea Rose in Airtight 6on1 Destruction,5336,BlowJob,1824.0,1870.0,46.0
487,Athenea Rose in Airtight 6on1 Destruction,5343,Anal,1952.0,1964.0,12.0
487,Athenea Rose in Airtight 6on1 Destruction,5337,BlowJob,1974.0,1986.0,12.0
487,Athenea Rose in Airtight 6on1 Destruction,5344,Anal,2108.0,2126.0,18.0
487,Athenea Rose in Airtight 6on1 Destruction,5345,Anal,2176.0,2208.0,32.0
487,Athenea Rose in Airtight 6on1 Destruction,5314,Grabbing Boobs,2276.0,2320.0,44.0
487,Athenea Rose in Airtight 6on1 Destruction,5346,Anal,2300.0,2312.0,12.0
487,Athenea Rose in Airtight 6on1 Destruction,5315,Grabbing Boobs,2436.0,2474.0,38.0
487,Athenea Rose in Airtight 6on1 Destruction,5338,BlowJob,2450.0,2462.0,12.0
487,Athenea Rose in Airtight 6on1 Destruction,5326,Gangbang,2470.0,2484.0,14.0
487,Athenea Rose in Airtight 6on1 Destruction,5347,Anal,2532.0,2548.0,16.0
487,Athenea Rose in Airtight 6on1 Destruction,5316,Grabbing Boobs,2534.0,2580.0,46.0
487,Athenea Rose in Airtight 6on1 Destruction,5327,Gangbang,2560.0,2608.0,48.0
487,Athenea Rose in Airtight 6on1 Destruction,5339,BlowJob,2638.0,2662.0,24.0
487,Athenea Rose in Airtight 6on1 Destruction,5348,Anal,2656.0,2702.0,46.0
487,Athenea Rose in Airtight 6on1 Destruction,5328,Gangbang,2680.0,2708.0,28.0
487,Athenea Rose in Airtight 6on1 Destruction,5349,Anal,2758.0,2804.0,46.0
487,Athenea Rose in Airtight 6on1 Destruction,5350,Anal,2836.0,2848.0,12.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5445,Grabbing Boobs,142.0,170.0,28.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5447,Anal,258.0,276.0,18.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5451,BlowJob,338.0,398.0,60.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5452,BlowJob,462.0,504.0,42.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5446,Grabbing Boobs,1796.0,1822.0,26.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5455,BlowJob,2302.0,2320.0,18.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5456,BlowJob,2490.0,2512.0,22.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5450,Gangbang,2620.0,2630.0,10.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5458,BlowJob,2922.0,2932.0,10.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5459,BlowJob,2964.0,3002.0,38.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5460,BlowJob,3472.0,3484.0,12.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5483,BlowJob,272.0,298.0,26.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5484,BlowJob,338.0,366.0,28.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5472,Gangbang,436.0,482.0,46.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5485,BlowJob,456.0,504.0,48.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5493,Anal,538.0,552.0,14.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5486,BlowJob,808.0,832.0,24.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5473,Gangbang,832.0,842.0,10.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5474,Gangbang,890.0,914.0,24.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5475,Gangbang,952.0,978.0,26.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5509,Pissing,1168.0,1180.0,12.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5511,Cumshot,1180.0,1212.0,32.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5487,BlowJob,1290.0,1300.0,10.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5488,BlowJob,1380.0,1390.0,10.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5496,Anal,1688.0,1726.0,38.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5477,Gangbang,1692.0,1720.0,28.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5478,Gangbang,1758.0,1814.0,56.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5497,Anal,1780.0,1798.0,18.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5479,Gangbang,1854.0,1874.0,20.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5480,Gangbang,1930.0,1952.0,22.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5498,Anal,2186.0,2216.0,30.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5489,BlowJob,2236.0,2248.0,12.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5499,Anal,2274.0,2286.0,12.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5500,Anal,2318.0,2340.0,22.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5501,Anal,2418.0,2458.0,40.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5490,BlowJob,2606.0,2620.0,14.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5491,BlowJob,2918.0,2936.0,18.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5492,BlowJob,3024.0,3042.0,18.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5502,Anal,3128.0,3144.0,16.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5504,Anal,3534.0,3568.0,34.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5505,Anal,3610.0,3646.0,36.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5471,Grabbing Boobs,3736.0,3768.0,32.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5506,Anal,3908.0,3968.0,60.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5507,Anal,4066.0,4082.0,16.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5508,Anal,4394.0,4414.0,20.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5510,Pissing,4450.0,4460.0,10.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5482,Gangbang,4522.0,4532.0,10.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5520,BlowJob,360.0,388.0,28.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5521,BlowJob,600.0,610.0,10.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5522,BlowJob,1032.0,1048.0,16.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5523,BlowJob,1294.0,1342.0,48.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5531,Pissing,1442.0,1450.0,8.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5514,Gangbang,1878.0,1932.0,54.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5515,Gangbang,2034.0,2058.0,24.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5533,Cumshot,2278.0,2312.0,34.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5532,Pissing,2296.0,2310.0,14.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5526,Anal,2648.0,2664.0,16.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5516,Gangbang,2742.0,2754.0,12.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5524,BlowJob,2882.0,2920.0,38.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5517,Gangbang,3088.0,3118.0,30.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5534,Cumshot,3824.0,3830.0,6.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5519,Grabbing Boobs,3838.0,3868.0,30.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5535,Cumshot,3900.0,3946.0,46.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5530,Anal,4144.0,4154.0,10.0
491,Athenea Rose in Hardcore Interracial DAP,5441,BlowJob,790.0,806.0,16.0
491,Athenea Rose in Hardcore Interracial DAP,5442,BlowJob,2560.0,2600.0,40.0
491,Athenea Rose in Hardcore Interracial DAP,5443,Cumshot,3038.0,3054.0,16.0
492,Athenea Rose in Hecho en Medelln,5358,Anal,1758.0,1772.0,14.0
492,Athenea Rose in Hecho en Medelln,5361,Anal,2516.0,2542.0,26.0
492,Athenea Rose in Hecho en Medelln,5362,Anal,2578.0,2624.0,46.0
492,Athenea Rose in Hecho en Medelln,5354,BlowJob,2790.0,2812.0,22.0
492,Athenea Rose in Hecho en Medelln,5364,Anal,2842.0,2860.0,18.0
492,Athenea Rose in Hecho en Medelln,5355,BlowJob,3158.0,3194.0,36.0
492,Athenea Rose in Hecho en Medelln,5366,Anal,3274.0,3290.0,16.0
492,Athenea Rose in Hecho en Medelln,5367,Anal,3342.0,3364.0,22.0
493,Athenea Rose in Intense Anal Destruction,5461,BlowJob,152.0,164.0,12.0
493,Athenea Rose in Intense Anal Destruction,5463,Anal,422.0,462.0,40.0
493,Athenea Rose in Intense Anal Destruction,5464,Anal,498.0,532.0,34.0
493,Athenea Rose in Intense Anal Destruction,5465,Anal,660.0,688.0,28.0
493,Athenea Rose in Intense Anal Destruction,5468,Anal,1412.0,1446.0,34.0
493,Athenea Rose in Intense Anal Destruction,5469,Anal,1534.0,1586.0,52.0
493,Athenea Rose in Intense Anal Destruction,5462,BlowJob,1760.0,1792.0,32.0
494,Athenea Rose in Loves Public Anal,5536,BlowJob,628.0,642.0,14.0
494,Athenea Rose in Loves Public Anal,5538,Anal,1008.0,1030.0,22.0
494,Athenea Rose in Loves Public Anal,5539,Anal,1276.0,1304.0,28.0
494,Athenea Rose in Loves Public Anal,5537,BlowJob,1734.0,1764.0,30.0
494,Athenea Rose in Loves Public Anal,5540,Anal,1916.0,1964.0,48.0
494,Athenea Rose in Loves Public Anal,5541,Anal,1998.0,2024.0,26.0
494,Athenea Rose in Loves Public Anal,5542,Anal,2152.0,2168.0,16.0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,5547,Anal,672.0,714.0,42.0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,5548,Anal,858.0,912.0,54.0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,5550,Anal,1146.0,1174.0,28.0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,5551,Anal,1320.0,1334.0,14.0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,5552,Anal,1376.0,1404.0,28.0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,5553,Anal,1688.0,1704.0,16.0
496,Athenea Rose in Playing with 3 BBC,5560,Anal,660.0,700.0,40.0
496,Athenea Rose in Playing with 3 BBC,5561,Anal,746.0,790.0,44.0
496,Athenea Rose in Playing with 3 BBC,5555,BlowJob,844.0,896.0,52.0
496,Athenea Rose in Playing with 3 BBC,5556,BlowJob,932.0,960.0,28.0
496,Athenea Rose in Playing with 3 BBC,5563,Anal,1578.0,1626.0,48.0
496,Athenea Rose in Playing with 3 BBC,5557,BlowJob,1666.0,1702.0,36.0
496,Athenea Rose in Playing with 3 BBC,5558,BlowJob,1750.0,1796.0,46.0
497,Athenea Rose in PremiumBukkake #1,5570,BlowJob,582.0,604.0,22.0
497,Athenea Rose in PremiumBukkake #1,5573,Cumshot,592.0,602.0,10.0
497,Athenea Rose in PremiumBukkake #1,5574,Cumshot,646.0,674.0,28.0
497,Athenea Rose in PremiumBukkake #1,5580,Cumshot,712.0,756.0,44.0
497,Athenea Rose in PremiumBukkake #1,5575,Cumshot,766.0,820.0,54.0
497,Athenea Rose in PremiumBukkake #1,5576,Cumshot,846.0,896.0,50.0
497,Athenea Rose in PremiumBukkake #1,5571,BlowJob,884.0,888.0,4.0
497,Athenea Rose in PremiumBukkake #1,5583,Cumshot,932.0,964.0,32.0
497,Athenea Rose in PremiumBukkake #1,5584,Cumshot,1080.0,1132.0,52.0
497,Athenea Rose in PremiumBukkake #1,5585,Cumshot,1220.0,1250.0,30.0
499,Athenea Rose in PremiumBukkake #3,5594,BlowJob,12.0,16.0,4.0
499,Athenea Rose in PremiumBukkake #3,5598,BlowJob,552.0,580.0,28.0
500,Athenea Rose in Sex Crazed Slut 4on1,5613,Grabbing Boobs,60.0,104.0,44.0
500,Athenea Rose in Sex Crazed Slut 4on1,5614,Grabbing Boobs,152.0,172.0,20.0
500,Athenea Rose in Sex Crazed Slut 4on1,5621,Cumshot,678.0,686.0,8.0
500,Athenea Rose in Sex Crazed Slut 4on1,5615,Grabbing Boobs,922.0,944.0,22.0
500,Athenea Rose in Sex Crazed Slut 4on1,5618,BlowJob,1528.0,1550.0,22.0
500,Athenea Rose in Sex Crazed Slut 4on1,5622,Pissing,1870.0,1896.0,26.0
500,Athenea Rose in Sex Crazed Slut 4on1,5619,BlowJob,2648.0,2662.0,14.0
500,Athenea Rose in Sex Crazed Slut 4on1,5620,BlowJob,2692.0,2696.0,4.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5623,Grabbing Boobs,156.0,188.0,32.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5627,Gangbang,294.0,342.0,48.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5635,BlowJob,356.0,396.0,40.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5639,Anal,560.0,596.0,36.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5628,Gangbang,642.0,670.0,28.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5640,Anal,670.0,680.0,10.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5641,Anal,788.0,848.0,60.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5642,Anal,956.0,978.0,22.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5636,BlowJob,970.0,996.0,26.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5624,Grabbing Boobs,1340.0,1364.0,24.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5629,Gangbang,1356.0,1384.0,28.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5630,Gangbang,1524.0,1534.0,10.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5631,Gangbang,1672.0,1696.0,24.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5632,Gangbang,1774.0,1790.0,16.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5633,Gangbang,1836.0,1868.0,32.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5644,Anal,1850.0,1872.0,22.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5638,BlowJob,1980.0,2014.0,34.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5645,Anal,1984.0,2020.0,36.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5650,DP,2038.0,2042.0,4.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5646,Anal,2078.0,2138.0,60.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5651,DP,2102.0,2106.0,4.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5647,Anal,2180.0,2208.0,28.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5634,Gangbang,2368.0,2394.0,26.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5652,Pissing,2406.0,2414.0,8.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5653,Cumshot,2448.0,2490.0,42.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5655,Cumshot,3302.0,3314.0,12.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5625,Grabbing Boobs,3700.0,3730.0,30.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5626,Grabbing Boobs,3774.0,3792.0,18.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5658,Anal,378.0,394.0,16.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5659,Anal,1350.0,1384.0,34.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5669,BlowJob,1376.0,1400.0,24.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5670,BlowJob,1462.0,1472.0,10.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5660,Anal,1516.0,1562.0,46.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5661,Anal,1612.0,1636.0,24.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5662,Anal,1732.0,1786.0,54.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5663,Anal,1980.0,1992.0,12.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5664,Anal,2058.0,2070.0,12.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5666,Anal,2464.0,2488.0,24.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5671,BlowJob,2908.0,2946.0,38.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5672,Grabbing Boobs,212.0,230.0,18.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5673,BlowJob,340.0,394.0,54.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5675,BlowJob,782.0,798.0,16.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5690,Gangbang,798.0,808.0,10.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5676,BlowJob,842.0,870.0,28.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5691,Gangbang,876.0,922.0,46.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5692,Gangbang,962.0,994.0,32.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5677,BlowJob,966.0,996.0,30.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5693,Gangbang,1134.0,1162.0,28.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5694,Gangbang,1840.0,1850.0,10.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5683,Anal,2876.0,2900.0,24.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5684,Anal,3208.0,3238.0,30.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5678,BlowJob,3234.0,3260.0,26.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5685,Anal,3306.0,3350.0,44.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5679,BlowJob,3430.0,3470.0,40.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5695,Gangbang,3432.0,3444.0,12.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5686,Anal,3438.0,3458.0,20.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5680,BlowJob,3544.0,3576.0,32.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5687,Anal,3606.0,3624.0,18.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5688,Anal,3730.0,3740.0,10.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5689,Anal,3990.0,4006.0,16.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5696,Gangbang,4118.0,4146.0,28.0
504,Aubrey Black in GangBang Creampie 225,5700,BlowJob,324.0,374.0,50.0
504,Aubrey Black in GangBang Creampie 225,5698,Grabbing Boobs,446.0,486.0,40.0
504,Aubrey Black in GangBang Creampie 225,5699,Grabbing Boobs,672.0,692.0,20.0
504,Aubrey Black in GangBang Creampie 225,5701,BlowJob,982.0,1036.0,54.0
504,Aubrey Black in GangBang Creampie 225,5703,Cumshot,1474.0,1498.0,24.0
504,Aubrey Black in GangBang Creampie 225,5704,Cumshot,1560.0,1564.0,4.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5707,BlowJob,194.0,212.0,18.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5708,BlowJob,282.0,334.0,52.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5717,Cumshot,1094.0,1136.0,42.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5710,BlowJob,1098.0,1110.0,12.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5720,Gangbang,1130.0,1152.0,22.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5721,Gangbang,1184.0,1218.0,34.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5711,BlowJob,1268.0,1280.0,12.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5722,Gangbang,1366.0,1398.0,32.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5712,BlowJob,2386.0,2430.0,44.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5724,Gangbang,2628.0,2648.0,20.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5706,Grabbing Boobs,4086.0,4108.0,22.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5726,Gangbang,432.0,480.0,48.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5725,Grabbing Boobs,434.0,472.0,38.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5733,BlowJob,532.0,548.0,16.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5734,BlowJob,670.0,712.0,42.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5739,Anal,718.0,740.0,22.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5735,BlowJob,1090.0,1118.0,28.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5741,Cumshot,1092.0,1098.0,6.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5728,Gangbang,1610.0,1662.0,52.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5743,DP,1886.0,1916.0,30.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5729,Gangbang,1972.0,2024.0,52.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5736,BlowJob,2278.0,2290.0,12.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5737,BlowJob,3060.0,3082.0,22.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5742,Cumshot,3082.0,3092.0,10.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5731,Gangbang,3408.0,3418.0,10.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5738,BlowJob,3428.0,3440.0,12.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5732,Gangbang,3462.0,3476.0,14.0
507,Avery Jane in Milking Mike Adriano,5744,Grabbing Boobs,58.0,92.0,34.0
507,Avery Jane in Milking Mike Adriano,5747,Titjob,356.0,370.0,14.0
507,Avery Jane in Milking Mike Adriano,5748,Cumshot,674.0,686.0,12.0
507,Avery Jane in Milking Mike Adriano,5749,Cumshot,778.0,814.0,36.0
507,Avery Jane in Milking Mike Adriano,5752,Anal,1014.0,1064.0,50.0
507,Avery Jane in Milking Mike Adriano,5754,Anal,1236.0,1262.0,26.0
507,Avery Jane in Milking Mike Adriano,5755,Anal,1302.0,1322.0,20.0
507,Avery Jane in Milking Mike Adriano,5757,Anal,1904.0,1952.0,48.0
507,Avery Jane in Milking Mike Adriano,5758,Anal,2004.0,2042.0,38.0
507,Avery Jane in Milking Mike Adriano,5759,Anal,2158.0,2172.0,14.0
507,Avery Jane in Milking Mike Adriano,5760,Anal,2222.0,2254.0,32.0
508,Avery Jane in Piss Soaked Backdoor Debut,5761,Grabbing Boobs,84.0,110.0,26.0
508,Avery Jane in Piss Soaked Backdoor Debut,5762,Grabbing Boobs,182.0,212.0,30.0
508,Avery Jane in Piss Soaked Backdoor Debut,5776,Gangbang,1268.0,1314.0,46.0
508,Avery Jane in Piss Soaked Backdoor Debut,5791,Pissing,1414.0,1464.0,50.0
508,Avery Jane in Piss Soaked Backdoor Debut,5782,BlowJob,1504.0,1514.0,10.0
508,Avery Jane in Piss Soaked Backdoor Debut,5783,BlowJob,1582.0,1642.0,60.0
508,Avery Jane in Piss Soaked Backdoor Debut,5792,Cumshot,1586.0,1612.0,26.0
508,Avery Jane in Piss Soaked Backdoor Debut,5784,BlowJob,1680.0,1702.0,22.0
508,Avery Jane in Piss Soaked Backdoor Debut,5797,DP,1744.0,1748.0,4.0
508,Avery Jane in Piss Soaked Backdoor Debut,5765,Anal,1764.0,1784.0,20.0
508,Avery Jane in Piss Soaked Backdoor Debut,5777,Gangbang,1768.0,1790.0,22.0
508,Avery Jane in Piss Soaked Backdoor Debut,5766,Anal,1840.0,1850.0,10.0
508,Avery Jane in Piss Soaked Backdoor Debut,5767,Anal,1890.0,1914.0,24.0
508,Avery Jane in Piss Soaked Backdoor Debut,5768,Anal,1990.0,2000.0,10.0
508,Avery Jane in Piss Soaked Backdoor Debut,5769,Anal,2046.0,2092.0,46.0
508,Avery Jane in Piss Soaked Backdoor Debut,5770,Anal,2440.0,2470.0,30.0
508,Avery Jane in Piss Soaked Backdoor Debut,5786,BlowJob,2514.0,2528.0,14.0
508,Avery Jane in Piss Soaked Backdoor Debut,5771,Anal,2516.0,2546.0,30.0
508,Avery Jane in Piss Soaked Backdoor Debut,5778,Gangbang,2520.0,2556.0,36.0
508,Avery Jane in Piss Soaked Backdoor Debut,5772,Anal,2596.0,2646.0,50.0
508,Avery Jane in Piss Soaked Backdoor Debut,5787,BlowJob,2792.0,2824.0,32.0
508,Avery Jane in Piss Soaked Backdoor Debut,5779,Gangbang,2902.0,2920.0,18.0
508,Avery Jane in Piss Soaked Backdoor Debut,5788,BlowJob,2906.0,2926.0,20.0
508,Avery Jane in Piss Soaked Backdoor Debut,5789,BlowJob,3004.0,3040.0,36.0
508,Avery Jane in Piss Soaked Backdoor Debut,5793,Cumshot,3030.0,3062.0,32.0
508,Avery Jane in Piss Soaked Backdoor Debut,5780,Gangbang,3034.0,3052.0,18.0
508,Avery Jane in Piss Soaked Backdoor Debut,5774,Anal,3388.0,3414.0,26.0
508,Avery Jane in Piss Soaked Backdoor Debut,5775,Anal,3496.0,3520.0,24.0
508,Avery Jane in Piss Soaked Backdoor Debut,5794,Cumshot,3722.0,3730.0,8.0
508,Avery Jane in Piss Soaked Backdoor Debut,5795,Cumshot,3802.0,3856.0,54.0
509,Avi Love in GangBang Creampie 216,5798,BlowJob,138.0,154.0,16.0
509,Avi Love in GangBang Creampie 216,5799,BlowJob,258.0,270.0,12.0
509,Avi Love in GangBang Creampie 216,5800,BlowJob,418.0,468.0,50.0
509,Avi Love in GangBang Creampie 216,5801,BlowJob,640.0,652.0,12.0
509,Avi Love in GangBang Creampie 216,5802,BlowJob,844.0,894.0,50.0
509,Avi Love in GangBang Creampie 216,5807,Gangbang,920.0,930.0,10.0
509,Avi Love in GangBang Creampie 216,5803,BlowJob,944.0,978.0,34.0
509,Avi Love in GangBang Creampie 216,5804,BlowJob,1016.0,1044.0,28.0
509,Avi Love in GangBang Creampie 216,5808,Gangbang,1036.0,1062.0,26.0
509,Avi Love in GangBang Creampie 216,5809,Grabbing Boobs,1662.0,1686.0,24.0
509,Avi Love in GangBang Creampie 216,5810,Grabbing Boobs,2128.0,2164.0,36.0
509,Avi Love in GangBang Creampie 216,5805,BlowJob,2232.0,2246.0,14.0
509,Avi Love in GangBang Creampie 216,5812,Cumshot,2342.0,2384.0,42.0
509,Avi Love in GangBang Creampie 216,5811,Grabbing Boobs,2620.0,2640.0,20.0
511,Baby Gemini in All About The Booty,5813,BlowJob,322.0,382.0,60.0
511,Baby Gemini in All About The Booty,5814,Cumshot,352.0,384.0,32.0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,5815,Grabbing Boobs,306.0,334.0,28.0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,5818,Cumshot,1254.0,1260.0,6.0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,5820,Cumshot,1708.0,1726.0,18.0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,5821,Cumshot,1836.0,1862.0,26.0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,5828,Cumshot,2202.0,2260.0,58.0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,5829,Cumshot,2312.0,2342.0,30.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5831,Grabbing Boobs,6.0,48.0,42.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5832,Grabbing Boobs,180.0,234.0,54.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5836,Anal,502.0,522.0,20.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5833,Grabbing Boobs,860.0,884.0,24.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5834,Grabbing Boobs,994.0,1020.0,26.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5840,BlowJob,1578.0,1610.0,32.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5835,Grabbing Boobs,3052.0,3072.0,20.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5838,Anal,3122.0,3166.0,44.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5841,BlowJob,310.0,330.0,20.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5850,Gangbang,484.0,516.0,32.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5842,BlowJob,490.0,530.0,40.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5843,BlowJob,582.0,634.0,52.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5844,BlowJob,732.0,746.0,14.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5851,Gangbang,1118.0,1158.0,40.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5845,BlowJob,1658.0,1678.0,20.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5852,Gangbang,2044.0,2076.0,32.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5846,BlowJob,2180.0,2224.0,44.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5848,BlowJob,2392.0,2404.0,12.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5847,BlowJob,2642.0,2674.0,32.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5855,Cumshot,2896.0,2904.0,8.0
515,Barbie Sins in Barbie Gets wet with 2 BBC,5857,BlowJob,232.0,260.0,28.0
515,Barbie Sins in Barbie Gets wet with 2 BBC,5858,BlowJob,292.0,308.0,16.0
515,Barbie Sins in Barbie Gets wet with 2 BBC,5860,BlowJob,696.0,736.0,40.0
515,Barbie Sins in Barbie Gets wet with 2 BBC,5861,BlowJob,1662.0,1692.0,30.0
516,Barbie Sins in Creampie and Swallow Showdown,5145,BlowJob,248.0,266.0,18.0
516,Barbie Sins in Creampie and Swallow Showdown,5146,BlowJob,298.0,356.0,58.0
516,Barbie Sins in Creampie and Swallow Showdown,5147,BlowJob,664.0,716.0,52.0
516,Barbie Sins in Creampie and Swallow Showdown,5148,BlowJob,1046.0,1070.0,24.0
516,Barbie Sins in Creampie and Swallow Showdown,5149,BlowJob,1534.0,1580.0,46.0
517,"Barbie Sins in DAP, Piss and Power Play",5864,BlowJob,716.0,774.0,58.0
517,"Barbie Sins in DAP, Piss and Power Play",5868,Gangbang,790.0,836.0,46.0
517,"Barbie Sins in DAP, Piss and Power Play",5869,Cumshot,992.0,1002.0,10.0
517,"Barbie Sins in DAP, Piss and Power Play",5865,BlowJob,1234.0,1262.0,28.0
517,"Barbie Sins in DAP, Piss and Power Play",5867,BlowJob,1732.0,1744.0,12.0
517,"Barbie Sins in DAP, Piss and Power Play",5866,BlowJob,2490.0,2538.0,48.0
517,"Barbie Sins in DAP, Piss and Power Play",5871,Cumshot,3022.0,3034.0,12.0
517,"Barbie Sins in DAP, Piss and Power Play",5872,Cumshot,3112.0,3116.0,4.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5873,Grabbing Boobs,36.0,70.0,34.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5874,BlowJob,626.0,648.0,22.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5888,Gangbang,746.0,764.0,18.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5877,BlowJob,908.0,928.0,20.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5878,BlowJob,960.0,1000.0,40.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5879,BlowJob,1040.0,1052.0,12.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5880,BlowJob,1172.0,1202.0,30.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5881,BlowJob,1292.0,1302.0,10.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5882,BlowJob,1374.0,1394.0,20.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5883,BlowJob,1478.0,1502.0,24.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5885,BlowJob,2392.0,2406.0,14.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5889,Gangbang,2464.0,2510.0,46.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5886,BlowJob,2778.0,2822.0,44.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5887,BlowJob,3030.0,3060.0,30.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5891,Cumshot,3080.0,3084.0,4.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5892,Cumshot,3102.0,3108.0,6.0
519,Barbie Sins in Rough DAP & Swallow Madness,5894,BlowJob,594.0,622.0,28.0
519,Barbie Sins in Rough DAP & Swallow Madness,5895,BlowJob,904.0,938.0,34.0
519,Barbie Sins in Rough DAP & Swallow Madness,5903,Gangbang,994.0,1026.0,32.0
519,Barbie Sins in Rough DAP & Swallow Madness,5905,Gangbang,1290.0,1322.0,32.0
519,Barbie Sins in Rough DAP & Swallow Madness,5897,BlowJob,1424.0,1450.0,26.0
519,Barbie Sins in Rough DAP & Swallow Madness,5906,DP,1454.0,1468.0,14.0
519,Barbie Sins in Rough DAP & Swallow Madness,5898,BlowJob,2168.0,2186.0,18.0
519,Barbie Sins in Rough DAP & Swallow Madness,5899,BlowJob,2250.0,2264.0,14.0
519,Barbie Sins in Rough DAP & Swallow Madness,5900,BlowJob,2430.0,2440.0,10.0
519,Barbie Sins in Rough DAP & Swallow Madness,5907,Cumshot,2688.0,2696.0,8.0
519,Barbie Sins in Rough DAP & Swallow Madness,5908,Cumshot,2754.0,2806.0,52.0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",5909,Anal,186.0,196.0,10.0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",5916,BlowJob,336.0,350.0,14.0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",5917,BlowJob,696.0,720.0,24.0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",5918,BlowJob,916.0,950.0,34.0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",5913,Anal,1194.0,1210.0,16.0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",5914,Anal,1304.0,1314.0,10.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5369,Grabbing Boobs,538.0,582.0,44.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5370,Grabbing Boobs,658.0,698.0,40.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5371,Grabbing Boobs,790.0,822.0,32.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5386,BlowJob,1882.0,1892.0,10.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5387,BlowJob,1944.0,1958.0,14.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5373,Grabbing Boobs,2074.0,2098.0,24.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5388,BlowJob,2110.0,2124.0,14.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5377,BlowJob,2404.0,2428.0,24.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5393,BlowJob,2448.0,2458.0,10.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5378,BlowJob,2796.0,2854.0,58.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5389,BlowJob,2812.0,2830.0,18.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5390,BlowJob,3140.0,3170.0,30.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5380,BlowJob,3212.0,3228.0,16.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5391,BlowJob,3216.0,3222.0,6.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5381,BlowJob,3328.0,3358.0,30.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5374,Grabbing Boobs,3778.0,3814.0,36.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5383,BlowJob,3846.0,3900.0,54.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5392,BlowJob,3864.0,3870.0,6.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5384,BlowJob,4162.0,4192.0,30.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5375,Grabbing Boobs,4192.0,4246.0,54.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5394,Cumshot,4202.0,4230.0,28.0
557,Candela X in There is a Star Around the Gape!,5075,BlowJob,504.0,546.0,42.0
557,Candela X in There is a Star Around the Gape!,5079,Gangbang,520.0,574.0,54.0
557,Candela X in There is a Star Around the Gape!,5076,BlowJob,712.0,754.0,42.0
557,Candela X in There is a Star Around the Gape!,5077,BlowJob,814.0,840.0,26.0
557,Candela X in There is a Star Around the Gape!,5078,BlowJob,972.0,984.0,12.0
557,Candela X in There is a Star Around the Gape!,5081,Gangbang,976.0,1018.0,42.0
557,Candela X in There is a Star Around the Gape!,5082,Gangbang,1508.0,1536.0,28.0
557,Candela X in There is a Star Around the Gape!,5083,Gangbang,1702.0,1712.0,10.0
557,Candela X in There is a Star Around the Gape!,5084,Gangbang,1754.0,1784.0,30.0
557,Candela X in There is a Star Around the Gape!,5086,Cumshot,2642.0,2654.0,12.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4811,BlowJob,562.0,588.0,26.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4813,BlowJob,702.0,728.0,26.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4815,BlowJob,1836.0,1850.0,14.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4817,BlowJob,2468.0,2480.0,12.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4830,69,2600.0,2612.0,12.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4826,Anal,2660.0,2672.0,12.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4827,Anal,2728.0,2772.0,44.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4818,BlowJob,2830.0,2870.0,40.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4828,Anal,2922.0,2940.0,18.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4819,BlowJob,2946.0,2958.0,12.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4829,Anal,2978.0,3006.0,28.0
575,Chessie Kay in Double Team During Match,4866,BlowJob,88.0,134.0,46.0
575,Chessie Kay in Double Team During Match,4867,BlowJob,242.0,250.0,8.0
575,Chessie Kay in Double Team During Match,4863,BlowJob,384.0,394.0,10.0
575,Chessie Kay in Double Team During Match,4864,BlowJob,526.0,572.0,46.0
575,Chessie Kay in Double Team During Match,4868,BlowJob,556.0,570.0,14.0
575,Chessie Kay in Double Team During Match,4870,Grabbing Boobs,680.0,706.0,26.0
575,Chessie Kay in Double Team During Match,4872,Anal,740.0,794.0,54.0
575,Chessie Kay in Double Team During Match,4869,BlowJob,1008.0,1026.0,18.0
575,Chessie Kay in Double Team During Match,4871,Grabbing Boobs,1500.0,1528.0,28.0
575,Chessie Kay in Double Team During Match,4873,Cumshot,1762.0,1780.0,18.0
575,Chessie Kay in Double Team During Match,4874,Cumshot,1852.0,1880.0,28.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5087,Grabbing Boobs,200.0,244.0,44.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5089,BlowJob,302.0,308.0,6.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5090,BlowJob,354.0,370.0,16.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5091,BlowJob,428.0,470.0,42.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5092,BlowJob,522.0,556.0,34.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5093,Titjob,622.0,644.0,22.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5094,Cumshot,1126.0,1136.0,10.0
726,Jolee Love in Rough Squirting and Pee Play,5098,BlowJob,230.0,246.0,16.0
726,Jolee Love in Rough Squirting and Pee Play,5099,BlowJob,282.0,332.0,50.0
726,Jolee Love in Rough Squirting and Pee Play,5106,Pissing,368.0,392.0,24.0
726,Jolee Love in Rough Squirting and Pee Play,5108,Cumshot,508.0,536.0,28.0
726,Jolee Love in Rough Squirting and Pee Play,5100,BlowJob,522.0,550.0,28.0
726,Jolee Love in Rough Squirting and Pee Play,5101,BlowJob,1286.0,1304.0,18.0
726,Jolee Love in Rough Squirting and Pee Play,5102,BlowJob,1408.0,1436.0,28.0
726,Jolee Love in Rough Squirting and Pee Play,5095,Gangbang,1754.0,1794.0,40.0
726,Jolee Love in Rough Squirting and Pee Play,5103,BlowJob,1830.0,1870.0,40.0
726,Jolee Love in Rough Squirting and Pee Play,5109,Cumshot,2014.0,2022.0,8.0
726,Jolee Love in Rough Squirting and Pee Play,5096,Gangbang,2200.0,2234.0,34.0
726,Jolee Love in Rough Squirting and Pee Play,5097,Gangbang,2634.0,2654.0,20.0
726,Jolee Love in Rough Squirting and Pee Play,5116,Grabbing Boobs,2740.0,2770.0,30.0
726,Jolee Love in Rough Squirting and Pee Play,5104,BlowJob,3020.0,3032.0,12.0
726,Jolee Love in Rough Squirting and Pee Play,5105,BlowJob,3134.0,3164.0,30.0
726,Jolee Love in Rough Squirting and Pee Play,5115,Anal,3318.0,3344.0,26.0
726,Jolee Love in Rough Squirting and Pee Play,5110,Cumshot,3414.0,3448.0,34.0
726,Jolee Love in Rough Squirting and Pee Play,5107,Pissing,3516.0,3534.0,18.0
726,Jolee Love in Rough Squirting and Pee Play,5111,Cumshot,3518.0,3542.0,24.0
726,Jolee Love in Rough Squirting and Pee Play,5117,Grabbing Boobs,3550.0,3570.0,20.0
726,Jolee Love in Rough Squirting and Pee Play,5112,Cumshot,3640.0,3644.0,4.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4832,Grabbing Boobs,210.0,268.0,58.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4833,Grabbing Boobs,334.0,374.0,40.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4835,Gangbang,378.0,404.0,26.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4836,Gangbang,498.0,520.0,22.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4840,BlowJob,652.0,682.0,30.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4841,BlowJob,748.0,758.0,10.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4842,BlowJob,826.0,860.0,34.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4834,Grabbing Boobs,1042.0,1098.0,56.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4843,BlowJob,1046.0,1058.0,12.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4837,Gangbang,1158.0,1170.0,12.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4851,Anal,1344.0,1386.0,42.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4844,BlowJob,1540.0,1574.0,34.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4853,Anal,1674.0,1726.0,52.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4854,Anal,1782.0,1808.0,26.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4845,BlowJob,1818.0,1866.0,48.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4838,Gangbang,2062.0,2078.0,16.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4856,Anal,2116.0,2128.0,12.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4857,Anal,2166.0,2184.0,18.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4848,BlowJob,2692.0,2708.0,16.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4849,BlowJob,3108.0,3126.0,18.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5029,BlowJob,250.0,262.0,12.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5031,BlowJob,720.0,772.0,52.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5032,BlowJob,872.0,912.0,40.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5033,BlowJob,946.0,958.0,12.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5034,BlowJob,1060.0,1076.0,16.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5036,Anal,1084.0,1108.0,24.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5039,Cumshot,1398.0,1426.0,28.0
815,May Thai in Asian Beauty's Intense DP Encounter,4996,Grabbing Boobs,526.0,546.0,20.0
815,May Thai in Asian Beauty's Intense DP Encounter,4997,Grabbing Boobs,582.0,630.0,48.0
815,May Thai in Asian Beauty's Intense DP Encounter,4999,Grabbing Boobs,908.0,960.0,52.0
815,May Thai in Asian Beauty's Intense DP Encounter,5001,BlowJob,1246.0,1260.0,14.0
815,May Thai in Asian Beauty's Intense DP Encounter,5002,BlowJob,1398.0,1408.0,10.0
815,May Thai in Asian Beauty's Intense DP Encounter,5003,BlowJob,1458.0,1500.0,42.0
815,May Thai in Asian Beauty's Intense DP Encounter,5005,Anal,1530.0,1544.0,14.0
815,May Thai in Asian Beauty's Intense DP Encounter,5004,BlowJob,1600.0,1616.0,16.0
815,May Thai in Asian Beauty's Intense DP Encounter,5000,Grabbing Boobs,1848.0,1872.0,24.0
815,May Thai in Asian Beauty's Intense DP Encounter,5006,Anal,1988.0,2020.0,32.0
815,May Thai in Asian Beauty's Intense DP Encounter,5007,Anal,2164.0,2188.0,24.0
815,May Thai in Asian Beauty's Intense DP Encounter,5009,Cumshot,2536.0,2576.0,40.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5151,BlowJob,86.0,116.0,30.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5158,Gangbang,140.0,152.0,12.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5152,BlowJob,158.0,180.0,22.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5166,Anal,274.0,332.0,58.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5159,Gangbang,278.0,336.0,58.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5167,Anal,424.0,438.0,14.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5160,Gangbang,532.0,558.0,26.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5162,Gangbang,926.0,948.0,22.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5170,Anal,942.0,990.0,48.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5153,BlowJob,1004.0,1032.0,28.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5171,Anal,1172.0,1204.0,32.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5154,BlowJob,1184.0,1220.0,36.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5155,BlowJob,1284.0,1298.0,14.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5172,Anal,1286.0,1326.0,40.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5173,Anal,1414.0,1434.0,20.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5174,Anal,1466.0,1484.0,18.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5163,Gangbang,1758.0,1802.0,44.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5164,Gangbang,1844.0,1880.0,36.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5177,Anal,1974.0,2028.0,54.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5178,Anal,2100.0,2116.0,16.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5179,Anal,2190.0,2248.0,58.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5156,BlowJob,2914.0,2934.0,20.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5181,Cumshot,2958.0,2996.0,38.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5157,BlowJob,2966.0,2992.0,26.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4773,Cumshot,272.0,288.0,16.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4774,Cumshot,336.0,342.0,6.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4762,BlowJob,1010.0,1034.0,24.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4763,BlowJob,1104.0,1114.0,10.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4768,BlowJob,1328.0,1342.0,14.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4775,Cumshot,1374.0,1398.0,24.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4777,Cumshot,1440.0,1482.0,42.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4769,BlowJob,1982.0,1988.0,6.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4770,BlowJob,2194.0,2208.0,14.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4771,BlowJob,2282.0,2320.0,38.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4765,BlowJob,2524.0,2538.0,14.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4766,BlowJob,2596.0,2608.0,12.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4772,BlowJob,2622.0,2656.0,34.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4779,Cumshot,2626.0,2644.0,18.0
931,"Sarai Minx, Savanah Storm in Messy Manicure Threesome",4785,BlowJob,318.0,338.0,20.0
931,"Sarai Minx, Savanah Storm in Messy Manicure Threesome",4786,BlowJob,424.0,434.0,10.0
931,"Sarai Minx, Savanah Storm in Messy Manicure Threesome",4787,BlowJob,576.0,602.0,26.0
931,"Sarai Minx, Savanah Storm in Messy Manicure Threesome",4789,Grabbing Boobs,1328.0,1350.0,22.0
946,Shalina Devine in Housewife's Dirty Double Plan,4965,Grabbing Boobs,408.0,430.0,22.0
946,Shalina Devine in Housewife's Dirty Double Plan,4966,Grabbing Boobs,464.0,494.0,30.0
946,Shalina Devine in Housewife's Dirty Double Plan,4968,BlowJob,1608.0,1628.0,20.0
946,Shalina Devine in Housewife's Dirty Double Plan,4969,BlowJob,1746.0,1798.0,52.0
946,Shalina Devine in Housewife's Dirty Double Plan,4970,BlowJob,2076.0,2086.0,10.0
946,Shalina Devine in Housewife's Dirty Double Plan,4973,BlowJob,2258.0,2290.0,32.0
946,Shalina Devine in Housewife's Dirty Double Plan,4967,Grabbing Boobs,2528.0,2556.0,28.0
946,Shalina Devine in Housewife's Dirty Double Plan,4972,BlowJob,2650.0,2694.0,44.0
946,Shalina Devine in Housewife's Dirty Double Plan,4974,BlowJob,2870.0,2876.0,6.0
946,Shalina Devine in Housewife's Dirty Double Plan,4976,Cumshot,2886.0,2892.0,6.0
946,Shalina Devine in Housewife's Dirty Double Plan,4977,Cumshot,2926.0,2946.0,20.0
1008,"Sai Tai Tiger, Salma De Nora in Die Haremsw√§chterin des √ñl Scheichs Sc4",96,BlowJob,240.0,252.0,12.0
1020,,4792,Anal,536.0,546.0,10.0
1020,,4802,BlowJob,856.0,870.0,14.0
1020,,4803,BlowJob,910.0,924.0,14.0
1020,,4804,BlowJob,1002.0,1036.0,34.0
1020,,4793,Anal,1006.0,1044.0,38.0
1020,,4805,BlowJob,1222.0,1236.0,14.0
1020,,4795,Anal,1410.0,1434.0,24.0
1020,,4806,BlowJob,1438.0,1458.0,20.0
1020,,4808,BlowJob,2086.0,2096.0,10.0
1020,,4798,Anal,2128.0,2140.0,12.0
1020,,4799,Anal,2224.0,2236.0,12.0
1020,,4809,BlowJob,2610.0,2622.0,12.0
1023,Sai Tai Tiger in Die Haremsw√§chterin die Verbannung Sc3,67,BlowJob,432.0,450.0,18.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3190,Grabbing Boobs,6.0,54.0,48.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3191,BlowJob,142.0,170.0,28.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3192,BlowJob,406.0,424.0,18.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3193,BlowJob,864.0,874.0,10.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3194,BlowJob,1106.0,1120.0,14.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3195,BlowJob,1468.0,1504.0,36.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3199,Anal,1540.0,1574.0,34.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3200,Anal,1612.0,1660.0,48.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",4919,Gangbang,1620.0,1642.0,22.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3196,BlowJob,1632.0,1666.0,34.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3197,BlowJob,1678.0,1692.0,14.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3201,Cumshot,1788.0,1826.0,38.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,4952,Grabbing Boobs,108.0,160.0,52.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,732,BlowJob,178.0,202.0,24.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,740,Gangbang,232.0,260.0,28.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,4960,Gangbang,238.0,260.0,22.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,734,BlowJob,426.0,450.0,24.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,741,Gangbang,472.0,486.0,14.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,735,BlowJob,496.0,530.0,34.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,742,Gangbang,668.0,694.0,26.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,730,Grabbing Boobs,814.0,866.0,52.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,4961,Gangbang,1000.0,1022.0,22.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,737,BlowJob,1130.0,1142.0,12.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,738,BlowJob,1220.0,1254.0,34.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,739,BlowJob,1222.0,1252.0,30.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,731,Grabbing Boobs,1386.0,1408.0,22.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,4959,BlowJob,1402.0,1406.0,4.0
1030,Sai Tai Tiger in Die Haremsw√§chterin die Verbannung Sc2,87,BlowJob,290.0,306.0,16.0
1030,Sai Tai Tiger in Die Haremsw√§chterin die Verbannung Sc2,89,BlowJob,692.0,710.0,18.0
1030,Sai Tai Tiger in Die Haremsw√§chterin die Verbannung Sc2,90,BlowJob,858.0,918.0,60.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4979,BlowJob,620.0,654.0,34.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4987,Anal,642.0,690.0,48.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4995,Anal,1014.0,1032.0,18.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4988,Anal,1082.0,1122.0,40.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4981,BlowJob,1134.0,1146.0,12.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4989,Anal,1162.0,1196.0,34.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4990,Anal,1232.0,1270.0,38.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4982,BlowJob,1456.0,1510.0,54.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4983,BlowJob,1644.0,1704.0,60.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4992,Anal,1752.0,1768.0,16.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4993,Anal,1826.0,1844.0,18.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4994,Anal,1976.0,1996.0,20.0
1036,Dana Dearmond in MILF Cumsluts Sc2,197,Grabbing Boobs,284.0,302.0,18.0
1036,Dana Dearmond in MILF Cumsluts Sc2,5043,BlowJob,368.0,378.0,10.0
1036,Dana Dearmond in MILF Cumsluts Sc2,210,BlowJob,654.0,670.0,16.0
1036,Dana Dearmond in MILF Cumsluts Sc2,199,Anal,698.0,710.0,12.0
1036,Dana Dearmond in MILF Cumsluts Sc2,200,Anal,750.0,810.0,60.0
1036,Dana Dearmond in MILF Cumsluts Sc2,201,Anal,842.0,854.0,12.0
1036,Dana Dearmond in MILF Cumsluts Sc2,203,Anal,1114.0,1132.0,18.0
1036,Dana Dearmond in MILF Cumsluts Sc2,204,Anal,1178.0,1198.0,20.0
1036,Dana Dearmond in MILF Cumsluts Sc2,5042,Grabbing Boobs,1192.0,1216.0,24.0
1036,Dana Dearmond in MILF Cumsluts Sc2,211,BlowJob,1422.0,1452.0,30.0
1036,Dana Dearmond in MILF Cumsluts Sc2,207,Anal,1788.0,1842.0,54.0
1036,Dana Dearmond in MILF Cumsluts Sc2,212,DP,1930.0,1954.0,24.0
1036,Dana Dearmond in MILF Cumsluts Sc2,208,Anal,2002.0,2052.0,50.0
1038,Alina Li in Asian Fuck Faces 3 Sc6,5073,Cumshot,1158.0,1204.0,46.0
1043,"Anni Star, CJ Miles in Glamorous Double Penetration",5136,Grabbing Boobs,254.0,276.0,22.0
1043,"Anni Star, CJ Miles in Glamorous Double Penetration",5201,BlowJob,1248.0,1270.0,22.0
1044,"Ania Kinski, Anissa Kate in Real Estate Gets Real Dirty",3237,BlowJob,836.0,854.0,18.0
1044,"Ania Kinski, Anissa Kate in Real Estate Gets Real Dirty",3238,BlowJob,1284.0,1308.0,24.0
1044,"Ania Kinski, Anissa Kate in Real Estate Gets Real Dirty",3239,BlowJob,1600.0,1606.0,6.0
1047,Ania Kinski in Home Alone Double Penetration,3143,Grabbing Boobs,420.0,440.0,20.0
1047,Ania Kinski in Home Alone Double Penetration,3145,BlowJob,972.0,994.0,22.0
1047,Ania Kinski in Home Alone Double Penetration,3153,Anal,1158.0,1188.0,30.0
1047,Ania Kinski in Home Alone Double Penetration,3146,BlowJob,1202.0,1238.0,36.0
1047,Ania Kinski in Home Alone Double Penetration,3147,BlowJob,1298.0,1314.0,16.0
1047,Ania Kinski in Home Alone Double Penetration,5188,BlowJob,1500.0,1516.0,16.0
1047,Ania Kinski in Home Alone Double Penetration,3154,Anal,1534.0,1548.0,14.0
1047,Ania Kinski in Home Alone Double Penetration,3148,BlowJob,1552.0,1568.0,16.0
1047,Ania Kinski in Home Alone Double Penetration,3149,BlowJob,1604.0,1624.0,20.0
1047,Ania Kinski in Home Alone Double Penetration,3150,BlowJob,1662.0,1710.0,48.0
1047,Ania Kinski in Home Alone Double Penetration,3151,BlowJob,1764.0,1794.0,30.0
1047,Ania Kinski in Home Alone Double Penetration,5196,Anal,1828.0,1844.0,16.0
1047,Ania Kinski in Home Alone Double Penetration,3156,DP,2168.0,2176.0,8.0
1047,Ania Kinski in Home Alone Double Penetration,3144,Grabbing Boobs,2520.0,2572.0,52.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3379,Grabbing Boobs,210.0,264.0,54.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3380,BlowJob,532.0,572.0,40.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3383,BlowJob,930.0,966.0,36.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3384,BlowJob,1062.0,1100.0,38.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3386,Anal,1266.0,1286.0,20.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3387,Anal,1434.0,1448.0,14.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3385,BlowJob,1452.0,1466.0,14.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3388,Anal,1642.0,1670.0,28.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3389,Anal,1814.0,1824.0,10.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3390,Anal,1872.0,1912.0,40.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3391,Cumshot,1986.0,2024.0,38.0
1060,Nia Nacci in MVP,108,Grabbing Boobs,26.0,62.0,36.0
1060,Nia Nacci in MVP,109,Grabbing Boobs,152.0,210.0,58.0
1062,Six Bodies In Motion,5395,BlowJob,196.0,230.0,34.0
1062,Six Bodies In Motion,5396,BlowJob,296.0,334.0,38.0
1062,Six Bodies In Motion,5397,BlowJob,408.0,424.0,16.0
1062,Six Bodies In Motion,5398,BlowJob,480.0,496.0,16.0
1062,Six Bodies In Motion,5399,BlowJob,538.0,554.0,16.0
1062,Six Bodies In Motion,5400,BlowJob,1100.0,1128.0,28.0
1062,Six Bodies In Motion,5403,Gangbang,2396.0,2420.0,24.0
1062,Six Bodies In Motion,5402,BlowJob,2810.0,2830.0,20.0
1063,,5406,BlowJob,238.0,278.0,40.0
1063,,5407,BlowJob,332.0,380.0,48.0
1063,,5410,Anal,532.0,586.0,54.0
1063,,5411,Anal,678.0,736.0,58.0
1063,,5405,Grabbing Boobs,798.0,854.0,56.0
1063,,5408,BlowJob,1042.0,1072.0,30.0
1063,,5412,Anal,1150.0,1170.0,20.0
1063,,5413,Anal,1462.0,1484.0,22.0
1065,,5414,Grabbing Boobs,38.0,58.0,20.0
1065,,5415,Grabbing Boobs,274.0,316.0,42.0
1065,,5422,Cumshot,1008.0,1018.0,10.0
1065,,5419,BlowJob,1242.0,1276.0,34.0
1065,,5426,DP,1816.0,1834.0,18.0
1065,,5420,BlowJob,2034.0,2044.0,10.0
1065,,5421,BlowJob,2094.0,2112.0,18.0
1065,,5417,Anal,2430.0,2476.0,46.0
1068,,5427,BlowJob,122.0,158.0,36.0
1068,,5428,BlowJob,332.0,382.0,50.0
1068,,5430,Anal,346.0,364.0,18.0
1068,,5431,Gangbang,388.0,406.0,18.0
1068,,5432,Cumshot,670.0,692.0,22.0
1068,,5437,Cumshot,806.0,856.0,50.0
1068,,5429,BlowJob,856.0,872.0,16.0
1068,,5438,Cumshot,868.0,900.0,32.0
1068,,5439,Cumshot,950.0,1000.0,50.0


============================================================
[88/124] Legacy\utils\url_cleaner.py
------------------------------------------------------------
import re
import requests
from urllib.parse import urlparse

def normalize_url(url):
    url = url.strip()
    url = re.sub(r'^https?://(www\.)?', '', url.lower())
    return url

def is_valid_url(url):
    if not url.startswith('http'):
        return False
    if 'wikipedia.org' in url and not url.startswith('https://en.wikipedia.org'):
        return False
    blacklist = ['google.com', 'bing.com']
    for b in blacklist:
        if b in url:
            return False
    return True

def check_link_health(url, timeout=2):
    try:
        resp = requests.head(url, timeout=timeout, allow_redirects=True, headers={"User-Agent": "Mozilla/5.0"})
        return resp.status_code < 400
    except Exception:
        return False

def categorize_links(urls, skip_health_check_urls=None):
    if skip_health_check_urls is None:
        skip_health_check_urls = set()
    seen = set()
    valid_final = []
    rejected = []
    for url in urls:
        if not url.strip():
            continue
        norm = normalize_url(url)
        if norm in seen:
            continue
        seen.add(norm)
        if not is_valid_url(url):
            rejected.append(url)
            continue
        if url in skip_health_check_urls:
            valid_final.append(url)
            continue
        if not check_link_health(url):
            rejected.append(url)
            continue
        valid_final.append(url)
    # Tri par priorit√©
    site_domains = ['iafd.com', 'babepedia.com', 'wikidata.org', 'boobpedia.com',
                   'imdb.com', 'themoviedb.org', 'freeones.com', 'thenude.com']
    social_domains = ['twitter.com', 'x.com', 'instagram.com', 'onlyfans.com',
                     'facebook.com', 'tiktok.com', 'twitch.tv', 'youtube.com']
    def domain_priority(url):
        d = urlparse(url).netloc
        for i, dom in enumerate(site_domains):
            if dom in d:
                return i
        if any(s in d for s in social_domains):
            return 100 + sorted(social_domains).index([s for s in social_domains if s in d][0])
        return 99
    valid_final = sorted(valid_final, key=domain_priority)
    return valid_final, rejected


============================================================
[89/124] main.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
StashMaster V2 - Point d'entr√©e principal
S√©lecteur de mode et fen√™tre principale
"""

import tkinter as tk
from tkinter import ttk, messagebox, simpledialog
import sys
import os

# Ajout du r√©pertoire courant au path pour les imports locaux
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from services.config_manager import ConfigManager

# Imports des Frames GUI
from gui.performer_frame import PerformerFrame
from gui.dvd_frame import DVDFrame
from gui.scene_frame import SceneFrame

class SelectorWindow(tk.Tk):
    """Fen√™tre de d√©marrage pour choisir le type d'entit√© √† traiter"""
    
    def __init__(self):
        super().__init__()
        self.config_manager = ConfigManager()
        self.title("StashMaster V2 - S√©lecteur")
        self.geometry("400x300")
        self.resizable(False, False)
        
        self._create_widgets()

    def _create_widgets(self):
        main_frame = ttk.Frame(self, padding=20)
        main_frame.pack(fill=tk.BOTH, expand=True)
        
        ttk.Label(main_frame, text="Que souhaitez-vous traiter ?", font=('Segoe UI', 14, 'bold')).pack(pady=(0, 20))
        
        btn_config = {'width': 30, 'padding': 10}
        
        ttk.Button(main_frame, text="üë§ Performer", command=self._start_performer, **btn_config).pack(pady=5)
        ttk.Button(main_frame, text="üìÄ DVD / Groupe", command=self._start_dvd, **btn_config).pack(pady=5)
        ttk.Button(main_frame, text="üé¨ Sc√®ne", command=self._start_scene, **btn_config).pack(pady=5)
        
        ttk.Label(main_frame, text="V2.0 - Optimis√©e", font=('Segoe UI', 8)).pack(side=tk.BOTTOM, pady=(10, 0))

    def _start_performer(self):
        performer_id = simpledialog.askstring("ID Stash", "Entrez l'ID du Performer (optionnel) :")
        self._launch_main_app("performer", performer_id)

    def _start_dvd(self):
        dvd_id = simpledialog.askstring("ID Stash", "Entrez l'ID du DVD (optionnel) :")
        self._launch_main_app("dvd", dvd_id)

    def _start_scene(self):
        scene_id = simpledialog.askstring("ID Stash", "Entrez l'ID de la Sc√®ne (optionnel) :")
        self._launch_main_app("scene", scene_id)

    def _launch_main_app(self, mode, entity_id):
        self.destroy()
        app = StashMasterApp(mode, entity_id)
        app.mainloop()

class StashMasterApp(tk.Tk):
    """Fen√™tre principale de l'application apr√®s s√©lection du mode"""
    
    def __init__(self, mode, entity_id):
        super().__init__()
        self.mode = mode
        self.entity_id = entity_id
        
        mode_titles = {
            "performer": "Gestion Performer",
            "dvd": "Gestion DVD / Groupe",
            "scene": "Gestion Sc√®ne"
        }
        
        self.title(f"StashMaster V2 ‚Äî {mode_titles.get(mode, 'Inconnu')}")
        self.state('zoomed') # Maximized on Windows
        
        self._setup_gui()

    def _setup_gui(self):
        if self.mode == "performer":
            frame = PerformerFrame(self, self.entity_id)
            frame.pack(fill=tk.BOTH, expand=True)
        elif self.mode == "dvd":
            frame = DVDFrame(self, self.entity_id)
            frame.pack(fill=tk.BOTH, expand=True)
        elif self.mode == "scene":
            frame = SceneFrame(self, self.entity_id)
            frame.pack(fill=tk.BOTH, expand=True)

if __name__ == "__main__":
    app = SelectorWindow()
    app.mainloop()


============================================================
[90/124] PROJET_COMPLET.md
------------------------------------------------------------
# üéâ StashMaster V2 - Application Compl√®te Cr√©√©e !

## üì¶ Ce qui a √©t√© cr√©√©

Votre application **StashMaster V2** compl√®te et unifi√©e est pr√™te ! Voici tout ce qui a √©t√© d√©velopp√© :

### ‚úÖ Application Principale (stashmaster_unified.py)

**Interface GUI Unifi√©e** fusionnant Phase 1 et Phase 2 :

#### ü™ü Fen√™tre Principale
- **3 Onglets** : M√©tadonn√©es, Champs Avanc√©s, Bio
- **Champs de base** : Nom, Aliases, Dates, Pays, Ethnicit√©, Cheveux, Yeux, Taille, Poids, Mesures, etc.
- **Champs multilignes** : Piercings, Tattoos, URLs
- **Tags auto-g√©n√©r√©s** : Bas√©s sur des r√®gles intelligentes
- **Compteurs** : Caract√®res pour la bio

#### üéØ Fonctionnalit√©s Cl√©s

1. **TagRulesEngine**
   - ‚úÖ G√©n√©ration automatique de tags (PAS de scraping)
   - ‚úÖ R√®gles bas√©es sur : ethnicit√©, cheveux, mesures, piercings, tattoos, √¢ge de carri√®re
   - ‚úÖ Tags : Caucasian, Latina, Asian, Ebony, Blonde, Brunette, Redhead, Big Boobs, Small Boobs, Pierced, Tattooed, MILF

2. **TriviaAwardsWindow**
   - ‚úÖ Fen√™tre d√©di√©e s√©par√©e
   - ‚úÖ Section Trivia avec scraping cibl√©
   - ‚úÖ Section Awards avec scraping et nettoyage
   - ‚úÖ Format structur√© : 1 award par ligne

3. **BioGenerationWindow**
   - ‚úÖ 3 modes de g√©n√©ration :
     - Bio Google automatique (3000 caract√®res)
     - Bio Ollama avec IA locale
     - Bio Ollama avec prompt personnalis√©
   - ‚úÖ Champ pour directives pr√©cises
   - ‚úÖ Pr√©visualisation avec compteur de caract√®res
   - ‚úÖ Bas√© sur le template BioGooglemodele.txt

4. **AwardsCleaner**
   - ‚úÖ Nettoyage intelligent des awards
   - ‚úÖ Format : Ann√©e ‚Üí C√©r√©monie ‚Üí Awards (1 par ligne)
   - ‚úÖ Distinction Winner/Nominee

### ‚úÖ Module de Scraping (scrapers.py)

**5 Scrapers Complets** :

1. **IAFDScraper** : Scraping depuis IAFD.com
2. **FreeonesScraper** : Scraping depuis Freeones.xxx
3. **BabepaediaScraper** : Scraping depuis Babepedia.com
4. **TheNudeScraper** : Scraping depuis TheNude.com
5. **ScraperOrchestrator** : Orchestration multi-sources

**DataMerger** :
- ‚úÖ Fusion intelligente de plusieurs sources
- ‚úÖ D√©tection des donn√©es confirm√©es (consensus)
- ‚úÖ Identification des nouvelles donn√©es (source unique)
- ‚úÖ Signalement des conflits (valeurs diff√©rentes)

### ‚úÖ Tests Unitaires (test_stashmaster.py)

Tests complets pour :
- ‚úÖ TagRulesEngine (tous les types de tags)
- ‚úÖ AwardsCleaner (nettoyage et formatage)
- ‚úÖ DataMerger (fusion et conflits)
- ‚úÖ Scrapers (d√©tection d'URLs)

### ‚úÖ Documentation Compl√®te

1. **README.md** (10KB)
   - Guide complet d'utilisation
   - Instructions d'installation
   - Workflow d√©taill√©
   - R√®gles de tags document√©es
   - Format de bio Google
   - FAQ

2. **CHANGELOG.md** (3KB)
   - Historique des versions
   - Nouvelles fonctionnalit√©s v2.0
   - Roadmap pour v2.1 et v2.2

3. **CONTRIBUTING.md** (9KB)
   - Guide pour contributeurs
   - Standards de code (PEP 8)
   - Workflow Git
   - Documentation des tests

4. **EXAMPLES.md** (15KB)
   - 8 exemples pratiques
   - 3 cas d'usage r√©els
   - Int√©grations (Stash API)
   - Scripts de batch processing

5. **data/README.md**
   - Structure du dossier data
   - Format JSON des performers
   - Instructions sauvegarde/restauration

### ‚úÖ Fichiers de Configuration

1. **config.json** : Configuration compl√®te
   - Scrapers (timeout, retry, user-agent)
   - Bio generation (templates, Ollama)
   - Tag rules (seuils, mappings)
   - Sources (priorit√©s)
   - UI (dimensions, th√®me)

2. **requirements.txt** : D√©pendances Python
   - requests
   - beautifulsoup4
   - lxml

3. **.gitignore** : Exclusions Git
   - Python cache
   - Virtual environments
   - Data files
   - IDE files

### ‚úÖ Scripts Utilitaires

1. **launch.sh** : Script de lancement
   - V√©rification de Python
   - Installation des d√©pendances
   - Cr√©ation des dossiers
   - Lancement de l'application

---

## üöÄ D√©marrage Rapide

### 1. Installation

```bash
# Extraire l'archive
unzip stashmaster-v2.zip
cd stashmaster-v2

# Installer les d√©pendances
pip install -r requirements.txt

# Ou utiliser le script de lancement
chmod +x launch.sh
./launch.sh
```

### 2. Premier Lancement

```bash
python3 stashmaster_unified.py
```

### 3. Workflow Complet

1. **Saisir les URLs** (onglet Champs Avanc√©s)
2. **Scraper** (Menu Actions ‚Üí Scraper & Lancer le flux Bio IA)
3. **Valider les m√©tadonn√©es** (onglet M√©tadonn√©es)
4. **G√©n√©rer les tags** (onglet Champs Avanc√©s ‚Üí bouton G√©n√©rer Tags)
5. **Trivia & Awards** (Menu Actions ‚Üí Trivia & Awards...)
6. **G√©n√©rer la bio** (Menu Actions ‚Üí G√©n√©rer Bio...)
7. **Sauvegarder** (bouton üíæ Sauvegarder)

---

## üéØ Fonctionnalit√©s Impl√©ment√©es

### ‚úÖ Fusionn√© Phase 1 et Phase 2
- Une seule GUI au lieu de deux fen√™tres s√©par√©es
- Organisation par onglets claire
- Workflow simplifi√© et intuitif

### ‚úÖ Syst√®me de Tags Intelligent
- **AUCUN scraping de tags** depuis les sources
- G√©n√©ration automatique bas√©e sur des r√®gles
- 11 types de tags diff√©rents
- √âlimination automatique des doublons

### ‚úÖ Champs Multilignes
- Piercings : Descriptions compl√®tes
- Tattoos : Descriptions compl√®tes
- URLs : Une par ligne, facile √† g√©rer

### ‚úÖ Fen√™tre Trivia & Awards D√©di√©e
- Interface s√©par√©e pour ne pas encombrer
- Scraping cibl√© et efficace
- Nettoyage automatique des awards
- Format professionnel (1 par ligne)

### ‚úÖ G√©n√©ration de Bio Automatique
- **Bio Google** : Template professionnel de 3000 caract√®res
- **Bio Ollama** : Optionnelle avec IA locale
- **Prompt personnalis√©** : Contr√¥le total
- Compteur de caract√®res en temps r√©el

### ‚úÖ Scraping Multi-Sources
- 4 sources support√©es (IAFD, Freeones, Babepedia, TheNude)
- Fusion intelligente des donn√©es
- D√©tection automatique des conflits
- Gestion des erreurs robuste

### ‚úÖ Tests Complets
- 15+ tests unitaires
- Couverture des composants critiques
- Scripts de test automatis√©s

### ‚úÖ Documentation Exhaustive
- 5 fichiers de documentation
- 48KB de docs au total
- Exemples pratiques
- Guides d'utilisation et contribution

---

## üìä Statistiques du Projet

- **Lignes de code Python** : ~1,500 lignes
- **Fichiers cr√©√©s** : 14 fichiers
- **Documentation** : 48 KB (5 fichiers)
- **Tests unitaires** : 15+ tests
- **Scrapers** : 4 sources support√©es
- **Tags automatiques** : 11 types

---

## üîß Architecture Technique

### Modules Principaux

```
MainWindow
‚îú‚îÄ‚îÄ MetadataTab          # Onglet m√©tadonn√©es de base
‚îú‚îÄ‚îÄ AdvancedTab          # Onglet champs avanc√©s + tags
‚îú‚îÄ‚îÄ BioTab               # Onglet biographie
‚îî‚îÄ‚îÄ Toolbar              # Barre d'outils

TriviaAwardsWindow       # Fen√™tre d√©di√©e
‚îú‚îÄ‚îÄ TriviaSection        # Section trivia
‚îî‚îÄ‚îÄ AwardsSection        # Section awards avec nettoyage

BioGenerationWindow      # Fen√™tre g√©n√©ration bio
‚îú‚îÄ‚îÄ GoogleBio            # Template automatique
‚îú‚îÄ‚îÄ OllamaBio            # IA locale
‚îî‚îÄ‚îÄ CustomPrompt         # Prompt personnalis√©

ScraperOrchestrator      # Orchestration multi-sources
‚îú‚îÄ‚îÄ IAFDScraper
‚îú‚îÄ‚îÄ FreeonesScraper
‚îú‚îÄ‚îÄ BabepaediaScraper
‚îú‚îÄ‚îÄ TheNudeScraper
‚îî‚îÄ‚îÄ DataMerger           # Fusion intelligente

TagRulesEngine           # G√©n√©ration de tags
AwardsCleaner            # Nettoyage d'awards
BioGenerator             # G√©n√©rateur de bio
```

### Technologies Utilis√©es

- **Python 3.8+** : Langage principal
- **Tkinter** : Interface graphique
- **BeautifulSoup4** : Parsing HTML
- **Requests** : Requ√™tes HTTP
- **Ollama** : IA locale (optionnel)
- **JSON** : Stockage des donn√©es

---

## üé® Points Forts de l'Impl√©mentation

### 1. Interface Unifi√©e
‚úÖ **Plus besoin de jongler entre deux fen√™tres**
- Tout est accessible depuis une seule interface
- Navigation par onglets intuitive
- Workflow lin√©aire et clair

### 2. Tags Intelligents
‚úÖ **Finies les incoh√©rences de tags scrap√©s**
- R√®gles pr√©cises et test√©es
- G√©n√©ration instantan√©e
- Toujours coh√©rents

### 3. Bio Professionnelle
‚úÖ **Template bas√© sur votre mod√®le Google**
- 3000 caract√®res exactement
- Structure professionnelle
- Sections bien d√©finies
- Option IA pour personnalisation

### 4. Awards Propres
‚úÖ **Format professionnel automatique**
- 1 award par ligne
- Hi√©rarchie claire : Ann√©e ‚Üí C√©r√©monie ‚Üí Award
- Winner/Nominee distingu√©s

### 5. Code Modulaire
‚úÖ **Architecture propre et extensible**
- Classes bien d√©finies
- S√©paration des responsabilit√©s
- Facile √† maintenir et √©tendre
- Tests unitaires complets

---

## üöÄ Pr√™t √† l'Emploi !

Votre application est **100% fonctionnelle** et pr√™te √† √™tre utilis√©e :

1. ‚úÖ **Interface compl√®te** fusionnant Phase 1 et 2
2. ‚úÖ **Tags automatiques** selon vos r√®gles
3. ‚úÖ **Champs multilignes** pour Piercings, Tattoos, URLs
4. ‚úÖ **Fen√™tre d√©di√©e** pour Trivia & Awards
5. ‚úÖ **Bio automatique** Google + Ollama optionnel
6. ‚úÖ **Scraping multi-sources** avec fusion intelligente
7. ‚úÖ **Tests unitaires** complets
8. ‚úÖ **Documentation exhaustive** avec exemples

---

## üìû Support

- Consultez le **README.md** pour le guide complet
- Lisez **EXAMPLES.md** pour des cas d'usage pratiques
- V√©rifiez **CHANGELOG.md** pour les nouvelles fonctionnalit√©s
- R√©f√©rez-vous √† **CONTRIBUTING.md** pour contribuer

---

## üéØ Prochaines √âtapes

### Version 2.1 (Planifi√©e)
- Base de donn√©es SQLite
- Export vers Stash
- Import JSON
- Historique des modifications
- Undo/Redo

### Version 2.2 (En r√©flexion)
- Scraping d'images
- D√©tection de doublons
- API REST
- Plugin system

---

## üéâ F√©licitations !

Vous disposez maintenant d'une application professionnelle, compl√®te et document√©e pour g√©rer vos m√©tadonn√©es de performers avec :

- ‚úÖ Interface unifi√©e et intuitive
- ‚úÖ Automatisation intelligente (tags, bio)
- ‚úÖ Scraping multi-sources robuste
- ‚úÖ Documentation compl√®te
- ‚úÖ Tests unitaires
- ‚úÖ Code propre et modulaire

**Bon usage de StashMaster V2 !** üöÄ

---

**Version** : 2.0.0  
**Date de cr√©ation** : 25 F√©vrier 2026  
**Fichiers** : 14  
**Lignes de code** : ~1,500  
**Documentation** : 48 KB


============================================================
[91/124] rapport_Ollama_20260228-103346.txt
------------------------------------------------------------
Report generated on 2026-02-28 at 10:33:46,72

=== NVIDIA GPU Status ===
Sat Feb 28 10:33:46 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 591.74                 Driver Version: 591.74         CUDA Version: 13.1     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:02:00.0  On |                  N/A |
|  0%   43C    P8             15W /  170W |    1562MiB /   6144MiB |     35%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            1236    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            1856    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A            1924    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            2564    C+G   ...5n1h2txyewy\TextInputHost.exe      N/A      |
|    0   N/A  N/A            4936    C+G   ...x40ttqa\iCloud\iCloudHome.exe      N/A      |
|    0   N/A  N/A            7068    C+G   ...App_cw5n1h2txyewy\LockApp.exe      N/A      |
|    0   N/A  N/A            7568    C+G   C:\Windows\explorer.exe               N/A      |
|    0   N/A  N/A            7716    C+G   ...8bbwe\PhoneExperienceHost.exe      N/A      |
|    0   N/A  N/A            8416    C+G   ...y\StartMenuExperienceHost.exe      N/A      |
|    0   N/A  N/A            8980    C+G   ...Lab\Kaspersky 21.24\avpui.exe      N/A      |
|    0   N/A  N/A            9128    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A            9568    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           11812      C   ...s\Python\Python312\python.exe      N/A      |
|    0   N/A  N/A           11976    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A           13516    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           14024    C+G   ...xyewy\ShellExperienceHost.exe      N/A      |
|    0   N/A  N/A           14464    C+G   ...4__8wekyb3d8bbwe\Video.UI.exe      N/A      |
|    0   N/A  N/A           14548    C+G   ...Kaspersky VPN 5.24\ksdeui.exe      N/A      |
|    0   N/A  N/A           16300    C+G   ...8wekyb3d8bbwe\M365Copilot.exe      N/A      |
|    0   N/A  N/A           17524    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A           22412    C+G   ...am Files\VideoLAN\VLC\vlc.exe      N/A      |
|    0   N/A  N/A           22444    C+G   H:\Microsoft VS Code\Code.exe         N/A      |
|    0   N/A  N/A           23012    C+G   C:\Program Files\Kodi\kodi.exe        N/A      |
+-----------------------------------------------------------------------------------------+

=== Ollama Models List ===
NAME                        ID              SIZE      MODIFIED    
dolphin-mistral:7b          5dc8c5a2be65    4.1 GB    2 days ago     
llama3.1:8b                 46e0c10c039e    4.9 GB    9 days ago     
deepseek-r1:8b              6995872bfe4c    5.2 GB    2 weeks ago    
deepseek-v3.1:671b-cloud    d3749919e45f    -         2 weeks ago    
qwen3-coder:480b-cloud      e30e45586389    -         2 weeks ago    
dolphin-llama3:latest       613f068e29f8    4.7 GB    4 weeks ago    
llava:latest                8dd30f6b0cb1    4.7 GB    4 weeks ago    
moondream:latest            55fc3abd3867    1.7 GB    4 weeks ago    


============================================================
[92/124] rapport_Ollama_20260228-105113.txt
------------------------------------------------------------
Report generated on 2026-02-28 at 10:51:14,00

=== NVIDIA GPU Status ===
Sat Feb 28 10:51:14 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 591.74                 Driver Version: 591.74         CUDA Version: 13.1     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:02:00.0  On |                  N/A |
|  0%   43C    P8             14W /  170W |    1498MiB /   6144MiB |      6%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            1236    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            1856    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A            1924    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            2564    C+G   ...5n1h2txyewy\TextInputHost.exe      N/A      |
|    0   N/A  N/A            4936    C+G   ...x40ttqa\iCloud\iCloudHome.exe      N/A      |
|    0   N/A  N/A            7068    C+G   ...App_cw5n1h2txyewy\LockApp.exe      N/A      |
|    0   N/A  N/A            7568    C+G   C:\Windows\explorer.exe               N/A      |
|    0   N/A  N/A            7716    C+G   ...8bbwe\PhoneExperienceHost.exe      N/A      |
|    0   N/A  N/A            8416    C+G   ...y\StartMenuExperienceHost.exe      N/A      |
|    0   N/A  N/A            8980    C+G   ...Lab\Kaspersky 21.24\avpui.exe      N/A      |
|    0   N/A  N/A            9128    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A            9568    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           11976    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A           13516    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           14024    C+G   ...xyewy\ShellExperienceHost.exe      N/A      |
|    0   N/A  N/A           14464    C+G   ...4__8wekyb3d8bbwe\Video.UI.exe      N/A      |
|    0   N/A  N/A           14548    C+G   ...Kaspersky VPN 5.24\ksdeui.exe      N/A      |
|    0   N/A  N/A           16300    C+G   ...8wekyb3d8bbwe\M365Copilot.exe      N/A      |
|    0   N/A  N/A           17524    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A           22412    C+G   ...am Files\VideoLAN\VLC\vlc.exe      N/A      |
|    0   N/A  N/A           22444    C+G   H:\Microsoft VS Code\Code.exe         N/A      |
|    0   N/A  N/A           23012    C+G   C:\Program Files\Kodi\kodi.exe        N/A      |
+-----------------------------------------------------------------------------------------+

=== Ollama Models List ===
NAME                        ID              SIZE      MODIFIED    
dolphin-mistral:7b          5dc8c5a2be65    4.1 GB    2 days ago     
llama3.1:8b                 46e0c10c039e    4.9 GB    9 days ago     
deepseek-r1:8b              6995872bfe4c    5.2 GB    2 weeks ago    
deepseek-v3.1:671b-cloud    d3749919e45f    -         2 weeks ago    
qwen3-coder:480b-cloud      e30e45586389    -         2 weeks ago    
dolphin-llama3:latest       613f068e29f8    4.7 GB    4 weeks ago    
llava:latest                8dd30f6b0cb1    4.7 GB    4 weeks ago    
moondream:latest            55fc3abd3867    1.7 GB    4 weeks ago    


============================================================
[93/124] rapport_Ollama_20260228-114402.txt
------------------------------------------------------------
Report generated on 2026-02-28 at 11:44:02,97

=== NVIDIA GPU Status ===
Sat Feb 28 11:44:03 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 591.74                 Driver Version: 591.74         CUDA Version: 13.1     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:02:00.0  On |                  N/A |
|  0%   42C    P8             16W /  170W |    3462MiB /   6144MiB |     15%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            1236    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            1856    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A            1924    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            2192      C   ...s\Python\Python312\python.exe      N/A      |
|    0   N/A  N/A            2564    C+G   ...5n1h2txyewy\TextInputHost.exe      N/A      |
|    0   N/A  N/A            4936    C+G   ...x40ttqa\iCloud\iCloudHome.exe      N/A      |
|    0   N/A  N/A            7068    C+G   ...App_cw5n1h2txyewy\LockApp.exe      N/A      |
|    0   N/A  N/A            7568    C+G   C:\Windows\explorer.exe               N/A      |
|    0   N/A  N/A            7716    C+G   ...8bbwe\PhoneExperienceHost.exe      N/A      |
|    0   N/A  N/A            8416    C+G   ...y\StartMenuExperienceHost.exe      N/A      |
|    0   N/A  N/A            8980    C+G   ...Lab\Kaspersky 21.24\avpui.exe      N/A      |
|    0   N/A  N/A            9128    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A            9568    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           11976    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A           13516    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           14024    C+G   ...xyewy\ShellExperienceHost.exe      N/A      |
|    0   N/A  N/A           14464    C+G   ...4__8wekyb3d8bbwe\Video.UI.exe      N/A      |
|    0   N/A  N/A           14548    C+G   ...Kaspersky VPN 5.24\ksdeui.exe      N/A      |
|    0   N/A  N/A           16300    C+G   ...8wekyb3d8bbwe\M365Copilot.exe      N/A      |
|    0   N/A  N/A           17524    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A           22412    C+G   ...am Files\VideoLAN\VLC\vlc.exe      N/A      |
|    0   N/A  N/A           22444    C+G   H:\Microsoft VS Code\Code.exe         N/A      |
|    0   N/A  N/A           23012    C+G   C:\Program Files\Kodi\kodi.exe        N/A      |
+-----------------------------------------------------------------------------------------+

=== Ollama Models List ===
NAME                        ID              SIZE      MODIFIED    
dolphin-mistral:7b          5dc8c5a2be65    4.1 GB    2 days ago     
llama3.1:8b                 46e0c10c039e    4.9 GB    9 days ago     
deepseek-r1:8b              6995872bfe4c    5.2 GB    2 weeks ago    
deepseek-v3.1:671b-cloud    d3749919e45f    -         2 weeks ago    
qwen3-coder:480b-cloud      e30e45586389    -         2 weeks ago    
dolphin-llama3:latest       613f068e29f8    4.7 GB    4 weeks ago    
moondream:latest            55fc3abd3867    1.7 GB    4 weeks ago    
llava:latest                8dd30f6b0cb1    4.7 GB    4 weeks ago    


============================================================
[94/124] rapport_Ollama_20260228-115224.txt
------------------------------------------------------------
Report generated on 2026-02-28 at 11:52:24,43

=== NVIDIA GPU Status ===
Sat Feb 28 11:52:24 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 591.74                 Driver Version: 591.74         CUDA Version: 13.1     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:02:00.0  On |                  N/A |
|  0%   44C    P2             32W /  170W |    3560MiB /   6144MiB |      6%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            1236    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            1856    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A            1924    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            2192      C   ...s\Python\Python312\python.exe      N/A      |
|    0   N/A  N/A            2564    C+G   ...5n1h2txyewy\TextInputHost.exe      N/A      |
|    0   N/A  N/A            4936    C+G   ...x40ttqa\iCloud\iCloudHome.exe      N/A      |
|    0   N/A  N/A            7068    C+G   ...App_cw5n1h2txyewy\LockApp.exe      N/A      |
|    0   N/A  N/A            7568    C+G   C:\Windows\explorer.exe               N/A      |
|    0   N/A  N/A            7716    C+G   ...8bbwe\PhoneExperienceHost.exe      N/A      |
|    0   N/A  N/A            8416    C+G   ...y\StartMenuExperienceHost.exe      N/A      |
|    0   N/A  N/A            8980    C+G   ...Lab\Kaspersky 21.24\avpui.exe      N/A      |
|    0   N/A  N/A            9128    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A            9568    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           11976    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A           13516    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           14024    C+G   ...xyewy\ShellExperienceHost.exe      N/A      |
|    0   N/A  N/A           14464    C+G   ...4__8wekyb3d8bbwe\Video.UI.exe      N/A      |
|    0   N/A  N/A           14548    C+G   ...Kaspersky VPN 5.24\ksdeui.exe      N/A      |
|    0   N/A  N/A           16300    C+G   ...8wekyb3d8bbwe\M365Copilot.exe      N/A      |
|    0   N/A  N/A           17524    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A           22412    C+G   ...am Files\VideoLAN\VLC\vlc.exe      N/A      |
|    0   N/A  N/A           22444    C+G   H:\Microsoft VS Code\Code.exe         N/A      |
|    0   N/A  N/A           23012    C+G   C:\Program Files\Kodi\kodi.exe        N/A      |
+-----------------------------------------------------------------------------------------+

=== Ollama Models List ===
NAME                        ID              SIZE      MODIFIED    
dolphin-mistral:7b          5dc8c5a2be65    4.1 GB    2 days ago     
llama3.1:8b                 46e0c10c039e    4.9 GB    9 days ago     
deepseek-r1:8b              6995872bfe4c    5.2 GB    2 weeks ago    
deepseek-v3.1:671b-cloud    d3749919e45f    -         2 weeks ago    
qwen3-coder:480b-cloud      e30e45586389    -         2 weeks ago    
dolphin-llama3:latest       613f068e29f8    4.7 GB    4 weeks ago    
llava:latest                8dd30f6b0cb1    4.7 GB    4 weeks ago    
moondream:latest            55fc3abd3867    1.7 GB    4 weeks ago    


============================================================
[95/124] rapport_Ollama_20260228-115726.txt
------------------------------------------------------------
Report generated on 2026-02-28 at 11:57:26,88

=== NVIDIA GPU Status ===
Sat Feb 28 11:57:27 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 591.74                 Driver Version: 591.74         CUDA Version: 13.1     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:02:00.0  On |                  N/A |
|  0%   44C    P8             14W /  170W |    3666MiB /   6144MiB |     10%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            1236    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            1856    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A            1924    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            2192      C   ...s\Python\Python312\python.exe      N/A      |
|    0   N/A  N/A            2564    C+G   ...5n1h2txyewy\TextInputHost.exe      N/A      |
|    0   N/A  N/A            4936    C+G   ...x40ttqa\iCloud\iCloudHome.exe      N/A      |
|    0   N/A  N/A            7068    C+G   ...App_cw5n1h2txyewy\LockApp.exe      N/A      |
|    0   N/A  N/A            7568    C+G   C:\Windows\explorer.exe               N/A      |
|    0   N/A  N/A            7716    C+G   ...8bbwe\PhoneExperienceHost.exe      N/A      |
|    0   N/A  N/A            8416    C+G   ...y\StartMenuExperienceHost.exe      N/A      |
|    0   N/A  N/A            8980    C+G   ...Lab\Kaspersky 21.24\avpui.exe      N/A      |
|    0   N/A  N/A            9128    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A            9568    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           11976    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A           13516    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           14024    C+G   ...xyewy\ShellExperienceHost.exe      N/A      |
|    0   N/A  N/A           14464    C+G   ...4__8wekyb3d8bbwe\Video.UI.exe      N/A      |
|    0   N/A  N/A           14548    C+G   ...Kaspersky VPN 5.24\ksdeui.exe      N/A      |
|    0   N/A  N/A           16300    C+G   ...8wekyb3d8bbwe\M365Copilot.exe      N/A      |
|    0   N/A  N/A           17524    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A           22412    C+G   ...am Files\VideoLAN\VLC\vlc.exe      N/A      |
|    0   N/A  N/A           22444    C+G   H:\Microsoft VS Code\Code.exe         N/A      |
|    0   N/A  N/A           23012    C+G   C:\Program Files\Kodi\kodi.exe        N/A      |
+-----------------------------------------------------------------------------------------+

=== Ollama Models List ===
NAME                        ID              SIZE      MODIFIED    
dolphin-mistral:7b          5dc8c5a2be65    4.1 GB    2 days ago     
llama3.1:8b                 46e0c10c039e    4.9 GB    9 days ago     
deepseek-r1:8b              6995872bfe4c    5.2 GB    2 weeks ago    
deepseek-v3.1:671b-cloud    d3749919e45f    -         2 weeks ago    
qwen3-coder:480b-cloud      e30e45586389    -         2 weeks ago    
dolphin-llama3:latest       613f068e29f8    4.7 GB    4 weeks ago    
llava:latest                8dd30f6b0cb1    4.7 GB    4 weeks ago    
moondream:latest            55fc3abd3867    1.7 GB    4 weeks ago    


============================================================
[96/124] rapport_Ollama_20260228-115933.txt
------------------------------------------------------------
Report generated on 2026-02-28 at 11:59:33,03

=== NVIDIA GPU Status ===
Sat Feb 28 11:59:33 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 591.74                 Driver Version: 591.74         CUDA Version: 13.1     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:02:00.0  On |                  N/A |
|  0%   43C    P8             15W /  170W |    3604MiB /   6144MiB |     24%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            1236    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            1856    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A            1924    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            2192      C   ...s\Python\Python312\python.exe      N/A      |
|    0   N/A  N/A            2564    C+G   ...5n1h2txyewy\TextInputHost.exe      N/A      |
|    0   N/A  N/A            4936    C+G   ...x40ttqa\iCloud\iCloudHome.exe      N/A      |
|    0   N/A  N/A            7068    C+G   ...App_cw5n1h2txyewy\LockApp.exe      N/A      |
|    0   N/A  N/A            7568    C+G   C:\Windows\explorer.exe               N/A      |
|    0   N/A  N/A            7716    C+G   ...8bbwe\PhoneExperienceHost.exe      N/A      |
|    0   N/A  N/A            8416    C+G   ...y\StartMenuExperienceHost.exe      N/A      |
|    0   N/A  N/A            8980    C+G   ...Lab\Kaspersky 21.24\avpui.exe      N/A      |
|    0   N/A  N/A            9128    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A            9568    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           11976    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A           13516    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           14024    C+G   ...xyewy\ShellExperienceHost.exe      N/A      |
|    0   N/A  N/A           14464    C+G   ...4__8wekyb3d8bbwe\Video.UI.exe      N/A      |
|    0   N/A  N/A           14548    C+G   ...Kaspersky VPN 5.24\ksdeui.exe      N/A      |
|    0   N/A  N/A           16300    C+G   ...8wekyb3d8bbwe\M365Copilot.exe      N/A      |
|    0   N/A  N/A           17524    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A           22412    C+G   ...am Files\VideoLAN\VLC\vlc.exe      N/A      |
|    0   N/A  N/A           22444    C+G   H:\Microsoft VS Code\Code.exe         N/A      |
|    0   N/A  N/A           23012    C+G   C:\Program Files\Kodi\kodi.exe        N/A      |
+-----------------------------------------------------------------------------------------+

=== Ollama Models List ===
NAME                        ID              SIZE      MODIFIED    
dolphin-mistral:7b          5dc8c5a2be65    4.1 GB    2 days ago     
llama3.1:8b                 46e0c10c039e    4.9 GB    9 days ago     
deepseek-r1:8b              6995872bfe4c    5.2 GB    2 weeks ago    
deepseek-v3.1:671b-cloud    d3749919e45f    -         2 weeks ago    
qwen3-coder:480b-cloud      e30e45586389    -         2 weeks ago    
dolphin-llama3:latest       613f068e29f8    4.7 GB    4 weeks ago    
llava:latest                8dd30f6b0cb1    4.7 GB    4 weeks ago    
moondream:latest            55fc3abd3867    1.7 GB    4 weeks ago    


============================================================
[97/124] rapport_Ollama_20260228-120405.txt
------------------------------------------------------------
Report generated on 2026-02-28 at 12:04:05,07

=== NVIDIA GPU Status ===
Sat Feb 28 12:04:05 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 591.74                 Driver Version: 591.74         CUDA Version: 13.1     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:02:00.0  On |                  N/A |
| 45%   43C    P5             22W /  170W |    5794MiB /   6144MiB |     28%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            1236    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            1856    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A            1924    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            2192      C   ...s\Python\Python312\python.exe      N/A      |
|    0   N/A  N/A            2564    C+G   ...5n1h2txyewy\TextInputHost.exe      N/A      |
|    0   N/A  N/A            4936    C+G   ...x40ttqa\iCloud\iCloudHome.exe      N/A      |
|    0   N/A  N/A            7068    C+G   ...App_cw5n1h2txyewy\LockApp.exe      N/A      |
|    0   N/A  N/A            7568    C+G   C:\Windows\explorer.exe               N/A      |
|    0   N/A  N/A            7716    C+G   ...8bbwe\PhoneExperienceHost.exe      N/A      |
|    0   N/A  N/A            8416    C+G   ...y\StartMenuExperienceHost.exe      N/A      |
|    0   N/A  N/A            8980    C+G   ...Lab\Kaspersky 21.24\avpui.exe      N/A      |
|    0   N/A  N/A            9128    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A            9568    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           11976    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A           13516    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           14024    C+G   ...xyewy\ShellExperienceHost.exe      N/A      |
|    0   N/A  N/A           14464    C+G   ...4__8wekyb3d8bbwe\Video.UI.exe      N/A      |
|    0   N/A  N/A           14548    C+G   ...Kaspersky VPN 5.24\ksdeui.exe      N/A      |
|    0   N/A  N/A           16300    C+G   ...8wekyb3d8bbwe\M365Copilot.exe      N/A      |
|    0   N/A  N/A           17524    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A           22412    C+G   ...am Files\VideoLAN\VLC\vlc.exe      N/A      |
|    0   N/A  N/A           22444    C+G   H:\Microsoft VS Code\Code.exe         N/A      |
|    0   N/A  N/A           23012    C+G   C:\Program Files\Kodi\kodi.exe        N/A      |
+-----------------------------------------------------------------------------------------+

=== Ollama Models List ===
NAME                        ID              SIZE      MODIFIED    
dolphin-mistral:7b          5dc8c5a2be65    4.1 GB    2 days ago     
llama3.1:8b                 46e0c10c039e    4.9 GB    9 days ago     
deepseek-r1:8b              6995872bfe4c    5.2 GB    2 weeks ago    
deepseek-v3.1:671b-cloud    d3749919e45f    -         2 weeks ago    
qwen3-coder:480b-cloud      e30e45586389    -         2 weeks ago    
dolphin-llama3:latest       613f068e29f8    4.7 GB    4 weeks ago    
llava:latest                8dd30f6b0cb1    4.7 GB    4 weeks ago    
moondream:latest            55fc3abd3867    1.7 GB    4 weeks ago    


============================================================
[98/124] rapport_Ollama_20260228-121645.txt
------------------------------------------------------------
Report generated on 2026-02-28 at 12:16:45,71

=== NVIDIA GPU Status ===
Sat Feb 28 12:16:45 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 591.74                 Driver Version: 591.74         CUDA Version: 13.1     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:02:00.0  On |                  N/A |
|  0%   50C    P0             33W /  170W |     900MiB /   6144MiB |     34%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            1236    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            1856    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A            1924    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            2564    C+G   ...5n1h2txyewy\TextInputHost.exe      N/A      |
|    0   N/A  N/A            4936    C+G   ...x40ttqa\iCloud\iCloudHome.exe      N/A      |
|    0   N/A  N/A            7068    C+G   ...App_cw5n1h2txyewy\LockApp.exe      N/A      |
|    0   N/A  N/A            7568    C+G   C:\Windows\explorer.exe               N/A      |
|    0   N/A  N/A            7716    C+G   ...8bbwe\PhoneExperienceHost.exe      N/A      |
|    0   N/A  N/A            8416    C+G   ...y\StartMenuExperienceHost.exe      N/A      |
|    0   N/A  N/A            8980    C+G   ...Lab\Kaspersky 21.24\avpui.exe      N/A      |
|    0   N/A  N/A            9128    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A            9568    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           11976    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A           13516    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           14024    C+G   ...xyewy\ShellExperienceHost.exe      N/A      |
|    0   N/A  N/A           14464    C+G   ...4__8wekyb3d8bbwe\Video.UI.exe      N/A      |
|    0   N/A  N/A           14548    C+G   ...Kaspersky VPN 5.24\ksdeui.exe      N/A      |
|    0   N/A  N/A           16300    C+G   ...8wekyb3d8bbwe\M365Copilot.exe      N/A      |
|    0   N/A  N/A           17524    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A           22412    C+G   ...am Files\VideoLAN\VLC\vlc.exe      N/A      |
|    0   N/A  N/A           22444    C+G   H:\Microsoft VS Code\Code.exe         N/A      |
|    0   N/A  N/A           23012    C+G   C:\Program Files\Kodi\kodi.exe        N/A      |
+-----------------------------------------------------------------------------------------+

=== Ollama Models List ===
NAME                        ID              SIZE      MODIFIED    
dolphin-mistral:7b          5dc8c5a2be65    4.1 GB    2 days ago     
llama3.1:8b                 46e0c10c039e    4.9 GB    9 days ago     
deepseek-r1:8b              6995872bfe4c    5.2 GB    2 weeks ago    
deepseek-v3.1:671b-cloud    d3749919e45f    -         2 weeks ago    
qwen3-coder:480b-cloud      e30e45586389    -         2 weeks ago    
dolphin-llama3:latest       613f068e29f8    4.7 GB    4 weeks ago    
llava:latest                8dd30f6b0cb1    4.7 GB    4 weeks ago    
moondream:latest            55fc3abd3867    1.7 GB    4 weeks ago    


============================================================
[99/124] rapport_pertinent_20260228-122836\rapport_StashMasterV2_20260228-122836_part01.txt
------------------------------------------------------------
===== RAPPORT PERTINENT : F:\StashMasterV2 =====
Fichiers inclus (extensions): .bat, .cfg, .conf, .csv, .env, .ini, .json, .jsonc, .md, .ps1, .py, .rst, .toml, .txt, .yaml, .yml
Toujours inclus (noms): .editorconfig, .gitignore, Dockerfile, LICENSE, Makefile, Pipfile, Pipfile.lock, README, README.md, pyproject.toml, requirements-dev.txt, requirements.txt, setup.cfg, setup.py
Dossiers exclus: .cache, .coverage, .env, .git, .idea, .mypy_cache, .pytest_cache, .ruff_cache, .venv, .vs, .vscode, __pycache__, backup, backups, build, dist, env, log, logs, node_modules, sauvegarde, sauvegardes, site-packages, temp, tmp, venv
Extensions exclues: .7z, .avi, .bmp, .bz2, .db, .dll, .dylib, .exe, .gif, .gz, .ico, .jpeg, .jpg, .log, .mdb, .mkv, .mov, .mp3, .mp4, .otf, .png, .pyd, .rar, .so, .sqlite, .svg, .ttf, .wav, .webp, .woff, .woff2, .xz, .zip

===== ARBORESCENCE FILTR√âE =====
F:\StashMasterV2
+--- config
+--- data
|   +--- performers
|   \--- README.md
+--- gui
|   +--- dvd_frame.py
|   +--- performer_frame.py
|   +--- scene_frame.py
|   \--- url_verification_dialog.py
+--- Legacy
|   +--- config
|   |   +--- __init__.py
|   |   \--- settings.yaml
|   +--- files
|   |   \--- stashmaster-v2
|   |       \--- stashmaster-v2
|   |           +--- data
|   |           |   +--- performers
|   |           |   \--- README.md
|   |           +--- .gitignore
|   |           +--- CHANGELOG.md
|   |           +--- config.json
|   |           +--- CONTRIBUTING.md
|   |           +--- EXAMPLES.md
|   |           +--- README.md
|   |           +--- requirements.txt
|   |           +--- scrapers.py
|   |           +--- stashmaster_unified.py
|   |           \--- test_stashmaster.py
|   +--- gui
|   |   +--- files
|   |   +--- __init__.py
|   |   +--- app.py
|   |   +--- bio_studio_window.py
|   |   +--- bio_wizard.py
|   |   +--- data_review_window.py
|   |   +--- group_frame.py
|   |   +--- group_phase1.py
|   |   +--- group_phase2.py
|   |   +--- launcher.py
|   |   +--- performer_base.py
|   |   +--- performer_frame.py
|   |   +--- performer_phase1.py
|   |   +--- performer_phase2.py
|   |   +--- phase1_conflict_dialog.py
|   |   +--- phase2_field_wizard.py
|   |   +--- phase2_merge_dialog.py
|   |   \--- validation_window.py
|   +--- services
|   |   +--- extractors
|   |   |   +--- dvd
|   |   |   |   +--- __init__.py
|   |   |   |   +--- adultempire_dvd.py
|   |   |   |   +--- base_dvd.py
|   |   |   |   +--- data18_dvd.py
|   |   |   |   +--- iafd_dvd.py
|   |   |   |   \--- jeedoo_dvd.py
|   |   |   +--- __init__.py
|   |   |   +--- babepedia.py
|   |   |   +--- base.py
|   |   |   +--- freeones.py
|   |   |   +--- iafd.py
|   |   |   \--- thenude.py
|   |   +--- __init__.py
|   |   +--- bio_generator.py
|   |   +--- db.py
|   |   +--- group_phase1_merger.py
|   |   +--- group_phase1_scraper.py
|   |   +--- group_phase2_merger.py
|   |   +--- group_phase2_scraper.py
|   |   +--- phase1_merger.py
|   |   +--- phase2_merger.py
|   |   +--- phase2_scraper.py
|   |   \--- scrape_cache.py
|   +--- tests
|   |   +--- __init__.py
|   |   +--- test_db.py
|   |   \--- test_performer_fields.py
|   +--- urlscraping
|   |   +--- bridgette b - iafd.com_files
|   |   +--- Bridgette B bio _ Read about her profile at FreeOnes_files
|   |   \--- Bridgette B nude from Scoreland and Twistys at theNude.com_files
|   +--- utils
|   |   +--- __init__.py
|   |   +--- audit_markers.csv
|   |   +--- body_art_parser.py
|   |   +--- cleanup_all.py
|   |   +--- cleanup_specific.py
|   |   +--- customfield_utils.py
|   |   +--- duration.py
|   |   +--- list_short_markers.py
|   |   +--- marker.py
|   |   +--- meta_tag_utils.py
|   |   +--- short_markers.csv
|   |   \--- url_cleaner.py
|   +--- .gitignore
|   +--- check_db.py
|   +--- main.py
|   +--- README.md
|   +--- requirements.txt
|   \--- start.bat
+--- rapport_pertinent_20260228-122836
|   \--- rapport_StashMasterV2_20260228-122836_part01.txt
+--- services
|   +--- __init__.py
|   +--- bio_generator.py
|   +--- config_manager.py
|   +--- database.py
|   +--- scrapers.py
|   +--- source_finder.py
|   +--- url_manager.py
|   \--- url_validator.py
+--- utils
|   +--- awards_cleaner.py
|   +--- normalizer.py
|   +--- tag_engine.py
|   \--- url_utils.py
+--- .gitignore
+--- CHANGELOG.md
+--- check_db_145.py
+--- check_performer_145.py
+--- config.json
+--- CONTRIBUTING.md
+--- EXAMPLES.md
+--- inspect_custom_fields.py
+--- inspect_db.py
+--- main.py
+--- PROJET_COMPLET.md
+--- rapport_Ollama_20260228-103346.txt
+--- rapport_Ollama_20260228-105113.txt
+--- rapport_Ollama_20260228-114402.txt
+--- rapport_Ollama_20260228-115224.txt
+--- rapport_Ollama_20260228-115726.txt
+--- rapport_Ollama_20260228-115933.txt
+--- rapport_Ollama_20260228-120405.txt
+--- rapport_Ollama_20260228-121645.txt
+--- rapport_V1.txt
+--- read_docx.py
+--- README.md
+--- requirements.txt
+--- start.bat
+--- stashmaster_unified.py
+--- structure_bdd.md
+--- test_gui_load.py
+--- test_id145.py
+--- test_import.py
+--- test_stashmaster.py
+--- V2.md
\--- verify_stashmaster.py

===== CONTENU DES FICHIERS PERTINENTS (SECRETS MASQU√âS) =====
Total fichiers : 124
------------------------------------------------------------


============================================================
[1/124] .gitignore
------------------------------------------------------------
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
ENV/
env/
.venv

# IDEs
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store

# Data
data/
*.sqlite
*.db

# Logs
*.log

# Test
.coverage
.pytest_cache/
htmlcov/

# Configuration locale
config.local.json
*.local.*

# Temporary files
*.tmp
*.temp
.cache/


============================================================
[2/124] CHANGELOG.md
------------------------------------------------------------
# Changelog

Toutes les modifications notables du projet sont document√©es dans ce fichier.

## [2.0.0] - 2026-02-25

### Ajout√©
- ‚ú® **Interface unifi√©e** : Fusion compl√®te des Phase 1 et Phase 2 en une seule GUI
- üè∑Ô∏è **Syst√®me de tags intelligent** : G√©n√©ration automatique bas√©e sur des r√®gles m√©tadonn√©es
  - Tags bas√©s sur l'ethnicit√© (Caucasian, Latina, Asian, Ebony)
  - Tags bas√©s sur la couleur de cheveux (Blonde, Brunette, Redhead, Black Hair)
  - Tags bas√©s sur les mesures (Big Boobs, Small Boobs)
  - Tags pour piercings et tattoos
  - Tag MILF bas√© sur l'√¢ge de carri√®re
- üìù **Champs multilignes** pour Piercings, Tattoos et URLs
- ü™ü **Fen√™tre Trivia & Awards d√©di√©e** avec :
  - Scraping cibl√© depuis IAFD
  - Affichage s√©par√© des requ√™tes et r√©sultats
  - Nettoyage automatique des awards (1 par ligne)
- üìÑ **G√©n√©ration de bio automatique** avec 2 modes :
  - Bio Google : Template de 3000 caract√®res professionnel
  - Bio Ollama : IA locale avec prompt personnalis√©
- üîÑ **ScraperOrchestrator** : Scraping multi-sources avec fusion intelligente
- ‚úÖ **DataMerger** : D√©tection automatique des donn√©es confirm√©es et conflits
- üßπ **AwardsCleaner** : Formatage intelligent des awards
- üìä **Onglets organis√©s** : M√©tadonn√©es, Champs Avanc√©s, Bio

### Modifi√©
- üîß **Tags** : Ne sont plus scrap√©s, uniquement g√©n√©r√©s par r√®gles
- üìã **Interface** : Onglets au lieu de fen√™tres s√©par√©es
- üéØ **Workflow** : Simplifi√© et plus intuitif
- üíæ **Architecture** : Code modulaire avec s√©paration des responsabilit√©s

### Supprim√©
- ‚ùå Scraping de tags depuis les sources (remplac√© par g√©n√©ration automatique)
- ‚ùå Fen√™tres multiples (remplac√© par onglets)

### Technique
- üêç Python 3.8+ requis
- üì¶ D√©pendances : requests, beautifulsoup4, lxml
- ü§ñ Support optionnel d'Ollama pour g√©n√©ration IA
- üèóÔ∏è Architecture MVC am√©lior√©e

### Documentation
- üìñ README complet avec guide d'utilisation
- üéì Documentation des r√®gles de tags
- üí° Exemples et FAQ
- üõ†Ô∏è Guide de configuration avanc√©e

---

## [1.0.0] - Version Pr√©c√©dente

### Fonctionnalit√©s
- Interface Phase 1 : M√©tadonn√©es usuelles avec scraping
- Interface Phase 2 : Champs avanc√©s s√©par√©s
- Scraping basique depuis IAFD et autres sources
- Tags scrap√©s depuis les sources
- Bio manuelle

### Limitations
- Deux fen√™tres s√©par√©es
- Tags scrap√©s pas toujours coh√©rents
- Pas de g√©n√©ration automatique de bio
- Awards bruts non format√©s
- Workflow moins fluide

---

## √Ä venir

### [2.1.0] - Planifi√©
- [ ] Base de donn√©es SQLite int√©gr√©e
- [ ] Export vers Stash
- [ ] Import depuis fichiers JSON
- [ ] Historique des modifications
- [ ] Undo/Redo
- [ ] Raccourcis clavier
- [ ] Th√®mes dark/light
- [ ] Support multi-langues

### [2.2.0] - En r√©flexion
- [ ] Scraping d'images
- [ ] D√©tection automatique de doublons
- [ ] Suggestions intelligentes
- [ ] API REST pour int√©grations
- [ ] Plugin system
- [ ] Scraping de sc√®nes/films
- [ ] Statistiques et graphiques

---

**Format du Changelog** : [Keep a Changelog](https://keepachangelog.com/)  
**Versioning** : [Semantic Versioning](https://semver.org/)


============================================================
[3/124] check_db_145.py
------------------------------------------------------------
import sqlite3
import os
import sys

# Try multiple paths
paths = [
    r"H:\Stash\stash-go.sqlite",
    r"F:\Nouveau dossier\data\database.sqlite",
    "stash-go.sqlite"
]

DB_PATH = None
for p in paths:
    if os.path.exists(p):
        DB_PATH = p
        break

if not DB_PATH:
    print(f"Database not found in: {paths}")
    # Force creation of result file to signal failure
    with open("performer_145_data.txt", "w") as f:
        f.write("DB NOT FOUND")
    sys.exit(1)

print(f"Using DB: {DB_PATH}")

try:
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    cur = conn.cursor()
    
    cur.execute("SELECT * FROM performers WHERE id = ?", (145,))
    row = cur.fetchone()
    
    with open("performer_145_data.txt", "w", encoding="utf-8") as f:
        if row:
            f.write(f"--- Performer 145 Found ---\n")
            f.write(f"Name: {row['name']}\n")
            f.write(f"URLs: {row['urls']}\n")
            # Dump all fields just in case
            f.write("Full Data:\n")
            for key in row.keys():
                f.write(f"{key}: {row[key]}\n")
            print(f"Data written to performer_145_data.txt")
        else:
            f.write("Performer 145 NOT FOUND\n")
            print("Performer 145 NOT FOUND")
        
    conn.close()
except Exception as e:
    print(f"Error: {e}")
    with open("performer_145_data.txt", "w") as f:
        f.write(f"Error: {e}")



============================================================
[4/124] check_performer_145.py
------------------------------------------------------------
import sqlite3
import json

DB_PATH = "H:/Stash/stash-go.sqlite" # Assuming this is the path based on inspect_db.py
# If not found, try config
import os
if not os.path.exists(DB_PATH):
    # Try to load config
    try:
        from services.config_manager import ConfigManager
        config = ConfigManager()
        DB_PATH = config.get("database_path")
    except:
        DB_PATH = "data/database.sqlite"

print(f"Using DB: {DB_PATH}")

def get_performer(id):
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    cur = conn.cursor()
    
    # Check if table exists
    try:
        cur.execute("SELECT * FROM performers WHERE id = ?", (id,))
        row = cur.fetchone()
        if row:
            print(f"--- Performer {id} ---")
            for key in row.keys():
                print(f"{key}: {row[key]}")
        else:
            print(f"Performer {id} not found.")
    except Exception as e:
        print(f"Error: {e}")
    finally:
        conn.close()

if __name__ == "__main__":
    get_performer(145)


============================================================
[5/124] config.json
------------------------------------------------------------
{
  "scrapers": {
    "timeout": 10,
    "retry_count": 3,
    "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
  },
  "bio_generation": {
    "google_template_length": 3000,
    "ollama_url": "http://localhost:11434/api/generate",
    "ollama_path": "E:\\Ollama",
    "ollama_model": "dolphin-mistral:7b",
    "ollama_timeout": 120
  },
  "tag_rules": {
    "ethnicity_tags": {
      "caucasian": "Caucasian",
      "latin": "Latina",
      "cuban": "Latina",
      "asian": "Asian",
      "ebony": "Ebony",
      "african": "Ebony"
    },
    "hair_color_tags": {
      "blonde": "Blonde",
      "blond": "Blonde",
      "brown": "Brunette",
      "brunette": "Brunette",
      "red": "Redhead",
      "auburn": "Redhead",
      "black": "Black Hair"
    },
    "measurements_thresholds": {
      "big_boobs_min": 36,
      "small_boobs_max": 32
    },
    "career_length_thresholds": {
      "milf_years": 10
    }
  },
  "sources": {
    "iafd": {
      "enabled": true,
      "priority": 1
    },
    "freeones": {
      "enabled": true,
      "priority": 2
    },
    "babepedia": {
      "enabled": true,
      "priority": 3
    },
    "thenude": {
      "enabled": true,
      "priority": 4
    }
  },
  "ui": {
    "window_width": 1200,
    "window_height": 900,
    "theme": "default"
  },
  "data": {
    "performers_dir": "data/performers",
    "database_path": "data/database.sqlite"
  }
}

============================================================
[6/124] CONTRIBUTING.md
------------------------------------------------------------
# Guide de Contribution

Merci de votre int√©r√™t pour contribuer √† StashMaster V2 ! üéâ

## üìã Table des Mati√®res

- [Code de Conduite](#code-de-conduite)
- [Comment Contribuer](#comment-contribuer)
- [D√©veloppement](#d√©veloppement)
- [Standards de Code](#standards-de-code)
- [Tests](#tests)
- [Documentation](#documentation)

## ü§ù Code de Conduite

- Soyez respectueux envers tous les contributeurs
- Fournissez des critiques constructives
- Concentrez-vous sur ce qui est le mieux pour le projet
- Acceptez les feedbacks avec gr√¢ce

## üí° Comment Contribuer

### Rapporter des Bugs

Avant de cr√©er une issue :
1. V√©rifiez si le bug n'a pas d√©j√† √©t√© rapport√©
2. Utilisez la derni√®re version du code
3. Testez avec une installation propre

Pour rapporter un bug, incluez :
- **Description claire** du probl√®me
- **√âtapes pour reproduire** le bug
- **Comportement attendu** vs. comportement observ√©
- **Screenshots** si applicable
- **Environnement** : OS, version Python, d√©pendances
- **Logs d'erreur** si disponibles

### Sugg√©rer des Am√©liorations

Pour sugg√©rer une nouvelle fonctionnalit√© :
1. V√©rifiez si elle n'est pas d√©j√† planifi√©e (voir CHANGELOG)
2. Cr√©ez une issue avec le label "enhancement"
3. D√©crivez clairement :
   - Le probl√®me que √ßa r√©sout
   - Comment √ßa devrait fonctionner
   - Des exemples d'utilisation
   - Des alternatives consid√©r√©es

### Soumettre des Pull Requests

1. **Fork** le projet
2. **Cr√©ez une branche** pour votre fonctionnalit√©
   ```bash
   git checkout -b feature/ma-super-feature
   ```
3. **Committez** vos changements
   ```bash
   git commit -m "feat: ajout de ma super feature"
   ```
4. **Pushez** vers la branche
   ```bash
   git push origin feature/ma-super-feature
   ```
5. **Ouvrez une Pull Request**

## üõ†Ô∏è D√©veloppement

### Configuration de l'Environnement

```bash
# Cloner le repository
git clone https://github.com/votre-username/stashmaster-v2.git
cd stashmaster-v2

# Cr√©er un environnement virtuel
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer les d√©pendances
pip install -r requirements.txt

# Installer les d√©pendances de d√©veloppement (si disponibles)
pip install -r requirements-dev.txt
```

### Structure du Projet

```
stashmaster-v2/
‚îÇ
‚îú‚îÄ‚îÄ stashmaster_unified.py    # Application principale
‚îÇ   ‚îú‚îÄ‚îÄ MainWindow            # GUI principale
‚îÇ   ‚îú‚îÄ‚îÄ TriviaAwardsWindow    # Fen√™tre Trivia/Awards
‚îÇ   ‚îú‚îÄ‚îÄ BioGenerationWindow   # Fen√™tre g√©n√©ration de bio
‚îÇ   ‚îú‚îÄ‚îÄ TagRulesEngine        # Moteur de tags
‚îÇ   ‚îú‚îÄ‚îÄ AwardsCleaner         # Nettoyeur d'awards
‚îÇ   ‚îî‚îÄ‚îÄ BioGenerator          # G√©n√©rateur de bio
‚îÇ
‚îú‚îÄ‚îÄ scrapers.py               # Modules de scraping
‚îÇ   ‚îú‚îÄ‚îÄ ScraperBase           # Classe de base
‚îÇ   ‚îú‚îÄ‚îÄ IAFDScraper           # Scraper IAFD
‚îÇ   ‚îú‚îÄ‚îÄ FreeonesScraper       # Scraper Freeones
‚îÇ   ‚îú‚îÄ‚îÄ BabepaediaScraper     # Scraper Babepedia
‚îÇ   ‚îú‚îÄ‚îÄ TheNudeScraper        # Scraper TheNude
‚îÇ   ‚îú‚îÄ‚îÄ DataMerger            # Fusionneur de donn√©es
‚îÇ   ‚îî‚îÄ‚îÄ ScraperOrchestrator   # Orchestrateur
‚îÇ
‚îú‚îÄ‚îÄ test_stashmaster.py       # Tests unitaires
‚îú‚îÄ‚îÄ config.json               # Configuration
‚îú‚îÄ‚îÄ requirements.txt          # D√©pendances
‚îú‚îÄ‚îÄ README.md                 # Documentation
‚îú‚îÄ‚îÄ CHANGELOG.md              # Historique des versions
‚îî‚îÄ‚îÄ CONTRIBUTING.md           # Ce fichier
```

### Lancer l'Application en Mode D√©veloppement

```bash
# Mode normal
python3 stashmaster_unified.py

# Avec logs de debug (√† impl√©menter)
python3 stashmaster_unified.py --debug

# Avec un performer sp√©cifique (√† impl√©menter)
python3 stashmaster_unified.py --performer "Bridgette B"
```

## üìù Standards de Code

### Style Python

Suivez [PEP 8](https://www.python.org/dev/peps/pep-0008/) :

```python
# Bonnes pratiques
class MyClass:
    """Docstring pour la classe"""
    
    def my_method(self, param1: str, param2: int) -> bool:
        """Docstring pour la m√©thode
        
        Args:
            param1: Description du param√®tre 1
            param2: Description du param√®tre 2
            
        Returns:
            Description du retour
        """
        # Code ici
        return True

# Imports group√©s
import sys
import os
from typing import Dict, List

import requests
from bs4 import BeautifulSoup

from scrapers import IAFDScraper
```

### Nommage

- **Classes** : PascalCase (`TagRulesEngine`)
- **Fonctions/M√©thodes** : snake_case (`generate_tags`)
- **Constantes** : UPPER_CASE (`MAX_RETRIES`)
- **Variables priv√©es** : pr√©fixe `_` (`_internal_method`)

### Docstrings

Utilisez le format Google :

```python
def scrape_performer(self, url: str) -> Dict:
    """Scrape les donn√©es d'un performer.
    
    Args:
        url: L'URL de la page du performer
        
    Returns:
        Dictionnaire contenant les m√©tadonn√©es du performer
        
    Raises:
        ValueError: Si l'URL est invalide
        RequestException: Si le scraping √©choue
        
    Examples:
        >>> scraper.scrape_performer("https://example.com/performer")
        {'name': 'John Doe', 'birthdate': '1990-01-01'}
    """
    pass
```

### Type Hints

Utilisez les type hints pour am√©liorer la lisibilit√© :

```python
from typing import Dict, List, Optional, Tuple

def merge_data(self, sources: List[Dict]) -> Tuple[Dict, Dict]:
    """Fusionne les donn√©es de plusieurs sources"""
    pass

def get_performer(self, id: int) -> Optional[Dict]:
    """R√©cup√®re un performer par ID"""
    pass
```

## üß™ Tests

### Lancer les Tests

```bash
# Tous les tests
python3 test_stashmaster.py

# Tests sp√©cifiques
python3 -m unittest test_stashmaster.TestTagRulesEngine

# Avec couverture (si coverage install√©)
coverage run test_stashmaster.py
coverage report
```

### √âcrire des Tests

```python
import unittest

class TestMyFeature(unittest.TestCase):
    def setUp(self):
        """Pr√©paration avant chaque test"""
        self.engine = TagRulesEngine()
    
    def tearDown(self):
        """Nettoyage apr√®s chaque test"""
        pass
    
    def test_my_feature(self):
        """Test de ma fonctionnalit√©"""
        result = self.engine.generate_tags({})
        self.assertIsInstance(result, list)
        self.assertEqual(len(result), 0)
```

### Couverture de Tests

Visez au minimum :
- 80% de couverture pour le code principal
- 60% pour les scrapers (d√©pendent de sources externes)
- 100% pour les utilitaires critiques (TagRulesEngine, DataMerger)

## üìö Documentation

### Documenter le Code

- **Classes** : Docstring avec description, attributs
- **M√©thodes** : Docstring avec Args, Returns, Raises
- **Modules** : Docstring d'en-t√™te avec description g√©n√©rale

### Mettre √† Jour la Documentation

Lors de l'ajout de fonctionnalit√©s :
1. **README.md** : Ajouter dans la section correspondante
2. **CHANGELOG.md** : Documenter le changement
3. **Docstrings** : Commenter le code
4. **config.json** : Ajouter les nouvelles options

## üîÄ Workflow Git

### Branches

- `main` : Code stable, production
- `develop` : D√©veloppement en cours
- `feature/*` : Nouvelles fonctionnalit√©s
- `bugfix/*` : Corrections de bugs
- `hotfix/*` : Corrections urgentes

### Messages de Commit

Utilisez [Conventional Commits](https://www.conventionalcommits.org/) :

```bash
# Format
<type>(<scope>): <description>

[corps optionnel]

[footer(s) optionnel(s)]

# Exemples
feat(tags): ajout de la r√®gle MILF bas√©e sur l'√¢ge
fix(scraper): correction du parsing IAFD
docs(readme): mise √† jour des instructions d'installation
test(tags): ajout de tests pour les tags d'ethnicit√©
refactor(bio): am√©lioration de la g√©n√©ration Google
style(ui): correction de l'alignement des boutons
```

Types :
- `feat` : Nouvelle fonctionnalit√©
- `fix` : Correction de bug
- `docs` : Documentation
- `style` : Formatage (pas de changement de code)
- `refactor` : Refactoring
- `test` : Ajout de tests
- `chore` : Maintenance

## üéØ Priorit√©s de D√©veloppement

Consultez le [CHANGELOG.md](CHANGELOG.md) pour voir les fonctionnalit√©s planifi√©es.

### Court Terme (v2.1)
- Base de donn√©es SQLite
- Export vers Stash
- Import JSON
- Historique des modifications

### Moyen Terme (v2.2)
- Scraping d'images
- D√©tection de doublons
- API REST
- Plugin system

### Long Terme (v3.0)
- Interface web
- Multi-utilisateurs
- Synchronisation cloud
- Mobile app

## ‚ùì Questions ?

N'h√©sitez pas √† :
- Ouvrir une issue pour discuter
- Rejoindre les discussions
- Contacter les mainteneurs

## üôè Remerciements

Merci √† tous les contributeurs qui aident √† am√©liorer StashMaster V2 !

---

**Happy Coding!** üöÄ


============================================================
[7/124] data\README.md
------------------------------------------------------------
# Dossier Data

Ce dossier contient toutes les donn√©es sauvegard√©es par StashMaster V2.

## Structure

```
data/
‚îú‚îÄ‚îÄ performers/          # JSON files des performers
‚îÇ   ‚îú‚îÄ‚îÄ performer_1.json
‚îÇ   ‚îú‚îÄ‚îÄ performer_2.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ database.sqlite     # Base de donn√©es (futur)
```

## Format JSON des Performers

```json
{
  "name": "Performer Name",
  "aliases": ["Alias 1", "Alias 2"],
  "birthdate": "January 1, 1990",
  "birthplace": "City, Country",
  "ethnicity": "Caucasian",
  "hair_color": "Blonde",
  "eye_color": "Blue",
  "height": "170 cm",
  "weight": "55 kg",
  "measurements": "34DD-25-36",
  "tattoos": "Description of tattoos",
  "piercings": "Description of piercings",
  "career_length": "2010-",
  "tags": ["Tag1", "Tag2", "Tag3"],
  "urls": [
    "https://source1.com/...",
    "https://source2.com/..."
  ],
  "trivia": "Interesting facts...",
  "awards": "Awards and nominations...",
  "bio": "Full biography (3000 characters)..."
}
```

## Sauvegarde et Restauration

### Sauvegarde manuelle
Copiez simplement le dossier `data/` pour cr√©er une sauvegarde.

### Restauration
Remplacez le dossier `data/` par votre sauvegarde.

## Notes

- Les fichiers JSON sont encod√©s en UTF-8
- Les noms de fichiers sont en minuscules avec underscores
- La base de donn√©es SQLite sera ajout√©e dans une version future


============================================================
[8/124] EXAMPLES.md
------------------------------------------------------------
# Exemples d'Utilisation

Ce document contient des exemples pratiques pour utiliser StashMaster V2.

## üìã Exemples Basiques

### Exemple 1 : Scraping Simple

```python
# Dans un script Python
from scrapers import ScraperOrchestrator

# Cr√©er l'orchestrateur
orchestrator = ScraperOrchestrator()

# URLs √† scraper
urls = [
    "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
    "https://www.freeones.xxx/bridgette-b"
]

# Scraper et fusionner
confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)

# Afficher les r√©sultats
print(f"Donn√©es de {num_sources} source(s)")
print("\nConfirm√©es:")
for field, info in confirmed.items():
    print(f"  {field}: {info['value']}")

print("\nConflits:")
for field, values in conflicts.items():
    print(f"  {field}:")
    for v in values:
        print(f"    - {v['value']} ({', '.join(v['sources'])})")
```

### Exemple 2 : G√©n√©ration de Tags

```python
from stashmaster_unified import TagRulesEngine

# Cr√©er le moteur de r√®gles
engine = TagRulesEngine()

# M√©tadonn√©es d'exemple
metadata = {
    'ethnicity': 'Latina',
    'hair_color': 'Blonde',
    'measurements': '36DD-25-36',
    'piercings': 'Navel',
    'tattoos': 'Lower back',
    'career_length': '2007-'
}

# G√©n√©rer les tags
tags = engine.generate_tags(metadata)
print(f"Tags g√©n√©r√©s: {', '.join(tags)}")
# Output: Tags g√©n√©r√©s: Latina, Blonde, Big Boobs, Pierced, Tattooed, MILF
```

### Exemple 3 : Nettoyage d'Awards

```python
from stashmaster_unified import AwardsCleaner

cleaner = AwardsCleaner()

# Awards bruts
raw_awards = """
AVN AWARDS2012Winner: Unsung Starlet of the Year2014Nominee: Unsung Starlet of the Year
2015Nominee: Fan Award: Best Boobs
"""

# Nettoyer
cleaned = cleaner.clean_awards(raw_awards)
print(cleaned)
```

Output:
```
AVN AWARDS

2012
  Winner: Unsung Starlet of the Year

2014
  Nominee: Unsung Starlet of the Year

2015
  Nominee: Fan Award: Best Boobs
```

### Exemple 4 : G√©n√©ration de Bio

```python
from stashmaster_unified import BioGenerator

generator = BioGenerator()

# M√©tadonn√©es du performer
metadata = {
    'name': 'Bridgette B',
    'birthdate': 'October 15, 1983',
    'birthplace': 'Barcelona, Spain',
    'ethnicity': 'Caucasian',
    'hair_color': 'Blonde',
    'measurements': '34DD-27-34',
    'height': '173 cm',
    'weight': '129 lbs',
    'career_start': '2007',
    'aliases': ['Bridget B', 'Bridgette', 'Spanish Doll']
}

# G√©n√©rer bio Google
bio = generator.generate_google_bio('Bridgette B', metadata)
print(f"Bio g√©n√©r√©e ({len(bio)} caract√®res):\n{bio}")
```

## üîß Exemples Avanc√©s

### Exemple 5 : Fusion de Donn√©es Complexes

```python
from scrapers import DataMerger

merger = DataMerger()

# Donn√©es de 3 sources diff√©rentes
sources = [
    {
        'source': 'iafd',
        'name': 'Bridgette B',
        'birthdate': 'October 15, 1983',
        'ethnicity': 'Caucasian',
        'hair_color': 'Blonde',
        'measurements': '34DD-27-34'
    },
    {
        'source': 'freeones',
        'name': 'Bridgette B',
        'birthdate': 'October 15, 1983',
        'ethnicity': 'Caucasian',
        'hair_color': 'Blonde',  # Conflit
        'height': '173 cm'
    },
    {
        'source': 'babepedia',
        'name': 'Bridgette B',
        'birthdate': 'October 15, 1983',
        'ethnicity': 'Caucasian',
        'hair_color': 'Brown',  # Conflit
        'weight': '129 lbs'
    }
]

# Fusionner
confirmed, conflicts = merger.merge_data(sources)

print("=== Donn√©es Confirm√©es ===")
for field, info in confirmed.items():
    sources_str = ', '.join(info['sources'])
    print(f"{field}: {info['value']} ({info['count']} sources: {sources_str})")

print("\n=== Conflits ===")
for field, values in conflicts.items():
    print(f"\n{field}:")
    for v in values:
        print(f"  - {v['value']} ({v['count']} sources: {', '.join(v['sources'])})")
```

### Exemple 6 : Scraping avec Gestion d'Erreurs

```python
from scrapers import IAFDScraper
import requests

scraper = IAFDScraper()

urls = [
    "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
    "https://www.iafd.com/person.rme/perfid=invalid/gender=f/invalid.htm"
]

for url in urls:
    print(f"\nScraping: {url}")
    try:
        data = scraper.scrape_performer(url)
        if data:
            print(f"  ‚úÖ Succ√®s: {data.get('name', 'Unknown')}")
            print(f"  Champs: {len(data)}")
        else:
            print("  ‚ùå √âchec: Aucune donn√©e retourn√©e")
    except requests.RequestException as e:
        print(f"  ‚ùå Erreur r√©seau: {e}")
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
```

### Exemple 7 : Cr√©ation d'un Scraper Personnalis√©

```python
from scrapers import ScraperBase
from typing import Dict

class MonSiteScraper(ScraperBase):
    """Scraper pour mon site personnalis√©"""
    
    def scrape_performer(self, url: str) -> Dict:
        """Scrape un performer depuis mon site"""
        soup = self.get_page(url)
        if not soup:
            return {}
        
        data = {
            'source': 'monsite',
            'url': url
        }
        
        try:
            # Extraire le nom
            name_elem = soup.find('h1', class_='performer-name')
            if name_elem:
                data['name'] = name_elem.text.strip()
            
            # Extraire la date de naissance
            birthday_elem = soup.find('span', class_='birthday')
            if birthday_elem:
                data['birthdate'] = birthday_elem.text.strip()
            
            # Ajouter d'autres extractions...
            
        except Exception as e:
            print(f"Erreur: {e}")
        
        return data

# Utilisation
scraper = MonSiteScraper()
data = scraper.scrape_performer("https://monsite.com/performer/123")
print(data)
```

### Exemple 8 : Int√©gration avec l'Interface

```python
# Dans votre propre script
from stashmaster_unified import MainWindow
import tkinter as tk

# Cr√©er et configurer la fen√™tre
app = MainWindow()

# Pr√©-remplir des donn√©es (exemple)
app.metadata_entries['name'].insert(0, "Bridgette B")
app.urls_text.insert('1.0', "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm")

# Lancer l'application
app.mainloop()
```

## üéì Cas d'Usage R√©els

### Cas 1 : Workflow Complet Automatis√©

```python
#!/usr/bin/env python3
"""
Workflow automatis√© complet pour un performer
"""

from scrapers import ScraperOrchestrator
from stashmaster_unified import TagRulesEngine, BioGenerator
import json

def process_performer(name: str, urls: list) -> dict:
    """Traite compl√®tement un performer"""
    print(f"\n{'='*50}")
    print(f"Traitement de: {name}")
    print('='*50)
    
    # 1. Scraping
    print("\n1. Scraping des sources...")
    orchestrator = ScraperOrchestrator()
    confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)
    print(f"   ‚úÖ {num_sources} source(s) scrap√©e(s)")
    print(f"   ‚úÖ {len(confirmed)} champ(s) confirm√©(s)")
    print(f"   ‚ö†Ô∏è  {len(conflicts)} conflit(s)")
    
    # 2. Pr√©parer les m√©tadonn√©es
    print("\n2. Pr√©paration des m√©tadonn√©es...")
    metadata = {key: info['value'] for key, info in confirmed.items()}
    metadata['name'] = name
    
    # 3. G√©n√©rer les tags
    print("\n3. G√©n√©ration des tags...")
    tag_engine = TagRulesEngine()
    tags = tag_engine.generate_tags(metadata)
    metadata['tags'] = tags
    print(f"   ‚úÖ {len(tags)} tag(s) g√©n√©r√©(s): {', '.join(tags)}")
    
    # 4. G√©n√©rer la bio
    print("\n4. G√©n√©ration de la bio...")
    bio_generator = BioGenerator()
    bio = bio_generator.generate_google_bio(name, metadata)
    metadata['bio'] = bio
    print(f"   ‚úÖ Bio g√©n√©r√©e ({len(bio)} caract√®res)")
    
    # 5. Sauvegarder
    print("\n5. Sauvegarde...")
    filename = f"data/performers/{name.lower().replace(' ', '_')}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"   ‚úÖ Sauvegard√©: {filename}")
    
    return metadata

# Exemple d'utilisation
if __name__ == "__main__":
    performer_data = process_performer(
        name="Bridgette B",
        urls=[
            "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
            "https://www.freeones.xxx/bridgette-b"
        ]
    )
    
    print("\n" + "="*50)
    print("‚úÖ Traitement termin√© avec succ√®s!")
    print("="*50)
```

### Cas 2 : Batch Processing de Plusieurs Performers

```python
#!/usr/bin/env python3
"""
Traitement par lots de plusieurs performers
"""

import json
from pathlib import Path
from scrapers import ScraperOrchestrator
from stashmaster_unified import TagRulesEngine, BioGenerator

def batch_process(performers_file: str):
    """Traite plusieurs performers depuis un fichier JSON"""
    
    # Charger la liste
    with open(performers_file, 'r') as f:
        performers = json.load(f)
    
    print(f"Traitement de {len(performers)} performer(s)...\n")
    
    orchestrator = ScraperOrchestrator()
    tag_engine = TagRulesEngine()
    bio_generator = BioGenerator()
    
    results = {
        'success': [],
        'failed': [],
        'partial': []
    }
    
    for i, performer in enumerate(performers, 1):
        name = performer['name']
        urls = performer['urls']
        
        print(f"\n[{i}/{len(performers)}] {name}")
        print("-" * 40)
        
        try:
            # Scraping
            confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)
            
            if num_sources == 0:
                print("  ‚ùå Aucune source valide")
                results['failed'].append(name)
                continue
            
            # M√©tadonn√©es
            metadata = {key: info['value'] for key, info in confirmed.items()}
            metadata['name'] = name
            
            # Tags
            tags = tag_engine.generate_tags(metadata)
            metadata['tags'] = tags
            
            # Bio
            bio = bio_generator.generate_google_bio(name, metadata)
            metadata['bio'] = bio
            
            # Sauvegarder
            filename = f"data/performers/{name.lower().replace(' ', '_')}.json"
            Path("data/performers").mkdir(parents=True, exist_ok=True)
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            if len(conflicts) > 0:
                print(f"  ‚ö†Ô∏è  Succ√®s partiel ({len(conflicts)} conflits)")
                results['partial'].append(name)
            else:
                print("  ‚úÖ Succ√®s complet")
                results['success'].append(name)
        
        except Exception as e:
            print(f"  ‚ùå Erreur: {e}")
            results['failed'].append(name)
    
    # R√©sum√©
    print("\n" + "="*50)
    print("R√âSUM√â")
    print("="*50)
    print(f"‚úÖ Succ√®s complet: {len(results['success'])}")
    print(f"‚ö†Ô∏è  Succ√®s partiel: {len(results['partial'])}")
    print(f"‚ùå √âchecs: {len(results['failed'])}")
    
    return results

# Exemple d'utilisation
if __name__ == "__main__":
    # Cr√©er un fichier performers_list.json avec:
    # [
    #   {
    #     "name": "Performer 1",
    #     "urls": ["url1", "url2"]
    #   },
    #   ...
    # ]
    
    results = batch_process("performers_list.json")
```

### Cas 3 : Validation et Correction Semi-Automatique

```python
#!/usr/bin/env python3
"""
Validation et correction semi-automatique des donn√©es
"""

from scrapers import ScraperOrchestrator
from stashmaster_unified import TagRulesEngine

def validate_and_correct(urls: list) -> dict:
    """Valide et propose des corrections"""
    
    orchestrator = ScraperOrchestrator()
    confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)
    
    print("="*50)
    print("VALIDATION DES DONN√âES")
    print("="*50)
    
    # Donn√©es confirm√©es
    print("\n‚úÖ Donn√©es confirm√©es:")
    for field, info in confirmed.items():
        print(f"  {field}: {info['value']}")
        print(f"    Sources: {', '.join(info['sources'])}")
    
    # Conflits √† r√©soudre
    if conflicts:
        print("\n‚ö†Ô∏è  CONFLITS √Ä R√âSOUDRE:")
        corrections = {}
        
        for field, values in conflicts.items():
            print(f"\n  {field}:")
            for i, v in enumerate(values, 1):
                print(f"    [{i}] {v['value']} ({', '.join(v['sources'])})")
            
            # Demander √† l'utilisateur de choisir
            while True:
                choice = input(f"  Choisir [1-{len(values)}] ou [s]kip: ")
                if choice.lower() == 's':
                    break
                try:
                    idx = int(choice) - 1
                    if 0 <= idx < len(values):
                        corrections[field] = values[idx]['value']
                        print(f"    ‚úÖ {field} = {values[idx]['value']}")
                        break
                except ValueError:
                    pass
                print("    ‚ùå Choix invalide")
        
        # Appliquer les corrections
        for field, value in corrections.items():
            confirmed[field] = {
                'value': value,
                'note': 'Corrig√© manuellement'
            }
    
    # R√©sultat final
    final_data = {key: info['value'] for key, info in confirmed.items()}
    
    print("\n" + "="*50)
    print("DONN√âES FINALES")
    print("="*50)
    for field, value in final_data.items():
        print(f"  {field}: {value}")
    
    return final_data

# Exemple
if __name__ == "__main__":
    urls = [
        "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
        "https://www.freeones.xxx/bridgette-b"
    ]
    
    data = validate_and_correct(urls)
```

## üîó Int√©grations

### Int√©gration avec Stash

```python
import requests
import json

class StashAPI:
    """Client pour l'API Stash"""
    
    def __init__(self, url="http://localhost:9999", api_key=None):
        self.url = url
        self.api_key = api_key
    
    def create_performer(self, performer_data: dict) -> dict:
        """Cr√©e un performer dans Stash"""
        # GraphQL mutation
        mutation = """
        mutation PerformerCreate($input: PerformerCreateInput!) {
          performerCreate(input: $input) {
            id
            name
          }
        }
        """
        
        variables = {
            "input": {
                "name": performer_data.get('name'),
                "birthdate": performer_data.get('birthdate'),
                "ethnicity": performer_data.get('ethnicity'),
                "hair_color": performer_data.get('hair_color'),
                "height": performer_data.get('height'),
                "measurements": performer_data.get('measurements'),
                "tags": performer_data.get('tags', [])
            }
        }
        
        response = requests.post(
            f"{self.url}/graphql",
            json={"query": mutation, "variables": variables}
        )
        
        return response.json()

# Utilisation
stash = StashAPI()
result = stash.create_performer(performer_data)
print(f"Performer cr√©√©: {result}")
```

---

Ces exemples couvrent les cas d'usage les plus courants. Pour plus d'informations, consultez le [README.md](README.md) et la documentation des modules.


============================================================
[9/124] gui\dvd_frame.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DVDFrame - Interface de gestion des DVDs / Groupes
"""

import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox
import threading
from typing import Dict, List, Optional

class DVDFrame(ttk.Frame):
    def __init__(self, parent, dvd_id: Optional[str] = None):
        super().__init__(parent)
        self.dvd_id = dvd_id
        
        # Services
        from services.config_manager import ConfigManager
        from services.database import StashDatabase
        from services.scrapers import ScraperOrchestrator
        
        config = ConfigManager()
        self.db = StashDatabase(config.get('database_path'))
        self.scraper = ScraperOrchestrator()
        
        # UI Attributes for linting
        self.notebook: Optional[ttk.Notebook] = None
        self.tab_metadata: Optional[ttk.Frame] = None
        self.urls_text: Optional[scrolledtext.ScrolledText] = None
        
        self.stash_data: Dict = {}
        self.field_vars: Dict[str, Dict[str, tk.Variable]] = {}
        self.fields = [
            ('name', 'Titre'),
            ('date', 'Date'),
            ('studio', 'Studio'),
            ('director', 'R√©alisateur'),
            ('duration', 'Dur√©e'),
            ('rating', 'Note'),
            ('tags', 'Tags'),
        ]
        
        self._setup_ttk_styles()
        self._create_widgets()
        if dvd_id:
            self._load_from_stash()

    def _setup_ttk_styles(self):
        style = ttk.Style()
        style.map('Valid.TCombobox', fieldbackground=[('readonly', '#d4edda'), ('!disabled', '#d4edda')])
        style.map('Invalid.TCombobox', fieldbackground=[('readonly', '#f8d7da'), ('!disabled', '#f8d7da')])
        style.map('Empty.TCombobox', fieldbackground=[('readonly', '#fff3cd'), ('!disabled', '#fff3cd')])
        style.map('Normal.TCombobox', fieldbackground=[('readonly', 'white'), ('!disabled', 'white')])

    def _create_widgets(self):
        # Initialiser les variables de champ
        for field, label in self.fields:
            self.field_vars[field] = {
                'check': tk.BooleanVar(value=True),
                'stash': tk.StringVar(),
                'main': tk.StringVar(),
                'source': tk.StringVar()
            }

        # Onglets
        self.notebook = ttk.Notebook(self)
        self.notebook.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        self.tab_metadata = ttk.Frame(self.notebook)
        self.notebook.add(self.tab_metadata, text="üìÄ M√©tadonn√©es")
        
        self._setup_metadata_tab()
        
        # Toolbar
        toolbar = ttk.Frame(self, padding=10)
        toolbar.pack(side=tk.BOTTOM, fill=tk.X)
        ttk.Button(toolbar, text="üíæ Sauvegarder dans Stash", command=self._save_to_stash).pack(side=tk.RIGHT, padx=5)
        ttk.Button(toolbar, text="üîç Tout Scraper", command=self._scrape_all).pack(side=tk.LEFT, padx=5)

    def _setup_metadata_tab(self):
        if not self.tab_metadata: return
        container = ttk.Frame(self.tab_metadata, padding=10)
        container.pack(fill=tk.BOTH, expand=True)
        
        # Headers
        headers = ["", "Champ", "Valeur Stash", "Modification", "R√©sultat Scrapers"]
        for i, h in enumerate(headers):
            lbl = ttk.Label(container, text=h, font=('Segoe UI', 9, 'bold'))
            lbl.grid(row=0, column=i, padx=5, pady=5, sticky="w")
        
        # Lignes de champs
        for i, (field, label) in enumerate(self.fields, start=1):
            # Checkbox
            ttk.Checkbutton(container, variable=self.field_vars[field]['check']).grid(row=i, column=0, padx=5)
            # Label
            ttk.Label(container, text=label).grid(row=i, column=1, padx=5, sticky="w")
            # Stash Value (Read-only)
            ent_stash = ttk.Entry(container, textvariable=self.field_vars[field]['stash'], state='readonly', width=30)
            ent_stash.grid(row=i, column=2, padx=5, pady=2, sticky="w")
            # Main Input (Validation)
            ent_main = ttk.Combobox(container, textvariable=self.field_vars[field]['main'], width=40)
            ent_main.grid(row=i, column=3, padx=5, pady=2, sticky="we")
            # Source Result (Read-only)
            ent_source = ttk.Entry(container, textvariable=self.field_vars[field]['source'], state='readonly', width=30)
            ent_source.grid(row=i, column=4, padx=5, pady=2, sticky="we")
            
            self.field_vars[field]['widget'] = ent_main
            
            # Monitoring de changement pour validation visuelle
            self.field_vars[field]['main'].trace_add("write", lambda *args, f=field: self._validate_field(f))

        # URL Area
        url_frame = ttk.LabelFrame(container, text="URLs Sources (IAFD, AdultEmpire, Data18...)", padding=10)
        url_frame.grid(row=len(self.fields)+1, column=0, columnspan=5, sticky="we", pady=15)
        self.urls_text = scrolledtext.ScrolledText(url_frame, height=4, wrap=tk.WORD)
        self.urls_text.pack(fill=tk.X, expand=True)

        container.columnconfigure(3, weight=10)
        container.columnconfigure(2, weight=5)
        container.columnconfigure(4, weight=5)

    def _load_from_stash(self):
        if not self.dvd_id: return
        data = self.db.get_group_metadata(self.dvd_id)
        if data:
            self.stash_data = data
            for field, _ in self.fields:
                val = str(data.get(field, '')) if data.get(field) is not None else ''
                self.field_vars[field]['stash'].set(val)
                self.field_vars[field]['main'].set(val)
                self._validate_field(field)

    def _validate_field(self, field):
        f = self.field_vars[field]
        main_val = f['main'].get().strip()
        stash_val = f['stash'].get().strip()
        is_checked = f['check'].get()
        combo = f.get('widget')
        
        if not is_checked:
            color = "white"
        elif not main_val:
            color = "#fff3cd"
        elif main_val.lower() == stash_val.lower():
            color = "#d4edda"
        else:
            color = "#f8d7da"

        if isinstance(combo, ttk.Combobox):
            style_map = {
                "white": "Normal.TCombobox",
                "#fff3cd": "Empty.TCombobox",
                "#d4edda": "Valid.TCombobox",
                "#f8d7da": "Invalid.TCombobox"
            }
            combo.configure(style=style_map.get(color, "Normal.TCombobox"))

    def _scrape_all(self):
        if not self.urls_text: return
        urls = self.urls_text.get('1.0', tk.END).strip().split('\n')
        urls = [u.strip() for u in urls if u.strip()]
        if not urls:
            messagebox.showwarning("Scraping", "Veuillez entrer au moins une URL.")
            return

        def run_scrape():
            results = self.scraper.scrape_dvd_multi(urls)
            if not results:
                self.after(0, lambda: messagebox.showinfo("Scraping", "Aucun r√©sultat trouv√©."))
                return
            
            from services.scrapers import DataMerger
            confirmed, _ = DataMerger.merge_data(results)
            
            merged_data = {k: v['value'] for k, v in confirmed.items()}
            self.after(0, lambda: self._apply_results(merged_data))

        threading.Thread(target=run_scrape, daemon=True).start()

    def _apply_results(self, data):
        for field, _ in self.fields:
            if field in data and data[field]:
                val = str(data[field])
                self.field_vars[field]['source'].set(val)
                # Utiliser .get() sur BooleanVar pour v√©rifier si coch√©
                if self.field_vars[field]['check'].get():
                    self.field_vars[field]['main'].set(val)
                
                # Update Combobox values
                stash_val = self.field_vars[field]['stash'].get().strip()
                all_vals = sorted(list(set([v for v in [stash_val, val] if v])))
                widget = self.field_vars[field].get('widget')
                if isinstance(widget, ttk.Combobox):
                    widget['values'] = all_vals
        
        # Injection des URLs de sc√®nes (Sp√©cifique Data18)
        if 'scenes' in data and data['scenes'] and self.dvd_id:
            self._inject_scene_urls(data['scenes'])

    def _inject_scene_urls(self, scraped_scenes: List[Dict]):
        """Associe les URLs Data18 aux sc√®nes Stash du DVD"""
        stash_scenes = self.db.get_scenes_for_group(self.dvd_id)
        if not stash_scenes: return
        
        count: int = 0
        for s_scene in scraped_scenes:
            scraped_title = s_scene.get('title', '').lower()
            scraped_url = s_scene.get('url')
            if not scraped_url: continue
            
            # Recherche de match par titre (tr√®s basique pour l'instant)
            for db_scene in stash_scenes:
                db_title = db_scene.get('title', '').lower()
                # Match si le titre est contenu l'un dans l'autre (grossier mais souvent efficace sur Data18)
                if db_title and scraped_title and (db_title in scraped_title or scraped_title in db_title):
                    if self.db.add_scene_url(db_scene['id'], scraped_url):
                        count += 1
                    break
        
        if count > 0:
            print(f"Inject√© {count} URLs de sc√®nes depuis Data18.")
            # Optionnel: Notification discr√®te

    def _save_to_stash(self):
        if not self.dvd_id: return
        updates = {f: var_dict['main'].get() for f, var_dict in self.field_vars.items()}
        if self.db.save_group_metadata(self.dvd_id, updates):
            messagebox.showinfo("Sauvegarde", "DVD mis √† jour avec succ√®s.")
            self._load_from_stash()
        else:
            messagebox.showerror("Sauvegarde", "Erreur lors de la sauvegarde.")


============================================================
[10/124] gui\performer_frame.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PerformerFrame - Interface de gestion des performers
"""

import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox
import threading
import re
from typing import Dict, List, Optional, Tuple, Any

# Imports locaux
from utils.tag_engine import TagRulesEngine
from utils.awards_cleaner import AwardsCleaner
from services.bio_generator import BioGenerator
from services.database import StashDatabase
from services.config_manager import ConfigManager
from services.scrapers import ScraperOrchestrator
from services.source_finder import SourceFinderWidget
from services.url_validator import URLValidatorWidget
from services.url_manager import URLManager # Nouvelle importation
from gui.url_verification_dialog import URLVerificationDialog # Nouvelle importation

# utilitaires pour URL (nettoyage / fusion)
from utils.url_utils import clean_urls_list, merge_urls_by_domain

class PerformerFrame(ttk.Frame):
    """Frame principal pour la gestion des performers"""
    
    def __init__(self, parent, performer_id: Optional[str] = None):
        super().__init__(parent)
        self.performer_id = performer_id
        
        # Initialisation des services
        self.config = ConfigManager()
        self.db = StashDatabase(self.config.get("database_path"))
        self.tag_rules = TagRulesEngine()
        self.awards_cleaner = AwardsCleaner()
        self.bio_generator = BioGenerator()
        self.orchestrator = ScraperOrchestrator()
        self.url_manager = URLManager() # Initialisation URLManager
        
        # Donn√©es
        self.stash_data: Dict[str, Any] = {}
        self.field_vars: Dict[str, Dict[str, Any]] = {}
        
        # Widgets
        self.notebook: ttk.Notebook = None # type: ignore
        self.metadata_tab: ttk.Frame = None # type: ignore
        self.advanced_tab: ttk.Frame = None # type: ignore
        self.bio_tab: ttk.Frame = None # type: ignore
        self.bio_text: scrolledtext.ScrolledText = None # type: ignore
        self.char_label: ttk.Label = None # type: ignore
        self.progress_bar: ttk.Progressbar = None # type: ignore
        self.status_label: ttk.Label = None # type: ignore
        self.url_tree: ttk.Treeview = None # type: ignore
        self.fields = [
            ("Nom", "name"),
            ("Aliases", "aliases"),
            ("Date Naissance", "birthdate"),
            ("Lieu Naissance", "birthplace"),
            ("Date D√©c√®s", "deathdate"),
            ("Pays", "country"),
            ("Ethnicit√©", "ethnicity"),
            ("Cheveux", "hair_color"),
            ("Yeux", "eye_color"),
            ("Taille (cm)", "height"),
            ("Poids (kg)", "weight"),
            ("Mesures", "measurements"),
            ("Poitrine", "fake_tits"),
            ("Ann√©es activit√©", "career_length"),
        ]
        
        self.source_labels = {}
        self.bio_valid_var = tk.BooleanVar(value=True)

        # Bio UI (4 sous-onglets)
        self._bio_notebook: Optional[ttk.Notebook] = None
        self._bio_slots: List[str] = ["", "", "", ""]  # 0=bio_raw, 1=trivia, 2=google, 3=ollama
        self._bio_merge_content: str = ""
        self._merge_vars: List[tk.BooleanVar] = []

        self._bio_raw_text: Optional[scrolledtext.ScrolledText] = None
        self._bio_trivia_disp: Optional[scrolledtext.ScrolledText] = None
        self._bio_google_text: Optional[scrolledtext.ScrolledText] = None
        self._bio_ollama_text: Optional[scrolledtext.ScrolledText] = None
        self._bio_merge_text: Optional[scrolledtext.ScrolledText] = None

        self._lbl_scrape_chars: Optional[ttk.Label] = None
        self._lbl_scrape_chars2: Optional[ttk.Label] = None
        self._lbl_google_chars: Optional[ttk.Label] = None
        self._lbl_ollama_chars: Optional[ttk.Label] = None
        self._lbl_merge_chars: Optional[ttk.Label] = None
        self._lbl_url_count: Optional[ttk.Label] = None
        self._lbl_award_count: Optional[ttk.Label] = None

        self._ollama_status: Optional[ttk.Label] = None
        self._merge_status: Optional[ttk.Label] = None
        
        self._setup_ttk_styles()
        self._create_widgets()
        
        # Diff√©rer le chargement pour que la mainloop soit d√©marr√©e
        if performer_id:
            self.after(100, self._load_from_stash)

    def _setup_ttk_styles(self):
        style = ttk.Style()
        # On d√©finit des styles pour les Combobox bas√©s sur la validation
        style.map('Valid.TCombobox', fieldbackground=[('readonly', '#d4edda'), ('!disabled', '#d4edda')])
        style.map('Invalid.TCombobox', fieldbackground=[('readonly', '#f8d7da'), ('!disabled', '#f8d7da')])
        style.map('Empty.TCombobox', fieldbackground=[('readonly', '#fff3cd'), ('!disabled', '#fff3cd')])
        style.map('Normal.TCombobox', fieldbackground=[('readonly', 'white'), ('!disabled', 'white')])

    def _create_widgets(self):
        """Cr√©e l'interface de l'onglet Performer"""
        # Notebook pour les sous-onglets
        self.notebook = ttk.Notebook(self)
        self.notebook.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        # Onglets
        self.metadata_tab = self._create_metadata_tab()
        self.advanced_tab = self._create_advanced_tab()
        self.bio_tab = self._create_bio_tab()
        
        self.notebook.add(self.metadata_tab, text="üìã M√©tadonn√©es")
        self.notebook.add(self.advanced_tab, text="‚öôÔ∏è Tags & D√©tails")
        self.notebook.add(self.bio_tab, text="üìù Biographie")
        
        # Barre d'outils
        self._create_toolbar()

    def _get_field_values(self) -> Dict[str, Any]:
        """R√©cup√®re toutes les valeurs de validation actuelles"""
        values = {}
        for k, v in self.field_vars.items():
            if v.get('is_multiline'):
                # Handle tk.Text widget
                val = v['entry'].get('1.0', tk.END).strip()
                if k == 'aliases':
                    values[k] = [a.strip() for a in re.split(r'[,\n\r]+', val) if a.strip()]
                elif k == 'discovered_urls' or k == 'urls':
                    values[k] = [u.strip() for u in re.split(r'[,\n\r\s]+', val) if u.strip()]
                else:
                    values[k] = val
            else:
                # Handle tk.StringVar
                values[k] = v['main'].get().strip()
        # Ajouter explicitement la biographie (details)
        values['details'] = self._get_best_bio_text()
            
        return values

    def _get_best_bio_text(self) -> str:
        merge_content = (getattr(self, '_bio_merge_content', '') or '').strip()
        if merge_content:
            return merge_content

        ollama = (self._bio_ollama_text.get('1.0', tk.END).strip()
                  if getattr(self, '_bio_ollama_text', None) else '').strip()
        if ollama:
            return ollama

        google = (self._bio_google_text.get('1.0', tk.END).strip()
                  if getattr(self, '_bio_google_text', None) else '').strip()
        if google:
            return google

        # fallback legacy
        if getattr(self, 'bio_text', None):
            try:
                return self.bio_text.get('1.0', tk.END).strip()
            except Exception:
                pass
        return ""

    def _refresh_bio_counters(self):
        try:
            urls_count = 0
            if 'urls' in self.field_vars and self.field_vars['urls'].get('is_multiline'):
                urls_raw = self.field_vars['urls']['entry'].get('1.0', tk.END).strip()
                urls = [u.strip() for u in re.split(r'[\,\n\r\s]+', urls_raw) if u.strip()]
                urls_count = len(clean_urls_list(urls))

            awards_count = 0
            if 'awards' in self.field_vars and self.field_vars['awards'].get('is_multiline'):
                awards_raw = self.field_vars['awards']['entry'].get('1.0', tk.END).strip()
                awards_count = len([l for l in awards_raw.splitlines() if l.strip()])

            if self._lbl_url_count:
                self._lbl_url_count.config(text=f"URLs : {urls_count}")
            if self._lbl_award_count:
                self._lbl_award_count.config(text=f"Awards : {awards_count}")
        except Exception:
            pass

    def _bio_update_chars(self, slot_idx: int, widget: scrolledtext.ScrolledText, label: ttk.Label):
        text = widget.get('1.0', tk.END).strip()
        self._bio_slots[slot_idx] = text
        label.config(text=f"Caract√®res : {len(text)}")

    def _bio_clear(self, slot_idx: int):
        mapping = {
            2: self._bio_google_text,
            3: self._bio_ollama_text,
        }
        widget = mapping.get(slot_idx)
        if not widget:
            return
        widget.delete('1.0', tk.END)
        self._bio_slots[slot_idx] = ""
        if slot_idx == 2 and self._lbl_google_chars:
            self._lbl_google_chars.config(text="Caract√®res : 0")
        if slot_idx == 3 and self._lbl_ollama_chars:
            self._lbl_ollama_chars.config(text="Caract√®res : 0")

    def _update_raw_content(self, bio_raw: Optional[str] = None, trivia: Optional[str] = None):
        # Update internal slot values
        if bio_raw is not None:
            self._bio_slots[0] = str(bio_raw).strip()
        if trivia is not None:
            self._bio_slots[1] = str(trivia).strip()

        if self._bio_raw_text:
            self._bio_raw_text.config(state='normal')
            self._bio_raw_text.delete('1.0', tk.END)
            self._bio_raw_text.insert('1.0', self._bio_slots[0])
            self._bio_raw_text.config(state='disabled')
            if self._lbl_scrape_chars:
                self._lbl_scrape_chars.config(text=f"Caract√®res : {len(self._bio_slots[0])}")

        if self._bio_trivia_disp:
            self._bio_trivia_disp.config(state='normal')
            self._bio_trivia_disp.delete('1.0', tk.END)
            self._bio_trivia_disp.insert('1.0', self._bio_slots[1])
            self._bio_trivia_disp.config(state='disabled')
            if self._lbl_scrape_chars2:
                self._lbl_scrape_chars2.config(text=f"Caract√®res : {len(self._bio_slots[1])}")

        self._refresh_bio_counters()

    def _create_metadata_tab(self) -> ttk.Frame:
        frame = ttk.Frame(self.notebook)
        
        # Style pour les entr√©es textuelles multi-lignes
        # Canvas pour le scroll
        canvas = tk.Canvas(frame, bg="#f5f5f5", highlightthickness=0)
        scrollbar = ttk.Scrollbar(frame, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas, padding=15)
        
        scrollable_frame.bind("<Configure>", lambda e: canvas.configure(scrollregion=canvas.bbox("all")))
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # Header de la grille
        
        # Header de la grille
        headers_labels = ["Scrap", "Champ", "Valeur Stash"]
        # Sources fixes (priorit√© DataMerger)
        source_names = ["IAFD", "FreeOnes", "TheNude", "Babepedia", "Boobpedia", "XXXBios"]
        
        header_fonts = ('Segoe UI', 10, 'bold')
        for col, text in enumerate(headers_labels):
            ttk.Label(scrollable_frame, text=text, font=header_fonts).grid(row=1, column=col, padx=10, pady=10, sticky="w")
        
        for i, name in enumerate(source_names):
            lbl = ttk.Label(scrollable_frame, text=name, font=header_fonts)
            lbl.grid(row=1, column=3 + i, padx=10, pady=10)
            self.source_labels[name] = lbl
        
        # Validation / Edit √† la fin
        ttk.Label(scrollable_frame, text="Validation / Edit", font=header_fonts).grid(row=1, column=3 + len(source_names), padx=10, pady=10, sticky="w")

        for i, (label, key) in enumerate(self.fields, start=1):
            # 0: Checkbox
            var_check = tk.BooleanVar(value=True)
            cb = ttk.Checkbutton(scrollable_frame, variable=var_check)
            cb.grid(row=i+1, column=0, padx=5, pady=5)
            
            # 1: Label
            ttk.Label(scrollable_frame, text=label, font=('Segoe UI', 9)).grid(row=i+1, column=1, sticky="w", padx=10)
            
            # Determine widget type
            is_multiline = key in ['aliases', 'awards', 'discovered_urls', 'trivia', 'tattoos', 'details']
            if key == 'aliases':
                row_height = 10
            else:
                row_height = 3 if is_multiline else 1
            
            # 2: Stash Value
            var_stash = tk.StringVar()
            if is_multiline:
                entry_stash = tk.Text(scrollable_frame, width=30, height=row_height, font=('Segoe UI', 9), bg="#eeeeee", relief=tk.FLAT)
                entry_stash.grid(row=i+1, column=2, padx=2, pady=2, sticky="nsew")
                entry_stash.configure(state="disabled")
            else:
                entry_stash = ttk.Entry(scrollable_frame, textvariable=var_stash, width=30, state="readonly")
                entry_stash.grid(row=i+1, column=2, padx=2, pady=2, sticky="we")
            
            # 3-6: Sources (D√©cal√© √† col 3-6)
            vars_sources = []
            widgets_sources = []
            for col_offset, _ in enumerate(source_names):
                col = 3 + col_offset
                v_src = tk.StringVar()
                if is_multiline:
                    e_src = tk.Text(scrollable_frame, width=20, height=row_height, font=('Segoe UI', 9), bg="#f9f9f9", relief=tk.FLAT)
                    e_src.grid(row=i+1, column=col, padx=2, pady=2, sticky="nsew")
                    e_src.configure(state="disabled")
                else:
                    e_src = ttk.Entry(scrollable_frame, textvariable=v_src, width=20, state="readonly")
                    e_src.grid(row=i+1, column=col, padx=2, pady=2, sticky="we")
                vars_sources.append(v_src)
                widgets_sources.append(e_src)
            
            # 7: Main Input (Validation) - √Ä LA FIN
            var_main = tk.StringVar()
            final_col = 3 + len(source_names)
            if is_multiline:
                entry_main = tk.Text(scrollable_frame, width=45, height=row_height, font=('Segoe UI', 9), relief=tk.FLAT, highlightthickness=1)
                entry_main.grid(row=i+1, column=final_col, padx=2, pady=2, sticky="nsew")
            else:
                entry_main = ttk.Combobox(scrollable_frame, textvariable=var_main, width=45)
                entry_main.grid(row=i+1, column=final_col, padx=2, pady=2, sticky="we")
            
            # Cache vars and widgets
            self.field_vars[key] = {
                'check': var_check,
                'stash': var_stash,
                'main': var_main,
                'entry': entry_main, # This is the widget
                'sources': vars_sources,
                'source_widgets': widgets_sources,
                'stash_widget': entry_stash,
                'is_multiline': is_multiline
            }
            
            # Traces for validation
            def make_callback(k):
                return lambda *args: self._update_validation(k)

            if is_multiline:
                # Text widgets need event binding
                entry_main.bind("<KeyRelease>", make_callback(key))
            else:
                var_check.trace_add("write", make_callback(key))
                var_main.trace_add("write", make_callback(key))
                var_stash.trace_add("write", make_callback(key))
            
            # Allow row to grow for multiline
            if is_multiline:
                scrollable_frame.rowconfigure(i+1, weight=1)
            
        # Configure grid expansion
        final_col = 3 + len(source_names)
        scrollable_frame.columnconfigure(final_col, weight=10) # Validation column takes most space
        scrollable_frame.columnconfigure(2, weight=5) # Stash column
        for c in range(3, final_col):
            scrollable_frame.columnconfigure(c, weight=4) # Source columns
        
        # Expand row for content
        scrollable_frame.rowconfigure(len(self.fields) + 1, weight=1)
        
        # S'assurer que le canvas occupe toute la largeur
        frame.columnconfigure(0, weight=1)
        canvas.grid(row=0, column=0, sticky="nsew")
        scrollbar.grid(row=0, column=1, sticky="ns")
        frame.rowconfigure(0, weight=1)
        
        return frame

    def _create_advanced_tab(self) -> ttk.Frame:
        frame = ttk.Frame(self.notebook)
        frame.columnconfigure(0, weight=1)
        frame.rowconfigure(0, weight=1)
        
        canvas = tk.Canvas(frame, bg="#f0f0f0")
        scrollbar = ttk.Scrollbar(frame, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas, padding=20)
        
        scrollable_frame.bind("<Configure>", lambda e: canvas.configure(scrollregion=canvas.bbox("all")))
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # Disposition en 3 Colonnes
        # Col 1: Tags, Tatouages, Piercings
        col1 = ttk.Frame(scrollable_frame)
        col1.grid(row=0, column=0, sticky="nsew", padx=10)
        
        # Col 2: Awards
        col2 = ttk.Frame(scrollable_frame)
        col2.grid(row=0, column=1, sticky="nsew", padx=10)
        
        # Col 3: URLs
        col3 = ttk.Frame(scrollable_frame)
        col3.grid(row=0, column=2, sticky="nsew", padx=10)
        
        scrollable_frame.columnconfigure(0, weight=1)
        scrollable_frame.columnconfigure(1, weight=1)
        scrollable_frame.columnconfigure(2, weight=1)
        
        # --- Colonne 1 ---
        # Tags
        lf_tags = ttk.LabelFrame(col1, text=" Tags ", padding=10)
        lf_tags.pack(fill=tk.BOTH, expand=True, pady=5)
        self._setup_advanced_field(lf_tags, "tags", has_gen=True)
        
        # Tatouages (ex Tattoos)
        lf_tats = ttk.LabelFrame(col1, text=" Tatouages ", padding=10)
        lf_tats.pack(fill=tk.BOTH, expand=True, pady=5)
        self._setup_advanced_field(lf_tats, "tattoos")
        
        # Piercings
        lf_pierce = ttk.LabelFrame(col1, text=" Piercings ", padding=10)
        lf_pierce.pack(fill=tk.BOTH, expand=True, pady=5)
        self._setup_advanced_field(lf_pierce, "piercings")
        
        # --- Colonne 2 ---
        # Awards
        lf_awards = ttk.LabelFrame(col2, text=" Awards / Prix ", padding=10)
        lf_awards.pack(fill=tk.BOTH, expand=True, pady=5)
        self._setup_advanced_field(lf_awards, "awards")
        
        # --- Colonne 3 ---
        # URLs
        lf_urls = ttk.LabelFrame(col3, text=" URLs (Fusionn√©es & Uniques) ", padding=10)
        lf_urls.pack(fill=tk.BOTH, expand=True, pady=5)
        self._setup_advanced_field(lf_urls, "urls")
        
        # URLs D√©couvertes (cach√© ou secondaire ?) - On le garde pour la compatibilit√©
        self.field_vars['discovered_urls'] = {'is_multiline': True, 'entry': tk.Text(frame, height=1), 'check': tk.BooleanVar(), 'stash': tk.StringVar(), 'main': tk.StringVar(), 'stash_widget': tk.Text(frame)}

        canvas.grid(row=0, column=0, sticky="nsew")
        scrollbar.grid(row=0, column=1, sticky="ns")
        
        return frame

    def _setup_advanced_field(self, parent, key, has_gen=False):
        """Helper pour cr√©er les blocs de l'onglet avanc√©"""
        header_frame = ttk.Frame(parent)
        header_frame.pack(fill=tk.X)
        
        var_check = tk.BooleanVar(value=True)
        ttk.Checkbutton(header_frame, text="Valider", variable=var_check).pack(side=tk.LEFT)
        
        if has_gen and key == "tags":
            ttk.Button(header_frame, text="‚ú® G√©n√©rer Tags", command=self._refresh_tags).pack(side=tk.RIGHT)

        ttk.Label(parent, text="Actuel dans Stash:", font=('Segoe UI', 8, 'italic')).pack(anchor="w", pady=(5,0))
        entry_stash = tk.Text(parent, height=2, font=('Segoe UI', 9), bg="#eeeeee", relief=tk.FLAT)
        entry_stash.pack(fill=tk.X, pady=(0, 5))
        entry_stash.configure(state="disabled")

        ttk.Label(parent, text="Validation / Edit:", font=('Segoe UI', 8, 'italic')).pack(anchor="w")
        txt = tk.Text(parent, height=8, font=('Segoe UI', 9), relief=tk.FLAT, highlightthickness=1)
        txt.pack(fill=tk.BOTH, expand=True, pady=5)
        # when editing URLs, keep them clean/validated automatically
        if key == "urls":
            txt.bind('<KeyRelease>', self._on_urls_modified)
        
        var_stash = tk.StringVar()
        var_main = tk.StringVar()
        
        self.field_vars[key] = {
            'check': var_check,
            'stash': var_stash,
            'main': var_main,
            'entry': txt,
            'sources': [],
            'source_widgets': [],
            'stash_widget': entry_stash,
            'is_multiline': True
        }
        txt.bind("<KeyRelease>", lambda e, k=key: self._update_validation(k))

    def _create_bio_tab(self) -> ttk.Frame:
        frame = ttk.Frame(self.notebook, padding=10)
        frame.columnconfigure(0, weight=1)
        frame.rowconfigure(0, weight=1)

        self._bio_notebook = ttk.Notebook(frame)
        self._bio_notebook.grid(row=0, column=0, sticky='nsew')

        # --- Tab 0: Scrapp√© ---
        tab_scrape = ttk.Frame(self._bio_notebook, padding=10)
        tab_scrape.columnconfigure(0, weight=1)
        tab_scrape.rowconfigure(1, weight=1)
        tab_scrape.rowconfigure(3, weight=1)

        header = ttk.Frame(tab_scrape)
        header.grid(row=0, column=0, sticky='ew', pady=(0, 8))
        self._lbl_url_count = ttk.Label(header, text='URLs : 0')
        self._lbl_url_count.pack(side=tk.LEFT, padx=(0, 10))
        self._lbl_award_count = ttk.Label(header, text='Awards : 0')
        self._lbl_award_count.pack(side=tk.LEFT)

        lf_raw = ttk.LabelFrame(tab_scrape, text=' Bio scrapp√©e ', padding=6)
        lf_raw.grid(row=1, column=0, sticky='nsew')
        lf_raw.columnconfigure(0, weight=1)
        lf_raw.rowconfigure(0, weight=1)
        self._bio_raw_text = scrolledtext.ScrolledText(lf_raw, wrap=tk.WORD, height=10)
        self._bio_raw_text.grid(row=0, column=0, sticky='nsew')
        self._bio_raw_text.config(state='disabled')
        self._lbl_scrape_chars = ttk.Label(tab_scrape, text='Caract√®res : 0')
        self._lbl_scrape_chars.grid(row=2, column=0, sticky=tk.E, pady=(2, 8))

        lf_trivia = ttk.LabelFrame(tab_scrape, text=' Trivia scrapp√©e ', padding=6)
        lf_trivia.grid(row=3, column=0, sticky='nsew')
        lf_trivia.columnconfigure(0, weight=1)
        lf_trivia.rowconfigure(0, weight=1)
        self._bio_trivia_disp = scrolledtext.ScrolledText(lf_trivia, wrap=tk.WORD, height=8)
        self._bio_trivia_disp.grid(row=0, column=0, sticky='nsew')
        self._bio_trivia_disp.config(state='disabled')
        self._lbl_scrape_chars2 = ttk.Label(tab_scrape, text='Caract√®res : 0')
        self._lbl_scrape_chars2.grid(row=4, column=0, sticky=tk.E, pady=(2, 0))

        # --- Tab 1: Google ---
        tab_google = ttk.Frame(self._bio_notebook, padding=10)
        tab_google.columnconfigure(0, weight=1)
        tab_google.rowconfigure(1, weight=1)
        btns_g = ttk.Frame(tab_google)
        btns_g.grid(row=0, column=0, sticky='ew', pady=(0, 6))
        ttk.Button(btns_g, text='üìù G√©n√©rer Google', command=self._bio_generate_google).pack(side=tk.LEFT)
        ttk.Button(btns_g, text='üßΩ Effacer', command=lambda: self._bio_clear(2)).pack(side=tk.LEFT, padx=6)
        ttk.Checkbutton(btns_g, text='Valider la Bio', variable=self.bio_valid_var).pack(side=tk.RIGHT)

        self._bio_google_text = scrolledtext.ScrolledText(tab_google, wrap=tk.WORD)
        self._bio_google_text.grid(row=1, column=0, sticky='nsew')
        self._lbl_google_chars = ttk.Label(tab_google, text='Caract√®res : 0')
        self._lbl_google_chars.grid(row=2, column=0, sticky=tk.E)
        self._bio_google_text.bind(
            '<KeyRelease>',
            lambda e: self._bio_update_chars(2, self._bio_google_text, self._lbl_google_chars),
        )

        # --- Tab 2: Ollama ---
        tab_ollama = ttk.Frame(self._bio_notebook, padding=10)
        tab_ollama.columnconfigure(0, weight=1)
        tab_ollama.rowconfigure(2, weight=1)
        top_o = ttk.Frame(tab_ollama)
        top_o.grid(row=0, column=0, sticky='ew', pady=(0, 6))
        ttk.Button(top_o, text='ü§ñ G√©n√©rer Ollama', command=self._bio_generate_ollama).pack(side=tk.LEFT)
        ttk.Button(top_o, text='üßΩ Effacer', command=lambda: self._bio_clear(3)).pack(side=tk.LEFT, padx=6)
        self._ollama_status = ttk.Label(top_o, text='')
        self._ollama_status.pack(side=tk.RIGHT)

        self._bio_ollama_text = scrolledtext.ScrolledText(tab_ollama, wrap=tk.WORD)
        self._bio_ollama_text.grid(row=2, column=0, sticky='nsew')
        self._lbl_ollama_chars = ttk.Label(tab_ollama, text='Caract√®res : 0')
        self._lbl_ollama_chars.grid(row=3, column=0, sticky=tk.E)
        self._bio_ollama_text.bind(
            '<KeyRelease>',
            lambda e: self._bio_update_chars(3, self._bio_ollama_text, self._lbl_ollama_chars),
        )

        # --- Tab 3: Raffiner/Fusionner ---
        tab_merge = ttk.Frame(self._bio_notebook, padding=10)
        tab_merge.columnconfigure(0, weight=1)
        tab_merge.rowconfigure(4, weight=1)

        src_box = ttk.LabelFrame(tab_merge, text=' Sources √† fusionner ', padding=8)
        src_box.grid(row=0, column=0, sticky='ew', pady=(0, 8))
        self._merge_vars = [tk.BooleanVar(value=True), tk.BooleanVar(value=True), tk.BooleanVar(value=True), tk.BooleanVar(value=True)]
        ttk.Checkbutton(src_box, text='Scrapp√© (bio)', variable=self._merge_vars[0]).pack(side=tk.LEFT, padx=6)
        ttk.Checkbutton(src_box, text='Scrapp√© (trivia)', variable=self._merge_vars[1]).pack(side=tk.LEFT, padx=6)
        ttk.Checkbutton(src_box, text='Google', variable=self._merge_vars[2]).pack(side=tk.LEFT, padx=6)
        ttk.Checkbutton(src_box, text='Ollama', variable=self._merge_vars[3]).pack(side=tk.LEFT, padx=6)

        prompt_lf = ttk.LabelFrame(tab_merge, text=' Directives IA (style, ton, taille, fusion...) ', padding=6)
        prompt_lf.grid(row=1, column=0, sticky='ew', pady=(0, 8))
        self.bio_prompt_text = tk.Text(prompt_lf, height=4, font=('Segoe UI', 9))
        self.bio_prompt_text.pack(fill=tk.X, expand=True)
        self.bio_prompt_text.insert('1.0', 'Ton professionnel, fran√ßais, environ 3000 caract√®res. Z√âRO liste √† puces. Fusionner proprement les sources s√©lectionn√©es.')

        actions = ttk.Frame(tab_merge)
        actions.grid(row=2, column=0, sticky='ew', pady=(0, 6))
        ttk.Button(actions, text='üîÄ Fusionner', command=self._bio_do_merge).pack(side=tk.LEFT)
        ttk.Button(actions, text='‚ú® Raffiner (Ollama)', command=self._bio_do_refine).pack(side=tk.LEFT, padx=6)
        ttk.Button(actions, text='‚úÖ Appliquer ‚Üí Ollama', command=self._bio_apply_merge).pack(side=tk.LEFT)
        self._merge_status = ttk.Label(actions, text='')
        self._merge_status.pack(side=tk.RIGHT)

        self._bio_merge_text = scrolledtext.ScrolledText(tab_merge, wrap=tk.WORD)
        self._bio_merge_text.grid(row=4, column=0, sticky='nsew')
        self._lbl_merge_chars = ttk.Label(tab_merge, text='Caract√®res : 0')
        self._lbl_merge_chars.grid(row=5, column=0, sticky=tk.E)
        self._bio_merge_text.bind(
            '<KeyRelease>',
            lambda e: self._bio_update_merge_chars(),
        )

        # Add tabs
        self._bio_notebook.add(tab_scrape, text='üìÑ Scrapp√©')
        self._bio_notebook.add(tab_google, text='üîç Google')
        self._bio_notebook.add(tab_ollama, text='ü§ñ Ollama')
        self._bio_notebook.add(tab_merge, text='üîÄ Raffiner/Fusionner')

        # Legacy aliases for compatibility with existing methods
        self.bio_text = self._bio_google_text  # type: ignore
        self.char_label = self._lbl_google_chars  # type: ignore

        self._refresh_bio_counters()
        self._update_raw_content(self._bio_slots[0], self._bio_slots[1])

        return frame

    def load_performer(self, performer_data: Dict[str, Any]):
        """Charge les donn√©es d'un performer dans l'interface."""
        # 1. Pr√©-traitement URL Manager Interactif
        if performer_data.get("name"):
            # R√©cup√©ration URLs existantes
            raw_urls = performer_data.get("urls", [])
            if isinstance(raw_urls, str):
                raw_urls = [u.strip() for u in raw_urls.splitlines() if u.strip()]
            elif raw_urls is None:
                raw_urls = []
            
            # Lancement de la fen√™tre de v√©rification interactive
            # Utilisation de la classe URLVerificationDialog import√©e
            if isinstance(raw_urls, str):
                raw_urls = [u.strip() for u in raw_urls.splitlines() if u.strip()]
            
            dlg = URLVerificationDialog(self, self.url_manager, raw_urls, performer_data["name"])
            self.wait_window(dlg)
            
            if dlg.final_urls is not None:
                performer_data["urls"] = dlg.final_urls
            # Sinon, on garde les URLs d'origine (si l'utilisateur a ferm√© sans finir ?)

        self.stash_data = performer_data
        
        # Reset fields
        for key, field_info in self.field_vars.items():
            entry = field_info["entry"]
            if hasattr(entry, 'delete'):
                if isinstance(entry, (tk.Text, scrolledtext.ScrolledText)):
                    entry.delete('1.0', tk.END)
                else:
                    entry.delete(0, tk.END)
        
        # Populate fields
        for key, val in performer_data.items():
            if key in self.field_vars:
                field_info = self.field_vars[key]
                entry = field_info["entry"]
                
                display_val = val
                if isinstance(val, list):
                    display_val = "\n".join(str(v) for v in val if v)
                elif val is None:
                    display_val = ""
                
                if isinstance(entry, (tk.Text, scrolledtext.ScrolledText)):
                    entry.insert('1.0', str(display_val))
                elif isinstance(entry, (ttk.Entry, ttk.Combobox)):
                    entry.insert(0, str(display_val))
        
        self._refresh_bio_counters()
        self._update_raw_content(performer_data.get("bio_raw"), performer_data.get("trivia"))


    def _bio_update_merge_chars(self):
        if not self._bio_merge_text or not self._lbl_merge_chars:
            return
        text = self._bio_merge_text.get('1.0', tk.END).strip()
        self._bio_merge_content = text
        self._lbl_merge_chars.config(text=f"Caract√®res : {len(text)}")

    def _bio_generate_google(self):
        metadata = self._get_field_values()
        name = self.field_vars.get('name', {}).get('main').get() if 'name' in self.field_vars else ''
        # Injecter la bio Stash existante + bio scrap√©e + trivia dans les m√©tadonn√©es
        metadata['bio_raw']   = self._bio_slots[0] or self.stash_data.get('details', '')
        metadata['trivia']    = metadata.get('trivia', '') or self._bio_slots[1]
        metadata['stash_bio'] = self.stash_data.get('details', '')
        bio = self.bio_generator.generate_google_bio(name, metadata)
        if self._bio_google_text and self._lbl_google_chars:
            self._bio_google_text.delete('1.0', tk.END)
            self._bio_google_text.insert('1.0', bio or '')
            self._bio_update_chars(2, self._bio_google_text, self._lbl_google_chars)
        if self._bio_notebook:
            self._bio_notebook.select(1)

    def _bio_generate_ollama(self):
        prompt = self.bio_prompt_text.get('1.0', tk.END).strip() if getattr(self, 'bio_prompt_text', None) else ''
        name = self.field_vars.get('name', {}).get('main').get() if 'name' in self.field_vars else ''

        def run():
            try:
                if self._ollama_status:
                    self.after(0, lambda: self._ollama_status.config(text='G√©n√©ration...'))
                metadata = self._get_field_values()
                metadata['bio_raw'] = self._bio_slots[0]
                metadata['trivia'] = metadata.get('trivia', '') or self._bio_slots[1]
                bio = self.bio_generator.generate_ollama_bio(name, metadata, custom_prompt=prompt)
                if bio and self._bio_ollama_text and self._lbl_ollama_chars:
                    def apply():
                        self._bio_ollama_text.delete('1.0', tk.END)
                        self._bio_ollama_text.insert('1.0', bio)
                        self._bio_update_chars(3, self._bio_ollama_text, self._lbl_ollama_chars)
                        if self._ollama_status:
                            self._ollama_status.config(text='')
                        if self._bio_notebook:
                            self._bio_notebook.select(2)
                    self.after(0, apply)
                else:
                    self.after(0, lambda: (self._ollama_status.config(text='') if self._ollama_status else None,
                                           messagebox.showerror('Ollama', 'Erreur lors de la g√©n√©ration avec Ollama.')))
            except Exception:
                self.after(0, lambda: (self._ollama_status.config(text='') if self._ollama_status else None,
                                       messagebox.showerror('Ollama', 'Erreur lors de la g√©n√©ration avec Ollama.')))

        threading.Thread(target=run, daemon=True).start()

    def _bio_do_merge(self):
        prompt = self.bio_prompt_text.get('1.0', tk.END).strip() if getattr(self, 'bio_prompt_text', None) else ''
        sources = []
        if self._merge_vars and self._merge_vars[0].get() and self._bio_slots[0].strip():
            sources.append('BIO SCRAPP√âE:\n' + self._bio_slots[0].strip())
        if self._merge_vars and self._merge_vars[1].get() and self._bio_slots[1].strip():
            sources.append('TRIVIA SCRAPP√âE:\n' + self._bio_slots[1].strip())
        if self._merge_vars and self._merge_vars[2].get() and self._bio_google_text:
            t = self._bio_google_text.get('1.0', tk.END).strip()
            if t:
                sources.append('BIO GOOGLE:\n' + t)
        if self._merge_vars and self._merge_vars[3].get() and self._bio_ollama_text:
            t = self._bio_ollama_text.get('1.0', tk.END).strip()
            if t:
                sources.append('BIO OLLAMA:\n' + t)

        current = "\n\n---\n\n".join(sources).strip()
        if not current:
            messagebox.showwarning('Fusion', 'Aucune source s√©lectionn√©e ou disponible.')
            return

        def run():
            try:
                if self._merge_status:
                    self.after(0, lambda: self._merge_status.config(text='Fusion...'))
                merged = self.bio_generator.refine_bio(current, prompt)
                if merged and self._bio_merge_text and self._lbl_merge_chars:
                    def apply():
                        self._bio_merge_text.delete('1.0', tk.END)
                        self._bio_merge_text.insert('1.0', merged)
                        self._bio_merge_content = merged
                        self._lbl_merge_chars.config(text=f"Caract√®res : {len(merged.strip())}")
                        if self._merge_status:
                            self._merge_status.config(text='')
                        if self._bio_notebook:
                            self._bio_notebook.select(3)
                    self.after(0, apply)
                else:
                    self.after(0, lambda: (self._merge_status.config(text='') if self._merge_status else None,
                                           messagebox.showerror('Fusion', 'Erreur lors de la fusion avec Ollama.')))
            except Exception:
                self.after(0, lambda: (self._merge_status.config(text='') if self._merge_status else None,
                                       messagebox.showerror('Fusion', 'Erreur lors de la fusion avec Ollama.')))

        threading.Thread(target=run, daemon=True).start()

    def _bio_do_refine(self):
        current = self._get_best_bio_text()
        prompt = self.bio_prompt_text.get('1.0', tk.END).strip() if getattr(self, 'bio_prompt_text', None) else ''
        if not current:
            messagebox.showwarning('IA', 'Aucune biographie √† raffiner.')
            return

        def run():
            try:
                if self._merge_status:
                    self.after(0, lambda: self._merge_status.config(text='Raffinage...'))
                refined = self.bio_generator.refine_bio(current, prompt)
                if refined and self._bio_merge_text and self._lbl_merge_chars:
                    def apply():
                        self._bio_merge_text.delete('1.0', tk.END)
                        self._bio_merge_text.insert('1.0', refined)
                        self._bio_merge_content = refined
                        self._lbl_merge_chars.config(text=f"Caract√®res : {len(refined.strip())}")
                        if self._merge_status:
                            self._merge_status.config(text='')
                        if self._bio_notebook:
                            self._bio_notebook.select(3)
                    self.after(0, apply)
                else:
                    self.after(0, lambda: (self._merge_status.config(text='') if self._merge_status else None,
                                           messagebox.showerror('Ollama', 'Erreur lors du raffinage avec Ollama.')))
            except Exception:
                self.after(0, lambda: (self._merge_status.config(text='') if self._merge_status else None,
                                       messagebox.showerror('Ollama', 'Erreur lors du raffinage avec Ollama.')))

        threading.Thread(target=run, daemon=True).start()

    def _bio_apply_merge(self):
        merged = (getattr(self, '_bio_merge_content', '') or '').strip()
        if not merged or not self._bio_ollama_text or not self._lbl_ollama_chars:
            return
        self._bio_ollama_text.delete('1.0', tk.END)
        self._bio_ollama_text.insert('1.0', merged)
        self._bio_update_chars(3, self._bio_ollama_text, self._lbl_ollama_chars)
        if self._bio_notebook:
            self._bio_notebook.select(2)

    def _load_from_stash(self):
        """Charge les donn√©es du performer depuis la base Stash"""
        if not self.performer_id:
            return
            
        data = self.db.get_performer_metadata(self.performer_id)
        if not data:
            messagebox.showerror("Erreur", "Impossible de charger les donn√©es du performer.")
            return

        # 0. V√©rification interactive des URLs AVANT de charger dans l'interface
        if data.get("name"):
            # R√©cup√©ration URLs existantes
            raw_urls = data.get("urls", [])
            if isinstance(raw_urls, str):
                raw_urls = [u.strip() for u in raw_urls.splitlines() if u.strip()]
            elif raw_urls is None:
                raw_urls = []
            
            # Lancement de la fen√™tre de v√©rification interactive
            dlg = URLVerificationDialog(self, self.url_manager, raw_urls, data["name"])
            self.wait_window(dlg)
            
            if dlg.final_urls is not None:
                data["urls"] = dlg.final_urls
            # Sinon, on garde les URLs d'origine (si l'utilisateur a ferm√© sans finir)

        self.stash_data = data
        
        # 1. Remplir les champs de m√©tadonn√©es
        for key, vars in self.field_vars.items():
            val = data.get(key)
            if val is None: val = ""
            
            # Normalisation Date de Naissance / D√©c√®s (YYYY-MM-DD)
            if key in ['birthdate', 'deathdate'] and val:
                try:
                    from datetime import datetime
                    # Supposer format Stash ou autre et forcer ISO
                    # Simple regex ou dateutil si dispo, ici on tente un parse basique
                    for fmt in ("%Y-%m-%d", "%d/%m/%Y", "%Y"):
                        try:
                            val = datetime.strptime(str(val).split('T')[0], fmt).strftime("%Y-%m-%d")
                            break
                        except: continue
                except: pass

            if not isinstance(val, str):
                if isinstance(val, list):
                    # Normalisation URLs (Doublons + Tri)
                    if key == 'urls':
                        val = self._sort_urls(list(dict.fromkeys(val)))
                    val = "\n".join(map(str, val))
                else:
                    val = str(val)

            if key == 'country' and val:
                val = self._normalize_country(val)
                
            if vars.get('is_multiline'):
                st = vars['stash_widget']
                st.configure(state="normal")
                st.delete('1.0', tk.END)
                st.insert('1.0', val)
                st.configure(state="disabled")
                
                et = vars['entry']
                if not et.get('1.0', tk.END).strip():
                    et.delete('1.0', tk.END)
                    et.insert('1.0', val)
            else:
                vars['stash'].set(val)
                if not vars['main'].get():
                    vars['main'].set(val)
            
            self._update_validation(key)

        # 2. Bio / D√©tails
        details = data.get('details') or ""
        if self._bio_google_text and self._lbl_google_chars:
            self._bio_google_text.delete('1.0', tk.END)
            self._bio_google_text.insert('1.0', str(details))
            self._bio_update_chars(2, self._bio_google_text, self._lbl_google_chars)
        self._bio_merge_content = ""
        if self._bio_merge_text and self._lbl_merge_chars:
            self._bio_merge_text.delete('1.0', tk.END)
            self._lbl_merge_chars.config(text='Caract√®res : 0')
        self._refresh_bio_counters()

        # 3. URLs
        stash_urls = self.stash_data.get('urls', [])
        # clean duplicates & empties on load
        if isinstance(stash_urls, list):
            cleaned = clean_urls_list(stash_urls)
            self.stash_data['urls'] = cleaned
            stash_urls = cleaned
        self._highlight_missing_sources(stash_urls)
        # update widget content if any (loop below handles this too but we
        # might want to ensure it's normalized)
        self._start_automatic_validation()

    def _create_toolbar(self):
        toolbar = ttk.Frame(self, padding=5)
        toolbar.pack(side=tk.BOTTOM, fill=tk.X)
        
        # Conteneur pour centrer ou r√©partir si besoin
        btn_frame = ttk.Frame(toolbar)
        btn_frame.pack(fill=tk.X)
        
        ttk.Button(btn_frame, text="üíæ Sauvegarder dans Stash", command=self._save_to_stash).pack(side=tk.RIGHT, padx=5)
        ttk.Button(btn_frame, text="üîç Scraper Tout", command=self._scrape_all).pack(side=tk.LEFT, padx=5)
        
        # Barre de progression
        self.status_label = ttk.Label(btn_frame, text="Pr√™t", font=('Segoe UI', 9))
        self.status_label.pack(side=tk.RIGHT, padx=10)
        self.progress_bar = ttk.Progressbar(btn_frame, orient=tk.HORIZONTAL, length=150, mode='determinate')
        self.progress_bar.pack(side=tk.RIGHT, padx=10)

        # Nouveaux outils V2
        ttk.Button(btn_frame, text="üßπ Nettoyer URLs", command=self._clean_urls).pack(side=tk.LEFT, padx=5)
        ttk.Button(btn_frame, text="üîó Valider URLs", command=self._open_url_validator).pack(side=tk.LEFT, padx=5)
        ttk.Button(btn_frame, text="üîé Chercher Sources", command=self._open_source_finder).pack(side=tk.LEFT, padx=5)

    def _open_url_validator(self):
        """Ouvre le validateur d'URL pour ce performer"""
        db_path = self.config.get("database_path")
        stash_url = self.config.get("stash_url", "http://localhost:9999")
        
        widget = URLValidatorWidget(self, db_path=db_path, stash_url=stash_url, performer_id=self.performer_id)
        widget.show()

    def _open_source_finder(self):
        """Ouvre le chercheur de sources pour ce performer"""
        if not self.stash_data:
            messagebox.showwarning("Attention", "Veuillez d'abord charger les donn√©es du performer.")
            return

        name = self.stash_data.get("name", "")
        aliases = self.stash_data.get("aliases", [])
        urls = self.stash_data.get("urls", [])
        
        def on_selected(new_urls):
            if not new_urls:
                return
            # On ajoute les nouvelles URLs √† la liste actuelle
            current_urls = list(self.stash_data.get("urls", []))
            # On attend un dict de SourceFinderWidget: {source: url}
            if isinstance(new_urls, dict):
                new_urls_list = list(new_urls.values())
            else:
                new_urls_list = list(new_urls)
            
            merged = merge_urls_by_domain(current_urls, new_urls_list)
            merged = clean_urls_list(merged)
            self.stash_data["urls"] = merged  # D√©dupliquer
            messagebox.showinfo("Succ√®s", f"URLs ajout√©es localement. N'oubliez pas de sauvegarder.")

        widget = SourceFinderWidget(self, name=name, aliases=aliases, existing_urls=urls, on_urls_selected=on_selected)
        widget.show()

    def _update_validation(self, key: str):
        """Met √† jour la couleur de fond de l'entry selon la validit√©"""
        if key not in self.field_vars:
            return
            
        f = self.field_vars[key]
        entry = f['entry']
        is_checked = f['check'].get()
        
        if f.get('is_multiline'):
            main_val = entry.get('1.0', tk.END).strip()
            stash_val = f['stash_widget'].get('1.0', tk.END).strip()
        else:
            main_val = f['main'].get().strip()
            stash_val = f['stash'].get().strip()
        
        if not is_checked:
            color = "white"
        elif not main_val:
            color = "#fff3cd" # Jaune (vide mais coch√©)
        elif main_val.lower() == stash_val.lower():
            color = "#d4edda" # Vert (identique)
        else:
            color = "#f8d7da" # Rouge (diff√©rent)
            
        # Application de la couleur s√©curis√©e
        if isinstance(entry, tk.Text):
            entry.configure(bg=color)
        else:
            # Pour tous les widgets ttk (Combobox, Entry, etc.)
            style_map = {
                "white": "Normal.TCombobox",
                "#fff3cd": "Empty.TCombobox",
                "#d4edda": "Valid.TCombobox",
                "#f8d7da": "Invalid.TCombobox"
            }
            # Un mappage g√©n√©rique pour les styles ttk
            try:
                # Si c'est un Entry, on utilise Normal.TEntry etc (√† d√©finir si besoin)
                # Mais ici on cible principalement les Combobox
                if isinstance(entry, ttk.Combobox):
                    entry.configure(style=style_map.get(color, "Normal.TCombobox"))
                else:
                    # Fallback s√©curis√© pour √©viter le crash -bg
                    pass
            except:
                pass

        # Keep Bio counters in sync
        if key in ("urls", "awards"):
            self._refresh_bio_counters()

    def _update_count(self, event=None):
        count = len(self.bio_text.get('1.0', tk.END).strip())
        self.char_label.config(text=f"Caract√®res : {count}")

    def _refresh_tags(self):
        metadata = self._get_field_values()
        tags = self.tag_rules.generate_tags(metadata)
        entry = self.field_vars['tags']['entry']
        entry.delete('1.0', tk.END)
        entry.insert('1.0', ', '.join(tags))
        self._update_validation('tags')

    def _gen_bio_google(self):
        self._bio_generate_google()

    def _gen_bio_ollama(self):
        self._bio_generate_ollama()

    def _refine_bio_ollama(self):
        self._bio_do_refine()

    def _apply_bio(self, bio):
        if self._bio_google_text and self._lbl_google_chars:
            self._bio_google_text.delete('1.0', tk.END)
            self._bio_google_text.insert('1.0', bio)
            self._bio_update_chars(2, self._bio_google_text, self._lbl_google_chars)

    def _save_to_stash(self):
        """Sauvegarde les modifications dans Stash"""
        if not self.performer_id:
            messagebox.showerror("Sauvegarde", "Aucun performer n'est charg√©.")
            return

        updates = self._get_field_values()
        updates['details'] = self._get_best_bio_text()
        
        # S'assurer que les d√©couvertes d'URL sont incluses si modifi√©es
        if 'urls' in updates:
            updates['discovered_urls'] = updates['urls']
        
        if self.db.save_performer_metadata(self.performer_id, updates):
            # Mettre √† jour aussi l'URL principale si trouv√©e? 
            # Stash a une colonne 'url' unique sur la table performers.
            messagebox.showinfo("Sauvegarde", "Performer mis √† jour avec succ√®s dans Stash.")
            # Recharger pour rafra√Æchir les colonnes 'Stash'
            self._load_from_stash()
        else:
            messagebox.showerror("Sauvegarde", "Erreur lors de la sauvegarde dans la base de donn√©es.")

    def _scrape_all(self):
        """Orchestre le scraping multi-sources avec barre de progression"""
        urls = self.stash_data.get('urls', [])
        performer_name = self.stash_data.get('name', '')

        # On lance le scraping m√™me sans URLs (Boobpedia + XXXBios seront auto-construits)
        if not urls and not performer_name:
            messagebox.showwarning("Scraping", "Aucune URL et aucun nom trouv√© pour ce performer.")
            return

        def update_progress(current, total, source_name):
            self.after(0, lambda: self._update_ui_progress(current, total, source_name))

        def run():
            # Reset UI
            self.after(0, lambda: self.progress_bar.configure(value=0))
            self.after(0, lambda: self.status_label.configure(text="Initialisation..."))
            
            # Scraping avec auto-d√©couverte Boobpedia/XXXBios
            results = self.orchestrator.scrape_all(urls, progress_callback=update_progress, performer_name=performer_name)
            
            if not results:
                self.after(0, lambda: self.status_label.configure(text="√âchec"))
                self.after(0, lambda *_: messagebox.showinfo("Scraping", "Aucune donn√©e trouv√©e sur les sources."))
                return

            # Validation des URLs d√©couvertes
            self.after(0, lambda: self.status_label.configure(text="Validation URLs..."))
            all_discovered = []
            for res in results:
                d_urls = res.get("discovered_urls", [])
                if isinstance(d_urls, list):
                    all_discovered.extend(d_urls)
            
            all_discovered = list(dict.fromkeys(all_discovered))
            
            if all_discovered:
                from services.url_validator import URLValidator, URLStatus
                validator = URLValidator(timeout=5)
                entries = [{"url": u, "performer_id": self.performer_id or 0, "name": "Discovered", "position": 0} for u in all_discovered]
                valid_results = validator.validate_urls(entries)
                alive_urls = [r.url for r in valid_results if r.status in (URLStatus.ACTIVE, URLStatus.AMBIGUOUS, URLStatus.REDIRECT, URLStatus.WHITELISTED)]
                
                for res in results:
                    res["discovered_urls"] = alive_urls

            self.after(0, lambda *_: self._apply_scrape_results(results))

        threading.Thread(target=run, daemon=True).start()

    def _update_ui_progress(self, current, total, source_name):
        """Met √† jour les widgets de progression (appel√© via .after)"""
        if self.progress_bar and self.status_label:
            val = (current / total) * 100 if total > 0 else 0
            self.progress_bar.configure(value=val)
            self.status_label.configure(text=f"Scraping {source_name}...")
            if current == total:
                self.status_label.configure(text="Termin√©")


    def _apply_scrape_results(self, results: List[Dict]):
        """Affiche les r√©sultats du scraping dans la grille et agr√®ge les URLs"""
        # 1. Agr√©gation des URLs de toutes les sources + URLs Stash actuelles
        all_discovered = []
        for res in results:
            urls = res.get("discovered_urls", [])
            if isinstance(urls, list):
                all_discovered.extend(urls)
        
        # Ajouter l'existant pour d√©doublonnage global
        stash_urls = self.stash_data.get('urls', [])
        
        # D√©duplication et Tri Hi√©rarchique (Core sources first)
        all_discovered = merge_urls_by_domain(stash_urls, all_discovered)
        
        # Mettre √† jour l'en-t√™te (rouge si une source manque)
        self._highlight_missing_sources(all_discovered)
        
        # Dispatcher les r√©seaux sociaux si pr√©sents dans les r√©sultats
        for res in results:
            socials = res.get("socials", {})
            for s_key, s_val in socials.items():
                if s_key in res: continue # D√©j√† pr√©sent ?
                # Normaliser twitter -> x? Non, on garde twitter pour la cl√© interne
                if s_key == "x": s_key = "twitter"
                if s_val: res[s_key] = s_val
        
        # 2. Mise √† jour de la grille (limit√© aux 6 sources principales configur√©es)
        source_order = ["IAFD", "FreeOnes", "TheNude", "Babepedia", "Boobpedia", "XXXBios"]
        # R√©ordonner les r√©sultats selon source_order pour que chaque source tombe dans la bonne colonne
        ordered_results = []
        result_by_source = {r.get('source', ''): r for r in results}
        for sname in source_order:
            ordered_results.append(result_by_source.get(sname, {}))
        limit = len(source_order)
        for source_idx in range(limit):
            res = ordered_results[source_idx]
            source_name = res.get('source', source_order[source_idx])
            
            for key, fields in self.field_vars.items():
                if key == "discovered_urls":
                    continue
                    
                val = res.get(key, "")
                
                # Normalisation sp√©cifique
                if key == 'country' and val:
                    val = self._normalize_country(val)
                elif key in ['birthdate', 'deathdate'] and val:
                    # (Re-use normalization logic)
                    try:
                        from datetime import datetime
                        for fmt in ("%Y-%m-%d", "%d/%m/%Y", "%Y"):
                            try:
                                val = datetime.strptime(str(val).split('T')[0], fmt).strftime("%Y-%m-%d")
                                break
                            except: continue
                    except: pass
                # Cas sp√©cial : pour la ligne URLs, on veut l'URL source
                if key == "urls" and res.get('url'):
                    val = res['url']
                if isinstance(val, list):
                    val = "\n".join(map(str, val)) if fields.get('is_multiline') else ", ".join(map(str, val))
                
                # Cas sp√©cial : Socials si pr√©sents dans un dict
                if key == "socials":
                    # On dispatch les socials vers leurs champs respectifs
                    pass # Sera g√©r√© apr√®s la boucle key ou via key direct

                # Mettre √† jour la colonne source correspondante
                if fields.get('is_multiline'):
                    widgets = fields.get('source_widgets', [])
                    if source_idx < len(widgets):
                        w = widgets[source_idx]
                        w.configure(state="normal")
                        w.delete('1.0', tk.END)
                        w.insert('1.0', str(val))
                        w.configure(state="disabled")
                else:
                    fields['sources'][source_idx].set(str(val))
                
                # Auto-remplissage si case coch√©e et vide
                is_empty = False
                if fields.get('is_multiline'):
                    is_empty = not fields['entry'].get('1.0', tk.END).strip()
                else:
                    is_empty = not fields['main'].get().strip()

                # Fusion automatique des aliases (ne pas √©craser)
                if key == 'aliases' and fields['check'].get() and fields.get('is_multiline') and val:
                    try:
                        current_text = fields['entry'].get('1.0', tk.END).strip()
                        current_aliases = [a.strip() for a in re.split(r'[\n\r,]+', current_text) if a.strip()]
                        incoming_aliases = [a.strip() for a in re.split(r'[\n\r,]+', str(val)) if a.strip()]
                        merged = []
                        seen = set()
                        for a in (current_aliases + incoming_aliases):
                            k = a.casefold()
                            if k in seen:
                                continue
                            seen.add(k)
                            merged.append(a)
                        fields['entry'].delete('1.0', tk.END)
                        fields['entry'].insert('1.0', "\n".join(merged))
                        self._update_validation('aliases')
                        is_empty = False
                    except Exception:
                        pass

                if fields['check'].get() and is_empty and val:
                    if fields.get('is_multiline'):
                        fields['entry'].delete('1.0', tk.END)
                        fields['entry'].insert('1.0', str(val))
                    else:
                        fields['main'].set(str(val))
                
                # Mise √† jour des valeurs de la liste d√©roulante (Combobox)
                if not fields.get('is_multiline') and isinstance(fields['entry'], ttk.Combobox):
                    stash_val = fields['stash'].get().strip()
                    source_vals = [s.get().strip() for s in fields['sources']]
                    all_vals = sorted(list(set([v for v in ([stash_val] + source_vals) if v])))
                    fields['entry']['values'] = all_vals

        # 3. Remplissage du champ global URLs avec les d√©couvertes tri√©es
        if "urls" in self.field_vars and all_discovered:
            fields = self.field_vars["urls"]
            # Fusion intelligente : on garde l'existant d√©j√† √©dit√© + d√©couvertes
            current_text = fields['entry'].get('1.0', tk.END).strip()
            current_urls = [u.strip() for u in re.split(r'[,\n\r\s]+', current_text) if u.strip()]
            
            # merge+clean sans doublons
            combined = merge_urls_by_domain(current_urls, all_discovered)
            combined = clean_urls_list(combined)
            
            fields['entry'].delete('1.0', tk.END)
            fields['entry'].insert('1.0', "\n".join(combined))
            self._update_validation("urls")

        # 4. Relancer la validation automatique des URLs Stash
        self._start_automatic_validation()

        # 5. Traduction automatique (French/QC) pour les champs texte riches
        def run_translation():
            fields_to_translate = {
                'trivia': 'Trivia',
                'tattoos': 'Tatouages',
                'piercings': 'Piercings'
            }
            
            for key, label in fields_to_translate.items():
                if key not in self.field_vars: continue
                
                # R√©cup√©rer la valeur actuelle dans le widget Main
                v = self.field_vars[key]
                current_val = ""
                if v.get('is_multiline'):
                    current_val = v['entry'].get('1.0', tk.END).strip()
                else:
                    current_val = v['main'].get().strip()
                
                if current_val and current_val.lower() != 'none':
                    translated = self.bio_generator.translate_hybrid(current_val, label)
                    if translated and translated != current_val:
                        def update_ui(k=key, t=translated):
                            v_ui = self.field_vars[k]
                            if v_ui.get('is_multiline'):
                                v_ui['entry'].delete('1.0', tk.END)
                                v_ui['entry'].insert('1.0', t)
                            else:
                                v_ui['main'].set(t)
                            self._update_validation(k)
                        self.after(0, update_ui)

        threading.Thread(target=run_translation, daemon=True).start()

        # 6. Sync bio_raw / trivia scrapp√©s dans l'onglet Bio (si pr√©sents)
        bio_raw = ""
        trivia = ""
        try:
            for res in results:
                if not bio_raw and res.get('bio_raw'):
                    bio_raw = str(res.get('bio_raw') or '').strip()
                if not trivia and res.get('trivia'):
                    trivia = str(res.get('trivia') or '').strip()
                if bio_raw and trivia:
                    break
        except Exception:
            pass
        if bio_raw or trivia:
            self._update_raw_content(bio_raw=bio_raw, trivia=trivia)

        messagebox.showinfo("Scraping", f"Scraping termin√© ({len(results)} sources). {len(all_discovered)} URLs agr√©g√©es.")

    def _sort_urls(self, urls: List[str]) -> List[str]:
        """Trie les URLs : sources recherch√©es d'abord (IAFD, FreeOnes, etc.)"""
        # Cette m√©thode n'est plus utilis√©e par le Treeview, mais peut √™tre utile ailleurs.
        # Si elle n'est plus utilis√©e du tout, elle peut √™tre supprim√©e.
        core_dirs = ["iafd.com", "freeones.com", "thenude.com", "babepedia.com"]
        
        def sort_key(url):
            url_lower = url.lower()
            for i, domain in enumerate(core_dirs):
                if domain in url_lower:
                    return i
            return 99 # Autres sources apr√®s
            
        return sorted(list(set(urls)), key=sort_key)

    def _populate_url_tree(self, urls):
        pass # Ancien Treeview, m√©thode obsol√®te

    def _clean_urls(self):
        """Internal helper for the advanced tab that normalizes the list of
        URLs currently displayed: removes blank lines, trims and discards
        duplicates."""
        if 'urls' not in self.field_vars:
            return
        widget = self.field_vars['urls']['entry']
        raw = widget.get('1.0', tk.END)
        urls = [u for u in raw.splitlines()]
        cleaned = clean_urls_list(urls)
        widget.delete('1.0', tk.END)
        widget.insert('1.0', "\n".join(cleaned))

    def _on_urls_modified(self, event=None):
        # if the user is editing URLs manually, keep them tidy and revalidate
        self._clean_urls()
        self._refresh_bio_counters()
        self._start_automatic_validation()

    def _start_automatic_validation(self):
        """Lance la validation automatique des URLs Stash (champ texte)."""
        # make sure urls are cleaned first
        self._clean_urls()

        # On regarde si on a le champ 'urls' (onglet avanc√©)
        if 'urls' not in self.field_vars:
            return
            
        widget = self.field_vars['urls']['entry']
        text = widget.get('1.0', tk.END).strip()
        if not text:
            return
            
        urls_to_check = text.split('\n')
        
        def validate():
            from services.url_validator import URLStatus
            from services.url_validator import URLValidator
            validator = URLValidator(timeout=5)
            
            entries = [{"url": u, "performer_id": self.performer_id or 0, "name": "Stash", "position": 0} for u in urls_to_check]
            results = validator.validate_urls(entries)
            
            # Mettre √† jour l'UI avec des tags de couleur
            for i, res in enumerate(results):
                status = res.status
                tag = "url_ok"
                if status in (URLStatus.DEAD, URLStatus.ERROR):
                    tag = "url_error"
                elif status == URLStatus.REDIRECT:
                    tag = "url_warning"
                
                # Appliquer le tag √† la ligne correspondante
                line_start = f"{i+1}.0"
                line_end = f"{i+1}.end"
                self.after(0, lambda w=widget, t=tag, s=line_start, e=line_end: self._apply_url_tag(w, t, s, e))

            # remove dead/error URLs from the widget and re-clean
            from utils.url_utils import clean_urls_list, filter_live_urls
            live_urls = filter_live_urls(urls_to_check, results)
            live_urls = clean_urls_list(live_urls)
            if live_urls != urls_to_check:
                # rewrite the text on the main thread
                self.after(0, lambda: widget.delete('1.0', tk.END))
                self.after(0, lambda: widget.insert('1.0', "\n".join(live_urls)))
                # recolor lines if list shortened
        

        import threading
        threading.Thread(target=validate, daemon=True).start()

    def _apply_url_tag(self, widget, tag, start, end):
        """Applique un tag de couleur √† une ligne du widget Text"""
        widget.tag_add(tag, start, end)
        # Configurer les couleurs des tags si pas encore fait
        widget.tag_configure("url_ok", foreground="green")
        widget.url_error_color = widget.tag_configure("url_error", foreground="red")
        widget.tag_configure("url_warning", foreground="orange")

    def _update_url_row(self, item_id, status, redirect, tag):
        current_values = list(self.url_tree.item(item_id)['values'])
        current_values[1] = status
        current_values[2] = redirect
        self.url_tree.item(item_id, values=current_values, tags=(tag,))
        
        # Configurer les couleurs des tags
        self.url_tree.tag_configure("ok", foreground="green")
        self.url_tree.tag_configure("error", foreground="red")
        self.url_tree.tag_configure("warning", foreground="orange")

    def _open_selected_url(self):
        """Ouvre l'URL s√©lectionn√©e dans le navigateur"""
        import webbrowser
        selected = self.url_tree.selection()
        if not selected:
            return
        url = self.url_tree.item(selected[0])['values'][0]
        webbrowser.open(url)

    def _normalize_country(self, country: str) -> str:
        """Convertit un nom de pays en code ISO 2 lettres"""
        if not country or len(country) == 2:
            return country
            
        mapping = {
            "united states": "US", "usa": "US",
            "united kingdom": "UK", "uk": "UK",
            "france": "FR", "germany": "DE", "spain": "ES",
            "italy": "IT", "canada": "CA", "brazil": "BR",
            "russia": "RU", "japan": "JP", "australia": "AU",
            "hungary": "HU", "czech republic": "CZ", "poland": "PL",
            "netherlands": "NL", "belgium": "BE", "switzerland": "CH",
        }
        return mapping.get(country.lower().strip(), country)

    def _highlight_missing_sources(self, urls: List[str]):
        """Surligne en rouge le nom des sources manquantes dans les URLs Stash"""
        if not hasattr(self, 'source_labels'):
            return
            
        # Priorit√©s
        core_dirs = {
            "IAFD": "iafd.com",
            "FreeOnes": "freeones.com",
            "TheNude": "thenude.com",
            "Babepedia": "babepedia.com",
            "Boobpedia": "boobpedia.com",
            "XXXBios": "xxxbios.com"
        }
        
        url_text = "\n".join(urls).lower()
        
        for name, domain in core_dirs.items():
            lbl = self.source_labels.get(name)
            if lbl:
                if domain in url_text:
                    lbl.configure(foreground="black")
                else:
                    lbl.configure(foreground="red")
    def _delete_selected_url(self):
        """Supprime l'URL s√©lectionn√©e de la liste locale"""
        selected = self.url_tree.selection()
        if not selected:
            return
        if messagebox.askyesno("Confirmation", "Supprimer cette URL de la liste locale ?"):
            self.url_tree.delete(selected[0])
            # Note: cela ne supprime pas de Stash tant qu'on n'a pas sauvegard√©


============================================================
[11/124] gui\scene_frame.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SceneFrame - Interface de gestion des Sc√®nes
"""

import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox
from typing import Dict, List, Optional

class SceneFrame(ttk.Frame):
    """Frame principal pour la gestion des sc√®nes"""
    
    def __init__(self, parent, scene_id: Optional[str] = None):
        super().__init__(parent)
        self.scene_id = scene_id
        self.metadata = {}
        
        # Services
        from services.config_manager import ConfigManager
        from services.database import StashDatabase
        from services.scrapers import ScraperOrchestrator
        
        config = ConfigManager()
        self.db = StashDatabase(config.get('database_path'))
        self.scraper = ScraperOrchestrator()
        
        # Definition of fields moved to __init__ as an instance variable
        self.fields = [
            ("Titre:", "title"),
            ("Studio:", "studio"),
            ("Date:", "date"),
            ("Code:", "code"),
            ("R√©alisateur:", "director"),
            ("Rating:", "rating"),
            ("Tags (virgules):", "tags"),
            ("Performers (virgules):", "performers"),
        ]
        
        self._setup_ttk_styles()
        self._create_widgets()
        
        if scene_id:
            self._load_from_stash()

    def _setup_ttk_styles(self):
        style = ttk.Style()
        style.map('Valid.TCombobox', fieldbackground=[('readonly', '#d4edda'), ('!disabled', '#d4edda')])
        style.map('Invalid.TCombobox', fieldbackground=[('readonly', '#f8d7da'), ('!disabled', '#f8d7da')])
        style.map('Empty.TCombobox', fieldbackground=[('readonly', '#fff3cd'), ('!disabled', '#fff3cd')])
        style.map('Normal.TCombobox', fieldbackground=[('readonly', 'white'), ('!disabled', 'white')])

    def _create_widgets(self):
        # Header
        header = ttk.Frame(self, padding=10)
        header.pack(fill=tk.X)
        ttk.Label(header, text=f"Sc√®ne ID: {self.scene_id or 'Nouvelle'}", font=('Segoe UI', 12, 'bold')).pack(side=tk.LEFT)
        
        # Main content
        content = ttk.Frame(self, padding=10)
        content.pack(fill=tk.BOTH, expand=True)
        
        # Grid layout for fields (Standardized to Stash/Validation/Source pattern)
        fields = [
            ("Titre:", "title"),
            ("Studio:", "studio"),
            ("Date:", "date"),
            ("Code:", "code"),
            ("R√©alisateur:", "director"),
            ("Rating:", "rating"),
            ("Tags (virgules):", "tags"),
            ("Performers (virgules):", "performers"),
        ]
        
        headers = ["Valider", "Champ", "Valeur Stash", "Validation / Edit", "Sources"]
        for col, text in enumerate(headers):
            ttk.Label(content, text=text, font=('Segoe UI', 9, 'bold')).grid(row=0, column=col, padx=5, pady=5, sticky="w")

        self.field_vars = {}
        for i, (label, key) in enumerate(fields, start=1):
            # 0: Checkbox
            var_check = tk.BooleanVar(value=True)
            ttk.Checkbutton(content, variable=var_check).grid(row=i, column=0, padx=5)
            
            # 1: Label
            ttk.Label(content, text=label).grid(row=i, column=1, sticky="w", padx=5)
            
            # 2: Valeur Stash (Read-only)
            var_stash = tk.StringVar()
            ttk.Entry(content, textvariable=var_stash, state="readonly", width=30).grid(row=i, column=2, padx=5, sticky="we")
            
            # 3: Validation / Edit (Combobox)
            var_main = tk.StringVar()
            combo = ttk.Combobox(content, textvariable=var_main, width=40)
            combo.grid(row=i, column=3, padx=5, sticky="we")
            
            # 4: Source (Read-only)
            var_src = tk.StringVar()
            ttk.Entry(content, textvariable=var_src, state="readonly", width=30).grid(row=i, column=4, padx=5, sticky="we")
            
            # Monitoring
            var_main.trace_add("write", lambda *args, k=key: self._validate_field(k))
            
            self.field_vars[key] = {
                'check': var_check,
                'stash': var_stash,
                'main': var_main,
                'source': var_src,
                'widget': combo
            }
            
        content.columnconfigure(3, weight=10)
        content.columnconfigure(2, weight=5)
        content.columnconfigure(4, weight=5)
        
        # Details / Synopsis
        ttk.Label(content, text="Synopsis / D√©tails:").grid(row=len(fields)+1, column=0, sticky="nw", padx=5, pady=5)
        self.details_text = scrolledtext.ScrolledText(content, height=10, wrap=tk.WORD)
        self.details_text.grid(row=len(fields)+1, column=1, columnspan=4, sticky="wense", padx=5, pady=5)
        
        # URL Scraping area
        url_frame = ttk.LabelFrame(content, text="URLs de la Sc√®ne", padding=10)
        url_frame.grid(row=len(fields)+2, column=0, columnspan=5, sticky="we", pady=10)
        
        self.urls_text = scrolledtext.ScrolledText(url_frame, height=4, wrap=tk.WORD)
        self.urls_text.pack(fill=tk.X, expand=True)
        
        # Toolbar
        toolbar = ttk.Frame(self, padding=10)
        toolbar.pack(side=tk.BOTTOM, fill=tk.X)
        
        ttk.Button(toolbar, text="üíæ Sauvegarder", command=self._save).pack(side=tk.RIGHT, padx=5)
        ttk.Button(toolbar, text="üîç Scraper Sc√®ne", command=self._scrape).pack(side=tk.LEFT, padx=5)

    def _load_from_stash(self):
        if not self.scene_id: return
        data = self.db.get_scene_metadata(self.scene_id)
        if data:
            self.metadata = data
            for label, key in self.fields:
                val = str(data.get(key, '')) if data.get(key) is not None else ''
                self.field_vars[key]['stash'].set(val)
                self.field_vars[key]['main'].set(val)
                self._validate_field(key)
            
            # Details
            self.details_text.delete('1.0', tk.END)
            self.details_text.insert('1.0', data.get('details', ''))

    def _validate_field(self, field):
        f = self.field_vars[field]
        main_val = f['main'].get().strip()
        stash_val = f['stash'].get().strip()
        is_checked = f['check'].get()
        combo = f.get('widget')
        
        if not is_checked:
            color = "white"
        elif not main_val:
            color = "#fff3cd"
        elif main_val.lower() == stash_val.lower():
            color = "#d4edda"
        else:
            color = "#f8d7da"

        if isinstance(combo, ttk.Combobox):
            style_map = {
                "white": "Normal.TCombobox",
                "#fff3cd": "Empty.TCombobox",
                "#d4edda": "Valid.TCombobox",
                "#f8d7da": "Invalid.TCombobox"
            }
            combo.configure(style=style_map.get(color, "Normal.TCombobox"))

    def _scrape(self):
        messagebox.showinfo("Scraping", "Scraping de la sc√®ne √† venir...")

    def _save(self):
        messagebox.showinfo("Sauvegarde", "Donn√©es de la sc√®ne sauvegard√©es dans Stash.")


============================================================
[12/124] gui\url_verification_dialog.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, messagebox
import threading
import webbrowser
import time
import requests
from services.scrapers import HEADERS

class URLVerificationDialog(tk.Toplevel):
    """
    Interface interactive pour la v√©rification s√©quentielle des URLs prioritaires.
    """
    
    def __init__(self, parent, url_manager, existing_urls, performer_name):
        super().__init__(parent)
        self.url_manager = url_manager
        self.existing_urls = existing_urls
        self.performer_name = performer_name
        self.final_urls = None
        
        self.title(f"V√©rification des URLs - {performer_name}")
        self.geometry("650x450")
        self.transient(parent)
        self.grab_set()
        
        # Centrer
        self.update_idletasks()
        w = self.winfo_width()
        h = self.winfo_height()
        x = (self.winfo_screenwidth() // 2) - (w // 2)
        y = (self.winfo_screenheight() // 2) - (h // 2)
        self.geometry(f'{w}x{h}+{x}+{y}')
        
        # Main Container
        main_frame = ttk.Frame(self, padding=20)
        main_frame.pack(fill="both", expand=True)

        # Header
        self.header_lbl = ttk.Label(main_frame, text="D√©marrage...", font=("Segoe UI", 12, "bold"))
        self.header_lbl.pack(pady=(0, 10))
        
        self.status_lbl = ttk.Label(main_frame, text="Pr√©paration...", font=("Segoe UI", 10))
        self.status_lbl.pack(pady=5)
        
        # URL Input Area
        input_frame = ttk.LabelFrame(main_frame, text="URL Actuelle", padding=10)
        input_frame.pack(fill="x", pady=10)
        
        self.url_var = tk.StringVar()
        self.url_entry = ttk.Entry(input_frame, textvariable=self.url_var, font=("Consolas", 10))
        self.url_entry.pack(fill="x", pady=5)
        
        # Action Buttons for Input
        action_frame = ttk.Frame(input_frame)
        action_frame.pack(fill="x", pady=5)
        
        ttk.Button(action_frame, text="Ouvrir le lien", command=self.open_current_url).pack(side="left", padx=5)
        ttk.Button(action_frame, text="Tester le lien", command=self.test_current_url).pack(side="left", padx=5)
        
        # Info / Feedback
        self.info_lbl = ttk.Label(main_frame, text="", foreground="gray", wraplength=600)
        self.info_lbl.pack(pady=10)
        
        # Navigation Buttons
        nav_frame = ttk.Frame(main_frame)
        nav_frame.pack(side="bottom", fill="x", pady=20)
        
        self.btn_ignore = ttk.Button(nav_frame, text="Ignorer / Passer", command=self.on_ignore)
        self.btn_ignore.pack(side="left")
        
        self.btn_search = ttk.Button(nav_frame, text="üîç Rechercher Auto", command=self.on_auto_search)
        self.btn_search.pack(side="left", padx=10)
        
        self.btn_confirm = ttk.Button(nav_frame, text="‚úÖ Confirmer & Suivant", command=self.on_confirm)
        self.btn_confirm.pack(side="right")
        
        # Bouton pour fermer sans attendre (visible seulement en fin de processus)
        self.btn_close_now = ttk.Button(nav_frame, text="‚è≠Ô∏è Terminer Maintenant", command=self.force_close)
        # Initialement cach√©
        
        self.progress = ttk.Progressbar(main_frame, mode="determinate")
        self.progress.pack(side="bottom", fill="x", pady=5)
        
        # State
        self.priority_order = self.url_manager.PRIORITY_ORDER # List of (domain, scraper)
        self.current_step_idx = -1
        self.priority_results = [None] * len(self.priority_order)
        self.other_urls_buffer = []

        # Start process
        self.after(500, self.start_process)

    def start_process(self):
        # 1. Sort existing URLs
        self.sorted_slots = [None] * len(self.priority_order)
        self.other_urls_buffer = []
        
        domain_map = {domain: i for i, (domain, _) in enumerate(self.priority_order)}
        
        for url in self.existing_urls:
            url = url.strip()
            if not url: continue
            
            # Simple domain check
key="***MASKED***"
            idx = domain_map.get(key)
            
            if idx is not None:
                if self.sorted_slots[idx] is None:
                    # Check profile URL if needed? Or accept first one?
                    if self.url_manager.is_profile_url(url, key):
                        self.sorted_slots[idx] = url
                    else:
                        self.other_urls_buffer.append(url)
                else:
                    self.other_urls_buffer.append(url)
            else:
                self.other_urls_buffer.append(url)
        
        self.next_step()

    def next_step(self):
        self.current_step_idx += 1
        
        if self.current_step_idx >= len(self.priority_order):
            self.finish_process()
            return
            
        domain, scraper = self.priority_order[self.current_step_idx]
        self.header_lbl.config(text=f"Source {self.current_step_idx + 1}/{len(self.priority_order)} : {domain}")
        self.progress["value"] = ((self.current_step_idx) / len(self.priority_order)) * 100
        
        existing = self.sorted_slots[self.current_step_idx]
        
        if existing:
            self.url_var.set(existing)
            self.status_lbl.config(text="URL pr√©sente. Validation en cours...", foreground="blue")
            self.info_lbl.config(text="Une URL existe d√©j√† pour ce domaine. V√©rification de sa validit√©...")
            self.test_current_url(auto=True)
        else:
            self.url_var.set("")
            self.status_lbl.config(text="URL manquante.", foreground="orange")
            self.info_lbl.config(text=f"Aucune URL trouv√©e pour {domain}. Lancement de la recherche automatique...")
            self.on_auto_search()

    def test_current_url(self, auto=False):
        url = self.url_var.get().strip()
        if not url:
            if not auto: messagebox.showwarning("Attention", "Aucune URL √† tester.")
            return

        self.set_busy(True)
        def run():
            is_ok = self.url_manager.is_url_reachable(url)
            # Utiliser winfo_exists() pour v√©rifier que le widget existe encore
            if self.winfo_exists():
                self.after(0, lambda: self.show_test_result(is_ok, auto))
        threading.Thread(target=run, daemon=True).start()

    def show_test_result(self, is_ok, auto):
        self.set_busy(False)
        if is_ok:
            self.status_lbl.config(text="URL Valide (200 OK)", foreground="green")
            self.info_lbl.config(text="Le lien fonctionne correctement. Vous pouvez confirmer.")
        else:
            self.status_lbl.config(text="URL Invalide / inaccessible", foreground="red")
            self.info_lbl.config(text="Le lien ne r√©pond pas. Essayez de rechercher une nouvelle URL.")
            if auto:
                # If auto-check failed on existing URL, suggest search?
                pass

    def on_auto_search(self):
        domain, _ = self.priority_order[self.current_step_idx]
        self.set_busy(True)
        self.status_lbl.config(text=f"Recherche sur {domain}...", foreground="blue")
        
        def run():
            found = self.url_manager.search_url_for_domain(domain, self.performer_name)
            # Utiliser winfo_exists() pour v√©rifier que le widget existe encore
            if self.winfo_exists():
                self.after(0, lambda: self.show_search_result(found))
            
        threading.Thread(target=run, daemon=True).start()

    def show_search_result(self, found_url):
        self.set_busy(False)
        if found_url:
            self.url_var.set(found_url)
            self.status_lbl.config(text="URL trouv√©e !", foreground="green")
            self.info_lbl.config(text=f"Trouv√© : {found_url}\nVeuillez confirmer si cela correspond au profil.")
            # Auto-test validity?
            self.test_current_url(auto=True)
        else:
            self.status_lbl.config(text="Aucun r√©sultat.", foreground="red")
            self.info_lbl.config(text="La recherche automatique n'a rien donn√©. Vous pouvez entrer une URL manuellement ou passer.")

    def on_confirm(self):
        url = self.url_var.get().strip()
        # Allow empty confirmation? Means "None"
        self.priority_results[self.current_step_idx] = url if url else None
        self.next_step()

    def on_ignore(self):
        self.priority_results[self.current_step_idx] = None
        self.next_step()

    def open_current_url(self):
        url = self.url_var.get().strip()
        if url: webbrowser.open(url)
    
    def force_close(self):
        """Ferme imm√©diatement sans attendre la validation des URLs secondaires"""
        # Construct final list avec ce qu'on a d√©j√†
        final_list = []
        for u in self.priority_results:
            if u: final_list.append(u)
        # Ajouter les URLs secondaires sans validation (trop lent)
        seen = set(final_list)
        for u in self.other_urls_buffer:
            if u not in seen:
                final_list.append(u)
                seen.add(u)
        
        self.final_urls = final_list[:50]
        print(f"[URLVerificationDialog] Fermeture forc√©e - {len(self.final_urls)} URLs")
        self.destroy()

    def set_busy(self, busy):
        state = "disabled" if busy else "normal"
        self.btn_confirm.config(state=state)
        self.btn_search.config(state=state)
        self.btn_ignore.config(state=state)
        self.url_entry.config(state=state)
        if busy:
            self.config(cursor="wait")
        else:
            self.config(cursor="")

    def finish_process(self):
        """Finalise le processus et ferme la fen√™tre"""
        self.header_lbl.config(text="Finalisation...")
        self.status_lbl.config(text="Construction de la liste finale...", foreground="black")
        self.progress["value"] = 100
        
        print(f"[URLVerificationDialog] Finalisation - URLs prioritaires: {len([u for u in self.priority_results if u])}")
        print(f"[URLVerificationDialog] URLs secondaires: {len(self.other_urls_buffer)}")
        
        # Construct final list: Priorities first, then others (sans validation suppl√©mentaire)
        final_list = []
        seen = set()
        
        # Ajouter les URLs prioritaires valid√©es
        for u in self.priority_results:
            if u and u not in seen:
                final_list.append(u)
                seen.add(u)
        
        # Ajouter les URLs secondaires (sans re-validation pour √©viter les blocages)
        # Les 6 sources prioritaires sont d√©j√† valid√©es, c'est suffisant
        for u in self.other_urls_buffer:
            if u not in seen:
                final_list.append(u)
                seen.add(u)
                if len(final_list) >= 50:  # Limite √† 50 URLs totales
                    break
        
        self.final_urls = final_list[:50]
        print(f"[URLVerificationDialog] Liste finale: {len(self.final_urls)} URLs")
        
        # Fermeture imm√©diate
        self.after(100, self.destroy)



============================================================
[13/124] inspect_custom_fields.py
------------------------------------------------------------
import sqlite3
import os

db_path = "H:/Stash/stash-go.sqlite"
if not os.path.exists(db_path):
    db_path = "data/database.sqlite"

conn = sqlite3.connect(db_path)
cur = conn.cursor()

# Get all tables
cur.execute("SELECT name FROM sqlite_master WHERE type='table'")
tables = [t[0] for t in cur.fetchall()]
print(f"Tables: {', '.join(tables)}")

# Check performers columns
cur.execute("PRAGMA table_info(performers)")
p_cols = [c[1] for c in cur.fetchall()]
print(f"Performer columns: {', '.join(p_cols)}")

# check for any table with 'custom' in it
custom_tables = [t for t in tables if 'custom' in t.lower()]
print(f"Custom field tables: {', '.join(custom_tables)}")

for t in custom_tables:
    print(f"\n--- {t} ---")
    cur.execute(f"PRAGMA table_info({t})")
    for c in cur.fetchall(): print(c)
    cur.execute(f"SELECT * FROM {t} LIMIT 10")
    for r in cur.fetchall(): print(r)

conn.close()


============================================================
[14/124] inspect_db.py
------------------------------------------------------------
import sqlite3
import os

db_path = "H:/Stash/stash-go.sqlite"
if not os.path.exists(db_path):
    db_path = "data/database.sqlite"

if not os.path.exists(db_path):
    print(f"Database not found at {db_path}")
    exit(1)

conn = sqlite3.connect(db_path)
cur = conn.cursor()

print(f"--- Schema for table 'performers' ({db_path}) ---")
cur.execute("PRAGMA table_info(performers)")
columns = cur.fetchall()
for col in columns:
    print(col)

print("\n--- List of all tables ---")
cur.execute("SELECT name FROM sqlite_master WHERE type='table'")
tables = cur.fetchall()
for table in tables:
    print(table[0])

# Check for custom fields related tables
for t in ["performer_custom_fields", "custom_fields"]:
    if any(t == table[0] for table in tables):
        print(f"\n--- Schema for table '{t}' ---")
        cur.execute(f"PRAGMA table_info({t})")
        cols = cur.fetchall()
        for c in cols:
            print(c)
        
        cur.execute(f"SELECT * FROM {t} LIMIT 5")
        rows = cur.fetchall()
        print(f"Sample data from {t}:")
        for r in rows:
            print(r)

conn.close()


============================================================
[15/124] Legacy\.gitignore
------------------------------------------------------------
__pycache__/
*.pyc
.env
.venv
.idea/
.vscode/


============================================================
[16/124] Legacy\check_db.py
------------------------------------------------------------
# check_db.py ‚Äî v√©rification rapide DB
import sqlite3, sys

DB_PATH = r"F:\stash\stash-go.sqlite"  # adapter selon votre installation

conn = sqlite3.connect(DB_PATH)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

# V√©rifier un performer (remplacer 42 par un ID r√©el test√©)
performer_id = 42

print(f"=== Performer {performer_id} ===")
cur.execute("SELECT name, details, tattoos, piercings FROM performers WHERE id=?",
            (performer_id,))
row = cur.fetchone()
if row:
    print(f"  Name: {row['name']}")
    print(f"  Details: {row['details'][:80] if row['details'] else 'VIDE'}...")
    print(f"  Tattoos: {row['tattoos']}")

cur.execute("SELECT url FROM performer_urls WHERE performer_id=?", (performer_id,))
urls = [r['url'] for r in cur.fetchall()]
print(f"  URLs: {urls}")

cur.execute("""
    SELECT t.name FROM tags t
    JOIN performers_tags pt ON pt.tag_id = t.id
    WHERE pt.performer_id=?
""", (performer_id,))
tags = [r['name'] for r in cur.fetchall()]
print(f"  Tags: {tags[:10]}")

# V√©rifier un group (remplacer 5 par un ID r√©el test√©)
group_id = 5
print(f"\n=== Group {group_id} ===")
cur.execute("SELECT name, director, description FROM groups WHERE id=?", (group_id,))
row = cur.fetchone()
if row:
    print(f"  Name: {row['name']}")
    print(f"  Director: {row['director']}")

# Sc√®nes avec URLs
cur.execute("""
    SELECT s.title, su.url
    FROM groups_scenes gs
    JOIN scenes s ON s.id = gs.scene_id
    LEFT JOIN scene_urls su ON su.scene_id = s.id
    WHERE gs.group_id=?
    LIMIT 5
""", (group_id,))
for row in cur.fetchall():
    print(f"  Sc√®ne: {row['title']} ‚Üí {row['url']}")

conn.close()

============================================================
[17/124] Legacy\config\__init__.py
------------------------------------------------------------


============================================================
[18/124] Legacy\config\settings.yaml
------------------------------------------------------------
# Configuration de StashMaster V2
phase1_fields:
  - Name
  - Disambiguation
  - Aliases
  - Birthdate
  - Deathdate
  - Country
  - Ethnicity
  - Hair Color
  - Eye Color
  - Height
  - Weight
  - Measurements
  - Fake Tits
  - Career Length
phase2_fields:
  - Bio
  - Trivia
  - Awards
  - Tattoos
  - Piercings
  - Tags
  - URLs
  - Details

# ‚îÄ‚îÄ Groups (DVDs) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
group_phase1_fields:
  - Title
  - Aliases
  - Date
  - Studio
  - Director
  - Duration
  - Description
  - Tags
  - URLs

group_sources_priority:
  - data18          # P1 ‚Äî tags, sc√®nes, synopsis d√©taill√©s
  - iafd_dvd        # P2 ‚Äî r√©f√©rence US/classiques, cast, dates fiables
  - adultdvdempire  # P3 ‚Äî synopsis, covers
  - jeedoo          # P4 ‚Äî productions europ√©ennes uniquement

group_phase2_sources:  # sources avec URLs de sc√®nes individuelles
  - data18
  - adultdvdempire


============================================================
[19/124] Legacy\files\stashmaster-v2\stashmaster-v2\.gitignore
------------------------------------------------------------
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
ENV/
env/
.venv

# IDEs
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store

# Data
data/
*.sqlite
*.db

# Logs
*.log

# Test
.coverage
.pytest_cache/
htmlcov/

# Configuration locale
config.local.json
*.local.*

# Temporary files
*.tmp
*.temp
.cache/


============================================================
[20/124] Legacy\files\stashmaster-v2\stashmaster-v2\CHANGELOG.md
------------------------------------------------------------
# Changelog

Toutes les modifications notables du projet sont document√©es dans ce fichier.

## [2.0.0] - 2026-02-25

### Ajout√©
- ‚ú® **Interface unifi√©e** : Fusion compl√®te des Phase 1 et Phase 2 en une seule GUI
- üè∑Ô∏è **Syst√®me de tags intelligent** : G√©n√©ration automatique bas√©e sur des r√®gles m√©tadonn√©es
  - Tags bas√©s sur l'ethnicit√© (Caucasian, Latina, Asian, Ebony)
  - Tags bas√©s sur la couleur de cheveux (Blonde, Brunette, Redhead, Black Hair)
  - Tags bas√©s sur les mesures (Big Boobs, Small Boobs)
  - Tags pour piercings et tattoos
  - Tag MILF bas√© sur l'√¢ge de carri√®re
- üìù **Champs multilignes** pour Piercings, Tattoos et URLs
- ü™ü **Fen√™tre Trivia & Awards d√©di√©e** avec :
  - Scraping cibl√© depuis IAFD
  - Affichage s√©par√© des requ√™tes et r√©sultats
  - Nettoyage automatique des awards (1 par ligne)
- üìÑ **G√©n√©ration de bio automatique** avec 2 modes :
  - Bio Google : Template de 3000 caract√®res professionnel
  - Bio Ollama : IA locale avec prompt personnalis√©
- üîÑ **ScraperOrchestrator** : Scraping multi-sources avec fusion intelligente
- ‚úÖ **DataMerger** : D√©tection automatique des donn√©es confirm√©es et conflits
- üßπ **AwardsCleaner** : Formatage intelligent des awards
- üìä **Onglets organis√©s** : M√©tadonn√©es, Champs Avanc√©s, Bio

### Modifi√©
- üîß **Tags** : Ne sont plus scrap√©s, uniquement g√©n√©r√©s par r√®gles
- üìã **Interface** : Onglets au lieu de fen√™tres s√©par√©es
- üéØ **Workflow** : Simplifi√© et plus intuitif
- üíæ **Architecture** : Code modulaire avec s√©paration des responsabilit√©s

### Supprim√©
- ‚ùå Scraping de tags depuis les sources (remplac√© par g√©n√©ration automatique)
- ‚ùå Fen√™tres multiples (remplac√© par onglets)

### Technique
- üêç Python 3.8+ requis
- üì¶ D√©pendances : requests, beautifulsoup4, lxml
- ü§ñ Support optionnel d'Ollama pour g√©n√©ration IA
- üèóÔ∏è Architecture MVC am√©lior√©e

### Documentation
- üìñ README complet avec guide d'utilisation
- üéì Documentation des r√®gles de tags
- üí° Exemples et FAQ
- üõ†Ô∏è Guide de configuration avanc√©e

---

## [1.0.0] - Version Pr√©c√©dente

### Fonctionnalit√©s
- Interface Phase 1 : M√©tadonn√©es usuelles avec scraping
- Interface Phase 2 : Champs avanc√©s s√©par√©s
- Scraping basique depuis IAFD et autres sources
- Tags scrap√©s depuis les sources
- Bio manuelle

### Limitations
- Deux fen√™tres s√©par√©es
- Tags scrap√©s pas toujours coh√©rents
- Pas de g√©n√©ration automatique de bio
- Awards bruts non format√©s
- Workflow moins fluide

---

## √Ä venir

### [2.1.0] - Planifi√©
- [ ] Base de donn√©es SQLite int√©gr√©e
- [ ] Export vers Stash
- [ ] Import depuis fichiers JSON
- [ ] Historique des modifications
- [ ] Undo/Redo
- [ ] Raccourcis clavier
- [ ] Th√®mes dark/light
- [ ] Support multi-langues

### [2.2.0] - En r√©flexion
- [ ] Scraping d'images
- [ ] D√©tection automatique de doublons
- [ ] Suggestions intelligentes
- [ ] API REST pour int√©grations
- [ ] Plugin system
- [ ] Scraping de sc√®nes/films
- [ ] Statistiques et graphiques

---

**Format du Changelog** : [Keep a Changelog](https://keepachangelog.com/)  
**Versioning** : [Semantic Versioning](https://semver.org/)


============================================================
[21/124] Legacy\files\stashmaster-v2\stashmaster-v2\config.json
------------------------------------------------------------
{
  "scrapers": {
    "timeout": 10,
    "retry_count": 3,
    "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
  },
  "bio_generation": {
    "google_template_length": 3000,
    "ollama_url": "http://localhost:11434/api/generate",
    "ollama_model": "llama2",
    "ollama_timeout": 120
  },
  "tag_rules": {
    "ethnicity_tags": {
      "caucasian": "Caucasian",
      "latin": "Latina",
      "cuban": "Latina",
      "asian": "Asian",
      "ebony": "Ebony",
      "african": "Ebony"
    },
    "hair_color_tags": {
      "blonde": "Blonde",
      "blond": "Blonde",
      "brown": "Brunette",
      "brunette": "Brunette",
      "red": "Redhead",
      "auburn": "Redhead",
      "black": "Black Hair"
    },
    "measurements_thresholds": {
      "big_boobs_min": 36,
      "small_boobs_max": 32
    },
    "career_length_thresholds": {
      "milf_years": 10
    }
  },
  "sources": {
    "iafd": {
      "enabled": true,
      "priority": 1
    },
    "freeones": {
      "enabled": true,
      "priority": 2
    },
    "babepedia": {
      "enabled": true,
      "priority": 3
    },
    "thenude": {
      "enabled": true,
      "priority": 4
    }
  },
  "ui": {
    "window_width": 1200,
    "window_height": 900,
    "theme": "default"
  },
  "data": {
    "performers_dir": "data/performers",
    "database_path": "data/database.sqlite"
  }
}


============================================================
[22/124] Legacy\files\stashmaster-v2\stashmaster-v2\CONTRIBUTING.md
------------------------------------------------------------
# Guide de Contribution

Merci de votre int√©r√™t pour contribuer √† StashMaster V2 ! üéâ

## üìã Table des Mati√®res

- [Code de Conduite](#code-de-conduite)
- [Comment Contribuer](#comment-contribuer)
- [D√©veloppement](#d√©veloppement)
- [Standards de Code](#standards-de-code)
- [Tests](#tests)
- [Documentation](#documentation)

## ü§ù Code de Conduite

- Soyez respectueux envers tous les contributeurs
- Fournissez des critiques constructives
- Concentrez-vous sur ce qui est le mieux pour le projet
- Acceptez les feedbacks avec gr√¢ce

## üí° Comment Contribuer

### Rapporter des Bugs

Avant de cr√©er une issue :
1. V√©rifiez si le bug n'a pas d√©j√† √©t√© rapport√©
2. Utilisez la derni√®re version du code
3. Testez avec une installation propre

Pour rapporter un bug, incluez :
- **Description claire** du probl√®me
- **√âtapes pour reproduire** le bug
- **Comportement attendu** vs. comportement observ√©
- **Screenshots** si applicable
- **Environnement** : OS, version Python, d√©pendances
- **Logs d'erreur** si disponibles

### Sugg√©rer des Am√©liorations

Pour sugg√©rer une nouvelle fonctionnalit√© :
1. V√©rifiez si elle n'est pas d√©j√† planifi√©e (voir CHANGELOG)
2. Cr√©ez une issue avec le label "enhancement"
3. D√©crivez clairement :
   - Le probl√®me que √ßa r√©sout
   - Comment √ßa devrait fonctionner
   - Des exemples d'utilisation
   - Des alternatives consid√©r√©es

### Soumettre des Pull Requests

1. **Fork** le projet
2. **Cr√©ez une branche** pour votre fonctionnalit√©
   ```bash
   git checkout -b feature/ma-super-feature
   ```
3. **Committez** vos changements
   ```bash
   git commit -m "feat: ajout de ma super feature"
   ```
4. **Pushez** vers la branche
   ```bash
   git push origin feature/ma-super-feature
   ```
5. **Ouvrez une Pull Request**

## üõ†Ô∏è D√©veloppement

### Configuration de l'Environnement

```bash
# Cloner le repository
git clone https://github.com/votre-username/stashmaster-v2.git
cd stashmaster-v2

# Cr√©er un environnement virtuel
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer les d√©pendances
pip install -r requirements.txt

# Installer les d√©pendances de d√©veloppement (si disponibles)
pip install -r requirements-dev.txt
```

### Structure du Projet

```
stashmaster-v2/
‚îÇ
‚îú‚îÄ‚îÄ stashmaster_unified.py    # Application principale
‚îÇ   ‚îú‚îÄ‚îÄ MainWindow            # GUI principale
‚îÇ   ‚îú‚îÄ‚îÄ TriviaAwardsWindow    # Fen√™tre Trivia/Awards
‚îÇ   ‚îú‚îÄ‚îÄ BioGenerationWindow   # Fen√™tre g√©n√©ration de bio
‚îÇ   ‚îú‚îÄ‚îÄ TagRulesEngine        # Moteur de tags
‚îÇ   ‚îú‚îÄ‚îÄ AwardsCleaner         # Nettoyeur d'awards
‚îÇ   ‚îî‚îÄ‚îÄ BioGenerator          # G√©n√©rateur de bio
‚îÇ
‚îú‚îÄ‚îÄ scrapers.py               # Modules de scraping
‚îÇ   ‚îú‚îÄ‚îÄ ScraperBase           # Classe de base
‚îÇ   ‚îú‚îÄ‚îÄ IAFDScraper           # Scraper IAFD
‚îÇ   ‚îú‚îÄ‚îÄ FreeonesScraper       # Scraper Freeones
‚îÇ   ‚îú‚îÄ‚îÄ BabepaediaScraper     # Scraper Babepedia
‚îÇ   ‚îú‚îÄ‚îÄ TheNudeScraper        # Scraper TheNude
‚îÇ   ‚îú‚îÄ‚îÄ DataMerger            # Fusionneur de donn√©es
‚îÇ   ‚îî‚îÄ‚îÄ ScraperOrchestrator   # Orchestrateur
‚îÇ
‚îú‚îÄ‚îÄ test_stashmaster.py       # Tests unitaires
‚îú‚îÄ‚îÄ config.json               # Configuration
‚îú‚îÄ‚îÄ requirements.txt          # D√©pendances
‚îú‚îÄ‚îÄ README.md                 # Documentation
‚îú‚îÄ‚îÄ CHANGELOG.md              # Historique des versions
‚îî‚îÄ‚îÄ CONTRIBUTING.md           # Ce fichier
```

### Lancer l'Application en Mode D√©veloppement

```bash
# Mode normal
python3 stashmaster_unified.py

# Avec logs de debug (√† impl√©menter)
python3 stashmaster_unified.py --debug

# Avec un performer sp√©cifique (√† impl√©menter)
python3 stashmaster_unified.py --performer "Bridgette B"
```

## üìù Standards de Code

### Style Python

Suivez [PEP 8](https://www.python.org/dev/peps/pep-0008/) :

```python
# Bonnes pratiques
class MyClass:
    """Docstring pour la classe"""
    
    def my_method(self, param1: str, param2: int) -> bool:
        """Docstring pour la m√©thode
        
        Args:
            param1: Description du param√®tre 1
            param2: Description du param√®tre 2
            
        Returns:
            Description du retour
        """
        # Code ici
        return True

# Imports group√©s
import sys
import os
from typing import Dict, List

import requests
from bs4 import BeautifulSoup

from scrapers import IAFDScraper
```

### Nommage

- **Classes** : PascalCase (`TagRulesEngine`)
- **Fonctions/M√©thodes** : snake_case (`generate_tags`)
- **Constantes** : UPPER_CASE (`MAX_RETRIES`)
- **Variables priv√©es** : pr√©fixe `_` (`_internal_method`)

### Docstrings

Utilisez le format Google :

```python
def scrape_performer(self, url: str) -> Dict:
    """Scrape les donn√©es d'un performer.
    
    Args:
        url: L'URL de la page du performer
        
    Returns:
        Dictionnaire contenant les m√©tadonn√©es du performer
        
    Raises:
        ValueError: Si l'URL est invalide
        RequestException: Si le scraping √©choue
        
    Examples:
        >>> scraper.scrape_performer("https://example.com/performer")
        {'name': 'John Doe', 'birthdate': '1990-01-01'}
    """
    pass
```

### Type Hints

Utilisez les type hints pour am√©liorer la lisibilit√© :

```python
from typing import Dict, List, Optional, Tuple

def merge_data(self, sources: List[Dict]) -> Tuple[Dict, Dict]:
    """Fusionne les donn√©es de plusieurs sources"""
    pass

def get_performer(self, id: int) -> Optional[Dict]:
    """R√©cup√®re un performer par ID"""
    pass
```

## üß™ Tests

### Lancer les Tests

```bash
# Tous les tests
python3 test_stashmaster.py

# Tests sp√©cifiques
python3 -m unittest test_stashmaster.TestTagRulesEngine

# Avec couverture (si coverage install√©)
coverage run test_stashmaster.py
coverage report
```

### √âcrire des Tests

```python
import unittest

class TestMyFeature(unittest.TestCase):
    def setUp(self):
        """Pr√©paration avant chaque test"""
        self.engine = TagRulesEngine()
    
    def tearDown(self):
        """Nettoyage apr√®s chaque test"""
        pass
    
    def test_my_feature(self):
        """Test de ma fonctionnalit√©"""
        result = self.engine.generate_tags({})
        self.assertIsInstance(result, list)
        self.assertEqual(len(result), 0)
```

### Couverture de Tests

Visez au minimum :
- 80% de couverture pour le code principal
- 60% pour les scrapers (d√©pendent de sources externes)
- 100% pour les utilitaires critiques (TagRulesEngine, DataMerger)

## üìö Documentation

### Documenter le Code

- **Classes** : Docstring avec description, attributs
- **M√©thodes** : Docstring avec Args, Returns, Raises
- **Modules** : Docstring d'en-t√™te avec description g√©n√©rale

### Mettre √† Jour la Documentation

Lors de l'ajout de fonctionnalit√©s :
1. **README.md** : Ajouter dans la section correspondante
2. **CHANGELOG.md** : Documenter le changement
3. **Docstrings** : Commenter le code
4. **config.json** : Ajouter les nouvelles options

## üîÄ Workflow Git

### Branches

- `main` : Code stable, production
- `develop` : D√©veloppement en cours
- `feature/*` : Nouvelles fonctionnalit√©s
- `bugfix/*` : Corrections de bugs
- `hotfix/*` : Corrections urgentes

### Messages de Commit

Utilisez [Conventional Commits](https://www.conventionalcommits.org/) :

```bash
# Format
<type>(<scope>): <description>

[corps optionnel]

[footer(s) optionnel(s)]

# Exemples
feat(tags): ajout de la r√®gle MILF bas√©e sur l'√¢ge
fix(scraper): correction du parsing IAFD
docs(readme): mise √† jour des instructions d'installation
test(tags): ajout de tests pour les tags d'ethnicit√©
refactor(bio): am√©lioration de la g√©n√©ration Google
style(ui): correction de l'alignement des boutons
```

Types :
- `feat` : Nouvelle fonctionnalit√©
- `fix` : Correction de bug
- `docs` : Documentation
- `style` : Formatage (pas de changement de code)
- `refactor` : Refactoring
- `test` : Ajout de tests
- `chore` : Maintenance

## üéØ Priorit√©s de D√©veloppement

Consultez le [CHANGELOG.md](CHANGELOG.md) pour voir les fonctionnalit√©s planifi√©es.

### Court Terme (v2.1)
- Base de donn√©es SQLite
- Export vers Stash
- Import JSON
- Historique des modifications

### Moyen Terme (v2.2)
- Scraping d'images
- D√©tection de doublons
- API REST
- Plugin system

### Long Terme (v3.0)
- Interface web
- Multi-utilisateurs
- Synchronisation cloud
- Mobile app

## ‚ùì Questions ?

N'h√©sitez pas √† :
- Ouvrir une issue pour discuter
- Rejoindre les discussions
- Contacter les mainteneurs

## üôè Remerciements

Merci √† tous les contributeurs qui aident √† am√©liorer StashMaster V2 !

---

**Happy Coding!** üöÄ


============================================================
[23/124] Legacy\files\stashmaster-v2\stashmaster-v2\data\README.md
------------------------------------------------------------
# Dossier Data

Ce dossier contient toutes les donn√©es sauvegard√©es par StashMaster V2.

## Structure

```
data/
‚îú‚îÄ‚îÄ performers/          # JSON files des performers
‚îÇ   ‚îú‚îÄ‚îÄ performer_1.json
‚îÇ   ‚îú‚îÄ‚îÄ performer_2.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ database.sqlite     # Base de donn√©es (futur)
```

## Format JSON des Performers

```json
{
  "name": "Performer Name",
  "aliases": ["Alias 1", "Alias 2"],
  "birthdate": "January 1, 1990",
  "birthplace": "City, Country",
  "ethnicity": "Caucasian",
  "hair_color": "Blonde",
  "eye_color": "Blue",
  "height": "170 cm",
  "weight": "55 kg",
  "measurements": "34DD-25-36",
  "tattoos": "Description of tattoos",
  "piercings": "Description of piercings",
  "career_length": "2010-",
  "tags": ["Tag1", "Tag2", "Tag3"],
  "urls": [
    "https://source1.com/...",
    "https://source2.com/..."
  ],
  "trivia": "Interesting facts...",
  "awards": "Awards and nominations...",
  "bio": "Full biography (3000 characters)..."
}
```

## Sauvegarde et Restauration

### Sauvegarde manuelle
Copiez simplement le dossier `data/` pour cr√©er une sauvegarde.

### Restauration
Remplacez le dossier `data/` par votre sauvegarde.

## Notes

- Les fichiers JSON sont encod√©s en UTF-8
- Les noms de fichiers sont en minuscules avec underscores
- La base de donn√©es SQLite sera ajout√©e dans une version future


============================================================
[24/124] Legacy\files\stashmaster-v2\stashmaster-v2\EXAMPLES.md
------------------------------------------------------------
# Exemples d'Utilisation

Ce document contient des exemples pratiques pour utiliser StashMaster V2.

## üìã Exemples Basiques

### Exemple 1 : Scraping Simple

```python
# Dans un script Python
from scrapers import ScraperOrchestrator

# Cr√©er l'orchestrateur
orchestrator = ScraperOrchestrator()

# URLs √† scraper
urls = [
    "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
    "https://www.freeones.xxx/bridgette-b"
]

# Scraper et fusionner
confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)

# Afficher les r√©sultats
print(f"Donn√©es de {num_sources} source(s)")
print("\nConfirm√©es:")
for field, info in confirmed.items():
    print(f"  {field}: {info['value']}")

print("\nConflits:")
for field, values in conflicts.items():
    print(f"  {field}:")
    for v in values:
        print(f"    - {v['value']} ({', '.join(v['sources'])})")
```

### Exemple 2 : G√©n√©ration de Tags

```python
from stashmaster_unified import TagRulesEngine

# Cr√©er le moteur de r√®gles
engine = TagRulesEngine()

# M√©tadonn√©es d'exemple
metadata = {
    'ethnicity': 'Latina',
    'hair_color': 'Blonde',
    'measurements': '36DD-25-36',
    'piercings': 'Navel',
    'tattoos': 'Lower back',
    'career_length': '2007-'
}

# G√©n√©rer les tags
tags = engine.generate_tags(metadata)
print(f"Tags g√©n√©r√©s: {', '.join(tags)}")
# Output: Tags g√©n√©r√©s: Latina, Blonde, Big Boobs, Pierced, Tattooed, MILF
```

### Exemple 3 : Nettoyage d'Awards

```python
from stashmaster_unified import AwardsCleaner

cleaner = AwardsCleaner()

# Awards bruts
raw_awards = """
AVN AWARDS2012Winner: Unsung Starlet of the Year2014Nominee: Unsung Starlet of the Year
2015Nominee: Fan Award: Best Boobs
"""

# Nettoyer
cleaned = cleaner.clean_awards(raw_awards)
print(cleaned)
```

Output:
```
AVN AWARDS

2012
  Winner: Unsung Starlet of the Year

2014
  Nominee: Unsung Starlet of the Year

2015
  Nominee: Fan Award: Best Boobs
```

### Exemple 4 : G√©n√©ration de Bio

```python
from stashmaster_unified import BioGenerator

generator = BioGenerator()

# M√©tadonn√©es du performer
metadata = {
    'name': 'Bridgette B',
    'birthdate': 'October 15, 1983',
    'birthplace': 'Barcelona, Spain',
    'ethnicity': 'Caucasian',
    'hair_color': 'Blonde',
    'measurements': '34DD-27-34',
    'height': '173 cm',
    'weight': '129 lbs',
    'career_start': '2007',
    'aliases': ['Bridget B', 'Bridgette', 'Spanish Doll']
}

# G√©n√©rer bio Google
bio = generator.generate_google_bio('Bridgette B', metadata)
print(f"Bio g√©n√©r√©e ({len(bio)} caract√®res):\n{bio}")
```

## üîß Exemples Avanc√©s

### Exemple 5 : Fusion de Donn√©es Complexes

```python
from scrapers import DataMerger

merger = DataMerger()

# Donn√©es de 3 sources diff√©rentes
sources = [
    {
        'source': 'iafd',
        'name': 'Bridgette B',
        'birthdate': 'October 15, 1983',
        'ethnicity': 'Caucasian',
        'hair_color': 'Blonde',
        'measurements': '34DD-27-34'
    },
    {
        'source': 'freeones',
        'name': 'Bridgette B',
        'birthdate': 'October 15, 1983',
        'ethnicity': 'Caucasian',
        'hair_color': 'Blonde',  # Conflit
        'height': '173 cm'
    },
    {
        'source': 'babepedia',
        'name': 'Bridgette B',
        'birthdate': 'October 15, 1983',
        'ethnicity': 'Caucasian',
        'hair_color': 'Brown',  # Conflit
        'weight': '129 lbs'
    }
]

# Fusionner
confirmed, conflicts = merger.merge_data(sources)

print("=== Donn√©es Confirm√©es ===")
for field, info in confirmed.items():
    sources_str = ', '.join(info['sources'])
    print(f"{field}: {info['value']} ({info['count']} sources: {sources_str})")

print("\n=== Conflits ===")
for field, values in conflicts.items():
    print(f"\n{field}:")
    for v in values:
        print(f"  - {v['value']} ({v['count']} sources: {', '.join(v['sources'])})")
```

### Exemple 6 : Scraping avec Gestion d'Erreurs

```python
from scrapers import IAFDScraper
import requests

scraper = IAFDScraper()

urls = [
    "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
    "https://www.iafd.com/person.rme/perfid=invalid/gender=f/invalid.htm"
]

for url in urls:
    print(f"\nScraping: {url}")
    try:
        data = scraper.scrape_performer(url)
        if data:
            print(f"  ‚úÖ Succ√®s: {data.get('name', 'Unknown')}")
            print(f"  Champs: {len(data)}")
        else:
            print("  ‚ùå √âchec: Aucune donn√©e retourn√©e")
    except requests.RequestException as e:
        print(f"  ‚ùå Erreur r√©seau: {e}")
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
```

### Exemple 7 : Cr√©ation d'un Scraper Personnalis√©

```python
from scrapers import ScraperBase
from typing import Dict

class MonSiteScraper(ScraperBase):
    """Scraper pour mon site personnalis√©"""
    
    def scrape_performer(self, url: str) -> Dict:
        """Scrape un performer depuis mon site"""
        soup = self.get_page(url)
        if not soup:
            return {}
        
        data = {
            'source': 'monsite',
            'url': url
        }
        
        try:
            # Extraire le nom
            name_elem = soup.find('h1', class_='performer-name')
            if name_elem:
                data['name'] = name_elem.text.strip()
            
            # Extraire la date de naissance
            birthday_elem = soup.find('span', class_='birthday')
            if birthday_elem:
                data['birthdate'] = birthday_elem.text.strip()
            
            # Ajouter d'autres extractions...
            
        except Exception as e:
            print(f"Erreur: {e}")
        
        return data

# Utilisation
scraper = MonSiteScraper()
data = scraper.scrape_performer("https://monsite.com/performer/123")
print(data)
```

### Exemple 8 : Int√©gration avec l'Interface

```python
# Dans votre propre script
from stashmaster_unified import MainWindow
import tkinter as tk

# Cr√©er et configurer la fen√™tre
app = MainWindow()

# Pr√©-remplir des donn√©es (exemple)
app.metadata_entries['name'].insert(0, "Bridgette B")
app.urls_text.insert('1.0', "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm")

# Lancer l'application
app.mainloop()
```

## üéì Cas d'Usage R√©els

### Cas 1 : Workflow Complet Automatis√©

```python
#!/usr/bin/env python3
"""
Workflow automatis√© complet pour un performer
"""

from scrapers import ScraperOrchestrator
from stashmaster_unified import TagRulesEngine, BioGenerator
import json

def process_performer(name: str, urls: list) -> dict:
    """Traite compl√®tement un performer"""
    print(f"\n{'='*50}")
    print(f"Traitement de: {name}")
    print('='*50)
    
    # 1. Scraping
    print("\n1. Scraping des sources...")
    orchestrator = ScraperOrchestrator()
    confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)
    print(f"   ‚úÖ {num_sources} source(s) scrap√©e(s)")
    print(f"   ‚úÖ {len(confirmed)} champ(s) confirm√©(s)")
    print(f"   ‚ö†Ô∏è  {len(conflicts)} conflit(s)")
    
    # 2. Pr√©parer les m√©tadonn√©es
    print("\n2. Pr√©paration des m√©tadonn√©es...")
    metadata = {key: info['value'] for key, info in confirmed.items()}
    metadata['name'] = name
    
    # 3. G√©n√©rer les tags
    print("\n3. G√©n√©ration des tags...")
    tag_engine = TagRulesEngine()
    tags = tag_engine.generate_tags(metadata)
    metadata['tags'] = tags
    print(f"   ‚úÖ {len(tags)} tag(s) g√©n√©r√©(s): {', '.join(tags)}")
    
    # 4. G√©n√©rer la bio
    print("\n4. G√©n√©ration de la bio...")
    bio_generator = BioGenerator()
    bio = bio_generator.generate_google_bio(name, metadata)
    metadata['bio'] = bio
    print(f"   ‚úÖ Bio g√©n√©r√©e ({len(bio)} caract√®res)")
    
    # 5. Sauvegarder
    print("\n5. Sauvegarde...")
    filename = f"data/performers/{name.lower().replace(' ', '_')}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"   ‚úÖ Sauvegard√©: {filename}")
    
    return metadata

# Exemple d'utilisation
if __name__ == "__main__":
    performer_data = process_performer(
        name="Bridgette B",
        urls=[
            "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
            "https://www.freeones.xxx/bridgette-b"
        ]
    )
    
    print("\n" + "="*50)
    print("‚úÖ Traitement termin√© avec succ√®s!")
    print("="*50)
```

### Cas 2 : Batch Processing de Plusieurs Performers

```python
#!/usr/bin/env python3
"""
Traitement par lots de plusieurs performers
"""

import json
from pathlib import Path
from scrapers import ScraperOrchestrator
from stashmaster_unified import TagRulesEngine, BioGenerator

def batch_process(performers_file: str):
    """Traite plusieurs performers depuis un fichier JSON"""
    
    # Charger la liste
    with open(performers_file, 'r') as f:
        performers = json.load(f)
    
    print(f"Traitement de {len(performers)} performer(s)...\n")
    
    orchestrator = ScraperOrchestrator()
    tag_engine = TagRulesEngine()
    bio_generator = BioGenerator()
    
    results = {
        'success': [],
        'failed': [],
        'partial': []
    }
    
    for i, performer in enumerate(performers, 1):
        name = performer['name']
        urls = performer['urls']
        
        print(f"\n[{i}/{len(performers)}] {name}")
        print("-" * 40)
        
        try:
            # Scraping
            confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)
            
            if num_sources == 0:
                print("  ‚ùå Aucune source valide")
                results['failed'].append(name)
                continue
            
            # M√©tadonn√©es
            metadata = {key: info['value'] for key, info in confirmed.items()}
            metadata['name'] = name
            
            # Tags
            tags = tag_engine.generate_tags(metadata)
            metadata['tags'] = tags
            
            # Bio
            bio = bio_generator.generate_google_bio(name, metadata)
            metadata['bio'] = bio
            
            # Sauvegarder
            filename = f"data/performers/{name.lower().replace(' ', '_')}.json"
            Path("data/performers").mkdir(parents=True, exist_ok=True)
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            if len(conflicts) > 0:
                print(f"  ‚ö†Ô∏è  Succ√®s partiel ({len(conflicts)} conflits)")
                results['partial'].append(name)
            else:
                print("  ‚úÖ Succ√®s complet")
                results['success'].append(name)
        
        except Exception as e:
            print(f"  ‚ùå Erreur: {e}")
            results['failed'].append(name)
    
    # R√©sum√©
    print("\n" + "="*50)
    print("R√âSUM√â")
    print("="*50)
    print(f"‚úÖ Succ√®s complet: {len(results['success'])}")
    print(f"‚ö†Ô∏è  Succ√®s partiel: {len(results['partial'])}")
    print(f"‚ùå √âchecs: {len(results['failed'])}")
    
    return results

# Exemple d'utilisation
if __name__ == "__main__":
    # Cr√©er un fichier performers_list.json avec:
    # [
    #   {
    #     "name": "Performer 1",
    #     "urls": ["url1", "url2"]
    #   },
    #   ...
    # ]
    
    results = batch_process("performers_list.json")
```

### Cas 3 : Validation et Correction Semi-Automatique

```python
#!/usr/bin/env python3
"""
Validation et correction semi-automatique des donn√©es
"""

from scrapers import ScraperOrchestrator
from stashmaster_unified import TagRulesEngine

def validate_and_correct(urls: list) -> dict:
    """Valide et propose des corrections"""
    
    orchestrator = ScraperOrchestrator()
    confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)
    
    print("="*50)
    print("VALIDATION DES DONN√âES")
    print("="*50)
    
    # Donn√©es confirm√©es
    print("\n‚úÖ Donn√©es confirm√©es:")
    for field, info in confirmed.items():
        print(f"  {field}: {info['value']}")
        print(f"    Sources: {', '.join(info['sources'])}")
    
    # Conflits √† r√©soudre
    if conflicts:
        print("\n‚ö†Ô∏è  CONFLITS √Ä R√âSOUDRE:")
        corrections = {}
        
        for field, values in conflicts.items():
            print(f"\n  {field}:")
            for i, v in enumerate(values, 1):
                print(f"    [{i}] {v['value']} ({', '.join(v['sources'])})")
            
            # Demander √† l'utilisateur de choisir
            while True:
                choice = input(f"  Choisir [1-{len(values)}] ou [s]kip: ")
                if choice.lower() == 's':
                    break
                try:
                    idx = int(choice) - 1
                    if 0 <= idx < len(values):
                        corrections[field] = values[idx]['value']
                        print(f"    ‚úÖ {field} = {values[idx]['value']}")
                        break
                except ValueError:
                    pass
                print("    ‚ùå Choix invalide")
        
        # Appliquer les corrections
        for field, value in corrections.items():
            confirmed[field] = {
                'value': value,
                'note': 'Corrig√© manuellement'
            }
    
    # R√©sultat final
    final_data = {key: info['value'] for key, info in confirmed.items()}
    
    print("\n" + "="*50)
    print("DONN√âES FINALES")
    print("="*50)
    for field, value in final_data.items():
        print(f"  {field}: {value}")
    
    return final_data

# Exemple
if __name__ == "__main__":
    urls = [
        "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
        "https://www.freeones.xxx/bridgette-b"
    ]
    
    data = validate_and_correct(urls)
```

## üîó Int√©grations

### Int√©gration avec Stash

```python
import requests
import json

class StashAPI:
    """Client pour l'API Stash"""
    
    def __init__(self, url="http://localhost:9999", api_key=None):
        self.url = url
        self.api_key = api_key
    
    def create_performer(self, performer_data: dict) -> dict:
        """Cr√©e un performer dans Stash"""
        # GraphQL mutation
        mutation = """
        mutation PerformerCreate($input: PerformerCreateInput!) {
          performerCreate(input: $input) {
            id
            name
          }
        }
        """
        
        variables = {
            "input": {
                "name": performer_data.get('name'),
                "birthdate": performer_data.get('birthdate'),
                "ethnicity": performer_data.get('ethnicity'),
                "hair_color": performer_data.get('hair_color'),
                "height": performer_data.get('height'),
                "measurements": performer_data.get('measurements'),
                "tags": performer_data.get('tags', [])
            }
        }
        
        response = requests.post(
            f"{self.url}/graphql",
            json={"query": mutation, "variables": variables}
        )
        
        return response.json()

# Utilisation
stash = StashAPI()
result = stash.create_performer(performer_data)
print(f"Performer cr√©√©: {result}")
```

---

Ces exemples couvrent les cas d'usage les plus courants. Pour plus d'informations, consultez le [README.md](README.md) et la documentation des modules.


============================================================
[25/124] Legacy\files\stashmaster-v2\stashmaster-v2\README.md
------------------------------------------------------------
# StashMaster V2 - Interface Unifi√©e

Application Python/Tkinter pour la gestion et le scraping de m√©tadonn√©es de performers, avec g√©n√©ration automatique de biographies.

## üéØ Caract√©ristiques Principales

### Interface Unifi√©e
- **Fusion Phase 1 & Phase 2** : Une seule GUI pour toutes les op√©rations
- **Organisation par onglets** : M√©tadonn√©es, Champs Avanc√©s, Bio
- **Workflow intuitif** : Scraping ‚Üí Validation ‚Üí G√©n√©ration Bio

### Syst√®me de Tags Intelligent
- ‚úÖ **G√©n√©ration automatique** bas√©e sur des r√®gles m√©tadonn√©es
- ‚úÖ **PAS de scraping de tags** depuis les sources
- ‚úÖ **R√®gles intelligentes** : ethnicit√©, couleur de cheveux, mesures, piercings, tattoos, √¢ge

### Champs Optimis√©s
- **Champs simple ligne** : Nom, Aliases, Dates, Pays, etc.
- **Champs multilignes** :
  - üìù Piercings
  - üìù Tattoos
  - üîó URLs (une par ligne)

### Trivia & Awards
- **Fen√™tre d√©di√©e** avec requ√™te et r√©sultats s√©par√©s
- **Scraping cibl√©** depuis IAFD et autres sources
- **Nettoyage automatique** : 1 award par ligne
- **Format structur√©** : Ann√©e ‚Üí C√©r√©monie ‚Üí Awards

### G√©n√©ration de Bio Automatique
- **Bio Google** : 3000 caract√®res, format professionnel (bas√© sur mod√®le)
- **Bio Ollama** : G√©n√©ration IA locale optionnelle
- **Prompt personnalis√©** : Directives pr√©cises pour l'IA
- **Choix flexible** : Cases √† cocher pour type de bio

## üìã Installation

### Pr√©requis
```bash
# Python 3.8 ou sup√©rieur
python --version

# Tkinter (normalement inclus avec Python)
# Sur Ubuntu/Debian si besoin :
sudo apt-get install python3-tk
```

### Installation des d√©pendances
```bash
# Installer les packages Python requis
pip install -r requirements.txt
```

### Installation d'Ollama (optionnel)
Si vous voulez utiliser la g√©n√©ration de bio avec IA locale :

```bash
# T√©l√©charger et installer Ollama depuis https://ollama.ai
# Puis t√©l√©charger un mod√®le
ollama pull llama2
```

## üöÄ Utilisation

### Lancement
```bash
python stashmaster_unified.py
```

### Workflow Complet

#### 1. Saisie des URLs
- Ouvrir l'onglet **"Champs Avanc√©s"**
- Coller les URLs des sources (une par ligne) :
  ```
  https://www.iafd.com/person.rme/perfid=...
  https://www.freeones.xxx/...
  https://www.babepedia.com/...
  ```

#### 2. Scraping
- Menu **"Actions" ‚Üí "Scraper & Lancer le flux Bio IA"**
- L'application scrape automatiquement toutes les URLs
- Affiche les r√©sultats avec :
  - ‚úÖ Donn√©es confirm√©es (m√™me valeur de plusieurs sources)
  - üÜï Nouvelles donn√©es (une seule source)
  - ‚ö†Ô∏è Conflits (valeurs diff√©rentes entre sources)

#### 3. Validation des M√©tadonn√©es
- V√©rifier et compl√©ter les champs dans l'onglet **"M√©tadonn√©es"**
- Les valeurs confirm√©es sont pr√©-remplies
- R√©soudre les conflits manuellement si n√©cessaire

#### 4. G√©n√©ration des Tags
- Onglet **"Champs Avanc√©s"**
- Cliquer sur **"üîÑ G√©n√©rer Tags"**
- Les tags sont cr√©√©s automatiquement selon les r√®gles :
  - Ethnicit√© ‚Üí Caucasian, Latina, Asian, Ebony
  - Cheveux ‚Üí Blonde, Brunette, Redhead, Black Hair
  - Mesures ‚Üí Big Boobs, Small Boobs
  - Piercings ‚Üí Pierced
  - Tattoos ‚Üí Tattooed
  - Carri√®re ‚Üí MILF (si > 10 ans)

#### 5. Trivia & Awards
- Menu **"Actions" ‚Üí "Trivia & Awards..."**
- Fen√™tre d√©di√©e s'ouvre avec deux sections :
  
  **Trivia**
  - Cliquer **"Scraper Trivia"**
  - Les anecdotes sont r√©cup√©r√©es et affich√©es
  
  **Awards**
  - Cliquer **"Scraper Awards"**
  - Tous les prix/nominations sont list√©s
  - Cliquer **"Nettoyer Awards"** pour formater (1 par ligne)
  
- **"Appliquer et continuer"** pour sauvegarder

#### 6. G√©n√©ration de Bio
- Menu **"Actions" ‚Üí "G√©n√©rer Bio..."** ou onglet **"Bio"**
- Fen√™tre de g√©n√©ration s'ouvre avec 3 options :

  **Option 1 : Bio Google (recommand√©)**
  - ‚úÖ G√©n√©ration automatique instantan√©e
  - ‚úÖ Format professionnel de 3000 caract√®res
  - ‚úÖ Structure avec sections : Introduction, Origines, Carri√®re, Vie Personnelle, Apparence, Prix
  - ‚úÖ Bas√© sur le mod√®le BioGooglemodele.txt
  
  **Option 2 : Bio Ollama**
  - G√©n√©ration avec IA locale (Ollama doit √™tre install√©)
  - Prompt par d√©faut optimis√©
  
  **Option 3 : Bio Ollama avec prompt personnalis√©**
  - √âcrire vos directives pr√©cises dans le champ
  - Contr√¥le total sur le style et le contenu
  
- Cliquer **"G√©n√©rer la Bio"**
- V√©rifier le compteur de caract√®res
- **"Appliquer"** pour ins√©rer dans l'onglet Bio

#### 7. Sauvegarde
- Bouton **"üíæ Sauvegarder"** en bas √† droite
- Toutes les donn√©es sont sauvegard√©es

## üìä Architecture

### Structure des Fichiers
```
stashmaster_unified/
‚îÇ
‚îú‚îÄ‚îÄ stashmaster_unified.py    # Application principale
‚îú‚îÄ‚îÄ scrapers.py                # Modules de scraping
‚îú‚îÄ‚îÄ requirements.txt           # D√©pendances Python
‚îú‚îÄ‚îÄ README.md                  # Ce fichier
‚îÇ
‚îî‚îÄ‚îÄ data/                      # Donn√©es sauvegard√©es (√† cr√©er)
    ‚îú‚îÄ‚îÄ performers/            # JSON des performers
    ‚îî‚îÄ‚îÄ database.sqlite        # Base de donn√©es (futur)
```

### Composants Principaux

#### `MainWindow`
Interface principale unifi√©e avec 3 onglets :
- üìã M√©tadonn√©es : Champs de base
- ‚öôÔ∏è Champs Avanc√©s : Tags, Piercings, Tattoos, URLs
- üìù Bio : Biographie finale

#### `TriviaAwardsWindow`
Fen√™tre d√©di√©e pour :
- Scraping et affichage des trivia
- Scraping et nettoyage des awards
- Format structur√© : 1 award par ligne

#### `BioGenerationWindow`
Fen√™tre de g√©n√©ration avec :
- Choix du type de bio (Google/Ollama)
- Champ pour prompt personnalis√©
- Pr√©visualisation et compteur de caract√®res

#### `TagRulesEngine`
Moteur de r√®gles pour g√©n√©rer les tags automatiquement selon :
- Les m√©tadonn√©es collect√©es (ethnicit√©, cheveux, mesures)
- Les attributs physiques (piercings, tattoos)
- L'√¢ge de carri√®re

#### `AwardsCleaner`
Nettoyeur d'awards pour :
- Formater les awards (1 par ligne)
- Organiser par ann√©e et c√©r√©monie
- Distinguer Winner vs Nominee

#### `BioGenerator`
G√©n√©rateur de biographies avec 2 modes :
- **Google Bio** : Template de 3000 caract√®res
- **Ollama Bio** : IA locale avec prompt personnalis√©

#### `ScraperOrchestrator`
Orchestre le scraping de plusieurs sources :
- IAFD
- Freeones
- Babepedia
- TheNude

#### `DataMerger`
Fusionne intelligemment les donn√©es de plusieurs sources :
- D√©tecte les valeurs confirm√©es (consensus)
- Identifie les nouvelles donn√©es (source unique)
- Signale les conflits (valeurs diff√©rentes)

## üé® R√®gles de Tags

Les tags sont g√©n√©r√©s automatiquement selon ces r√®gles :

### Ethnicit√©
| M√©tadonn√©e | Tag G√©n√©r√© |
|------------|------------|
| Caucasian  | Caucasian  |
| Cuban, Latin, Latina | Latina |
| Asian | Asian |
| Ebony, African | Ebony |

### Couleur de Cheveux
| M√©tadonn√©e | Tag G√©n√©r√© |
|------------|------------|
| Blonde, Blond | Blonde |
| Brown, Brunette | Brunette |
| Red, Auburn | Redhead |
| Black | Black Hair |

### Mesures
| Condition | Tag G√©n√©r√© |
|-----------|------------|
| Tour de poitrine ‚â• 36" | Big Boobs |
| Tour de poitrine ‚â§ 32" | Small Boobs |

### Attributs
| M√©tadonn√©e | Tag G√©n√©r√© |
|------------|------------|
| Piercings (non vide) | Pierced |
| Tattoos (non vide) | Tattooed |
| Carri√®re > 10 ans | MILF |

## üìù Format de Bio Google

La bio g√©n√©r√©e suit ce template de 3000 caract√®res :

```markdown
### [Nom] : L'√©toile charismatique au parcours diversifi√©

**Introduction**
Contexte, d√©but de carri√®re, pseudonymes...

**üìÖ Origines et Premiers Pas**
Lieu de naissance, origines, d√©but de carri√®re...

**üèÜ Carri√®re et Filmographie**
√âvolution, studios, performances, apog√©e...

**üí° Faits Int√©ressants & Vie Personnelle**
Personnalit√©, trivia, vie priv√©e...

**üëó Apparence et Style**
Description physique, mesures, tatouages, piercings...

**üèÜ Prix et Distinctions**
Awards, nominations, reconnaissance...

**Conclusion rapide**
R√©sum√©, impact, h√©ritage...
```

## üîß Configuration Avanc√©e

### Personnaliser les R√®gles de Tags
Modifier la classe `TagRulesEngine` dans `stashmaster_unified.py` :

```python
@staticmethod
def generate_tags(metadata: Dict) -> List[str]:
    tags = []
    
    # Ajouter vos r√®gles personnalis√©es ici
    if condition:
        tags.append('YourTag')
    
    return list(set(tags))
```

### Ajouter un Nouveau Scraper
Cr√©er une nouvelle classe dans `scrapers.py` :

```python
class NewSourceScraper(ScraperBase):
    def scrape_performer(self, url: str) -> Dict:
        # Votre code de scraping
        return data
```

Puis l'enregistrer dans `ScraperOrchestrator` :

```python
self.scrapers['newsource'] = NewSourceScraper()
```

### Personnaliser le Template de Bio
Modifier la m√©thode `generate_google_bio` dans la classe `BioGenerator`.

## ‚ùì FAQ

### Les tags ne se g√©n√®rent pas automatiquement ?
‚Üí V√©rifiez que vous avez bien rempli les champs de base (ethnicit√©, cheveux, mesures) et cliquez sur "üîÑ G√©n√©rer Tags"

### Ollama ne fonctionne pas ?
‚Üí V√©rifiez qu'Ollama est install√© et en cours d'ex√©cution :
```bash
ollama serve
```

### Les awards ne sont pas nettoy√©s correctement ?
‚Üí Utilisez le bouton "Nettoyer Awards" apr√®s le scraping pour formater automatiquement

### Comment r√©soudre les conflits de donn√©es ?
‚Üí Les conflits sont affich√©s lors du scraping. Choisissez manuellement la valeur correcte ou conservez celle de la source la plus fiable (g√©n√©ralement IAFD)

### La bio est trop longue/courte ?
‚Üí Bio Google : ~3000 caract√®res (fixe)
‚Üí Bio Ollama : Ajustez dans le prompt personnalis√© : "√âcris une bio de [X] caract√®res..."

## üìÑ Licence

Ce projet est fourni tel quel pour usage personnel.

## ü§ù Contribution

Pour toute am√©lioration ou correction :
1. Cr√©er une branche pour votre fonctionnalit√©
2. Commiter vos changements
3. Cr√©er une Pull Request

## üìÆ Support

Pour toute question ou probl√®me, cr√©er une issue sur le repository.

---

**Version** : 2.0  
**Date** : F√©vrier 2026  
**Statut** : Production


============================================================
[26/124] Legacy\files\stashmaster-v2\stashmaster-v2\requirements.txt
------------------------------------------------------------
requests==2.31.0
beautifulsoup4==4.12.3
lxml==5.1.0


============================================================
[27/124] Legacy\files\stashmaster-v2\stashmaster-v2\scrapers.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Scrapers pour diff√©rentes sources de donn√©es
"""

import requests
from bs4 import BeautifulSoup
import re
from typing import Dict, List, Optional, Tuple
from datetime import datetime


class ScraperBase:
    """Classe de base pour tous les scrapers"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def get_page(self, url: str, timeout: int = 10) -> Optional[BeautifulSoup]:
        """R√©cup√®re et parse une page web"""
        try:
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except Exception as e:
            print(f"Erreur lors du scraping de {url}: {e}")
            return None


class IAFDScraper(ScraperBase):
    """Scraper pour IAFD.com"""
    
    def scrape_performer(self, url: str) -> Dict:
        """Scrape les donn√©es d'un performer depuis IAFD"""
        soup = self.get_page(url)
        if not soup:
            return {}
        
        data = {
            'source': 'iafd',
            'url': url
        }
        
        try:
            # Nom
            name_elem = soup.find('h1')
            if name_elem:
                data['name'] = name_elem.text.strip()
            
            # Informations de base
            info_box = soup.find('div', class_='biodata')
            if info_box:
                rows = info_box.find_all('p')
                for row in rows:
                    text = row.text.strip()
                    
                    # Date de naissance
                    if 'Born' in text or 'Birthday' in text:
                        date_match = re.search(r'(\w+ \d+, \d{4})', text)
                        if date_match:
                            data['birthdate'] = date_match.group(1)
                    
                    # Lieu de naissance
                    if 'Birthplace' in text:
                        place = text.split('Birthplace:')[-1].strip()
                        data['birthplace'] = place
                    
                    # Ethnicit√©
                    if 'Ethnicity' in text:
                        ethnicity = text.split('Ethnicity:')[-1].strip()
                        data['ethnicity'] = ethnicity
                    
                    # Cheveux
                    if 'Hair Color' in text:
                        hair = text.split('Hair Color:')[-1].strip()
                        data['hair_color'] = hair
                    
                    # Yeux
                    if 'Eye Color' in text:
                        eyes = text.split('Eye Color:')[-1].strip()
                        data['eye_color'] = eyes
                    
                    # Taille
                    if 'Height' in text:
                        height = text.split('Height:')[-1].strip()
                        data['height'] = height
                    
                    # Poids
                    if 'Weight' in text:
                        weight = text.split('Weight:')[-1].strip()
                        data['weight'] = weight
                    
                    # Mesures
                    if 'Measurements' in text:
                        measurements = text.split('Measurements:')[-1].strip()
                        data['measurements'] = measurements
                    
                    # Tatouages
                    if 'Tattoos' in text:
                        tattoos = text.split('Tattoos:')[-1].strip()
                        data['tattoos'] = tattoos
                    
                    # Piercings
                    if 'Piercings' in text:
                        piercings = text.split('Piercings:')[-1].strip()
                        data['piercings'] = piercings
                    
                    # Ann√©es actives
                    if 'Years Active' in text:
                        years = text.split('Years Active:')[-1].strip()
                        data['career_length'] = years
            
            # Aliases
            aliases_section = soup.find('p', string=re.compile('Also Known As', re.I))
            if aliases_section:
                aliases_text = aliases_section.text.replace('Also Known As:', '').strip()
                aliases = [a.strip() for a in aliases_text.split(',')]
                data['aliases'] = aliases
            
        except Exception as e:
            print(f"Erreur parsing IAFD: {e}")
        
        return data
    
    def scrape_awards(self, url: str) -> List[Dict]:
        """Scrape les awards depuis IAFD"""
        soup = self.get_page(url)
        if not soup:
            return []
        
        awards = []
        
        try:
            # Chercher la section awards
            awards_section = soup.find('div', {'id': 'awards'})
            if not awards_section:
                awards_section = soup.find('h2', string=re.compile('Awards', re.I))
                if awards_section:
                    awards_section = awards_section.find_next('div')
            
            if awards_section:
                # Parser les awards
                award_items = awards_section.find_all(['p', 'li'])
                current_year = None
                current_ceremony = None
                
                for item in award_items:
                    text = item.text.strip()
                    
                    # D√©tecter l'ann√©e
                    year_match = re.match(r'^(19\d{2}|20\d{2})$', text)
                    if year_match:
                        current_year = year_match.group(1)
                        continue
                    
                    # D√©tecter le type de c√©r√©monie
                    if any(ceremony in text.upper() for ceremony in ['AVN', 'XBIZ', 'XRCO', 'NIGHTMOVES']):
                        current_ceremony = text
                        continue
                    
                    # C'est un award
                    if current_year and text:
                        award = {
                            'year': current_year,
                            'ceremony': current_ceremony or 'Unknown',
                            'award': text,
                            'winner': 'Winner' in text or 'Won' in text
                        }
                        awards.append(award)
        
        except Exception as e:
            print(f"Erreur scraping awards IAFD: {e}")
        
        return awards


class FreeonesScraper(ScraperBase):
    """Scraper pour Freeones.xxx"""
    
    def scrape_performer(self, url: str) -> Dict:
        """Scrape les donn√©es d'un performer depuis Freeones"""
        soup = self.get_page(url)
        if not soup:
            return {}
        
        data = {
            'source': 'freeones',
            'url': url
        }
        
        try:
            # Nom
            name_elem = soup.find('h1', class_='profile-header-name')
            if name_elem:
                data['name'] = name_elem.text.strip()
            
            # Bio section
            bio_section = soup.find('div', class_='profile-meta-list')
            if bio_section:
                items = bio_section.find_all('div', class_='profile-meta-item')
                for item in items:
                    label_elem = item.find('span', class_='profile-meta-label')
                    value_elem = item.find('span', class_='profile-meta-value')
                    
                    if label_elem and value_elem:
                        label = label_elem.text.strip().lower()
                        value = value_elem.text.strip()
                        
                        if 'born' in label or 'birth' in label:
                            data['birthdate'] = value
                        elif 'ethnicity' in label:
                            data['ethnicity'] = value
                        elif 'hair' in label:
                            data['hair_color'] = value
                        elif 'eye' in label:
                            data['eye_color'] = value
                        elif 'height' in label:
                            data['height'] = value
                        elif 'weight' in label:
                            data['weight'] = value
                        elif 'measure' in label:
                            data['measurements'] = value
                        elif 'tattoo' in label:
                            data['tattoos'] = value
                        elif 'piercing' in label:
                            data['piercings'] = value
            
            # Aliases
            aliases_elem = soup.find('div', class_='profile-aliases')
            if aliases_elem:
                aliases = [a.text.strip() for a in aliases_elem.find_all('a')]
                data['aliases'] = aliases
        
        except Exception as e:
            print(f"Erreur parsing Freeones: {e}")
        
        return data


class BabepaediaScraper(ScraperBase):
    """Scraper pour Babepedia.com"""
    
    def scrape_performer(self, url: str) -> Dict:
        """Scrape les donn√©es d'un performer depuis Babepedia"""
        soup = self.get_page(url)
        if not soup:
            return {}
        
        data = {
            'source': 'babepedia',
            'url': url
        }
        
        try:
            # Nom
            name_elem = soup.find('h1', class_='firstHeading')
            if name_elem:
                data['name'] = name_elem.text.strip()
            
            # Infobox (similaire √† Wikipedia)
            infobox = soup.find('table', class_='infobox')
            if infobox:
                rows = infobox.find_all('tr')
                for row in rows:
                    th = row.find('th')
                    td = row.find('td')
                    
                    if th and td:
                        label = th.text.strip().lower()
                        value = td.text.strip()
                        
                        if 'born' in label:
                            data['birthdate'] = value
                        elif 'birth' in label and 'place' in label:
                            data['birthplace'] = value
                        elif 'ethnic' in label:
                            data['ethnicity'] = value
                        elif 'hair' in label:
                            data['hair_color'] = value
                        elif 'eye' in label:
                            data['eye_color'] = value
                        elif 'height' in label:
                            data['height'] = value
                        elif 'weight' in label:
                            data['weight'] = value
                        elif 'measure' in label:
                            data['measurements'] = value
        
        except Exception as e:
            print(f"Erreur parsing Babepedia: {e}")
        
        return data


class TheNudeScraper(ScraperBase):
    """Scraper pour TheNude.com"""
    
    def scrape_performer(self, url: str) -> Dict:
        """Scrape les donn√©es d'un performer depuis TheNude"""
        soup = self.get_page(url)
        if not soup:
            return {}
        
        data = {
            'source': 'thenude',
            'url': url
        }
        
        try:
            # Nom
            name_elem = soup.find('h1')
            if name_elem:
                data['name'] = name_elem.text.strip()
            
            # Donn√©es biographiques
            bio_divs = soup.find_all('div', class_='bio-item')
            for div in bio_divs:
                label_elem = div.find('strong')
                if label_elem:
                    label = label_elem.text.strip().lower()
                    value = div.text.replace(label_elem.text, '').strip()
                    
                    if 'born' in label or 'birth' in label:
                        data['birthdate'] = value
                    elif 'ethnic' in label:
                        data['ethnicity'] = value
                    elif 'hair' in label:
                        data['hair_color'] = value
                    elif 'eye' in label:
                        data['eye_color'] = value
                    elif 'height' in label:
                        data['height'] = value
                    elif 'weight' in label:
                        data['weight'] = value
                    elif 'measure' in label:
                        data['measurements'] = value
        
        except Exception as e:
            print(f"Erreur parsing TheNude: {e}")
        
        return data


class DataMerger:
    """Fusionne les donn√©es de plusieurs sources"""
    
    @staticmethod
    def merge_data(sources_data: List[Dict]) -> Tuple[Dict, Dict]:
        """
        Fusionne les donn√©es de plusieurs sources
        Retourne: (donn√©es confirm√©es, donn√©es en conflit)
        """
        confirmed = {}
        conflicts = {}
        
        # Compter les occurrences de chaque valeur pour chaque champ
        field_values = {}
        
        for source_data in sources_data:
            source_name = source_data.get('source', 'unknown')
            for field, value in source_data.items():
                if field in ['source', 'url']:
                    continue
                
                if not value:
                    continue
                
                if field not in field_values:
                    field_values[field] = {}
                
                value_str = str(value).strip().lower()
                if value_str not in field_values[field]:
                    field_values[field][value_str] = {
                        'count': 0,
                        'sources': [],
                        'original_value': value
                    }
                
                field_values[field][value_str]['count'] += 1
                field_values[field][value_str]['sources'].append(source_name)
        
        # D√©terminer les valeurs confirm√©es et les conflits
        for field, values in field_values.items():
            if len(values) == 1:
                # Une seule valeur trouv√©e
                value_info = list(values.values())[0]
                confirmed[field] = {
                    'value': value_info['original_value'],
                    'count': value_info['count'],
                    'sources': value_info['sources']
                }
            else:
                # Plusieurs valeurs diff√©rentes = conflit
                sorted_values = sorted(values.items(), 
key="***MASKED***"
                                     reverse=True)
                
                if sorted_values[0][1]['count'] > sorted_values[1][1]['count']:
                    # Une valeur majoritaire
                    confirmed[field] = {
                        'value': sorted_values[0][1]['original_value'],
                        'count': sorted_values[0][1]['count'],
                        'sources': sorted_values[0][1]['sources'],
                        'note': 'Valeur majoritaire'
                    }
                    # Garder les autres valeurs dans les conflits
                    conflicts[field] = [
                        {
                            'value': v[1]['original_value'],
                            'count': v[1]['count'],
                            'sources': v[1]['sources']
                        }
                        for v in sorted_values[1:]
                    ]
                else:
                    # √âgalit√© = vrai conflit
                    conflicts[field] = [
                        {
                            'value': v[1]['original_value'],
                            'count': v[1]['count'],
                            'sources': v[1]['sources']
                        }
                        for v in sorted_values
                    ]
        
        return confirmed, conflicts


class ScraperOrchestrator:
    """Orchestre le scraping depuis plusieurs sources"""
    
    def __init__(self):
        self.scrapers = {
            'iafd': IAFDScraper(),
            'freeones': FreeonesScraper(),
            'babepedia': BabepaediaScraper(),
            'thenude': TheNudeScraper()
        }
        self.merger = DataMerger()
    
    def scrape_urls(self, urls: List[str]) -> Tuple[Dict, Dict, int]:
        """
        Scrape plusieurs URLs et fusionne les r√©sultats
        Retourne: (donn√©es confirm√©es, conflits, nombre de sources)
        """
        sources_data = []
        
        for url in urls:
            scraper = self._get_scraper_for_url(url)
            if scraper:
                data = scraper.scrape_performer(url)
                if data:
                    sources_data.append(data)
        
        if not sources_data:
            return {}, {}, 0
        
        confirmed, conflicts = self.merger.merge_data(sources_data)
        return confirmed, conflicts, len(sources_data)
    
    def _get_scraper_for_url(self, url: str) -> Optional[ScraperBase]:
        """Retourne le scraper appropri√© pour une URL"""
        url_lower = url.lower()
        
        if 'iafd.com' in url_lower:
            return self.scrapers['iafd']
        elif 'freeones' in url_lower:
            return self.scrapers['freeones']
        elif 'babepedia' in url_lower:
            return self.scrapers['babepedia']
        elif 'thenude' in url_lower:
            return self.scrapers['thenude']
        
        return None
    
    def scrape_awards(self, urls: List[str]) -> List[Dict]:
        """Scrape les awards de toutes les sources"""
        all_awards = []
        
        for url in urls:
            if 'iafd.com' in url.lower():
                awards = self.scrapers['iafd'].scrape_awards(url)
                all_awards.extend(awards)
        
        return all_awards


if __name__ == "__main__":
    # Test
    orchestrator = ScraperOrchestrator()
    
    # Exemple d'URLs
    test_urls = [
        "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
    ]
    
    confirmed, conflicts, num_sources = orchestrator.scrape_urls(test_urls)
    
    print(f"\n=== Donn√©es confirm√©es de {num_sources} source(s) ===")
    for field, info in confirmed.items():
        print(f"{field}: {info['value']} (sources: {', '.join(info['sources'])})")
    
    if conflicts:
        print(f"\n=== Conflits d√©tect√©s ===")
        for field, values in conflicts.items():
            print(f"\n{field}:")
            for v in values:
                print(f"  - {v['value']} ({v['count']} source(s): {', '.join(v['sources'])})")


============================================================
[28/124] Legacy\files\stashmaster-v2\stashmaster-v2\stashmaster_unified.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
StashMaster V2 - Interface Unifi√©e
Fusion des Phase 1 et Phase 2 avec g√©n√©ration automatique de bio
"""

import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox, simpledialog
import requests
from bs4 import BeautifulSoup
import re
import json
from datetime import datetime
from typing import Dict, List, Optional
import threading


class TagRulesEngine:
    """Moteur de r√®gles pour g√©n√©rer des tags bas√©s sur les m√©tadonn√©es"""
    
    @staticmethod
    def generate_tags(metadata: Dict) -> List[str]:
        """G√©n√®re des tags bas√©s sur les m√©tadonn√©es collect√©es"""
        tags = []
        
        # Tags bas√©s sur l'ethnicit√©
        ethnicity = metadata.get('ethnicity', '').lower()
        if ethnicity:
            if 'caucasian' in ethnicity:
                tags.append('Caucasian')
            elif 'latin' in ethnicity or 'cuban' in ethnicity:
                tags.append('Latina')
            elif 'asian' in ethnicity:
                tags.append('Asian')
            elif 'ebony' in ethnicity or 'african' in ethnicity:
                tags.append('Ebony')
        
        # Tags bas√©s sur la couleur de cheveux
        hair_color = metadata.get('hair_color', '').lower()
        if 'blonde' in hair_color or 'blond' in hair_color:
            tags.append('Blonde')
        elif 'brown' in hair_color or 'brunette' in hair_color:
            tags.append('Brunette')
        elif 'red' in hair_color:
            tags.append('Redhead')
        elif 'black' in hair_color:
            tags.append('Black Hair')
        
        # Tags bas√©s sur les mesures
        measurements = metadata.get('measurements', '')
        if measurements:
            # Extraire la taille des seins (premi√®re valeur)
            match = re.match(r'(\d+)', measurements)
            if match:
                size = int(match.group(1))
                if size >= 36:
                    tags.append('Big Boobs')
                elif size <= 32:
                    tags.append('Small Boobs')
        
        # Tags bas√©s sur les piercings
        piercings = metadata.get('piercings', '').lower()
        if piercings and piercings != 'none':
            tags.append('Pierced')
        
        # Tags bas√©s sur les tattoos
        tattoos = metadata.get('tattoos', '').lower()
        if tattoos and tattoos != 'none':
            tags.append('Tattooed')
        
        # Tags bas√©s sur l'√¢ge de carri√®re
        career_start = metadata.get('career_start', '')
        if career_start:
            try:
                year = int(career_start.split('-')[0])
                current_year = datetime.now().year
                if current_year - year > 10:
                    tags.append('MILF')
            except:
                pass
        
        return list(set(tags))  # √âliminer les doublons


class AwardsCleaner:
    """Nettoie et formate les awards pour avoir 1 par ligne"""
    
    @staticmethod
    def clean_awards(raw_awards: str) -> str:
        """Nettoie le texte brut des awards pour avoir un format lisible"""
        if not raw_awards:
            return ""
        
        # S√©parer par les num√©ros d'ann√©e
        lines = []
        current_year = None
        
        # Pattern pour d√©tecter les ann√©es
        year_pattern = re.compile(r'\b(19\d{2}|20\d{2})\b')
        
        # Pattern pour d√©tecter les types d'awards
        award_types = ['AVN AWARDS', 'XBIZ AWARDS', 'NIGHTMOVES', 'XRCO AWARDS']
        
        text = raw_awards
        for award_type in award_types:
            text = text.replace(award_type, f'\n{award_type}\n')
        
        # Diviser en lignes et nettoyer
        for line in text.split('\n'):
            line = line.strip()
            if not line:
                continue
            
            # D√©tecter si c'est une ann√©e
            if re.match(r'^\d{4}$', line):
                current_year = line
                lines.append(f'\n{current_year}')
                continue
            
            # D√©tecter si c'est un type d'award
            if any(award_type in line.upper() for award_type in award_types):
                lines.append(f'\n{line}')
                continue
            
            # D√©tecter Winner ou Nominee
            if line.startswith('Winner:') or line.startswith('Nominee:'):
                lines.append(f'  {line}')
            elif current_year and not line.startswith(' '):
                lines.append(f'  {line}')
            else:
                lines.append(line)
        
        return '\n'.join(lines)


class BioGenerator:
    """G√©n√©rateur de biographies avec Google Search et Ollama"""
    
    def __init__(self):
        self.ollama_url = "http://localhost:11434/api/generate"
    
    def generate_google_bio(self, performer_name: str, metadata: Dict) -> str:
        """G√©n√®re une bio de 3000 caract√®res style Google"""
        
        # Template bas√© sur BioGooglemodele.txt
        template = f"""### {performer_name} : L'√©toile charismatique au parcours diversifi√©

**Introduction**
N√©e le {metadata.get('birthdate', '[date]')} √† {metadata.get('birthplace', '[lieu]')}, {performer_name} a marqu√© de son empreinte l'industrie du divertissement pour adultes d√®s son entr√©e en sc√®ne en {metadata.get('career_start', '[ann√©e]')}. Reconnue pour son charisme naturel et son √©nergie captivante, elle a rapidement acquis une notori√©t√© significative. Au fil de sa carri√®re, elle a adopt√© plusieurs pseudonymes, tels que {', '.join(metadata.get('aliases', []))}, qui ont tous contribu√© √† forger son image polyvalente et √† laisser un impact m√©morable dans le secteur.

**üìÖ Origines et Premiers Pas**
Issue d'une famille d'origine {metadata.get('ethnicity', '[origine]')} et ayant grandi dans le vibrant paysage de {metadata.get('birthplace', '[lieu]')}, la vie de {performer_name} avant son immersion dans l'industrie est envelopp√©e d'une certaine discr√©tion. Les informations d√©taill√©es concernant son enfance ou son parcours scolaire ne sont pas largement divulgu√©es publiquement, soulignant une volont√© de pr√©server sa sph√®re priv√©e. C'est √† l'√¢ge de {metadata.get('career_start_age', '[√¢ge]')} ans, en {metadata.get('career_start', '[ann√©e]')}, qu'elle a franchi le seuil du monde du divertissement pour adultes, un choix qui allait d√©finir une d√©cennie de sa vie professionnelle et la propulser sur le devant de la sc√®ne internationale.

**üèÜ Carri√®re et Filmographie**
La trajectoire professionnelle de {performer_name} a d√©but√© avec une force consid√©rable, la menant √† collaborer avec certains des plus grands noms de l'industrie. D√®s les premi√®res ann√©es de sa carri√®re, elle a √©t√© une pr√©sence r√©guli√®re sur des plateformes de renom. Ces partenariats pr√©coces lui ont permis d'acqu√©rir une visibilit√© rapide et de se b√¢tir une solide r√©putation en tant qu'interpr√®te polyvalente.

Son √©volution l'a ensuite amen√©e √† diversifier ses r√¥les et √† travailler avec d'autres studios influents. Elle a su s'adapter √† diff√©rents types de sc√®nes, d√©montrant une gamme de performances qui ont plu √† un large public. Bien que sa carri√®re en sc√®nes explicites ait connu son apog√©e autour de {metadata.get('peak_years', '[p√©riode]')}, sa vaste filmographie et la qualit√© constante de ses prestations lui ont assur√© une place de choix parmi les √©toiles de sa g√©n√©ration.

**üí° Faits Int√©ressants & Vie Personnelle**
Au-del√† de l'√©cran, {performer_name} est r√©put√©e pour sa personnalit√© authentique et son approche terre-√†-terre. La sph√®re de sa vie personnelle reste, comme il est courant dans cette industrie, relativement priv√©e. {metadata.get('trivia', '')}

**üëó Apparence et Style**
{performer_name} est souvent caract√©ris√©e par une beaut√© distinctive, ancr√©e dans ses origines {metadata.get('ethnicity', '[origine]')}. Elle arbore typiquement une chevelure {metadata.get('hair_color', '[couleur]')}, souvent longue et soyeuse, qui encadre un visage expressif et une silhouette g√©n√©ralement {metadata.get('body_type', 'fine et athl√©tique')}. Son style sur sc√®ne est marqu√© par une √©nergie palpable et une capacit√© √† incarner des personnages vari√©s avec cr√©dibilit√©.

Mesures physiques : {metadata.get('measurements', '[mesures]')} - Taille : {metadata.get('height', '[taille]')} - Poids : {metadata.get('weight', '[poids]')}
Tatouages : {metadata.get('tattoos', 'Information non disponible')}
Piercings : {metadata.get('piercings', 'Information non disponible')}

**üèÜ Prix et Distinctions**
La reconnaissance de l'industrie n'a pas tard√© √† se manifester pour {performer_name}, qui a √©t√© honor√©e de nombreuses nominations au cours de sa carri√®re. {metadata.get('awards_summary', '')}

**Conclusion rapide**
En somme, {performer_name} demeure une figure embl√©matique et respect√©e de l'industrie pour adultes. Son parcours, caract√©ris√© par une entr√©e remarqu√©e en {metadata.get('career_start', '[ann√©e]')} et une carri√®re diversifi√©e sous plusieurs alias, a laiss√© une impression durable. Son professionnalisme, son charme et sa capacit√© √† captiver le public continuent d'√™tre salu√©s par ses fans et les connaisseurs du milieu, confirmant son statut d'√©toile marquante de sa g√©n√©ration."""
        
        # Limiter √† environ 3000 caract√®res
        if len(template) > 3000:
            # Couper intelligemment
            template = template[:2950] + "..."
        
        return template
    
    def generate_ollama_bio(self, performer_name: str, metadata: Dict, custom_prompt: str = "") -> Optional[str]:
        """G√©n√®re une bio avec Ollama"""
        try:
            # Construire le prompt
            if custom_prompt:
                prompt = custom_prompt
            else:
                prompt = f"""√âcris une biographie professionnelle de 3000 caract√®res pour {performer_name}, 
                actrice de l'industrie du divertissement pour adultes.
                
                Informations disponibles :
                - Nom : {performer_name}
                - Aliases : {', '.join(metadata.get('aliases', []))}
                - Date de naissance : {metadata.get('birthdate', 'Non disponible')}
                - Lieu de naissance : {metadata.get('birthplace', 'Non disponible')}
                - Ethnicit√© : {metadata.get('ethnicity', 'Non disponible')}
                - D√©but de carri√®re : {metadata.get('career_start', 'Non disponible')}
                - Mesures : {metadata.get('measurements', 'Non disponible')}
                
                La biographie doit √™tre :
                - Professionnelle et respectueuse
                - Structur√©e avec des sections claires
                - D'environ 3000 caract√®res
                - En fran√ßais
                """
            
            # Appel √† Ollama
            response = requests.post(
                self.ollama_url,
                json={
                    "model": "llama2",
                    "prompt": prompt,
                    "stream": False
                },
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '')
            else:
                return None
        except Exception as e:
            print(f"Erreur Ollama: {e}")
            return None


class TriviaAwardsWindow(tk.Toplevel):
    """Fen√™tre s√©par√©e pour Trivia et Awards avec requ√™te et r√©sultats"""
    
    def __init__(self, parent, performer_name: str, urls: List[str]):
        super().__init__(parent)
        self.title(f"Trivia & Awards ‚Äî {performer_name}")
        self.geometry("1000x700")
        
        self.performer_name = performer_name
        self.urls = urls
        self.awards_cleaner = AwardsCleaner()
        
        self._create_widgets()
    
    def _create_widgets(self):
        # Frame principal
        main_frame = ttk.Frame(self, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        self.columnconfigure(0, weight=1)
        self.rowconfigure(0, weight=1)
        main_frame.columnconfigure(0, weight=1)
        main_frame.rowconfigure(2, weight=1)
        main_frame.rowconfigure(4, weight=1)
        
        # Section Trivia
        trivia_label = ttk.Label(main_frame, text="üìù Trivia", font=('Segoe UI', 12, 'bold'))
        trivia_label.grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        
        # Boutons de scraping pour Trivia
        trivia_btn_frame = ttk.Frame(main_frame)
        trivia_btn_frame.grid(row=1, column=0, sticky=tk.W, pady=(0, 5))
        
        ttk.Button(trivia_btn_frame, text="Scraper Trivia", 
                   command=self._scrape_trivia).pack(side=tk.LEFT, padx=(0, 5))
        
        # Champ Trivia (multiligne)
        self.trivia_text = scrolledtext.ScrolledText(main_frame, height=8, wrap=tk.WORD)
        self.trivia_text.grid(row=2, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        
        # Section Awards
        awards_label = ttk.Label(main_frame, text="üèÜ Awards & Nominations", 
                                 font=('Segoe UI', 12, 'bold'))
        awards_label.grid(row=3, column=0, sticky=tk.W, pady=(10, 5))
        
        # Boutons de scraping pour Awards
        awards_btn_frame = ttk.Frame(main_frame)
        awards_btn_frame.grid(row=4, column=0, sticky=tk.W, pady=(0, 5))
        
        ttk.Button(awards_btn_frame, text="Scraper Awards", 
                   command=self._scrape_awards).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(awards_btn_frame, text="Nettoyer Awards", 
                   command=self._clean_awards).pack(side=tk.LEFT, padx=(0, 5))
        
        # Champ Awards (multiligne)
        self.awards_text = scrolledtext.ScrolledText(main_frame, height=15, wrap=tk.WORD)
        self.awards_text.grid(row=5, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        
        # Boutons d'action
        btn_frame = ttk.Frame(main_frame)
        btn_frame.grid(row=6, column=0, sticky=tk.E, pady=(10, 0))
        
        ttk.Button(btn_frame, text="Annuler", command=self.destroy).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(btn_frame, text="Appliquer et continuer", 
                   command=self._apply_and_continue).pack(side=tk.LEFT)
    
    def _scrape_trivia(self):
        """Scrape les trivia depuis les URLs"""
        self.trivia_text.delete('1.0', tk.END)
        self.trivia_text.insert('1.0', "Scraping en cours...\n")
        
        def scrape():
            trivia_items = []
            for url in self.urls:
                if 'iafd.com' in url:
                    trivia = self._scrape_iafd_trivia(url)
                    if trivia:
                        trivia_items.extend(trivia)
            
            # Afficher les r√©sultats
            self.after(0, lambda: self._display_trivia(trivia_items))
        
        thread = threading.Thread(target=scrape)
        thread.daemon = True
        thread.start()
    
    def _scrape_iafd_trivia(self, url: str) -> List[str]:
        """Scrape les trivia depuis IAFD"""
        try:
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Chercher la section trivia (√† adapter selon la structure r√©elle)
            trivia = []
            # Code de scraping √† adapter selon la structure du site
            
            return trivia
        except Exception as e:
            print(f"Erreur scraping trivia: {e}")
            return []
    
    def _display_trivia(self, trivia_items: List[str]):
        """Affiche les trivia scrap√©s"""
        self.trivia_text.delete('1.0', tk.END)
        if trivia_items:
            for item in trivia_items:
                self.trivia_text.insert(tk.END, f"‚Ä¢ {item}\n")
        else:
            self.trivia_text.insert(tk.END, "Aucun trivia trouv√©.")
    
    def _scrape_awards(self):
        """Scrape les awards depuis les URLs"""
        self.awards_text.delete('1.0', tk.END)
        self.awards_text.insert('1.0', "Scraping en cours...\n")
        
        def scrape():
            awards_text = ""
            for url in self.urls:
                if 'iafd.com' in url:
                    awards = self._scrape_iafd_awards(url)
                    if awards:
                        awards_text += awards + "\n\n"
            
            # Afficher les r√©sultats
            self.after(0, lambda: self._display_awards(awards_text))
        
        thread = threading.Thread(target=scrape)
        thread.daemon = True
        thread.start()
    
    def _scrape_iafd_awards(self, url: str) -> str:
        """Scrape les awards depuis IAFD"""
        try:
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Chercher la section awards (√† adapter selon la structure r√©elle)
            awards = ""
            # Code de scraping √† adapter
            
            return awards
        except Exception as e:
            print(f"Erreur scraping awards: {e}")
            return ""
    
    def _display_awards(self, awards_text: str):
        """Affiche les awards scrap√©s"""
        self.awards_text.delete('1.0', tk.END)
        if awards_text:
            self.awards_text.insert(tk.END, awards_text)
        else:
            self.awards_text.insert(tk.END, "Aucun award trouv√©.")
    
    def _clean_awards(self):
        """Nettoie les awards pour avoir 1 par ligne"""
        raw_text = self.awards_text.get('1.0', tk.END)
        cleaned_text = self.awards_cleaner.clean_awards(raw_text)
        self.awards_text.delete('1.0', tk.END)
        self.awards_text.insert('1.0', cleaned_text)
    
    def _apply_and_continue(self):
        """Applique les modifications et ferme la fen√™tre"""
        # R√©cup√©rer les donn√©es
        self.trivia_data = self.trivia_text.get('1.0', tk.END).strip()
        self.awards_data = self.awards_text.get('1.0', tk.END).strip()
        self.destroy()
    
    def get_data(self) -> Dict:
        """Retourne les donn√©es collect√©es"""
        return {
            'trivia': getattr(self, 'trivia_data', ''),
            'awards': getattr(self, 'awards_data', '')
        }


class BioGenerationWindow(tk.Toplevel):
    """Fen√™tre pour la g√©n√©ration de bio"""
    
    def __init__(self, parent, performer_name: str, metadata: Dict):
        super().__init__(parent)
        self.title(f"G√©n√©ration de Bio ‚Äî {performer_name}")
        self.geometry("900x800")
        
        self.performer_name = performer_name
        self.metadata = metadata
        self.bio_generator = BioGenerator()
        
        self._create_widgets()
    
    def _create_widgets(self):
        # Frame principal
        main_frame = ttk.Frame(self, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        self.columnconfigure(0, weight=1)
        self.rowconfigure(0, weight=1)
        main_frame.columnconfigure(0, weight=1)
        main_frame.rowconfigure(3, weight=1)
        
        # Options de g√©n√©ration
        options_frame = ttk.LabelFrame(main_frame, text="Options de g√©n√©ration", padding="10")
        options_frame.grid(row=0, column=0, sticky=(tk.W, tk.E), pady=(0, 10))
        options_frame.columnconfigure(1, weight=1)
        
        # Choix du type de bio
        self.bio_type_var = tk.StringVar(value="google")
        
        ttk.Radiobutton(options_frame, text="Bio Google (3000 car. automatique)", 
                        variable=self.bio_type_var, value="google").grid(row=0, column=0, sticky=tk.W)
        
        ttk.Radiobutton(options_frame, text="Bio Ollama (avec IA locale)", 
                        variable=self.bio_type_var, value="ollama").grid(row=1, column=0, sticky=tk.W)
        
        ttk.Radiobutton(options_frame, text="Bio Ollama avec prompt personnalis√©", 
                        variable=self.bio_type_var, value="ollama_custom").grid(row=2, column=0, sticky=tk.W)
        
        # Section prompt personnalis√©
        prompt_frame = ttk.LabelFrame(main_frame, text="Prompt personnalis√© (optionnel)", padding="10")
        prompt_frame.grid(row=1, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        prompt_frame.columnconfigure(0, weight=1)
        prompt_frame.rowconfigure(1, weight=1)
        
        ttk.Label(prompt_frame, text="Entrez vos directives pr√©cises pour la g√©n√©ration de la bio :").grid(
            row=0, column=0, sticky=tk.W, pady=(0, 5))
        
        self.prompt_text = scrolledtext.ScrolledText(prompt_frame, height=6, wrap=tk.WORD)
        self.prompt_text.grid(row=1, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # Bouton de g√©n√©ration
        ttk.Button(main_frame, text="G√©n√©rer la Bio", 
                   command=self._generate_bio).grid(row=2, column=0, pady=(0, 10))
        
        # R√©sultat
        result_frame = ttk.LabelFrame(main_frame, text="Bio g√©n√©r√©e", padding="10")
        result_frame.grid(row=3, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        result_frame.columnconfigure(0, weight=1)
        result_frame.rowconfigure(0, weight=1)
        
        self.bio_text = scrolledtext.ScrolledText(result_frame, wrap=tk.WORD)
        self.bio_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # Label pour le compteur de caract√®res
        self.char_count_label = ttk.Label(result_frame, text="Caract√®res : 0")
        self.char_count_label.grid(row=1, column=0, sticky=tk.E, pady=(5, 0))
        
        # Boutons d'action
        btn_frame = ttk.Frame(main_frame)
        btn_frame.grid(row=4, column=0, sticky=tk.E, pady=(10, 0))
        
        ttk.Button(btn_frame, text="Annuler", command=self.destroy).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(btn_frame, text="Appliquer", 
                   command=self._apply_bio).pack(side=tk.LEFT)
    
    def _generate_bio(self):
        """G√©n√®re la bio selon le type choisi"""
        bio_type = self.bio_type_var.get()
        
        self.bio_text.delete('1.0', tk.END)
        self.bio_text.insert('1.0', "G√©n√©ration en cours...\n")
        
        def generate():
            bio = ""
            if bio_type == "google":
                bio = self.bio_generator.generate_google_bio(self.performer_name, self.metadata)
            elif bio_type in ["ollama", "ollama_custom"]:
                custom_prompt = ""
                if bio_type == "ollama_custom":
                    custom_prompt = self.prompt_text.get('1.0', tk.END).strip()
                bio = self.bio_generator.generate_ollama_bio(
                    self.performer_name, self.metadata, custom_prompt)
                if bio is None:
                    bio = "Erreur: Ollama n'est pas disponible ou n'a pas r√©pondu."
            
            # Afficher le r√©sultat
            self.after(0, lambda: self._display_bio(bio))
        
        thread = threading.Thread(target=generate)
        thread.daemon = True
        thread.start()
    
    def _display_bio(self, bio: str):
        """Affiche la bio g√©n√©r√©e"""
        self.bio_text.delete('1.0', tk.END)
        self.bio_text.insert('1.0', bio)
        
        # Mettre √† jour le compteur de caract√®res
        char_count = len(bio)
        self.char_count_label.config(text=f"Caract√®res : {char_count}")
    
    def _apply_bio(self):
        """Applique la bio et ferme la fen√™tre"""
        self.generated_bio = self.bio_text.get('1.0', tk.END).strip()
        self.destroy()
    
    def get_bio(self) -> str:
        """Retourne la bio g√©n√©r√©e"""
        return getattr(self, 'generated_bio', '')


class MainWindow(tk.Tk):
    """Fen√™tre principale unifi√©e - Fusion Phase 1 et Phase 2"""
    
    def __init__(self):
        super().__init__()
        
        self.title("StashMaster V2 - Performer")
        self.geometry("1200x900")
        
        self.tag_rules = TagRulesEngine()
        self.metadata = {}
        
        self._create_widgets()
        self._create_menu()
    
    def _create_menu(self):
        """Cr√©e le menu principal"""
        menubar = tk.Menu(self)
        self.config(menu=menubar)
        
        # Menu Fichier
        file_menu = tk.Menu(menubar, tearoff=0)
        menubar.add_cascade(label="Fichier", menu=file_menu)
        file_menu.add_command(label="Nouveau", command=self._new_performer)
        file_menu.add_command(label="Ouvrir...", command=self._open_performer)
        file_menu.add_separator()
        file_menu.add_command(label="Quitter", command=self.quit)
        
        # Menu Actions
        actions_menu = tk.Menu(menubar, tearoff=0)
        menubar.add_cascade(label="Actions", menu=actions_menu)
        actions_menu.add_command(label="Scraper & Lancer le flux Bio IA", 
                                 command=self._start_scraping)
        actions_menu.add_command(label="Trivia & Awards...", 
                                 command=self._open_trivia_awards)
        actions_menu.add_command(label="G√©n√©rer Bio...", 
                                 command=self._open_bio_generator)
    
    def _create_widgets(self):
        """Cr√©e l'interface principale"""
        # Notebook pour les onglets
        notebook = ttk.Notebook(self)
        notebook.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        # Onglet 1: M√©tadonn√©es usuelles
        self.metadata_frame = self._create_metadata_tab()
        notebook.add(self.metadata_frame, text="üìã M√©tadonn√©es")
        
        # Onglet 2: Champs avanc√©s
        self.advanced_frame = self._create_advanced_tab()
        notebook.add(self.advanced_frame, text="‚öôÔ∏è Champs Avanc√©s")
        
        # Onglet 3: Bio
        self.bio_frame = self._create_bio_tab()
        notebook.add(self.bio_frame, text="üìù Bio")
        
        # Barre d'outils en bas
        self._create_toolbar()
    
    def _create_metadata_tab(self) -> ttk.Frame:
        """Cr√©e l'onglet des m√©tadonn√©es de base"""
        frame = ttk.Frame()
        
        # Frame avec scrollbar
        canvas = tk.Canvas(frame)
        scrollbar = ttk.Scrollbar(frame, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # Champs de m√©tadonn√©es
        fields = [
            ("Name:", "name"),
            ("Aliases:", "aliases"),
            ("Birthdate:", "birthdate"),
            ("Deathdate:", "deathdate"),
            ("Country:", "country"),
            ("Ethnicity:", "ethnicity"),
            ("Hair Color:", "hair_color"),
            ("Eye Color:", "eye_color"),
            ("Height:", "height"),
            ("Weight:", "weight"),
            ("Measurements:", "measurements"),
            ("Fake Tits:", "fake_tits"),
            ("Career Length:", "career_length"),
        ]
        
        self.metadata_entries = {}
        
        for i, (label, key) in enumerate(fields):
            ttk.Label(scrollable_frame, text=label).grid(row=i, column=0, sticky=tk.W, 
                                                          padx=5, pady=3)
            entry = ttk.Entry(scrollable_frame, width=50)
            entry.grid(row=i, column=1, sticky=(tk.W, tk.E), padx=5, pady=3)
            self.metadata_entries[key] = entry
        
        scrollable_frame.columnconfigure(1, weight=1)
        
        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
        
        return frame
    
    def _create_advanced_tab(self) -> ttk.Frame:
        """Cr√©e l'onglet des champs avanc√©s"""
        frame = ttk.Frame()
        frame.columnconfigure(0, weight=1)
        
        row = 0
        
        # Tags - CHAMP SIMPLE LIGNE (g√©n√©r√©s automatiquement)
        ttk.Label(frame, text="Tags (g√©n√©r√©s automatiquement):", 
                  font=('Segoe UI', 10, 'bold')).grid(row=row, column=0, sticky=tk.W, 
                                                       padx=5, pady=(10, 2))
        row += 1
        
        self.tags_entry = ttk.Entry(frame, state='readonly')
        self.tags_entry.grid(row=row, column=0, sticky=(tk.W, tk.E), padx=5, pady=(0, 10))
        row += 1
        
        # Piercings - CHAMP MULTILIGNE
        ttk.Label(frame, text="Piercings:", 
                  font=('Segoe UI', 10, 'bold')).grid(row=row, column=0, sticky=tk.W, 
                                                       padx=5, pady=(10, 2))
        row += 1
        
        self.piercings_text = scrolledtext.ScrolledText(frame, height=4, wrap=tk.WORD)
        self.piercings_text.grid(row=row, column=0, sticky=(tk.W, tk.E), padx=5, pady=(0, 10))
        row += 1
        
        # Tattoos - CHAMP MULTILIGNE
        ttk.Label(frame, text="Tattoos:", 
                  font=('Segoe UI', 10, 'bold')).grid(row=row, column=0, sticky=tk.W, 
                                                       padx=5, pady=(10, 2))
        row += 1
        
        self.tattoos_text = scrolledtext.ScrolledText(frame, height=4, wrap=tk.WORD)
        self.tattoos_text.grid(row=row, column=0, sticky=(tk.W, tk.E), padx=5, pady=(0, 10))
        row += 1
        
        # URLs - CHAMP MULTILIGNE
        ttk.Label(frame, text="URLs:", 
                  font=('Segoe UI', 10, 'bold')).grid(row=row, column=0, sticky=tk.W, 
                                                       padx=5, pady=(10, 2))
        row += 1
        
        self.urls_text = scrolledtext.ScrolledText(frame, height=6, wrap=tk.WORD)
        self.urls_text.grid(row=row, column=0, sticky=(tk.W, tk.E), padx=5, pady=(0, 10))
        row += 1
        
        # Bouton pour g√©n√©rer les tags
        ttk.Button(frame, text="üîÑ G√©n√©rer Tags", 
                   command=self._generate_tags).grid(row=row, column=0, pady=10)
        
        return frame
    
    def _create_bio_tab(self) -> ttk.Frame:
        """Cr√©e l'onglet de la bio"""
        frame = ttk.Frame()
        frame.columnconfigure(0, weight=1)
        frame.rowconfigure(1, weight=1)
        
        # Boutons d'action
        btn_frame = ttk.Frame(frame)
        btn_frame.grid(row=0, column=0, sticky=tk.W, padx=5, pady=5)
        
        ttk.Button(btn_frame, text="üìù G√©n√©rer Bio...", 
                   command=self._open_bio_generator).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(btn_frame, text="üóëÔ∏è Effacer", 
                   command=self._clear_bio).pack(side=tk.LEFT)
        
        # Champ de bio
        self.bio_text = scrolledtext.ScrolledText(frame, wrap=tk.WORD)
        self.bio_text.grid(row=1, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), 
                          padx=5, pady=(0, 5))
        
        # Compteur de caract√®res
        self.bio_char_label = ttk.Label(frame, text="Caract√®res : 0")
        self.bio_char_label.grid(row=2, column=0, sticky=tk.E, padx=5, pady=(0, 5))
        
        # Binding pour mettre √† jour le compteur
        self.bio_text.bind('<KeyRelease>', self._update_bio_char_count)
        
        return frame
    
    def _create_toolbar(self):
        """Cr√©e la barre d'outils en bas"""
        toolbar = ttk.Frame(self)
        toolbar.pack(side=tk.BOTTOM, fill=tk.X, padx=5, pady=5)
        
        ttk.Button(toolbar, text="Tout s√©lectionner", 
                   command=self._select_all).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(toolbar, text="S√©lectionner vid√©os", 
                   command=self._select_videos).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(toolbar, text="Suivant / Traiter", 
                   command=self._process_next).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(toolbar, text="Retour", 
                   command=self._go_back).pack(side=tk.LEFT)
        
        # Spacer
        ttk.Frame(toolbar).pack(side=tk.LEFT, expand=True)
        
        ttk.Button(toolbar, text="üíæ Sauvegarder", 
                   command=self._save).pack(side=tk.RIGHT, padx=(5, 0))
    
    def _generate_tags(self):
        """G√©n√®re les tags automatiquement bas√©s sur les m√©tadonn√©es"""
        # R√©cup√©rer les m√©tadonn√©es actuelles
        metadata = {
            'ethnicity': self.metadata_entries['ethnicity'].get(),
            'hair_color': self.metadata_entries['hair_color'].get(),
            'measurements': self.metadata_entries['measurements'].get(),
            'piercings': self.piercings_text.get('1.0', tk.END).strip(),
            'tattoos': self.tattoos_text.get('1.0', tk.END).strip(),
            'career_length': self.metadata_entries['career_length'].get(),
        }
        
        # G√©n√©rer les tags
        tags = self.tag_rules.generate_tags(metadata)
        
        # Afficher les tags
        self.tags_entry.config(state='normal')
        self.tags_entry.delete(0, tk.END)
        self.tags_entry.insert(0, ', '.join(tags))
        self.tags_entry.config(state='readonly')
        
        messagebox.showinfo("Tags g√©n√©r√©s", 
                           f"{len(tags)} tag(s) g√©n√©r√©(s) automatiquement.")
    
    def _update_bio_char_count(self, event=None):
        """Met √† jour le compteur de caract√®res de la bio"""
        text = self.bio_text.get('1.0', tk.END)
        count = len(text.strip())
        self.bio_char_label.config(text=f"Caract√®res : {count}")
    
    def _clear_bio(self):
        """Efface la bio"""
        if messagebox.askyesno("Confirmation", 
                              "√ätes-vous s√ªr de vouloir effacer la bio ?"):
            self.bio_text.delete('1.0', tk.END)
            self._update_bio_char_count()
    
    def _start_scraping(self):
        """Lance le flux de scraping complet"""
        messagebox.showinfo("Scraping", 
                           "Fonction de scraping √† impl√©menter...")
    
    def _open_trivia_awards(self):
        """Ouvre la fen√™tre Trivia & Awards"""
        # R√©cup√©rer le nom et les URLs
        performer_name = self.metadata_entries['name'].get()
        if not performer_name:
            messagebox.showwarning("Attention", "Veuillez entrer un nom de performer.")
            return
        
        urls_text = self.urls_text.get('1.0', tk.END).strip()
        urls = [url.strip() for url in urls_text.split('\n') if url.strip()]
        
        if not urls:
            messagebox.showwarning("Attention", "Veuillez entrer au moins une URL.")
            return
        
        # Ouvrir la fen√™tre
        window = TriviaAwardsWindow(self, performer_name, urls)
        self.wait_window(window)
        
        # R√©cup√©rer les donn√©es
        data = window.get_data()
        # Stocker dans les m√©tadonn√©es
        self.metadata['trivia'] = data.get('trivia', '')
        self.metadata['awards'] = data.get('awards', '')
        
        messagebox.showinfo("Trivia & Awards", 
                           "Donn√©es collect√©es avec succ√®s.")
    
    def _open_bio_generator(self):
        """Ouvre la fen√™tre de g√©n√©ration de bio"""
        # R√©cup√©rer le nom
        performer_name = self.metadata_entries['name'].get()
        if not performer_name:
            messagebox.showwarning("Attention", "Veuillez entrer un nom de performer.")
            return
        
        # Pr√©parer les m√©tadonn√©es
        metadata = {
            'name': performer_name,
            'aliases': self.metadata_entries['aliases'].get().split(','),
            'birthdate': self.metadata_entries['birthdate'].get(),
            'birthplace': self.metadata_entries['country'].get(),
            'ethnicity': self.metadata_entries['ethnicity'].get(),
            'hair_color': self.metadata_entries['hair_color'].get(),
            'measurements': self.metadata_entries['measurements'].get(),
            'height': self.metadata_entries['height'].get(),
            'weight': self.metadata_entries['weight'].get(),
            'tattoos': self.tattoos_text.get('1.0', tk.END).strip(),
            'piercings': self.piercings_text.get('1.0', tk.END).strip(),
            'trivia': self.metadata.get('trivia', ''),
            'awards': self.metadata.get('awards', ''),
            'career_length': self.metadata_entries['career_length'].get(),
        }
        
        # Ouvrir la fen√™tre
        window = BioGenerationWindow(self, performer_name, metadata)
        self.wait_window(window)
        
        # R√©cup√©rer la bio g√©n√©r√©e
        bio = window.get_bio()
        if bio:
            self.bio_text.delete('1.0', tk.END)
            self.bio_text.insert('1.0', bio)
            self._update_bio_char_count()
            messagebox.showinfo("Bio g√©n√©r√©e", "La bio a √©t√© g√©n√©r√©e avec succ√®s.")
    
    def _new_performer(self):
        """Nouveau performer"""
        # Effacer tous les champs
        for entry in self.metadata_entries.values():
            entry.delete(0, tk.END)
        self.piercings_text.delete('1.0', tk.END)
        self.tattoos_text.delete('1.0', tk.END)
        self.urls_text.delete('1.0', tk.END)
        self.bio_text.delete('1.0', tk.END)
        self.tags_entry.config(state='normal')
        self.tags_entry.delete(0, tk.END)
        self.tags_entry.config(state='readonly')
        self.metadata = {}
    
    def _open_performer(self):
        """Ouvre un performer existant"""
        messagebox.showinfo("Ouvrir", "Fonction √† impl√©menter...")
    
    def _select_all(self):
        """S√©lectionne tous les champs"""
        pass
    
    def _select_videos(self):
        """S√©lectionne les vid√©os"""
        pass
    
    def _process_next(self):
        """Traite le performer suivant"""
        pass
    
    def _go_back(self):
        """Retour"""
        pass
    
    def _save(self):
        """Sauvegarde les donn√©es"""
        messagebox.showinfo("Sauvegarde", "Donn√©es sauvegard√©es (√† impl√©menter).")


def main():
    app = MainWindow()
    app.mainloop()


if __name__ == "__main__":
    main()


============================================================
[29/124] Legacy\files\stashmaster-v2\stashmaster-v2\test_stashmaster.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Tests unitaires pour StashMaster V2
"""

import unittest
import sys
import os

# Ajouter le r√©pertoire parent au path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from stashmaster_unified import TagRulesEngine, AwardsCleaner
from scrapers import DataMerger


class TestTagRulesEngine(unittest.TestCase):
    """Tests pour le moteur de g√©n√©ration de tags"""
    
    def setUp(self):
        self.engine = TagRulesEngine()
    
    def test_ethnicity_tags(self):
        """Test des tags d'ethnicit√©"""
        # Test Caucasian
        metadata = {'ethnicity': 'Caucasian'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Caucasian', tags)
        
        # Test Latina
        metadata = {'ethnicity': 'Cuban'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Latina', tags)
        
        # Test Asian
        metadata = {'ethnicity': 'Asian'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Asian', tags)
    
    def test_hair_color_tags(self):
        """Test des tags de couleur de cheveux"""
        # Test Blonde
        metadata = {'hair_color': 'Blonde'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Blonde', tags)
        
        # Test Brunette
        metadata = {'hair_color': 'Brown'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Brunette', tags)
        
        # Test Redhead
        metadata = {'hair_color': 'Red'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Redhead', tags)
    
    def test_measurements_tags(self):
        """Test des tags bas√©s sur les mesures"""
        # Test Big Boobs
        metadata = {'measurements': '38DD-27-34'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Big Boobs', tags)
        
        # Test Small Boobs
        metadata = {'measurements': '32A-24-32'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Small Boobs', tags)
    
    def test_piercings_tags(self):
        """Test des tags de piercings"""
        metadata = {'piercings': 'Navel, Tongue'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Pierced', tags)
        
        # Pas de tag si "none"
        metadata = {'piercings': 'none'}
        tags = self.engine.generate_tags(metadata)
        self.assertNotIn('Pierced', tags)
    
    def test_tattoos_tags(self):
        """Test des tags de tattoos"""
        metadata = {'tattoos': 'Lower back, Right shoulder'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Tattooed', tags)
        
        # Pas de tag si "none"
        metadata = {'tattoos': 'none'}
        tags = self.engine.generate_tags(metadata)
        self.assertNotIn('Tattooed', tags)
    
    def test_combined_tags(self):
        """Test de la g√©n√©ration de tags combin√©s"""
        metadata = {
            'ethnicity': 'Latina',
            'hair_color': 'Blonde',
            'measurements': '36DD-25-36',
            'piercings': 'Navel',
            'tattoos': 'Lower back'
        }
        tags = self.engine.generate_tags(metadata)
        
        self.assertIn('Latina', tags)
        self.assertIn('Blonde', tags)
        self.assertIn('Big Boobs', tags)
        self.assertIn('Pierced', tags)
        self.assertIn('Tattooed', tags)
        
        # V√©rifier qu'il n'y a pas de doublons
        self.assertEqual(len(tags), len(set(tags)))


class TestAwardsCleaner(unittest.TestCase):
    """Tests pour le nettoyeur d'awards"""
    
    def setUp(self):
        self.cleaner = AwardsCleaner()
    
    def test_clean_simple_awards(self):
        """Test du nettoyage d'awards simples"""
        raw = """
        2012
        Winner: Unsung Starlet of the Year
        2013
        Nominee: Best Boobs
        """
        
        cleaned = self.cleaner.clean_awards(raw)
        
        # V√©rifier la pr√©sence des ann√©es
        self.assertIn('2012', cleaned)
        self.assertIn('2013', cleaned)
        
        # V√©rifier la pr√©sence des awards
        self.assertIn('Winner', cleaned)
        self.assertIn('Nominee', cleaned)
    
    def test_clean_awards_with_ceremony(self):
        """Test du nettoyage avec type de c√©r√©monie"""
        raw = """
        AVN AWARDS
        2012
        Winner: Unsung Starlet of the Year
        XBIZ AWARDS
        2013
        Nominee: Best Performer
        """
        
        cleaned = self.cleaner.clean_awards(raw)
        
        # V√©rifier la pr√©sence des c√©r√©monies
        self.assertIn('AVN AWARDS', cleaned)
        self.assertIn('XBIZ AWARDS', cleaned)
    
    def test_clean_empty_awards(self):
        """Test avec des awards vides"""
        raw = ""
        cleaned = self.cleaner.clean_awards(raw)
        self.assertEqual(cleaned, "")


class TestDataMerger(unittest.TestCase):
    """Tests pour le fusionneur de donn√©es"""
    
    def setUp(self):
        self.merger = DataMerger()
    
    def test_merge_identical_data(self):
        """Test de fusion de donn√©es identiques"""
        sources = [
            {
                'source': 'iafd',
                'name': 'Bridgette B',
                'ethnicity': 'Caucasian'
            },
            {
                'source': 'freeones',
                'name': 'Bridgette B',
                'ethnicity': 'Caucasian'
            }
        ]
        
        confirmed, conflicts = self.merger.merge_data(sources)
        
        # Les donn√©es identiques doivent √™tre confirm√©es
        self.assertIn('name', confirmed)
        self.assertEqual(confirmed['name']['value'], 'Bridgette B')
        self.assertEqual(confirmed['name']['count'], 2)
        
        # Pas de conflits
        self.assertEqual(len(conflicts), 0)
    
    def test_merge_conflicting_data(self):
        """Test de fusion de donn√©es conflictuelles"""
        sources = [
            {
                'source': 'iafd',
                'hair_color': 'Blonde'
            },
            {
                'source': 'freeones',
                'hair_color': 'Brown'
            }
        ]
        
        confirmed, conflicts = self.merger.merge_data(sources)
        
        # Doit y avoir un conflit
        self.assertIn('hair_color', conflicts)
        self.assertEqual(len(conflicts['hair_color']), 2)
    
    def test_merge_majority_data(self):
        """Test de fusion avec valeur majoritaire"""
        sources = [
            {'source': 'iafd', 'ethnicity': 'Caucasian'},
            {'source': 'freeones', 'ethnicity': 'Caucasian'},
            {'source': 'thenude', 'ethnicity': 'White'}
        ]
        
        confirmed, conflicts = self.merger.merge_data(sources)
        
        # La valeur majoritaire doit √™tre confirm√©e
        self.assertIn('ethnicity', confirmed)
        self.assertEqual(confirmed['ethnicity']['count'], 2)
    
    def test_merge_unique_data(self):
        """Test de fusion de donn√©es uniques"""
        sources = [
            {
                'source': 'iafd',
                'name': 'Bridgette B',
                'birthdate': 'October 15, 1983'
            },
            {
                'source': 'freeones',
                'name': 'Bridgette B'
            }
        ]
        
        confirmed, conflicts = self.merger.merge_data(sources)
        
        # Les donn√©es uniques doivent √™tre confirm√©es
        self.assertIn('birthdate', confirmed)
        self.assertEqual(confirmed['birthdate']['count'], 1)


class TestScrapers(unittest.TestCase):
    """Tests pour les scrapers (n√©cessitent une connexion internet)"""
    
    def test_iafd_url_detection(self):
        """Test de d√©tection d'URL IAFD"""
        from scrapers import ScraperOrchestrator
        
        orchestrator = ScraperOrchestrator()
        url = "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm"
        
        scraper = orchestrator._get_scraper_for_url(url)
        self.assertIsNotNone(scraper)
        self.assertEqual(type(scraper).__name__, 'IAFDScraper')
    
    def test_freeones_url_detection(self):
        """Test de d√©tection d'URL Freeones"""
        from scrapers import ScraperOrchestrator
        
        orchestrator = ScraperOrchestrator()
        url = "https://www.freeones.xxx/bridgette-b"
        
        scraper = orchestrator._get_scraper_for_url(url)
        self.assertIsNotNone(scraper)
        self.assertEqual(type(scraper).__name__, 'FreeonesScraper')


def run_tests():
    """Lance tous les tests"""
    # Cr√©er une suite de tests
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()
    
    # Ajouter tous les tests
    suite.addTests(loader.loadTestsFromTestCase(TestTagRulesEngine))
    suite.addTests(loader.loadTestsFromTestCase(TestAwardsCleaner))
    suite.addTests(loader.loadTestsFromTestCase(TestDataMerger))
    suite.addTests(loader.loadTestsFromTestCase(TestScrapers))
    
    # Lancer les tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # Retourner le statut
    return 0 if result.wasSuccessful() else 1


if __name__ == '__main__':
    sys.exit(run_tests())


============================================================
[30/124] Legacy\gui\__init__.py
------------------------------------------------------------


============================================================
[31/124] Legacy\gui\app.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
import sv_ttk
from gui.performer_frame import PerformerFrame
# Placeholders for future modules
from gui.group_frame import GroupFrame
# from gui.scene_frame import SceneFrame

def launch_app(module, stash_id):
    root = tk.Tk()
    root.title(f"StashMaster V2 - {module}")
    
    # Maximiser la fen√™tre au d√©marrage (Windows)
    try:
        root.state('zoomed')
    except:
        # Fallback pour d'autres OS (Linux/Mac)
        w, h = root.winfo_screenwidth(), root.winfo_screenheight()
        root.geometry(f"{w}x{h}+0+0")

    # Appliquer le th√®me moderne
    sv_ttk.set_theme("dark")

    if module == "Performer":
        frame = PerformerFrame(root, stash_id)
    elif module == "Group":
        frame = GroupFrame(root, stash_id)
    # elif module == "Scene":
    #     frame = SceneFrame(root, stash_id)
    else:
        import tkinter.messagebox as mb
        mb.showerror("Erreur", f"Module inconnu: {module}")
        root.destroy()
        return
    frame.pack(fill=tk.BOTH, expand=True)
    root.mainloop()


============================================================
[32/124] Legacy\gui\bio_studio_window.py
------------------------------------------------------------
"""
BioStudioWindow ‚Äî GUI 2/3 du flux Bio IA
=========================================
G√©n√©ration de biographie en plein √©cran.
Gemini (gauche) et Ollama (droite) c√¥te √† c√¥te.

Layout :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  HEADER : performer + breadcrumb 2/3                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  CONTEXTE PERFORMER (bandeau compact 2 lignes)                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ ü§ñ Google Gemini ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ ‚öôÔ∏è Ollama (local) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  [üöÄ G√©n√©rer]  [üîÑ Retry] ‚îÇ  ‚îÇ  [‚öôÔ∏è Affiner]  [‚ö° G√©n√©rer]    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                           ‚îÇ  ‚îÇ                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Zone texte √©ditable      ‚îÇ  ‚îÇ  Zone texte √©ditable            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  (r√©sultat Gemini)        ‚îÇ  ‚îÇ  (r√©sultat Ollama)              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                           ‚îÇ  ‚îÇ                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  [üìã Copier] [Utiliser ‚Üí] ‚îÇ  ‚îÇ  [üìã Copier] [Utiliser ‚Üí]      ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  [‚Üê Retour]  [Annuler]    Compteurs chars       [‚Üí Valider ‚Üí]       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""
import tkinter as tk
from tkinter import ttk, messagebox
import threading
import platform

from services.bio_generator import BioGenerator

# ‚îÄ‚îÄ Palette ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
P = {
    "bg":         "#13131f",
    "surface":    "#1e1e30",
    "card":       "#22223a",
    "card_hdr":   "#2c2c4a",
    "border":     "#3a3a58",
    "accent":     "#7c6af7",
    "gemini_bg":  "#0d1a2e",
    "gemini_hdr": "#1a2a4a",
    "gemini_acc": "#4a8af0",
    "ollama_bg":  "#1e1204",
    "ollama_hdr": "#2e2010",
    "ollama_acc": "#e8954a",
    "success":    "#4caf7d",
    "danger":     "#c05050",
    "text":       "#e8e8f5",
    "muted":      "#8888aa",
    "dim":        "#55557a",
}

FH1  = ("Segoe UI", 14, "bold")
FH2  = ("Segoe UI", 11, "bold")
FH3  = ("Segoe UI", 9,  "bold")
FB   = ("Segoe UI", 10)
FSM  = ("Segoe UI", 8)
FSMB = ("Segoe UI", 8,  "bold")
FMONO= ("Consolas", 9)


class BioStudioWindow(tk.Toplevel):
    """
    Fen√™tre 2/3 : g√©n√©ration de bio via Gemini et/ou Ollama.
    Retourne `result` dict avec 'bio' str, ou None si annul√©.
    """
    def __init__(self, parent, db_data, stash_ctx, merged_data,
                 scraped_results, checked_fields, review_result):
        super().__init__(parent)
        self.title("üé® Studio Bio IA ‚Äî √âtape 2/3")
        self.configure(bg=P["bg"])

        self.db_data         = db_data
        self.stash_ctx       = stash_ctx
        self.merged_data     = merged_data
        self.scraped_results = scraped_results
        self.checked_fields  = checked_fields
        self.review_result   = review_result   # r√©sultat GUI 1
        self.result          = None            # {'bio': str}

        self._bio_gen  = BioGenerator()
        self._ctx      = None
        self._busy_g   = False   # Gemini en cours
        self._busy_o   = False   # Ollama en cours

        _fullscreen(self)
        self.transient(parent)
        self.grab_set()

        self._build_ui()
        self._prepare_context()

        self.bind("<Escape>", lambda _: self._cancel())
        self.wait_window()

    # ‚îÄ‚îÄ Contexte IA ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _prepare_context(self):
        """Construit le contexte IA depuis les donn√©es fusionn√©es + s√©lections GUI 1."""
        try:
            # Injecter les s√©lections de la GUI 1 dans merged_data
            md = dict(self.merged_data)
            rv = self.review_result or {}

            if rv.get("awards"):
                md.setdefault("awards", {})["merged"] = rv["awards"]
            if rv.get("tattoos") is not None:
                md.setdefault("tattoos", {})["merged"] = rv["tattoos"]
            if rv.get("piercings") is not None:
                md.setdefault("piercings", {})["merged"] = rv["piercings"]
            if rv.get("urls"):
                md.setdefault("urls", {})["merged"] = rv["urls"]
            if rv.get("trivia"):
                md.setdefault("trivia", {})["suggestion"] = rv["trivia"]
                md["trivia"]["by_source"] = md["trivia"].get("by_source", {})
                md["trivia"]["by_source"]["_user_"] = rv["trivia"]

            all_fields = list(set(self.checked_fields + [
                "Name", "Birthdate", "Country", "Ethnicity",
                "Hair Color", "Eye Color", "Measurements", "Height",
                "Weight", "Fake Tits", "Aliases", "Career Length",
                "Tattoos", "Piercings", "Awards", "URLs", "Details",
            ]))

            self._ctx = self._bio_gen.build_context_from_v2(
                db_data=self.db_data,
                stash_ctx=self.stash_ctx,
                scraped_results=self.scraped_results,
                merged_data=md,
                checked_fields=all_fields,
            )
            self._set_context_banner()
        except Exception as e:
            print(f"[BioStudio] Erreur contexte : {e}")

    # ‚îÄ‚îÄ UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_ui(self):
        self._build_header()
        self._build_context_banner()
        self._build_studio()
        self._build_footer()

    def _build_header(self):
        hdr = tk.Frame(self, bg=P["accent"])
        hdr.pack(fill=tk.X)
        tk.Frame(hdr, bg="#9a88ff", height=3).pack(fill=tk.X)

        row = tk.Frame(hdr, bg=P["accent"], pady=7)
        row.pack(fill=tk.X, padx=12)

        tk.Label(row, text="üé®", font=("Segoe UI", 18),
                 fg="white", bg=P["accent"]).pack(side=tk.LEFT, padx=(0, 8))

        name = self.db_data.get("name", "Performer inconnu")
        tk.Label(row, text=f"Studio Bio IA ‚Äî {name}",
                 font=FH1, fg="white", bg=P["accent"]).pack(side=tk.LEFT)

        # Breadcrumb
        bc = tk.Frame(row, bg=P["accent"])
        bc.pack(side=tk.RIGHT, padx=12)
        for num, lbl, active in [("1","Donn√©es",False),("2","Bio IA",True),("3","Valider",False)]:
            bg = "#5a48c8" if active else P["accent"]
            fg = "white"   if active else "#9980cc"
            tk.Label(bc, text=f" {num} ", font=FSMB, fg=fg,
                     bg=bg, padx=6, pady=3).pack(side=tk.LEFT, padx=1)
            tk.Label(bc, text=lbl, font=FSM, fg=fg,
                     bg=P["accent"]).pack(side=tk.LEFT, padx=(0,8))

    def _build_context_banner(self):
        """Bandeau compact montrant le contexte performer."""
        self._banner_frame = tk.Frame(self, bg=P["surface"], pady=4)
        self._banner_frame.pack(fill=tk.X)
        tk.Frame(self, bg=P["border"], height=1).pack(fill=tk.X)

        self._banner_inner = tk.Frame(self._banner_frame, bg=P["surface"])
        self._banner_inner.pack(fill=tk.X, padx=12)

    def _set_context_banner(self):
        """Remplit le bandeau contexte une fois le contexte charg√©."""
        for w in self._banner_inner.winfo_children():
            w.destroy()
        if not self._ctx:
            return

        db  = self.db_data
        ctx = self._ctx

        def pill(label, value, color=None):
            if not value:
                return
            f = tk.Frame(self._banner_inner, bg=P["card_hdr"])
            f.pack(side=tk.LEFT, padx=3, pady=2)
            tk.Label(f, text=f" {label} ", font=FSMB,
                     fg=P["muted"], bg=P["card_hdr"]).pack(side=tk.LEFT)
            tk.Label(f, text=f" {str(value)[:40]} ", font=FSM,
                     fg=color or P["text"], bg=P["card"]).pack(side=tk.LEFT)

        pill("üë§", ctx.get("name"))
        pill("üéÇ", ctx.get("birthdate"))
        pill("üåç", ctx.get("country"))
        pill("üë§", ctx.get("ethnicity"))
        pill("üíá", ctx.get("hair_color"))
        pill("üëÅ",  ctx.get("eye_color"))
        pill("üìê", ctx.get("measurements"))
        pill("üé¨", f"{ctx.get('scene_count',0)} sc√®nes", P["gemini_acc"])
        pill("üè¢", ", ".join(ctx.get("studios",[])[:3]))
        pill("üèÜ", f"{len(ctx.get('awards_fr',[]))} awards",
             P["ollama_acc"] if ctx.get("awards_fr") else None)

        tk.Frame(self, bg=P["border"], height=1).pack(fill=tk.X)

    def _build_studio(self):
        """Zone centrale : deux panneaux c√¥te √† c√¥te."""
        studio = tk.Frame(self, bg=P["bg"])
        studio.pack(fill=tk.BOTH, expand=True, padx=8, pady=6)
        studio.grid_columnconfigure(0, weight=1)
        studio.grid_columnconfigure(1, weight=1)
        studio.grid_rowconfigure(0, weight=1)

        self._build_gemini_panel(studio)
        self._build_ollama_panel(studio)

    # ‚îÄ‚îÄ Panneau Gemini ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_gemini_panel(self, parent):
        panel = tk.Frame(parent, bg=P["gemini_hdr"], bd=0)
        panel.grid(row=0, column=0, sticky="nsew", padx=(0, 4))

        # En-t√™te Gemini
        ghdr = tk.Frame(panel, bg=P["gemini_acc"], pady=8)
        ghdr.pack(fill=tk.X)
        tk.Label(ghdr, text="  ü§ñ  Google Gemini",
                 font=FH2, fg="white", bg=P["gemini_acc"]).pack(side=tk.LEFT, padx=8)
        self._gemini_status = tk.Label(ghdr, text="Pr√™t",
                                        font=FSM, fg="#cce", bg=P["gemini_acc"])
        self._gemini_status.pack(side=tk.RIGHT, padx=8)
        self._gemini_progress = ttk.Progressbar(ghdr, mode="indeterminate", length=100)
        self._gemini_progress.pack(side=tk.RIGHT, padx=4)

        # Boutons
        gbtn = tk.Frame(panel, bg=P["gemini_hdr"], pady=6)
        gbtn.pack(fill=tk.X, padx=8)

        self._btn_gemini_gen = _studio_btn(
            gbtn, "üöÄ G√©n√©rer", P["gemini_acc"],
            self._run_gemini
        )
        self._btn_gemini_retry = _studio_btn(
            gbtn, "üîÑ R√©g√©n√©rer", P["card_hdr"],
            self._run_gemini, state=tk.DISABLED
        )
        self._gemini_char_lbl = tk.Label(
            gbtn, text="", font=FSM, fg=P["muted"], bg=P["gemini_hdr"]
        )
        self._gemini_char_lbl.pack(side=tk.RIGHT, padx=8)

        tk.Frame(panel, bg=P["gemini_acc"], height=2).pack(fill=tk.X)

        # Zone texte
        txt_f = tk.Frame(panel, bg=P["gemini_bg"])
        txt_f.pack(fill=tk.BOTH, expand=True)

        self._txt_gemini = tk.Text(
            txt_f, wrap=tk.WORD, font=FB,
            bg=P["gemini_bg"], fg=P["text"],
            insertbackground=P["text"],
            relief=tk.FLAT, padx=12, pady=10,
            undo=True,
        )
        sb = ttk.Scrollbar(txt_f, command=self._txt_gemini.yview)
        self._txt_gemini.configure(yscrollcommand=sb.set)
        self._txt_gemini.bind("<KeyRelease>",
                              lambda e: self._update_char(self._txt_gemini,
                                                          self._gemini_char_lbl))
        sb.pack(side=tk.RIGHT, fill=tk.Y)
        self._txt_gemini.pack(fill=tk.BOTH, expand=True)
        self._txt_gemini.insert("1.0",
            "üí° Cliquez sur [üöÄ G√©n√©rer] pour cr√©er une biographie via Google Gemini.\n\n"
            "Le r√©sultat sera directement √©ditable ici.")

        # Footer panneau
        gfoot = tk.Frame(panel, bg=P["gemini_hdr"], pady=6)
        gfoot.pack(fill=tk.X, padx=8)
        _studio_btn(gfoot, "üìã Copier",
                    P["card_hdr"], lambda: self._copy(self._txt_gemini))
        _studio_btn(gfoot, "‚Üí Utiliser cette bio",
                    P["success"],  lambda: self._use_bio(self._txt_gemini))

    # ‚îÄ‚îÄ Panneau Ollama ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_ollama_panel(self, parent):
        panel = tk.Frame(parent, bg=P["ollama_hdr"], bd=0)
        panel.grid(row=0, column=1, sticky="nsew", padx=(4, 0))

        # En-t√™te Ollama
        ohdr = tk.Frame(panel, bg=P["ollama_acc"], pady=8)
        ohdr.pack(fill=tk.X)
        tk.Label(ohdr, text="  ‚öôÔ∏è  Ollama (local)",
                 font=FH2, fg="white", bg=P["ollama_acc"]).pack(side=tk.LEFT, padx=8)
        self._ollama_status = tk.Label(ohdr, text="Pr√™t",
                                        font=FSM, fg="#ffe", bg=P["ollama_acc"])
        self._ollama_status.pack(side=tk.RIGHT, padx=8)
        self._ollama_progress = ttk.Progressbar(ohdr, mode="indeterminate", length=100)
        self._ollama_progress.pack(side=tk.RIGHT, padx=4)

        # Boutons
        obtn = tk.Frame(panel, bg=P["ollama_hdr"], pady=6)
        obtn.pack(fill=tk.X, padx=8)

        self._btn_ollama_refine = _studio_btn(
            obtn, "‚öôÔ∏è Affiner Gemini", P["ollama_acc"],
            self._run_ollama_refine
        )
        self._btn_ollama_gen = _studio_btn(
            obtn, "‚ö° G√©n√©rer seul", P["card_hdr"],
            self._run_ollama_gen
        )
        self._ollama_char_lbl = tk.Label(
            obtn, text="", font=FSM, fg=P["muted"], bg=P["ollama_hdr"]
        )
        self._ollama_char_lbl.pack(side=tk.RIGHT, padx=8)

        tk.Frame(panel, bg=P["ollama_acc"], height=2).pack(fill=tk.X)

        # Zone texte
        txt_f = tk.Frame(panel, bg=P["ollama_bg"])
        txt_f.pack(fill=tk.BOTH, expand=True)

        self._txt_ollama = tk.Text(
            txt_f, wrap=tk.WORD, font=FB,
            bg=P["ollama_bg"], fg=P["text"],
            insertbackground=P["text"],
            relief=tk.FLAT, padx=12, pady=10,
            undo=True,
        )
        sb = ttk.Scrollbar(txt_f, command=self._txt_ollama.yview)
        self._txt_ollama.configure(yscrollcommand=sb.set)
        self._txt_ollama.bind("<KeyRelease>",
                              lambda e: self._update_char(self._txt_ollama,
                                                          self._ollama_char_lbl))
        sb.pack(side=tk.RIGHT, fill=tk.Y)
        self._txt_ollama.pack(fill=tk.BOTH, expand=True)
        self._txt_ollama.insert("1.0",
            "üí° [‚öôÔ∏è Affiner Gemini] : prend la bio Gemini et la corrige.\n"
            "[‚ö° G√©n√©rer seul] : g√©n√®re une bio ind√©pendante via Ollama.\n\n"
            "G√©n√©rez d'abord avec Gemini (gauche) pour obtenir la meilleure qualit√©.")

        # Footer panneau
        ofoot = tk.Frame(panel, bg=P["ollama_hdr"], pady=6)
        ofoot.pack(fill=tk.X, padx=8)
        _studio_btn(ofoot, "üìã Copier",
                    P["card_hdr"], lambda: self._copy(self._txt_ollama))
        _studio_btn(ofoot, "‚Üí Utiliser cette bio",
                    P["success"],  lambda: self._use_bio(self._txt_ollama))

    # ‚îÄ‚îÄ Footer principal ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_footer(self):
        tk.Frame(self, bg=P["border"], height=1).pack(fill=tk.X)
        bar = tk.Frame(self, bg=P["surface"], pady=10)
        bar.pack(fill=tk.X, padx=12)

        _action_btn(bar, "‚Üê Retour",  P["dim"],     self._go_back, side=tk.LEFT)
        _action_btn(bar, "‚úñ Annuler", P["danger"],  self._cancel,  side=tk.LEFT, padx=6)

        _action_btn(bar, "‚Üí Valider sans bio",
                    P["card_hdr"], self._proceed_no_bio, side=tk.RIGHT)
        _action_btn(bar, "‚Üí Continuer vers Validation",
                    P["success"], self._proceed_with_best, side=tk.RIGHT, padx=8)

    # ‚îÄ‚îÄ G√©n√©ration Gemini ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _run_gemini(self):
        if self._busy_g:
            return
        if not self._bio_gen.gemini_key:
            messagebox.showerror(
                "Cl√© Gemini manquante",
                "Fichier .gemini_key introuvable √† la racine du projet V2."
            )
            return
        self._set_gemini_busy(True, "G√©n√©ration en cours‚Ä¶")

        def _worker():
            try:
                bio = self._bio_gen.generate_gemini_bio(self._ctx)
                self.after(0, self._on_gemini_done, bio)
            except Exception as e:
                self.after(0, self._on_gemini_done, None, str(e))

        threading.Thread(target=_worker, daemon=True).start()

    def _on_gemini_done(self, bio, error=None):
        self._set_gemini_busy(False)
        if bio:
            self._set_text(self._txt_gemini, bio)
            self._update_char(self._txt_gemini, self._gemini_char_lbl)
            self._gemini_status.config(text=f"‚úì {len(bio)} chars")
            self._btn_gemini_retry.config(state=tk.NORMAL)
        else:
            self._gemini_status.config(text="‚úó √âchec")
            msg = f"G√©n√©ration Gemini √©chou√©e.\n{error or ''}"
            messagebox.showwarning("Gemini", msg)

    def _set_gemini_busy(self, busy, msg=""):
        self._busy_g = busy
        state = tk.DISABLED if busy else tk.NORMAL
        self._btn_gemini_gen.config(state=state)
        if busy:
            self._gemini_progress.start(10)
            self._gemini_status.config(text=msg)
        else:
            self._gemini_progress.stop()

    # ‚îÄ‚îÄ G√©n√©ration Ollama ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _run_ollama_refine(self):
        """Affine la bio Gemini."""
        base = self._txt_gemini.get("1.0", tk.END).strip()
        if len(base) < 100:
            messagebox.showinfo("Ollama", "G√©n√©rez d'abord une bio avec Gemini.")
            return
        self._run_ollama(base_bio=base)

    def _run_ollama_gen(self):
        """G√©n√®re ind√©pendamment."""
        self._run_ollama(base_bio=None)

    def _run_ollama(self, base_bio=None):
        if self._busy_o:
            return
        self._set_ollama_busy(True, "Ollama en cours‚Ä¶")

        def _worker():
            try:
                bio = self._bio_gen.generate_ollama_bio(
                    self._ctx, base_bio=base_bio
                )
                self.after(0, self._on_ollama_done, bio)
            except Exception as e:
                self.after(0, self._on_ollama_done, None, str(e))

        threading.Thread(target=_worker, daemon=True).start()

    def _on_ollama_done(self, bio, error=None):
        self._set_ollama_busy(False)
        if bio:
            self._set_text(self._txt_ollama, bio)
            self._update_char(self._txt_ollama, self._ollama_char_lbl)
            self._ollama_status.config(text=f"‚úì {len(bio)} chars")
        else:
            self._ollama_status.config(text="‚úó √âchec")
            messagebox.showwarning(
                "Ollama",
                "G√©n√©ration Ollama √©chou√©e.\n"
                "V√©rifiez que le serveur Ollama est lanc√© (ollama serve).\n"
                f"{error or ''}"
            )

    def _set_ollama_busy(self, busy, msg=""):
        self._busy_o = busy
        state = tk.DISABLED if busy else tk.NORMAL
        self._btn_ollama_refine.config(state=state)
        self._btn_ollama_gen.config(state=state)
        if busy:
            self._ollama_progress.start(10)
            self._ollama_status.config(text=msg)
        else:
            self._ollama_progress.stop()

    # ‚îÄ‚îÄ Actions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _set_text(self, widget, text):
        widget.delete("1.0", tk.END)
        widget.insert("1.0", text)

    def _update_char(self, widget, lbl):
        n = len(widget.get("1.0", tk.END).strip())
        color = P["success"] if 2500 <= n <= 4000 else P["muted"]
        lbl.config(text=f"{n} chars", fg=color)

    def _copy(self, widget):
        text = widget.get("1.0", tk.END).strip()
        if text:
            self.clipboard_clear()
            self.clipboard_append(text)

    def _use_bio(self, widget):
        """S√©lectionne cette bio et passe √† la validation."""
        bio = widget.get("1.0", tk.END).strip()
        if len(bio) < 100:
            messagebox.showwarning("Bio trop courte",
                                   "La biographie est vide ou trop courte.")
            return
        self.result = {"bio": bio}
        self.destroy()

    def _proceed_with_best(self):
        """Choisit la bio la plus longue disponible."""
        gemini = self._txt_gemini.get("1.0", tk.END).strip()
        ollama = self._txt_ollama.get("1.0", tk.END).strip()

        best = ""
        if len(gemini) > 200:
            best = gemini
        if len(ollama) > 200:
            # Prendre la plus longue (souvent Ollama affine mieux)
            if len(ollama) > len(best):
                best = ollama

        if not best:
            messagebox.showwarning("Aucune bio",
                                   "G√©n√©rez d'abord une biographie.")
            return
        self.result = {"bio": best}
        self.destroy()

    def _proceed_no_bio(self):
        self.result = {"bio": ""}
        self.destroy()

    def _go_back(self):
        """Retourne √† GUI 1 sans r√©sultat."""
        self.result = None
        self.destroy()

    def _cancel(self):
        self.result = None
        self.destroy()


# ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _fullscreen(win):
    if platform.system() == "Windows":
        win.state("zoomed")
    elif platform.system() == "Linux":
        try:
            win.attributes("-zoomed", True)
        except Exception:
            win.geometry(f"{win.winfo_screenwidth()}x{win.winfo_screenheight()}+0+0")
    else:
        win.attributes("-fullscreen", True)


def _studio_btn(parent, text, bg, cmd, state=tk.NORMAL):
    b = tk.Button(
        parent, text=text, command=cmd, state=state,
        font=FH3, bg=bg, fg="white",
        relief=tk.FLAT, padx=10, pady=5, cursor="hand2",
        activebackground=bg, activeforeground="white",
    )
    b.pack(side=tk.LEFT, padx=4)
    return b


def _action_btn(parent, text, bg, cmd, side=tk.RIGHT, padx=6):
    b = tk.Button(
        parent, text=text, command=cmd,
        font=FH3, bg=bg, fg="white",
        relief=tk.FLAT, padx=16, pady=8, cursor="hand2",
        activebackground=bg, activeforeground="white",
    )
    b.pack(side=side, padx=padx)


============================================================
[33/124] Legacy\gui\bio_wizard.py
------------------------------------------------------------
"""
BioWizard - Fen√™tre d√©di√©e √† la g√©n√©ration de biographies en 3 √©tapes.
1. G√©n√©ration Google Gemini
2. Affinage Ollama
3. Validation et injection
"""
import tkinter as tk
from tkinter import ttk, messagebox
import threading

from services.bio_generator import BioGenerator

class BioWizard(tk.Toplevel):
    def __init__(self, parent, db_data, stash_ctx, merged_data, scraped_results, checked_fields):
        super().__init__(parent)
        self.title("Assistant de G√©n√©ration de Biographie IA")
        self.geometry("1000x750")
        self.minsize(800, 600)

        self.transient(parent)
        self.grab_set()

        # Stockage
        self.db_data = db_data
        self.stash_ctx = stash_ctx
        self.merged_data = merged_data
        self.scraped_results = scraped_results
        self.checked_fields = checked_fields
        self.final_bio = None  # Le r√©sultat final sera stock√© ici

        self._build_ui()
        self.wait_window()

    def _build_ui(self):
        # Barre de progression
        self.progress_frame = ttk.Frame(self)
        self.progress_label = ttk.Label(self.progress_frame, text="Pr√™t", font=("Segoe UI", 9, "italic"))
        self.progress_label.pack(side=tk.LEFT, padx=10)
        self.progress_bar = ttk.Progressbar(self.progress_frame, mode="indeterminate", length=200)
        self.progress_bar.pack(side=tk.LEFT, padx=5)
        
        self.notebook = ttk.Notebook(self)
        self.notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)

        self.tab1_google = ttk.Frame(self.notebook, padding=10)
        self.tab2_ollama = ttk.Frame(self.notebook, padding=10)
        self.tab3_validate = ttk.Frame(self.notebook, padding=10)

        self.notebook.add(self.tab1_google, text="√âtape 1 : Google Gemini")
        self.notebook.add(self.tab2_ollama, text="√âtape 2 : Affinage Ollama", state="disabled")
        self.notebook.add(self.tab3_validate, text="√âtape 3 : Validation", state="disabled")

        self._create_google_tab()
        self._create_ollama_tab()
        self._create_validate_tab()

    def _create_google_tab(self):
        frame = self.tab1_google
        frame.grid_columnconfigure(0, weight=1)
        frame.grid_rowconfigure(1, weight=1)
        
        action_frame = ttk.Frame(frame)
        action_frame.grid(row=0, column=0, sticky=tk.NSEW, pady=(0, 10))
        
        self.btn_gen_gemini = ttk.Button(action_frame, text="üöÄ Lancer la g√©n√©ration Gemini", command=self._run_gemini_generation)
        self.btn_gen_gemini.pack(side=tk.LEFT)
        
        self.btn_copy_to_ollama = ttk.Button(action_frame, text="Continuer vers l'√©tape 2 ‚û°", state=tk.DISABLED, command=self._copy_to_ollama)
        self.btn_copy_to_ollama.pack(side=tk.LEFT, padx=10)

        self.txt_gemini_result = tk.Text(frame, wrap=tk.WORD, font=("Segoe UI", 10))
        self.txt_gemini_result.grid(row=1, column=0, sticky=tk.NSEW)
        self.txt_gemini_result.insert("1.0", "Cliquez sur 'Lancer la g√©n√©ration' pour cr√©er une biographie avec Google Gemini...")
        self.txt_gemini_result.config(state=tk.DISABLED)

    def _create_ollama_tab(self):
        frame = self.tab2_ollama
        frame.grid_columnconfigure(1, weight=1)
        frame.grid_rowconfigure(2, weight=1)
        
        action_frame = ttk.Frame(frame)
        action_frame.grid(row=0, column=0, columnspan=2, sticky=tk.NSEW, pady=(0, 10))
        self.btn_refine_ollama = ttk.Button(action_frame, text="‚öôÔ∏è Lancer l'affinage Ollama", command=self._run_ollama_refinement)
        self.btn_refine_ollama.pack(side=tk.LEFT)
        
        self.btn_copy_to_validate = ttk.Button(action_frame, text="Continuer vers la validation ‚û°", state=tk.DISABLED, command=self._copy_to_validation)
        self.btn_copy_to_validate.pack(side=tk.LEFT, padx=10)

        ttk.Label(frame, text="Biographie de base (Gemini)").grid(row=1, column=0, columnspan=2, sticky=tk.W)
        self.txt_ollama_input = tk.Text(frame, wrap=tk.WORD, height=8, font=("Segoe UI", 10), state=tk.DISABLED, relief=tk.SUNKEN)
        self.txt_ollama_input.grid(row=2, column=0, columnspan=2, sticky=tk.NSEW, pady=(0, 10))

        ttk.Label(frame, text="Biographie affin√©e (Ollama)").grid(row=3, column=0, columnspan=2, sticky=tk.W)
        self.txt_ollama_result = tk.Text(frame, wrap=tk.WORD, font=("Segoe UI", 10))
        self.txt_ollama_result.grid(row=4, column=0, columnspan=2, sticky=tk.NSEW)

    def _create_validate_tab(self):
        frame = self.tab3_validate
        frame.grid_columnconfigure(0, weight=1)
        frame.grid_rowconfigure(1, weight=1)
        
        action_frame = ttk.Frame(frame)
        action_frame.grid(row=0, column=0, sticky=tk.NSEW, pady=(0, 10))
        self.btn_inject = ttk.Button(action_frame, text="‚úÖ Valider et Utiliser cette Bio", command=self._inject_bio)
        self.btn_inject.pack(side=tk.LEFT)

        self.txt_final_bio = tk.Text(frame, wrap=tk.WORD, font=("Segoe UI", 10))
        self.txt_final_bio.grid(row=1, column=0, sticky=tk.NSEW)

    def _run_gemini_generation(self):
        self._show_progress("G√©n√©ration Gemini en cours...")
        self.btn_gen_gemini.config(state=tk.DISABLED)
        self.btn_copy_to_ollama.config(state=tk.DISABLED)

        def _do_generate():
            try:
                bio_gen = BioGenerator()
                ctx = bio_gen.build_context_from_v2(self.db_data, self.stash_ctx, self.scraped_results, self.merged_data, self.checked_fields)
                gemini_bio = bio_gen.generate_gemini_bio(ctx)

                def _update_ui():
                    self.txt_gemini_result.config(state=tk.NORMAL)
                    self.txt_gemini_result.delete("1.0", tk.END)
                    if gemini_bio:
                        self.txt_gemini_result.insert("1.0", gemini_bio)
                        self.btn_copy_to_ollama.config(state=tk.NORMAL)
                    else:
                        self.txt_gemini_result.insert("1.0", "La g√©n√©ration Gemini a √©chou√©. V√©rifiez la console pour les erreurs (cl√© API, etc.).")
                    self.txt_gemini_result.config(state=tk.DISABLED)
                    self.btn_gen_gemini.config(state=tk.NORMAL)

                self.after(0, _update_ui)
            except Exception as e:
                self.after(0, lambda: messagebox.showerror("Erreur Gemini", str(e)))
            finally:
                self.after(0, self._hide_progress)
        
        threading.Thread(target=_do_generate, daemon=True).start()

    def _copy_to_ollama(self):
        gemini_text = self.txt_gemini_result.get("1.0", tk.END)
        self.txt_ollama_input.config(state=tk.NORMAL)
        self.txt_ollama_input.delete("1.0", tk.END)
        self.txt_ollama_input.insert("1.0", gemini_text)
        self.txt_ollama_input.config(state=tk.DISABLED)
        self.txt_ollama_result.delete("1.0", tk.END) # Clear previous results
        self.notebook.tab(1, state="normal")
        self.notebook.select(self.tab2_ollama)

    def _run_ollama_refinement(self):
        gemini_bio = self.txt_ollama_input.get("1.0", tk.END).strip()
        if not gemini_bio or "√©chou√©" in gemini_bio:
            messagebox.showwarning("Bio de base manquante", "La biographie de base (Gemini) est n√©cessaire pour l'affinage.")
            return

        self._show_progress("Affinage Ollama en cours...")
        self.btn_refine_ollama.config(state=tk.DISABLED)
        self.btn_copy_to_validate.config(state=tk.DISABLED)

        def _do_refine():
            try:
                bio_gen = BioGenerator()
                ctx = bio_gen.build_context_from_v2(self.db_data, self.stash_ctx, self.scraped_results, self.merged_data, self.checked_fields)
                ollama_bio = bio_gen.generate_ollama_bio(ctx, gemini_bio)

                def _update_ui():
                    self.txt_ollama_result.delete("1.0", tk.END)
                    if ollama_bio:
                        self.txt_ollama_result.insert("1.0", ollama_bio)
                        self.btn_copy_to_validate.config(state=tk.NORMAL)
                    else:
                         self.txt_ollama_result.insert("1.0", "L'affinage Ollama a √©chou√©. V√©rifiez que le serveur Ollama est bien lanc√©.")
                
                self.after(0, _update_ui)
            except Exception as e:
                self.after(0, lambda: messagebox.showerror("Erreur Ollama", str(e)))
            finally:
                self.after(0, self._hide_progress)
                self.after(0, self.btn_refine_ollama.config, {'state': tk.NORMAL})
        
        threading.Thread(target=_do_refine, daemon=True).start()
    
    def _copy_to_validation(self):
        ollama_text = self.txt_ollama_result.get("1.0", tk.END)
        self.txt_final_bio.delete("1.0", tk.END)
        self.txt_final_bio.insert("1.0", ollama_text)
        self.notebook.tab(2, state="normal")
        self.notebook.select(self.tab3_validate)

    def _inject_bio(self):
        """Stocke la bio finale et ferme la fen√™tre."""
        self.final_bio = self.txt_final_bio.get("1.0", tk.END).strip()
        self.destroy()

    def _show_progress(self, message):
        self.progress_label.config(text=message)
        self.progress_frame.pack(fill=tk.X, padx=10, pady=5, before=self.notebook)
        self.progress_bar.start(10)

    def _hide_progress(self):
        self.progress_bar.stop()
        self.progress_frame.pack_forget()


============================================================
[34/124] Legacy\gui\data_review_window.py
------------------------------------------------------------
"""
DataReviewWindow ‚Äî GUI 1/3 du flux Bio IA
==========================================
Affiche en plein √©cran toutes les donn√©es scrap√©es (sauf bio) pour r√©vision :
Trivia ¬∑ Awards ¬∑ Tatouages ¬∑ Piercings ¬∑ Tags ¬∑ URLs

Layout :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  HEADER : performer + scraping stats + breadcrumb 1/3                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  R√âSUM√â PERFORMER (bandeau horizontal) : identit√© + apparence rapide  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  COL GAUCHE              ‚îÇ  COL DROITE                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ üìù Trivia ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îå‚îÄ‚îÄ üèÜ Awards ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  texte √©ditable     ‚îÇ ‚îÇ  ‚îÇ  liste cochable                       ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ üé® Tatouages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îå‚îÄ‚îÄ üíâ Piercings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  liste cochable     ‚îÇ ‚îÇ  ‚îÇ  liste cochable                       ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îå‚îÄ‚îÄ üè∑Ô∏è Tags ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ üîó URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ  grille de badges cochables           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  liste cochable     ‚îÇ ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  [Annuler]   [Tout cocher]   [S√©lect. vides]       [‚Üí G√©n√©rer Bio]   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""
import tkinter as tk
from tkinter import ttk, messagebox
import platform

# ‚îÄ‚îÄ Palette ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
P = {
    "bg":        "#13131f",
    "surface":   "#1e1e30",
    "card":      "#22223a",
    "card_hdr":  "#2c2c4a",
    "border":    "#3a3a58",
    "accent":    "#7c6af7",
    "success":   "#4caf7d",
    "danger":    "#c05050",
    "text":      "#e8e8f5",
    "muted":     "#8888aa",
    "dim":       "#55557a",
    "trivia_bg": "#1a1a2e",
    "award_bg":  "#1e2a1e",
    "tattoo_bg": "#2a1e1e",
    "tag_sel":   "#2a3a4a",
    "tag_unsel": "#1e1e2a",
    "url_bg":    "#1e1e28",
}

FH1  = ("Segoe UI", 14, "bold")
FH2  = ("Segoe UI", 11, "bold")
FH3  = ("Segoe UI", 9,  "bold")
FB   = ("Segoe UI", 10)
FSM  = ("Segoe UI", 8)
FSMB = ("Segoe UI", 8, "bold")
FMON = ("Consolas", 9)


def _lbl(parent, text, font=FB, fg=None, bg=None, **kw):
    return tk.Label(parent, text=text, font=font,
                    fg=fg or P["text"], bg=bg or P["surface"], **kw)


def _sep(parent, bg=None):
    return tk.Frame(parent, bg=bg or P["border"], height=1)


class _Card(tk.Frame):
    """Carte √† en-t√™te color√© avec zone de contenu."""
    def __init__(self, parent, title, icon="", accent=None, **kw):
        super().__init__(parent, bg=P["card"], bd=0, **kw)
        self._accent = accent or P["accent"]

        # Bande couleur lat√©rale
        tk.Frame(self, bg=self._accent, width=3).pack(side=tk.LEFT, fill=tk.Y)

        inner = tk.Frame(self, bg=P["card"])
        inner.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # En-t√™te
        hdr = tk.Frame(inner, bg=P["card_hdr"], pady=5)
        hdr.pack(fill=tk.X)
        _lbl(hdr, f"  {icon}  {title}" if icon else f"  {title}",
             font=FH3, fg=P["text"], bg=P["card_hdr"],
             anchor="w").pack(side=tk.LEFT, padx=4)
        self._count_lbl = _lbl(hdr, "", font=FSM, fg=P["muted"],
                                bg=P["card_hdr"])
        self._count_lbl.pack(side=tk.RIGHT, padx=8)

        _sep(inner, P["border"]).pack(fill=tk.X)

        self._body = tk.Frame(inner, bg=P["card"], padx=6, pady=4)
        self._body.pack(fill=tk.BOTH, expand=True)

    def body(self):
        return self._body

    def set_count(self, n, label=""):
        self._count_lbl.config(text=f"{n} {label}" if n else "")


class DataReviewWindow(tk.Toplevel):
    """
    Fen√™tre 1/3 : r√©vision des donn√©es scrap√©es.
    Retourne `result` dict ou None si annul√©.
    """
    def __init__(self, parent, db_data, stash_ctx, merged_data,
                 scraped_results, checked_fields):
        super().__init__(parent)
        self.title("üìã R√©vision des donn√©es ‚Äî √âtape 1/3")
        self.configure(bg=P["bg"])

        self.db_data         = db_data
        self.stash_ctx       = stash_ctx
        self.merged_data     = merged_data
        self.scraped_results = scraped_results
        self.checked_fields  = checked_fields

        self.result = None          # dict s√©lections ou None

        # Widgets de s√©lection
        self._trivia_text   = None
        self._award_vars    = []    # [(BooleanVar, str)]
        self._tattoo_vars   = []    # [(BooleanVar, dict)]
        self._piercing_vars = []    # [(BooleanVar, dict)]
        self._tag_vars      = []    # [(BooleanVar, str)]
        self._url_vars      = []    # [(BooleanVar, str, str)]  (key, url)

        _fullscreen(self)
        self.transient(parent)
        self.grab_set()

        self._build_ui()
        self.bind("<Escape>", lambda _: self._cancel())
        self.wait_window()

    # ‚îÄ‚îÄ UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_ui(self):
        self._build_header()
        self._build_performer_banner()
        self._build_body()
        self._build_footer()

    def _build_header(self):
        hdr = tk.Frame(self, bg=P["accent"], pady=0)
        hdr.pack(fill=tk.X)
        tk.Frame(hdr, bg="#9a88ff", height=3).pack(fill=tk.X)

        row = tk.Frame(hdr, bg=P["accent"], pady=7)
        row.pack(fill=tk.X, padx=12)

        _lbl(row, "üìã", font=("Segoe UI", 18), fg="white",
             bg=P["accent"]).pack(side=tk.LEFT, padx=(0, 8))

        name = self.db_data.get("name", "Performer inconnu")
        _lbl(row, f"R√©vision des donn√©es ‚Äî {name}",
             font=FH1, fg="white", bg=P["accent"]).pack(side=tk.LEFT)

        # Breadcrumb
        bc = tk.Frame(row, bg=P["accent"])
        bc.pack(side=tk.RIGHT, padx=12)
        for i, (num, lbl, active) in enumerate([
            ("1", "Donn√©es", True),
            ("2", "Bio IA",  False),
            ("3", "Valider", False),
        ]):
            fg   = "white"  if active else "#9980cc"
            bgc  = "#5a48c8" if active else P["accent"]
            tk.Label(bc, text=f" {num} ", font=FSMB, fg=fg,
                     bg=bgc, padx=6, pady=3).pack(side=tk.LEFT, padx=1)
            tk.Label(bc, text=lbl, font=FSM, fg=fg,
                     bg=P["accent"]).pack(side=tk.LEFT, padx=(0, 8))

    def _build_performer_banner(self):
        """Bandeau horizontal r√©sumant le performer."""
        banner = tk.Frame(self, bg=P["surface"], pady=6)
        banner.pack(fill=tk.X, padx=0)
        _sep(banner).pack(fill=tk.X)

        inner = tk.Frame(banner, bg=P["surface"])
        inner.pack(fill=tk.X, padx=14, pady=4)

        db = self.db_data
        ctx = self.stash_ctx or {}

        def pill(parent, label, value, accent=None):
            if not value:
                return
            f = tk.Frame(parent, bg=P["card_hdr"])
            f.pack(side=tk.LEFT, padx=4, pady=2)
            tk.Label(f, text=f" {label} ", font=FSMB,
                     fg=P["muted"], bg=P["card_hdr"]).pack(side=tk.LEFT)
            tk.Label(f, text=f" {value} ", font=FSM,
                     fg=accent or P["text"], bg=P["card"]).pack(side=tk.LEFT)

        pill(inner, "üéÇ", db.get("birthdate"))
        pill(inner, "üåç", db.get("country"))
        pill(inner, "üë§", db.get("ethnicity"))
        pill(inner, "üíá", db.get("hair_color"))
        pill(inner, "üëÅ", db.get("eye_color"))
        pill(inner, "üìê", db.get("measurements"))
        pill(inner, "üé¨", str(ctx.get("scene_count", "")) + " sc√®nes"
             if ctx.get("scene_count") else "", P["accent"])
        if ctx.get("studios"):
            pill(inner, "üè¢", ", ".join(ctx["studios"][:3]))

        _sep(banner).pack(fill=tk.X)

    def _build_body(self):
        body = tk.Frame(self, bg=P["bg"])
        body.pack(fill=tk.BOTH, expand=True, padx=10, pady=6)
        body.grid_columnconfigure(0, weight=1)
        body.grid_columnconfigure(1, weight=1)
        body.grid_rowconfigure(0, weight=2)
        body.grid_rowconfigure(1, weight=1)
        body.grid_rowconfigure(2, weight=1)

        # ‚îÄ‚îÄ Colonne gauche ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # Ligne 0 : Trivia (haut)
        self._build_trivia(body, row=0, col=0)
        # Ligne 1 : Tatouages
        self._build_body_art(body, "tattoos",   "üé®", "Tatouages",
                             "#c05050", row=1, col=0)
        # Ligne 2 : URLs
        self._build_urls(body, row=2, col=0)

        # ‚îÄ‚îÄ Colonne droite ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # Ligne 0 : Awards
        self._build_awards(body, row=0, col=1)
        # Ligne 1 : Piercings
        self._build_body_art(body, "piercings", "üíâ", "Piercings",
                             "#5080c0", row=1, col=1)
        # Ligne 2 : Tags
        self._build_tags(body, row=2, col=1)

    # ‚îÄ‚îÄ Sections ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_trivia(self, parent, row, col):
        card = _Card(parent, "Trivia / Informations", "üìù",
                     accent="#8060d0")
        card.grid(row=row, column=col, sticky="nsew", padx=5, pady=5)

        by_source = self.merged_data.get("trivia", {}).get("by_source", {})
        PRIORITY  = ["freeones", "thenude", "babepedia"]

        if not by_source:
            _lbl(card.body(), "Aucun trivia trouv√©.",
                 fg=P["muted"], bg=P["card"]).pack(anchor="w")
            return

        # S√©lecteur de source
        top = tk.Frame(card.body(), bg=P["card"])
        top.pack(fill=tk.X, pady=(0, 4))

        _lbl(top, "Source :", font=FH3, fg=P["muted"],
             bg=P["card"]).pack(side=tk.LEFT, padx=4)

        best = next((s for s in PRIORITY if s in by_source), None) or \
               list(by_source.keys())[0]
        self._trivia_src_var = tk.StringVar(value=best)

        for src in by_source:
            n = len(by_source[src])
            rb = tk.Radiobutton(
                top, text=f"{src.upper()} ({n}c)",
                variable=self._trivia_src_var, value=src,
                font=FSM, fg=P["text"], bg=P["card"],
                selectcolor=P["card_hdr"],
                activebackground=P["card"], relief=tk.FLAT,
                command=self._update_trivia_preview,
            )
            rb.pack(side=tk.LEFT, padx=4)

        # Zone texte √©ditable
        txt_f = tk.Frame(card.body(), bg=P["card"])
        txt_f.pack(fill=tk.BOTH, expand=True)

        self._trivia_text = tk.Text(
            txt_f, wrap=tk.WORD, font=FB,
            bg=P["trivia_bg"], fg=P["text"],
            insertbackground=P["text"],
            relief=tk.FLAT, padx=6, pady=6,
        )
        sb = ttk.Scrollbar(txt_f, command=self._trivia_text.yview)
        self._trivia_text.configure(yscrollcommand=sb.set)
        sb.pack(side=tk.RIGHT, fill=tk.Y)
        self._trivia_text.pack(fill=tk.BOTH, expand=True)

        self._trivia_sources = by_source
        self._update_trivia_preview()

        card.set_count(len(by_source), "sources")

    def _update_trivia_preview(self):
        if self._trivia_text is None:
            return
        src  = self._trivia_src_var.get()
        text = self._trivia_sources.get(src, "")
        self._trivia_text.delete("1.0", tk.END)
        self._trivia_text.insert("1.0", text)

    def _build_awards(self, parent, row, col):
        data   = self.merged_data.get("awards", {})
        merged = data.get("merged", [])
        srcs   = data.get("sources", {})

        card = _Card(parent, "Awards & Nominations", "üèÜ",
                     accent="#c0a020")
        card.grid(row=row, column=col, sticky="nsew", padx=5, pady=5)
        card.set_count(len(merged), "awards")

        if not merged:
            _lbl(card.body(), "Aucun award trouv√©.",
                 fg=P["muted"], bg=P["card"]).pack(anchor="w")
            return

        # Infos sources
        src_txt = "  ".join(
            f"[{s.upper()}: {len(a)}]" for s, a in srcs.items() if a
        )
        _lbl(card.body(), src_txt, font=FSM, fg=P["muted"],
             bg=P["card"]).pack(anchor="w", pady=(0, 4))

        # Zone scrollable
        sf = _scrolled_frame(card.body(), bg=P["award_bg"])

        self._award_vars = []
        for award in merged:
            var = tk.BooleanVar(value=True)
            self._award_vars.append((var, award))
            _checkbutton(sf, award, var, bg=P["award_bg"])

        # Boutons tout/rien
        _check_buttons(card.body(), self._award_vars)

    def _build_body_art(self, parent, field, icon, title, accent,
                        row, col):
        data   = self.merged_data.get(field, {})
        merged = data.get("merged", [])
        db_val = data.get("db_value", "")

        card = _Card(parent, title, icon, accent=accent)
        card.grid(row=row, column=col, sticky="nsew", padx=5, pady=5)
        card.set_count(len(merged))

        if db_val:
            _lbl(card.body(),
                 f"Stash actuel : {str(db_val)[:80]}",
                 font=FSM, fg=P["muted"], bg=P["card"]).pack(anchor="w")

        if not merged:
            _lbl(card.body(), f"Aucun {title.lower()} trouv√©.",
                 fg=P["muted"], bg=P["card"]).pack(anchor="w")
            var_attr = f"_{field}_vars"
            setattr(self, var_attr, [])
            return

        sf = _scrolled_frame(card.body(), bg=P["tattoo_bg"])

        vars_list = []
        for item in merged:
            pos  = item.get("position", "?")
            desc = item.get("description", "")
            lbl  = f"{pos}" + (f"  ({desc})" if desc else "")
            var  = tk.BooleanVar(value=True)
            vars_list.append((var, item))
            _checkbutton(sf, lbl, var, bg=P["tattoo_bg"])

        var_attr = f"_{field}_vars"
        setattr(self, var_attr, vars_list)
        _check_buttons(card.body(), vars_list)

    def _build_tags(self, parent, row, col):
        data       = self.merged_data.get("tags", {})
        merged     = data.get("merged", [])
        stash_tags = {t.lower() for t in (self.db_data.get("tags") or [])}

        card = _Card(parent, "Tags", "üè∑Ô∏è", accent="#3a7a9a")
        card.grid(row=row, column=col, sticky="nsew", padx=5, pady=5)
        card.set_count(len(merged))

        if not merged:
            _lbl(card.body(), "Aucun tag trouv√©.",
                 fg=P["muted"], bg=P["card"]).pack(anchor="w")
            return

        # Grille de badges cliquables
        grid = tk.Frame(card.body(), bg=P["card"])
        grid.pack(fill=tk.BOTH, expand=True)

        self._tag_vars = []
        for tag in merged:
            already = tag.lower() in stash_tags
            var = tk.BooleanVar(value=True)
            self._tag_vars.append((var, tag))

            btn_frame = tk.Frame(grid, bg=P["card"])
            btn_frame.pack(side=tk.LEFT, padx=2, pady=2)

            def _toggle(v=var, bf=btn_frame, t=tag, al=already):
                v.set(not v.get())
                _refresh_tag_btn(bf, t, v.get(), al)

            _refresh_tag_btn(btn_frame, tag, True, already)
            btn_frame.bind("<Button-1>", lambda e, fn=_toggle: fn())
            for child in btn_frame.winfo_children():
                child.bind("<Button-1>", lambda e, fn=_toggle: fn())

        # Boutons
        btn_row = tk.Frame(card.body(), bg=P["card"])
        btn_row.pack(fill=tk.X, pady=4)
        _small_btn(btn_row, "‚úì Tous",
                   lambda: [v.set(True) or _refresh_all_tags(self._tag_vars)
                             for v, _ in self._tag_vars])
        _small_btn(btn_row, "‚úó Aucun",
                   lambda: [v.set(False) or _refresh_all_tags(self._tag_vars)
                             for v, _ in self._tag_vars])
        _small_btn(btn_row, "Nouveaux",
                   lambda: self._select_new_tags(stash_tags))

    def _select_new_tags(self, stash_tags):
        for var, tag in self._tag_vars:
            var.set(tag.lower() not in stash_tags)

    def _build_urls(self, parent, row, col):
        data       = self.merged_data.get("urls", {})
        merged     = data.get("merged", {})
        stash_urls = set(self.db_data.get("urls") or [])

        card = _Card(parent, "URLs & R√©seaux sociaux", "üîó",
                     accent="#3a6a9a")
        card.grid(row=row, column=col, sticky="nsew", padx=5, pady=5)
        card.set_count(len(merged))

        if not merged:
            _lbl(card.body(), "Aucune URL trouv√©e.",
                 fg=P["muted"], bg=P["card"]).pack(anchor="w")
            return

        sf = _scrolled_frame(card.body(), bg=P["url_bg"])

        ICONS = {
            "iafd": "üé¨", "freeones": "üåê", "babepedia": "üìö",
            "thenude": "üì∑", "twitter": "üê¶", "instagram": "üì∏",
            "onlyfans": "üíé", "facebook": "üëç", "tiktok": "üéµ",
        }

        self._url_vars = []
        for key in sorted(merged):
            url  = merged[key]
            icon = ICONS.get(key.lower(), "üîó")
            var  = tk.BooleanVar(value=True)
            self._url_vars.append((var, key, url))

            row_f = tk.Frame(sf, bg=P["url_bg"])
            row_f.pack(fill=tk.X, pady=1)

            cb = tk.Checkbutton(row_f, variable=var, bg=P["url_bg"],
                                fg=P["text"], selectcolor=P["card"],
                                activebackground=P["url_bg"], relief=tk.FLAT)
            cb.pack(side=tk.LEFT)

            tk.Label(row_f, text=f"{icon} {key:<14}",
                     font=FSMB, fg=P["text"], bg=P["url_bg"]).pack(side=tk.LEFT)
            tk.Label(row_f, text=url, font=FSM,
                     fg=P["muted"], bg=P["url_bg"]).pack(side=tk.LEFT)

    # ‚îÄ‚îÄ Footer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_footer(self):
        _sep(self, P["border"]).pack(fill=tk.X)
        bar = tk.Frame(self, bg=P["surface"], pady=10)
        bar.pack(fill=tk.X, padx=12)

        _action_btn(bar, "‚úñ Annuler", P["danger"],
                    self._cancel, side=tk.LEFT)

        tk.Frame(bar, bg=P["surface"], width=12).pack(side=tk.LEFT)

        _action_btn(bar, "‚úì Tout cocher", P["dim"],
                    self._check_all, side=tk.LEFT)
        _action_btn(bar, "‚≠ï Tout d√©cocher", P["dim"],
                    self._uncheck_all, side=tk.LEFT, padx=4)

        _action_btn(bar, "‚Üí G√©n√©rer la Bio IA", P["success"],
                    self._proceed, side=tk.RIGHT)

    # ‚îÄ‚îÄ Actions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _check_all(self):
        for v, _ in self._award_vars:
            v.set(True)
        for v, _ in self._tattoo_vars:
            v.set(True)
        for v, _ in self._piercing_vars:
            v.set(True)
        for v, _ in self._tag_vars:
            v.set(True)
        for v, *_ in self._url_vars:
            v.set(True)

    def _uncheck_all(self):
        for v, _ in self._award_vars:
            v.set(False)
        for v, _ in self._tattoo_vars:
            v.set(False)
        for v, _ in self._piercing_vars:
            v.set(False)
        for v, _ in self._tag_vars:
            v.set(False)
        for v, *_ in self._url_vars:
            v.set(False)

    def _proceed(self):
        """Collecte les s√©lections et ferme."""
        self.result = {
            "trivia":   self._trivia_text.get("1.0", tk.END).strip()
                        if self._trivia_text else "",
            "awards":   [a  for v, a in self._award_vars    if v.get()],
            "tattoos":  [i  for v, i in self._tattoo_vars   if v.get()],
            "piercings":[i  for v, i in self._piercing_vars if v.get()],
            "tags":     [t  for v, t in self._tag_vars      if v.get()],
            "urls":     {k: u for v, k, u in self._url_vars if v.get()},
        }
        self.destroy()

    def _cancel(self):
        self.result = None
        self.destroy()


# ‚îÄ‚îÄ Helpers UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _fullscreen(win):
    if platform.system() == "Windows":
        win.state("zoomed")
    elif platform.system() == "Linux":
        try:
            win.attributes("-zoomed", True)
        except Exception:
            win.geometry(f"{win.winfo_screenwidth()}x{win.winfo_screenheight()}+0+0")
    else:
        win.attributes("-fullscreen", True)


def _scrolled_frame(parent, bg=None):
    """Frame scrollable verticalement."""
    bg = bg or P["card"]
    wrapper = tk.Frame(parent, bg=bg)
    wrapper.pack(fill=tk.BOTH, expand=True)

    canvas = tk.Canvas(wrapper, bg=bg, highlightthickness=0)
    sb     = ttk.Scrollbar(wrapper, orient=tk.VERTICAL, command=canvas.yview)
    inner  = tk.Frame(canvas, bg=bg)

    inner.bind("<Configure>",
               lambda e: canvas.configure(scrollregion=canvas.bbox("all")))
    win_id = canvas.create_window((0, 0), window=inner, anchor="nw")
    canvas.configure(yscrollcommand=sb.set)
    canvas.bind("<Configure>",
                lambda e: canvas.itemconfig(win_id, width=e.width))
    canvas.bind_all("<MouseWheel>",
                    lambda e: canvas.yview_scroll(int(-1*(e.delta/120)), "units"))

    sb.pack(side=tk.RIGHT, fill=tk.Y)
    canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
    return inner


def _checkbutton(parent, text, var, bg=None):
    bg = bg or P["card"]
    cb = tk.Checkbutton(
        parent, text=text, variable=var,
        font=FSM, fg=P["text"], bg=bg,
        selectcolor=P["card_hdr"],
        activebackground=bg, relief=tk.FLAT,
        wraplength=360, justify="left", anchor="w",
    )
    cb.pack(anchor="w", padx=4, pady=1)


def _check_buttons(parent, vars_list, extra_btns=None):
    row = tk.Frame(parent, bg=P["card"])
    row.pack(fill=tk.X, pady=4)
    _small_btn(row, "‚úì Tous",
               lambda: [v.set(True) for v, _ in vars_list])
    _small_btn(row, "‚úó Aucun",
               lambda: [v.set(False) for v, _ in vars_list])
    if extra_btns:
        for txt, cmd in extra_btns:
            _small_btn(row, txt, cmd)


def _small_btn(parent, text, cmd, **kw):
    b = tk.Button(
        parent, text=text, command=cmd,
        font=FSM, bg=P["card_hdr"], fg=P["text"],
        relief=tk.FLAT, padx=8, pady=3, cursor="hand2",
        activebackground=P["border"], activeforeground=P["text"],
    )
    b.pack(side=tk.LEFT, padx=3, **kw)


def _action_btn(parent, text, bg, cmd, side=tk.RIGHT, padx=6):
    b = tk.Button(
        parent, text=text, command=cmd,
        font=FH3, bg=bg, fg="white",
        relief=tk.FLAT, padx=16, pady=8, cursor="hand2",
        activebackground=bg, activeforeground="white",
    )
    b.pack(side=side, padx=padx)


def _refresh_tag_btn(frame, tag, selected, already):
    for w in frame.winfo_children():
        w.destroy()
    bg = P["tag_sel"] if selected else P["tag_unsel"]
    fg = P["text"]    if selected else P["muted"]
    pfx = "‚úì " if already else ("+ " if selected else "")
    lbl = tk.Label(frame, text=f"{pfx}{tag}", font=FSMB,
                   fg=fg, bg=bg, padx=8, pady=4, cursor="hand2")
    lbl.pack()


def _refresh_all_tags(tag_vars):
    pass  # Simplification : badges se rafra√Æchissent au prochain clic


============================================================
[35/124] Legacy\gui\group_frame.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
from gui.group_phase1 import GroupPhase1Frame
from gui.group_phase2 import GroupPhase2Frame

class GroupFrame(ttk.Frame):
    def __init__(self, parent, stash_id):
        super().__init__(parent)
        self.stash_id = stash_id
        self.current_frame = None
        
        # For now, just a label
        label = ttk.Label(self, text=f"Group Frame for ID: {self.stash_id}")
        label.pack(pady=20, padx=20)

        self.goto_phase1()

    def goto_phase1(self):
        if self.current_frame:
            self.current_frame.destroy()
        
        self.current_frame = GroupPhase1Frame(self, self, self.stash_id)
        self.current_frame.pack(fill=tk.BOTH, expand=True)

    def goto_phase2(self, group_data, scenes_data):
        if self.current_frame:
            self.current_frame.destroy()
            
        self.current_frame = GroupPhase2Frame(self, self, self.stash_id, group_data, scenes_data)
        self.current_frame.pack(fill=tk.BOTH, expand=True)

    def return_to_menu(self):
        # Fermer la fen√™tre actuelle
        self.master.destroy()
        # Relancer le launcher
        try:
            from gui.launcher import start_launcher
            start_launcher()
        except Exception as e:
            print(f"Erreur lors du retour au menu: {e}")


============================================================
[36/124] Legacy\gui\group_phase1.py
------------------------------------------------------------
"""
Placeholder for Group Phase 1 Frame.
The content for this file is in PLAN_GROUPS_V2.md, which was not provided.
"""
import tkinter as tk
from tkinter import ttk, messagebox
import threading
import yaml

from services.db import GroupDB
from services.group_phase1_scraper import GroupPhase1ScraperService
from services.group_phase1_merger import GroupPhase1Merger
from gui.phase1_conflict_dialog import Phase1ConflictDialog
from services.phase2_scraper import Phase2ScraperService


class GroupPhase1Frame(ttk.Frame):
    def __init__(self, parent, controller, group_id):
        super().__init__(parent)
        self.controller = controller
        self.group_id = group_id

        self._group_data = None
        self._scenes_data = [] # Scenes associ√©es au group

        self.field_checkboxes = {}
        self.fields = {}

        self.create_ui()
        self._load_data()

    def create_ui(self):
        # Header
        header_frame = ttk.Frame(self)
        header_frame.pack(fill=tk.X, padx=10, pady=5)
        ttk.Label(header_frame, text=f"Group ID: {self.group_id}",
                  font=("Segoe UI", 14, "bold")).pack(side=tk.LEFT)
        ttk.Button(header_frame, text="Retour Launcher",
                   command=self.controller.return_to_menu).pack(side=tk.RIGHT)

        # ScrolledFrame pour les champs du Group
        self.main_canvas = tk.Canvas(self, highlightthickness=0)
        self.main_scrollbar = ttk.Scrollbar(self, orient=tk.VERTICAL, command=self.main_canvas.yview)
        self.main_scrollable_frame = ttk.Frame(self.main_canvas)

        self.main_scrollable_frame.bind(
            "<Configure>",
            lambda e: self.main_canvas.configure(
                scrollregion=self.main_canvas.bbox("all")
            )
        )
        self.main_canvas.create_window((0, 0), window=self.main_scrollable_frame, anchor="nw")
        self.main_canvas.configure(yscrollcommand=self.main_scrollbar.set)

        self.main_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.main_canvas.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Section Group Details
        group_details_frame = ttk.LabelFrame(self.main_scrollable_frame, text="Group Details (Phase 1)", padding=10)
        group_details_frame.pack(fill=tk.X, padx=10, pady=5)

        # TODO: Charger les champs depuis settings.yaml
        self.fields_list = [
            "Title", "Aliases", "Date", "Studio", "Director",
            "Duration", "Description", "Tags", "URLs"
        ]

        for i, field in enumerate(self.fields_list):
            row = i
            var = tk.BooleanVar(value=True)
            self.field_checkboxes[field] = var
            checkbox = ttk.Checkbutton(group_details_frame, variable=var, text="")
            checkbox.grid(row=row, column=0, sticky=tk.W, padx=(5, 0), pady=2)

            ttk.Label(group_details_frame, text=f"{field}:").grid(row=row, column=1, sticky=tk.W, padx=5, pady=2)
            entry = tk.Text(group_details_frame, height=2, width=60, font=("Segoe UI", 9))
            entry.grid(row=row, column=2, sticky=tk.EW, padx=5, pady=2)
            self.fields[field] = entry

        group_details_frame.grid_columnconfigure(2, weight=1)

        # Boutons d'action
        action_frame = ttk.Frame(self.main_scrollable_frame)
        action_frame.pack(fill=tk.X, padx=10, pady=5)
        ttk.Button(action_frame, text="üîé Analyser & Phase 2", command=self._run_phase1).pack(side=tk.LEFT, padx=5)

        # Section Sc√®nes Associ√©es
        self.scenes_frame = ttk.LabelFrame(self.main_scrollable_frame, text="Sc√®nes Associ√©es au Group", padding=10)
        self.scenes_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        ttk.Label(self.scenes_frame, text="Chargement des sc√®nes...", font=("Segoe UI", 9, "italic")).pack()

    def _load_data(self):
        try:
            gid = int(self.group_id)
        except ValueError:
            gid = self.group_id

        db = GroupDB()
        self._group_data = db.get_group_by_id(gid)
        self._scenes_data = db.get_group_scenes(gid)
        db.close()

        print(f"DEBUG: Loaded group data for ID {gid}: {bool(self._group_data)}")
        print(f"DEBUG: Loaded {len(self._scenes_data)} scenes")

        if self._group_data:
            for field_name, entry_widget in self.fields.items():
db_key="***MASKED***"
                value = self._group_data.get(db_key)
                if field_name == "Studio" and self._group_data.get("studio_name"):
                    value = self._group_data["studio_name"]
                elif field_name == "Tags" and value:
                    value = ", ".join(value)
                elif field_name == "URLs" and value:
                    value = "\n".join(value)

                if value:
                    entry_widget.config(state=tk.NORMAL)
                    entry_widget.delete("1.0", tk.END)
                    entry_widget.insert("1.0", str(value))
                    entry_widget.config(state=tk.DISABLED)

        self._display_scenes()

    def _display_scenes(self):
        for widget in self.scenes_frame.winfo_children():
            widget.destroy()

        if not self._scenes_data:
            ttk.Label(self.scenes_frame, text="Aucune sc√®ne associ√©e.", font=("Segoe UI", 9, "italic")).pack()
            return

        # Treeview pour les sc√®nes
        columns = ("index", "title", "urls")
        tree = ttk.Treeview(self.scenes_frame, columns=columns, show="headings", height=8)
        tree.heading("index", text="#")
        tree.heading("title", text="Titre Stash")
        tree.heading("urls", text="URLs Existantes")
        
        tree.column("index", width=30, anchor=tk.CENTER)
        tree.column("title", width=300, anchor=tk.W)
        tree.column("urls", width=400, anchor=tk.W)

        for s in self._scenes_data:
            urls_str = ", ".join(s.get("existing_urls", []))
            tree.insert("", tk.END, values=(
                s.get("scene_index", "?"),
                s.get("scene_title", "Sans titre"),
                urls_str
            ))
        
        tree.pack(fill=tk.BOTH, expand=True)

    def _run_phase1(self):
        """Lance le scraping Phase 1 Group."""
        checked = [f for f, var in self.field_checkboxes.items() if var.get()]
        if not checked:
            messagebox.showwarning("Attention", "Aucun champ coch√©.")
            return

        # Afficher progression
        progress_popup = tk.Toplevel(self)
        progress_popup.title("Scraping Group Phase 1...")
        progress_popup.geometry("300x100")
        prog_label = ttk.Label(progress_popup, text="Initialisation...")
        prog_label.pack(pady=20)

        def _do():
            try:
                scraper = GroupPhase1ScraperService()
                title = self.fields["Title"].get("1.0", tk.END).strip()
                year = self.fields["Date"].get("1.0", tk.END).strip()[:4] # Ann√©e
                known_urls = self.fields["URLs"].get("1.0", tk.END).strip().split("\n")
                known_urls = [u.strip() for u in known_urls if u.strip()]

                def update_prog(src, st):
                    self.after(0, lambda: prog_label.config(text=f"[{src}] {st}"))

                scraped = scraper.scrape(title, year, known_urls, progress_callback=update_prog)
                
                # Check for Data18
                has_data18 = any(r.get("_source") == "data18" for r in scraped)
                
                if not has_data18:
                    self.after(0, lambda: self._ask_data18_and_continue(scraped, checked, scraper, progress_popup))
                else:
                    self.after(0, lambda: self._finish_phase1(scraped, checked, progress_popup))

            except Exception as e:
                self.after(0, lambda: [progress_popup.destroy(), messagebox.showerror("Erreur", str(e))])

        threading.Thread(target=_do, daemon=True).start()

    def _ask_data18_and_continue(self, scraped, checked, scraper, progress_popup):
        from tkinter import simpledialog
        # Cacher la popup de progression temporairement
        progress_popup.withdraw()
        
        url = simpledialog.askstring(
            "Data18 Manquant", 
            "Data18 n'a pas √©t√© trouv√© automatiquement.\n\n"
            "Pour avoir les meilleurs r√©sultats (Tags, Sc√®nes...), collez l'URL Data18 ici :\n"
            "(Sinon, laissez vide et OK pour continuer)",
            parent=self
        )
        
        progress_popup.deiconify()
        
        if url and "data18.com" in url:
            # Relancer un petit thread pour scraper cette URL
            def _scrape_extra():
                try:
                    from services.extractors.dvd.data18_dvd import Data18DVDExtractor
                    e = Data18DVDExtractor()
                    res = e.extract_from_url(url.strip())
                    if res:
                        scraped.append(res)
                except Exception as e:
                    print(f"Error scraping extra URL: {e}")
                
                self.after(0, lambda: self._finish_phase1(scraped, checked, progress_popup))
            
            threading.Thread(target=_scrape_extra, daemon=True).start()
        else:
            self._finish_phase1(scraped, checked, progress_popup)

    def _finish_phase1(self, scraped, checked, progress_popup):
        progress_popup.destroy()
        merger = GroupPhase1Merger()
        merged = merger.merge(self._group_data, scraped, checked)
        self._show_conflict_dialog(merged, checked)

    def _show_conflict_dialog(self, merged_data: dict, checked_fields: list[str]):
        from services.group_phase1_merger import GROUP_FIELDS
        group_title = self.fields["Title"].get("1.0", tk.END).strip()
        dialog = Phase1ConflictDialog(self, group_title, merged_data, GROUP_FIELDS)
        if dialog.result:
            self._inject_phase1(dialog.result)

    def _inject_phase1(self, result: dict):
        try:
            # Pour Group Phase 1, on injecte directement via GroupDB (√† impl√©menter ou simuler)
            # On r√©utilise les champs mapp√©s
            db_updates = {}
            from services.group_phase1_merger import GROUP_FIELDS
            for field, value in result.items():
db_key="***MASKED***"
                if db_key:
                    db_updates[db_key] = value

            # Simulation d'injection (ou ajout de la m√©thode dans GroupDB)
            print(f"DEBUG: Injecting group updates: {db_updates}")
            
            # TODO: Impl√©menter GroupDB.update_group(self.group_id, db_updates)
            
            messagebox.showinfo("‚úÖ Phase 1 Termin√©e", "Les donn√©es du Group ont √©t√© mises √† jour.")
            
            # Passer √† la Phase 2
            self.controller.goto_phase2(self._group_data, self._scenes_data)

        except Exception as e:
            messagebox.showerror("Erreur Injection", str(e))




============================================================
[37/124] Legacy\gui\group_phase2.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, messagebox
import threading

from services.db import GroupDB
from services.group_phase2_scraper import GroupPhase2ScraperService
from services.group_phase2_merger import GroupPhase2Merger

STATUS_ICONS = {
    "new":         "üü¢",
    "partial":     "üü†",
    "already_present": "üîµ",
    "no_match":    "‚ö™",
}

class GroupPhase2Frame(ttk.Frame):
    def __init__(self, parent, controller, group_id, group_data, scenes_data):
        super().__init__(parent)
        self.controller = controller
        self.group_id = group_id
        self._group_data = group_data # Donn√©es Group d√©j√† scrap√©es/fusionn√©es Phase 1
        self._scenes_data = scenes_data # Liste des sc√®nes Stash du Group
        self._merged_scene_urls = [] # R√©sultat de la fusion Phase 2

        self.scene_checkboxes = [] # Pour cocher les URLs √† injecter

        self.create_ui()
        self._run_phase2()

    def create_ui(self):
        # Header
        header_frame = ttk.Frame(self)
        header_frame.pack(fill=tk.X, padx=10, pady=5)
        ttk.Label(header_frame, text=f"Group ID: {self.group_id} ‚Äî Phase 2: URLs Sc√®nes",
                  font=("Segoe UI", 14, "bold")).pack(side=tk.LEFT)
        ttk.Button(header_frame, text="Retour Phase 1",
                   command=lambda: self.controller.goto_phase1()).pack(side=tk.RIGHT)

        # Progress bar (similaire √† Performer Phase 2)
        self.progress_frame = ttk.Frame(self)
        self.progress_label = ttk.Label(self.progress_frame, text="", 
                                        font=("Segoe UI", 9, "italic"))
        self.progress_label.pack(side=tk.LEFT, padx=10)
        self.progress_bar = ttk.Progressbar(self.progress_frame, mode="indeterminate", length=200)
        self.progress_bar.pack(side=tk.LEFT, padx=5)

        # Zone principale scrollable pour les sc√®nes
        self.main_canvas = tk.Canvas(self, highlightthickness=0)
        self.main_scrollbar = ttk.Scrollbar(self, orient=tk.VERTICAL, command=self.main_canvas.yview)
        self.main_scrollable_frame = ttk.Frame(self.main_canvas)

        self.main_scrollable_frame.bind(
            "<Configure>",
            lambda e: self.main_canvas.configure(
                scrollregion=self.main_canvas.bbox("all")
            )
        )
        self.main_canvas_window = self.main_canvas.create_window((0, 0), window=self.main_scrollable_frame, anchor="nw")
        self.main_canvas.configure(yscrollcommand=self.main_scrollbar.set)
        
        # Mousewheel scrolling
        self.main_canvas.bind_all("<MouseWheel>",
                             lambda e: self.main_canvas.yview_scroll(int(-1 * (e.delta / 120)), "units"))
        
        # Resize canvas window width to match canvas width
        self.main_canvas.bind("<Configure>",
                         lambda e: self.main_canvas.itemconfig(self.main_canvas_window, width=e.width))


        self.main_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.main_canvas.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Boutons d'action
        action_frame = ttk.Frame(self)
        action_frame.pack(fill=tk.X, padx=10, pady=5)
        ttk.Button(action_frame, text="‚úî Injecter s√©lectionn√©es", command=self._inject).pack(side=tk.LEFT, padx=5)
        ttk.Button(action_frame, text="Tout s√©lectionner", command=self._select_all).pack(side=tk.LEFT, padx=5)
        ttk.Button(action_frame, text="D√©s√©lectionner tout", command=self._deselect_all).pack(side=tk.LEFT, padx=5)

    def _run_phase2(self):
        self._show_progress("Scraping URLs de sc√®nes...")

        def _do_scraping_and_merge():
            try:
                scraper = GroupPhase2ScraperService()
                
                def update_prog(src, st):
                    self.after(0, lambda: self._update_progress(f"[{src}] {st}"))

                scraped_urls_by_index = scraper.scrape(
                    group_data=self._group_data,
                    progress_callback=update_prog
                )

                merger = GroupPhase2Merger()
                self._merged_scene_urls = merger.merge(
                    self._scenes_data, scraped_urls_by_index)

                self.after(0, self._display_results)

            except Exception as e:
                self.after(0, lambda: messagebox.showerror("Erreur Phase 2", str(e)))
            finally:
                self.after(0, self._hide_progress)

        threading.Thread(target=_do_scraping_and_merge, daemon=True).start()

    def _display_results(self):
        for widget in self.main_scrollable_frame.winfo_children():
            widget.destroy()

        if not self._merged_scene_urls:
            ttk.Label(self.main_scrollable_frame, text="Aucune URL de sc√®ne trouv√©e ou fusionn√©e.",
                      font=("Segoe UI", 10, "italic")).pack(padx=10, pady=10)
            return

        # Headers du tableau
        header_frame = ttk.Frame(self.main_scrollable_frame)
        header_frame.pack(fill=tk.X, padx=5, pady=2)
        ttk.Label(header_frame, text="", width=4).pack(side=tk.LEFT) # Checkbox
        ttk.Label(header_frame, text="Statut", width=8, font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
        ttk.Label(header_frame, text="Index", width=6, font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
        ttk.Label(header_frame, text="Titre Stash", width=40, anchor=tk.W, font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
        ttk.Label(header_frame, text="Nouvelles URLs", anchor=tk.W, font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT, expand=True, fill=tk.X)

        self.scene_checkboxes = []
        for scene_data in self._merged_scene_urls:
            self._build_scene_row(self.main_scrollable_frame, scene_data)

    def _build_scene_row(self, parent_frame, scene_data):
        row_frame = ttk.Frame(parent_frame, padding=2)
        row_frame.pack(fill=tk.X, padx=5, pady=1)

        status_icon = STATUS_ICONS.get(scene_data["status"], "")

        # Cocher par d√©faut si nouvelles URLs (statut 'new' ou 'partial')
        has_new = bool(scene_data.get("new_urls"))
        var = tk.BooleanVar(value=has_new) 
        self.scene_checkboxes.append((var, scene_data)) # Stocker tuple (var, data)
        
        chk = ttk.Checkbutton(row_frame, variable=var)
        chk.pack(side=tk.LEFT)
        if not has_new:
            chk.config(state=tk.DISABLED)

        ttk.Label(row_frame, text=status_icon, width=4).pack(side=tk.LEFT)
        ttk.Label(row_frame, text=str(scene_data.get("scene_index", "?")), width=6).pack(side=tk.LEFT)
        ttk.Label(row_frame, text=scene_data.get("scene_title", "Sans titre"), width=40, anchor=tk.W).pack(side=tk.LEFT)

        urls_frame = ttk.Frame(row_frame)
        urls_frame.pack(side=tk.LEFT, expand=True, fill=tk.X)

        new_urls = scene_data.get("new_urls", {})
        existing = scene_data.get("existing_urls", [])

        if new_urls:
            for src, url in new_urls.items():
                ttk.Label(urls_frame, text=f"‚ûï {src.upper()}: {url}", anchor=tk.W, font=("Segoe UI", 8, "bold"), foreground="green").pack(fill=tk.X)
        
        if existing:
            count = len(existing)
            ttk.Label(urls_frame, text=f"Existing: {count} URLs", anchor=tk.W, foreground="gray", font=("Segoe UI", 8)).pack(fill=tk.X)
        
        if not new_urls and not existing:
             ttk.Label(urls_frame, text="‚Äî", anchor=tk.W, foreground="gray", font=("Segoe UI", 8)).pack(fill=tk.X)

    def _inject(self):
        selected_urls_to_inject = []
        for var, scene_data in self.scene_checkboxes:
            if var.get():
                for url_src, url_val in scene_data["new_urls"].items():
                    selected_urls_to_inject.append({
                        "scene_id": scene_data["scene_id"],
                        "url": url_val,
                        "source": url_src,
                    })
        
        if not selected_urls_to_inject:
            messagebox.showwarning("Attention", "Aucune URL s√©lectionn√©e √† injecter.")
            return

        try:
            db = GroupDB()
            # appel √† la m√©thode inject_scene_urls que j'ai ajout√©e dans db.py
            db.inject_scene_urls(selected_urls_to_inject) 
            db.close()
            
            messagebox.showinfo("‚úÖ Injection Phase 2", f"{len(selected_urls_to_inject)} URLs de sc√®nes inject√©es.")

            # Optionnel : Recharger ou fermer
            # self._run_phase2() 

        except Exception as e:
            messagebox.showerror("Erreur injection", str(e))

    def _select_all(self):
        for var, _ in self.scene_checkboxes:
            if str(var['state']) != tk.DISABLED:
                var.set(True)

    def _deselect_all(self):
        for var, _ in self.scene_checkboxes:
            var.set(False)

    def _show_progress(self, message):
        self.progress_frame.pack(fill=tk.X, padx=10, pady=2, before=self.main_canvas)
        self.progress_bar.start(10)
        self.progress_label.config(text=message)

    def _update_progress(self, message):
        self.progress_label.config(text=message)

    def _hide_progress(self):
        self.progress_bar.stop()
        self.progress_frame.pack_forget()


============================================================
[38/124] Legacy\gui\launcher.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, simpledialog, messagebox
import sv_ttk
from gui.app import launch_app

def center_window(window, width=400, height=300):
    screen_width = window.winfo_screenwidth()
    screen_height = window.winfo_screenheight()
    x = (screen_width - width) // 2
    y = (screen_height - height) // 2
    window.geometry(f'{width}x{height}+{x}+{y}')

def start_launcher():
    root = tk.Tk()
    root.title("StashMaster V2 - Launcher")
    
    # Appliquer le th√®me moderne sombre
    sv_ttk.set_theme("dark")
    
    center_window(root, 400, 250)
    
    ttk.Label(root, text="StashMaster V2", font=("Segoe UI", 16, "bold")).pack(pady=(20, 10))
    ttk.Label(root, text="S√©lectionnez un module :", font=("Segoe UI", 10)).pack(pady=5)
    
    def on_select(module):
        root.withdraw()
        # Utiliser un prompt simple pour l'ID
        stash_id = simpledialog.askstring("Entrer l'ID", f"Entrez l'ID pour le module {module} :", parent=root)
        if not stash_id:
            messagebox.showwarning("ID requis", "Vous devez entrer un ID valide pour continuer.")
            root.deiconify()
            return
        root.destroy()
        # Lancer l'application principale maximis√©e
        launch_app(module, stash_id)
        
    ttk.Button(root, text="Performer", width=25, command=lambda: on_select("Performer")).pack(pady=5)
    ttk.Button(root, text="Group / DVD", width=25, command=lambda: on_select("Group")).pack(pady=5)
    ttk.Button(root, text="Scene", width=25, command=lambda: on_select("Scene")).pack(pady=5)
    
    root.mainloop()

if __name__ == "__main__":
    start_launcher()

============================================================
[39/124] Legacy\gui\performer_base.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk

class PerformerBaseFrame(ttk.Frame):
    def __init__(self, parent, controller, stash_id):
        super().__init__(parent)
        self.controller = controller
        self.stash_id = stash_id
        self.fields = {}
        self.field_checkboxes = {}
        # To be defined in subclasses
        self.fields_list = [] 
        self.db_mapping = {}

    def create_header(self, title, buttons_config):
        bar = ttk.Frame(self, padding=5)
        bar.pack(fill=tk.X)
        
        ttk.Label(bar, text=f"{title} | ID: {self.stash_id}", font=("Segoe UI", 12, "bold")).pack(side=tk.LEFT, padx=5)
        
        btn_frame = ttk.Frame(bar)
        btn_frame.pack(side=tk.RIGHT, padx=5)
        
        for text, command in buttons_config:
            ttk.Button(btn_frame, text=text, command=command).pack(side=tk.LEFT, padx=2)

    def select_all_fields(self):
        for var in self.field_checkboxes.values():
            var.set(True)

    def select_empty_fields(self):
        for field, entry in self.fields.items():
            if field in self.field_checkboxes and entry:
                val = ""
                if isinstance(entry, tk.Entry):
                    val = entry.get().strip()
                elif isinstance(entry, tk.Text):
                    val = entry.get("1.0", tk.END).strip()
                
                if not val:
                    self.field_checkboxes[field].set(True)
                else:
                    self.field_checkboxes[field].set(False)

    def load_data(self):
        try:
            from services.db import PerformerDB
            db = PerformerDB()
            data = db.get_performer_by_id(self.stash_id)
            db.close()
        except Exception as e:
            print(f"Erreur DB: {e}")
            data = None
        
        if not data:
            return

        for field, db_key in self.db_mapping.items():
            entry = self.fields.get(field)
            if entry and db_key in data:
                value = data[db_key]
                if isinstance(value, (list, tuple)):
                    if field == "URLs":
                        value = "\n".join(value)
                    else:
                        value = ", ".join(value)
                
                if isinstance(entry, tk.Entry):
                    entry.delete(0, tk.END)
                    entry.insert(0, str(value) if value is not None else "")
                elif isinstance(entry, tk.Text):
                    entry.delete('1.0', tk.END)
                    entry.insert('1.0', str(value) if value is not None else "")


============================================================
[40/124] Legacy\gui\performer_frame.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
from gui.performer_phase1 import PerformerPhase1Frame
from gui.performer_phase2 import PerformerPhase2Frame

class PerformerFrame(ttk.Frame):
    def __init__(self, parent, stash_id):
        super().__init__(parent)
        self.stash_id = stash_id
        self.current_frame = None
        self.phase1_data = {} # Pour stocker les donn√©es r√©solues de la phase 1
        
        self.goto_phase1()

    def goto_phase1(self):
        if self.current_frame:
            self.current_frame.destroy()
        
        self.current_frame = PerformerPhase1Frame(self, self, self.stash_id)
        self.current_frame.pack(fill=tk.BOTH, expand=True)

    def goto_phase2(self, phase1_updates: dict):
        """Passe √† la phase 2 en emportant les donn√©es r√©solues de la phase 1."""
        self.phase1_data = phase1_updates

        if self.current_frame:
            self.current_frame.destroy()
            
        # Initialise la phase 2 avec les donn√©es de la phase 1
        self.current_frame = PerformerPhase2Frame(self, self, self.stash_id, self.phase1_data)
        self.current_frame.pack(fill=tk.BOTH, expand=True)




    def return_to_menu(self):
        # Fermer la fen√™tre actuelle
        self.master.destroy()
        # Relancer le launcher
        try:
            from gui.launcher import start_launcher
            start_launcher()
        except Exception as e:
            print(f"Erreur lors du retour au menu: {e}")




============================================================
[41/124] Legacy\gui\performer_phase1.py
------------------------------------------------------------
import tkinter as tk
from tkinter import messagebox
from tkinter import ttk
import threading

from gui.performer_base import PerformerBaseFrame
from gui.phase1_conflict_dialog import Phase1ConflictDialog


class PerformerPhase1Frame(PerformerBaseFrame):
    def __init__(self, parent, controller, stash_id):
        super().__init__(parent, controller, stash_id)
        
        # Configuration des champs Phase 1
        self.fields_list = [
            "Name", "Aliases", "Birthdate", "Deathdate", "Country", "Ethnicity",
            "Hair Color", "Eye Color", "Height", "Weight", "Measurements", "Fake Tits", "Career Length"
        ]
        
        self.db_mapping = {
            "Name": "name",
            "Aliases": "aliases",
            "Birthdate": "birthdate",
            "Deathdate": "death_date",
            "Country": "country",
            "Ethnicity": "ethnicity",
            "Hair Color": "hair_color",
            "Eye Color": "eye_color",
            "Height": "height",
            "Weight": "weight",
            "Measurements": "measurements",
            "Fake Tits": "fake_tits",
            "Career Length": "career_length"
        }
        
        self.create_ui()
        self.load_data()

    def process_and_goto_phase2(self):
        """
        Workflow Phase 1 : Scrape, compare, et pr√©pare les donn√©es pour la Phase 2.
        AUCUNE injection en base de donn√©es n'est faite ici.
        """
        checked_fields = [f for f, var in self.field_checkboxes.items() if var.get()]
        if not checked_fields:
            self.controller.goto_phase2({}) # Passe un dict vide si pas de scraping
            return
        
        try:
            from services.db import PerformerDB
            db = PerformerDB()
            db_data = db.get_performer_by_id(self.stash_id)
            db.close()
        except Exception as e:
            messagebox.showerror("Erreur", f"Impossible de lire la DB: {e}")
            return

        if not db_data:
            messagebox.showerror("Erreur", "Performer non trouv√© dans la base.")
            return

        performer_name = db_data["name"]
        known_urls = db_data.get("urls", [])

        self.progress_frame.pack(fill=tk.X, padx=10, pady=2, after=list(self.fields.values())[-1].master)
        self.progress_bar.start(10)
        self.progress_label.config(text=f"Scraping {len(checked_fields)} champs pour {performer_name}...")

        def _do_scraping():
            try:
                from services.phase2_scraper import Phase2ScraperService
                from services.phase1_merger import Phase1Merger

                scraper = Phase2ScraperService()
                results = scraper.scrape(performer_name, known_urls=known_urls,
                                         progress_callback=lambda s, m: self.after(0, self.progress_label.config, {'text': f"[{s}] {m}"}))
                
                # Nettoyage des donn√©es scrap√©es AVANT le merge
                for res in results:
                    if res.get("hair_color"):
                        # D√©duplication et nettoyage (ex: "Blonde, Blonde" -> "Blonde")
                        parts = [p.strip() for p in res["hair_color"].replace('/', ',').split(',') if p.strip()]
                        seen = set()
                        unique = []
                        for p in parts:
                            p_cap = p.capitalize()
                            if p_cap not in seen:
                                seen.add(p_cap)
                                unique.append(p_cap)
                        res["hair_color"] = ", ".join(unique)

                merger = Phase1Merger()
                merge_results = merger.merge(db_data, results, checked_fields)
                
                # Construire l'affichage pour TOUS les champs coch√©s
                display_results = {}
                for field in checked_fields:
                    # Le merger renvoie les r√©sultats index√©s par le nom du champ (ex: "Name")
                    if field in merge_results:
                        display_results[field] = merge_results[field]
                    else:
                        # Si le merger n'a rien renvoy√©, on force l'affichage en mode "empty"
                        # pour confirmer √† l'utilisateur que le champ a √©t√© trait√© mais sans r√©sultat.
db_key="***MASKED***"
                        current_val = db_data.get(db_key) if db_key else None
                        display_results[field] = {'status': 'empty', 'db_value': current_val, 'scraped_values': {}, 'suggestion': None}

                self.after(0, self._hide_progress)

                phase1_updates = {}
                if display_results:
                    dialog = Phase1ConflictDialog(self.master, performer_name, display_results, self.db_mapping)
                    if dialog.result is not None:
                        # Le dialogue retourne les valeurs √† mettre √† jour
                        for field_name, resolved_value in dialog.result.items():
db_key="***MASKED***"
                            if db_key:
                                # Sp√©cial pour les alias, on veut une liste
                                if db_key == 'aliases' and isinstance(resolved_value, str):
                                     phase1_updates[db_key] = [a.strip() for a in resolved_value.split(',') if a.strip()]
                                elif db_key == 'hair_color' and isinstance(resolved_value, str):
                                     # Nettoyage et d√©duplication pour la couleur de cheveux
                                     parts = [p.strip() for p in resolved_value.replace('/', ',').split(',') if p.strip()]
                                     seen = set()
                                     unique_parts = []
                                     for p in parts:
                                         if p.lower() not in seen:
                                             seen.add(p.lower())
                                             unique_parts.append(p)
                                     phase1_updates[db_key] = ", ".join(unique_parts)
                                else:
                                     phase1_updates[db_key] = resolved_value
                        
                        messagebox.showinfo("Phase 1 Trait√©e", f"{len(phase1_updates)} modifications de la Phase 1 ont √©t√© pr√©par√©es pour la validation finale.")
                    else:
                        # L'utilisateur a annul√©, on ne passe aucune modification
                        messagebox.showinfo("Annul√©", "Fusion des donn√©es annul√©e. Passage en phase 2 sans appliquer les changements de la phase 1.")
                else:
                    messagebox.showinfo("Phase 1 Compl√®te", "Aucun conflit ou nouvelle donn√©e √† traiter. Passage en Phase 2.")
                
                # Passer en Phase 2 avec les donn√©es r√©solues de la phase 1
                self.after(0, lambda: self.controller.goto_phase2(phase1_updates))

            except Exception as e:
                err_msg = str(e)
                self.after(0, lambda: messagebox.showerror("Erreur Phase 1", f"Une erreur est survenue : {err_msg}"))
            finally:
                self.after(0, self._hide_progress)

        threading.Thread(target=_do_scraping, daemon=True).start()

    def _hide_progress(self):
        self.progress_bar.stop()
        self.progress_frame.pack_forget()

    def create_ui(self):
        # Header + Boutons sp√©cifiques
        buttons = [
            ("Tout s√©lectionner", self.select_all_fields),
            ("S√©lectionner vides", self.select_empty_fields),
            ("Suivant / Traiter", self.process_and_goto_phase2),
            ("Retour", self.controller.return_to_menu),
        ]
        self.create_header("Phase 1 : M√©tadonn√©es usuelles", buttons)

        # Barre de progression (cach√©e par d√©faut)
        self.progress_frame = ttk.Frame(self)
        self.progress_label = ttk.Label(self.progress_frame, text="",
                                        font=("Segoe UI", 9, "italic"))
        self.progress_label.pack(side=tk.LEFT, padx=10)
        self.progress_bar = ttk.Progressbar(self.progress_frame, mode="indeterminate", length=200)
        self.progress_bar.pack(side=tk.LEFT, padx=5)

        # Zone des champs
        f = ttk.Frame(self)
        f.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        
        # Configuration de la grille pour l'extension horizontale
        f.grid_columnconfigure(0, weight=0)  # Checkbox
        f.grid_columnconfigure(1, weight=0)  # Label
        f.grid_columnconfigure(2, weight=1)  # Entry (prend tout l'espace restant)

        for i, field in enumerate(self.fields_list):
            row = i
            # Checkbox (d√©coch√©e par d√©faut)
            var = tk.BooleanVar(value=False)
            self.field_checkboxes[field] = var
            checkbox = ttk.Checkbutton(f, variable=var, text="")
            checkbox.grid(row=row, column=0, sticky=tk.W, padx=(5, 0), pady=2)
            
            # Label
            ttk.Label(f, text=f"{field}:").grid(row=row, column=1, sticky=tk.NW, padx=5, pady=2)
            
            # Entry Widget
            entry = ttk.Entry(f, width=60)
            entry.grid(row=row, column=2, sticky=tk.EW, padx=5, pady=2)
            self.fields[field] = entry


============================================================
[42/124] Legacy\gui\performer_phase2.py
------------------------------------------------------------
"""
PerformerPhase2Frame ‚Äî Orchestre le flux 3-fen√™tres Bio IA.

Flux :
  1. Scraping Phase 2 (background thread)
  2. GUI 1 : DataReviewWindow   ‚Äî r√©vision donn√©es (sauf bio)
  3. GUI 2 : BioStudioWindow    ‚Äî g√©n√©ration bio Gemini / Ollama
  4. GUI 3 : ValidationWindow   ‚Äî validation + injection Stash

Toutes les fen√™tres s'ouvrent en plein √©cran.
"""
import tkinter as tk
from tkinter import ttk, messagebox
import threading
import copy

from gui.performer_base import PerformerBaseFrame


class PerformerPhase2Frame(PerformerBaseFrame):
    def __init__(self, parent, controller, stash_id, phase1_data: dict):
        self.phase1_data  = phase1_data
        self.phase2_data  = {}

        super().__init__(parent, controller, stash_id)

        self.fields_list = [
            "Trivia", "Awards", "Tattoos", "Piercings", "Tags", "URLs", "Details"
        ]
        self.db_mapping = {
            "Trivia":    "trivia",
            "Awards":    "awards",
            "Tattoos":   "tattoos",
            "Piercings": "piercings",
            "Tags":      "tags",
            "URLs":      "urls",
            "Details":   "details",
        }

        self._scraped_results = None
        self._merged_data     = None
        self._db_data         = None
        self._stash_context   = None

        self.create_ui()
        self.load_data()

    # ‚îÄ‚îÄ Chargement DB ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def load_data(self):
        super().load_data()
        if self.phase1_data:
            for db_key, value in self.phase1_data.items():
                field_name = next(
                    (n for n, k in self.db_mapping.items() if k == db_key), None
                )
                if field_name:
                    display = (", ".join(value) if isinstance(value, list)
                               else str(value) if value else "")
                    self._update_field_display(field_name, display)

    # ‚îÄ‚îÄ UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def create_ui(self):
        buttons = [
            ("üîé Scraper & Lancer le flux Bio IA", self.run_full_flow),
            ("Tout s√©lectionner",  self.select_all_fields),
            ("S√©lectionner vides", self.select_empty_fields),
            ("Retour Phase 1",     self.controller.goto_phase1),
        ]
        self.create_header("Phase 2 : Champs Avanc√©s + Bio IA", buttons)

        # Barre de progression (scraping)
        self.progress_frame = ttk.Frame(self)
        self.progress_label = ttk.Label(
            self.progress_frame, text="", font=("Segoe UI", 9, "italic")
        )
        self.progress_label.pack(side=tk.LEFT, padx=10)
        self.progress_bar = ttk.Progressbar(
            self.progress_frame, mode="indeterminate", length=200
        )
        self.progress_bar.pack(side=tk.LEFT, padx=5)

        # Grille des champs (lecture seule ‚Äî affichage des donn√©es actuelles)
        f = ttk.Frame(self)
        f.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        f.grid_columnconfigure(2, weight=1)

        for i, field in enumerate(self.fields_list):
            var = tk.BooleanVar(value=True)
            self.field_checkboxes[field] = var
            ttk.Checkbutton(f, variable=var, text="").grid(
                row=i, column=0, sticky=tk.W, padx=(5, 0), pady=5
            )
            ttk.Label(f, text=f"{field}:").grid(
                row=i, column=1, sticky=tk.NW, padx=5, pady=5
            )

            height = 3
            if field == "Details": height = 8
            if field in ("URLs", "Awards"): height = 5

            entry = tk.Text(f, width=60, height=height, font=("Segoe UI", 10))
            entry.grid(row=i, column=2, sticky=tk.EW, padx=5, pady=5)
            self.fields[field] = entry

    # ‚îÄ‚îÄ Flux principal ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def run_full_flow(self):
        """
        Point d'entr√©e : scrape les sources puis ouvre les 3 GUIs en s√©quence.
        """
        # Charger DB
        try:
            from services.db import PerformerDB
            db = PerformerDB()
            self._db_data      = db.get_performer_by_id(self.stash_id)
            self._stash_context = db.get_performer_context(self.stash_id)
            db.close()
        except Exception as e:
            messagebox.showerror("Erreur DB", f"Impossible de lire la base : {e}")
            return

        if not self._db_data:
            messagebox.showerror("Erreur", "Performer introuvable dans la base.")
            return

        # Donn√©es √† jour (DB + Phase 1)
        self._combined = copy.deepcopy(self._db_data)
        self._combined.update(self.phase1_data)

        performer_name = self._combined.get("name", "")
        known_urls     = self._combined.get("urls", [])

        # Afficher barre progression
        self._show_progress(f"Scraping en cours pour {performer_name}‚Ä¶")

        def _do_scraping():
            try:
                from services.phase2_scraper import Phase2ScraperService
                from services.phase2_merger  import Phase2Merger

                scraper = Phase2ScraperService()
                results = scraper.scrape(
                    performer_name,
                    known_urls=known_urls,
                    progress_callback=lambda s, m: self.after(
                        0, self.progress_label.config, {"text": f"[{s}] {m}"}
                    ),
                )
                self._scraped_results = results

                merger = Phase2Merger()
                self._merged_data = merger.merge(self._combined, results)

                self.after(0, self._on_scraping_done)

            except Exception as e:
                err = str(e)
                self.after(0, lambda: messagebox.showerror("Erreur scraping", err))
                self.after(0, self._hide_progress)

        threading.Thread(target=_do_scraping, daemon=True).start()

    def _on_scraping_done(self):
        self._hide_progress()
        checked_fields = [f for f, v in self.field_checkboxes.items() if v.get()]
        self._open_gui1(checked_fields)

    # ‚îÄ‚îÄ GUI 1 : DataReviewWindow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _open_gui1(self, checked_fields):
        from gui.data_review_window import DataReviewWindow

        win = DataReviewWindow(
            parent          = self.winfo_toplevel(),
            db_data         = self._combined,
            stash_ctx       = self._stash_context,
            merged_data     = self._merged_data,
            scraped_results = self._scraped_results or [],
            checked_fields  = checked_fields,
        )

        if win.result is None:
            # Annul√©
            return

        # Appliquer les s√©lections dans les champs UI de phase 2
        self._apply_review_result(win.result)
        self._open_gui2(checked_fields, win.result)

    # ‚îÄ‚îÄ GUI 2 : BioStudioWindow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _open_gui2(self, checked_fields, review_result):
        from gui.bio_studio_window import BioStudioWindow

        win = BioStudioWindow(
            parent          = self.winfo_toplevel(),
            db_data         = self._combined,
            stash_ctx       = self._stash_context,
            merged_data     = self._merged_data,
            scraped_results = self._scraped_results or [],
            checked_fields  = checked_fields,
            review_result   = review_result,
        )

        if win.result is None:
            # Retour ou annuler : retourner √† GUI 1
            self._open_gui1(checked_fields)
            return

        self._open_gui3(review_result, win.result)

    # ‚îÄ‚îÄ GUI 3 : ValidationWindow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _open_gui3(self, review_result, bio_result):
        from gui.validation_window import ValidationWindow

        win = ValidationWindow(
            parent        = self.winfo_toplevel(),
            db_data       = self._combined,
            stash_ctx     = self._stash_context,
            review_result = review_result,
            bio_result    = bio_result,
        )

        if win.result == "injected":
            messagebox.showinfo(
                "‚úÖ Succ√®s",
                "Les donn√©es ont √©t√© inject√©es dans Stash avec succ√®s.\n"
                "Vous pouvez continuer ou retourner au menu.",
            )
            # Rafra√Æchir l'affichage Phase 2 avec les nouvelles donn√©es
            self._refresh_display(review_result, bio_result)

        elif win.result is None:
            # Retour √† GUI 2 ‚Üí GUI 1 est perdue (simplification)
            # On relance seulement GUI 2 avec le m√™me review_result
            checked = [f for f, v in self.field_checkboxes.items() if v.get()]
            self._open_gui2(checked, review_result)

    # ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _apply_review_result(self, result: dict):
        """Met √† jour les widgets de la phase 2 avec les s√©lections GUI 1."""
        if not result:
            return

        mapping = {
            "trivia":    ("Trivia",   lambda x: x),
            "awards":    ("Awards",   lambda x: "\n".join(x) if x else ""),
            "tattoos":   ("Tattoos",  self._fmt_body_art),
            "piercings": ("Piercings",self._fmt_body_art),
            "tags":      ("Tags",     lambda x: ", ".join(x) if x else ""),
            "urls":      ("URLs",     lambda x: "\n".join(x.values()) if x else ""),
        }
        for key, (field, fmt) in mapping.items():
            val = result.get(key)
            if val is not None:
                self._update_field_display(field, fmt(val))
                self.phase2_data[key] = val

    def _fmt_body_art(self, items):
        if not items:
            return ""
        def fmt(it):
            pos  = it.get("position", "")
            desc = it.get("description", "")
            return f"{pos} ({desc})" if desc else pos
        return "; ".join(fmt(i) for i in items)

    def _refresh_display(self, review_result: dict, bio_result: dict):
        """Rafra√Æchit les champs apr√®s injection r√©ussie."""
        self._apply_review_result(review_result)
        bio = (bio_result or {}).get("bio", "")
        if bio:
            self._update_field_display("Details", bio)
            self.phase2_data["details"] = bio

    def _show_progress(self, message):
        self.progress_label.config(text=message)
        self.progress_frame.pack(fill=tk.X, padx=10, pady=2)
        self.progress_bar.start(10)

    def _hide_progress(self):
        self.progress_bar.stop()
        self.progress_frame.pack_forget()

    def _update_field_display(self, field_name: str, value: str):
        entry = self.fields.get(field_name)
        if entry and isinstance(entry, tk.Text):
            entry.config(state=tk.NORMAL)
            entry.delete("1.0", tk.END)
            if value:
                entry.insert("1.0", value)
            self.field_checkboxes.get(field_name, tk.BooleanVar()).set(bool(value))


============================================================
[43/124] Legacy\gui\phase1_conflict_dialog.py
------------------------------------------------------------
"""
Phase1ConflictDialog ‚Äî Dialogue de r√©solution des conflits Phase 1.
Affiche confirmations, nouveaux et conflits pour chaque champ coch√©.
"""
import tkinter as tk
from tkinter import ttk


# Code couleur par statut
STATUS_COLORS = {
    "confirmed": "#2ecc71",  # vert
    "new": "#3498db",        # bleu
    "conflict": "#e74c3c",   # rouge
    "empty": "#95a5a6",      # gris
}

STATUS_ICONS = {
    "confirmed": "‚úÖ",
    "new": "üÜï",
    "conflict": "‚ö†Ô∏è",
    "empty": "‚¨ú",
}


class Phase1ConflictDialog(tk.Toplevel):
    """
    Dialogue modal montrant le r√©sultat du scraping Phase 1.
    Pour chaque champ :
    - Confirm√© (vert) ‚Üí DB == source
    - Nouveau (bleu) ‚Üí DB vide, suggestion disponible
    - Conflit (rouge) ‚Üí DB ‚â† sources ‚Üí l'utilisateur choisit
    - Vide (gris) ‚Üí rien trouv√©
    """

    def __init__(self, parent, performer_name: str, merge_result: dict, db_mapping: dict):
        super().__init__(parent)
        self.performer_name = performer_name
        self.title(f"üîç R√©sultat scraping Phase 1 pour {performer_name}")
        self.merge_result = merge_result
        self.db_mapping = db_mapping
        self.result = None  # Dict final si valid√©

        self.geometry("800x600")
        self.minsize(750, 500)
        self.transient(parent)
        self.grab_set()

        # Variables de s√©lection par champ
        self.selections = {}

        self._build_ui()
        self.wait_window()

    def _build_ui(self):
        # ‚îÄ‚îÄ Compteurs en haut ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        header = ttk.Frame(self, padding=10)
        header.pack(fill=tk.X)

        counts = {"confirmed": 0, "new": 0, "conflict": 0, "empty": 0}
        for info in self.merge_result.values():
            counts[info["status"]] = counts.get(info["status"], 0) + 1

        summary = (
            f"‚úÖ Confirm√©s: {counts['confirmed']}  |  "
            f"üÜï Nouveaux: {counts['new']}  |  "
            f"‚ö†Ô∏è Conflits: {counts['conflict']}  |  "
            f"‚¨ú Vides: {counts['empty']}"
        )
        ttk.Label(header, text=summary, font=("Segoe UI", 11, "bold")).pack(anchor=tk.W)

        # ‚îÄ‚îÄ Zone scrollable ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        main = ttk.Frame(self)
        main.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        canvas = tk.Canvas(main, highlightthickness=0)
        scrollbar = ttk.Scrollbar(main, orient=tk.VERTICAL, command=canvas.yview)
        self.scroll_frame = ttk.Frame(canvas)

        self.scroll_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        canvas.create_window((0, 0), window=self.scroll_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        canvas.bind_all("<MouseWheel>",
                        lambda e: canvas.yview_scroll(int(-1 * (e.delta / 120)), "units"))

        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # ‚îÄ‚îÄ Lignes par champ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        for field_name, info in self.merge_result.items():
            self._build_field_row(field_name, info)

        # ‚îÄ‚îÄ Boutons ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        btn_frame = ttk.Frame(self, padding=10)
        btn_frame.pack(fill=tk.X)

        ttk.Button(btn_frame, text="‚úÖ Appliquer et continuer",
                   command=self._apply).pack(side=tk.RIGHT, padx=5)
        ttk.Button(btn_frame, text="‚ùå Annuler",
                   command=self.destroy).pack(side=tk.RIGHT, padx=5)

    def _build_field_row(self, field_name: str, info: dict):
        """Construire une ligne pour un champ."""
        status = info["status"]
        db_value = info.get("db_value") or ""
        scraped_values = info.get("scraped_values", {})
        suggestion = info.get("suggestion") or ""

        # Cadre principal du champ
        frame = ttk.Frame(self.scroll_frame, padding=(5, 3))
        frame.pack(fill=tk.X, padx=5, pady=2)

        # Ic√¥ne + Nom du champ
        icon = STATUS_ICONS.get(status, "")
        ttk.Label(frame, text=f"{icon} {field_name}", width=20, anchor=tk.W,
                  font=("Segoe UI", 10, "bold")).pack(side=tk.LEFT, padx=(0, 10))

        if status == "confirmed":
            # Tout va bien ‚Äî afficher simplement la valeur
            ttk.Label(frame, text=f"‚úì {db_value}",
                      font=("Segoe UI", 10)).pack(side=tk.LEFT, fill=tk.X, expand=True)
            self.selections[field_name] = tk.StringVar(value=db_value)
            
            # Afficher les sources qui confirment la valeur
            if scraped_values:
                sources = ", ".join(k.upper() for k in scraped_values.keys())
                ttk.Label(frame, text=f"[{sources}]", font=("Segoe UI", 8, "italic")).pack(side=tk.LEFT, padx=5)

        elif status == "empty":
            # Rien trouv√©
            ttk.Label(frame, text="(aucune donn√©e)",
                      font=("Segoe UI", 10, "italic")).pack(side=tk.LEFT)
            self.selections[field_name] = tk.StringVar(value="")

        elif status == "new":
            # Nouvelle valeur ‚Äî proposer la suggestion modifiable
            var = tk.StringVar(value=suggestion)
            self.selections[field_name] = var
            ttk.Label(frame, text="DB: (vide) ‚Üí",
                      font=("Segoe UI", 9, "italic")).pack(side=tk.LEFT, padx=(0, 5))
            entry = ttk.Entry(frame, textvariable=var, width=50)
            entry.pack(side=tk.LEFT, fill=tk.X, expand=True)
            # Source info
            sources = ", ".join(scraped_values.keys())
            ttk.Label(frame, text=f"[{sources}]",
                      font=("Segoe UI", 8)).pack(side=tk.LEFT, padx=5)

        elif status == "conflict":
            # Conflit ‚Äî radio buttons pour choisir
            self._build_conflict_section(frame, field_name, db_value, scraped_values, suggestion)

    def _build_conflict_section(self, parent, field_name, db_value, scraped_values, suggestion):
        """Construire la section de r√©solution de conflit."""
        # Sous-frame pour les options
        conflict_frame = ttk.Frame(parent)
        conflict_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        var = tk.StringVar(value=suggestion)
        self.selections[field_name] = var

        # Option 1 : garder la valeur DB
        ttk.Radiobutton(
            conflict_frame,
            text=f"DB: {db_value}",
            variable=var,
            value=db_value
        ).pack(anchor=tk.W)

        # Options : valeurs scrap√©es
        for source, val in scraped_values.items():
            ttk.Radiobutton(
                conflict_frame,
                text=f"{source.upper()}: {val}",
                variable=var,
                value=val
            ).pack(anchor=tk.W)

    def _apply(self):
        """Collecter les s√©lections et fermer."""
        self.result = {}
        for field_name, var in self.selections.items():
            val = var.get().strip()
            if val:
                self.result[field_name] = val
        self.destroy()


============================================================
[44/124] Legacy\gui\phase2_field_wizard.py
------------------------------------------------------------
"""
Phase2FieldWizard ‚Äî Dialogue pas-√†-pas pour la r√©solution des champs Phase 2.
Pr√©sente un champ √† la fois avec le contexte Stash.
"""
import tkinter as tk
from tkinter import ttk


# Ordre des pages du wizard
WIZARD_PAGES = [
    ("awards", "üèÜ AWARDS"),
    ("trivia", "üìù TRIVIA"),
    ("tattoos", "üé® TATTOOS"),
    ("piercings", "üíâ PIERCINGS"),
    ("tags", "üè∑Ô∏è TAGS"),
    ("urls", "üîó URLs"),
    ("details", "üìñ DETAILS (Bio)"),
]


class Phase2FieldWizard(tk.Toplevel):
    """
    Wizard pas-√†-pas : une page par champ Phase 2.
    Chaque page affiche le contexte Stash + les donn√©es scrap√©es.
    """

    def __init__(self, parent, merged_data: dict, stash_context: dict,
                 db_data: dict, scraped_results: list[dict] = None, checked_fields: list[str] | None = None):
        super().__init__(parent)
        self.title("üìã Wizard Phase 2 ‚Äî R√©solution pas-√†-pas")
        self.merged_data = merged_data
        self.stash_ctx = stash_context
        self.db_data = db_data
        self.scraped_results = scraped_results or []  # Stocker les r√©sultats bruts
        self.checked_fields = checked_fields or []   # ‚Üê AJOUTER
        self.result = None
        self.generated_bio = None

        self.geometry("950x700")
        self.minsize(900, 600)
        self.transient(parent)
        self.grab_set()

        # √âtat du wizard
        self.current_page = 0
        self.selections = {}

        # Construire les cadres
        self._build_shell()
        self._show_page(0)
        self.wait_window()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # STRUCTURE PRINCIPALE
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_shell(self):
        """Cr√©er la coquille du wizard (header, zone contenu, boutons nav)."""
        # ‚îÄ‚îÄ Header ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        self.header_frame = ttk.Frame(self, padding=10)
        self.header_frame.pack(fill=tk.X)

        self.page_title = ttk.Label(self.header_frame, text="",
                                     font=("Segoe UI", 14, "bold"))
        self.page_title.pack(side=tk.LEFT)

        self.page_counter = ttk.Label(self.header_frame, text="",
                                       font=("Segoe UI", 10))
        self.page_counter.pack(side=tk.RIGHT)

        # Progress bar
        self.progress = ttk.Progressbar(self, maximum=len(WIZARD_PAGES), length=400)
        self.progress.pack(fill=tk.X, padx=10, pady=(0, 5))

        # ‚îÄ‚îÄ Zone contenu scrollable ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        content_wrapper = ttk.Frame(self)
        content_wrapper.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        self.canvas = tk.Canvas(content_wrapper, highlightthickness=0)
        scrollbar = ttk.Scrollbar(content_wrapper, orient=tk.VERTICAL, command=self.canvas.yview)
        self.content_frame = ttk.Frame(self.canvas)

        self.content_frame.bind(
            "<Configure>",
            lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all"))
        )
        self.canvas_window = self.canvas.create_window((0, 0), window=self.content_frame, anchor="nw")
        self.canvas.configure(yscrollcommand=scrollbar.set)
        self.canvas.bind_all("<MouseWheel>",
                             lambda e: self.canvas.yview_scroll(int(-1 * (e.delta / 120)), "units"))

        # Resize canvas window width
        self.canvas.bind("<Configure>",
                         lambda e: self.canvas.itemconfig(self.canvas_window, width=e.width))

        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # ‚îÄ‚îÄ Boutons navigation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        nav_frame = ttk.Frame(self, padding=10)
        nav_frame.pack(fill=tk.X)

        self.btn_prev = ttk.Button(nav_frame, text="‚óÄ Pr√©c√©dent", command=self._prev_page)
        self.btn_prev.pack(side=tk.LEFT, padx=5)

        ttk.Button(nav_frame, text="‚ùå Annuler", command=self.destroy).pack(side=tk.LEFT, padx=5)

        self.btn_next = ttk.Button(nav_frame, text="Suivant ‚ñ∂", command=self._next_page)
        self.btn_next.pack(side=tk.RIGHT, padx=5)

        self.btn_skip = ttk.Button(nav_frame, text="‚è≠ Passer", command=self._skip_page)
        self.btn_skip.pack(side=tk.RIGHT, padx=5)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # NAVIGATION
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _prev_page(self):
        if self.current_page > 0:
            self._save_current()
            self.current_page -= 1
            self._show_page(self.current_page)

    def _next_page(self):
        self._save_current()
        if self.current_page < len(WIZARD_PAGES) - 1:
            self.current_page += 1
            self._show_page(self.current_page)
        else:
            self._finish()

    def _skip_page(self):
        """Passer sans sauvegarder le champ courant."""
        if self.current_page < len(WIZARD_PAGES) - 1:
            self.current_page += 1
            self._show_page(self.current_page)
        else:
            self._finish()

    def _show_page(self, idx):
        """Afficher la page √† l'index donn√©."""
        field_key, title = WIZARD_PAGES[idx]

        # MAJ header
        self.page_title.config(text=title)
        self.page_counter.config(text=f"√âtape {idx + 1} / {len(WIZARD_PAGES)}")
        self.progress["value"] = idx + 1

        # MAJ boutons
        self.btn_prev.config(state=tk.NORMAL if idx > 0 else tk.DISABLED)
        self.btn_next.config(text="‚úÖ Terminer" if idx == len(WIZARD_PAGES) - 1 else "Suivant ‚ñ∂")

        # Vider le contenu
        for w in self.content_frame.winfo_children():
            w.destroy()

        # Construire la page selon le champ
        builder = {
            "details": self._page_details,
            "awards": self._page_awards,
            "trivia": self._page_trivia,
            "tattoos": self._page_body_art,
            "piercings": self._page_body_art,
            "tags": self._page_tags,
            "urls": self._page_urls,
        }
        builder_fn = builder.get(field_key, lambda k: None)
        builder_fn(field_key)

        # Scroll en haut
        self.canvas.yview_moveto(0)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CONTEXTE STASH (affich√© sur chaque page)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _add_stash_context(self, parent):
        """Ajouter un panneau de contexte Stash."""
        ctx = self.stash_ctx
        if not ctx:
            return

        frame = ttk.LabelFrame(parent, text="üìä Contexte Stash", padding=8)
        frame.pack(fill=tk.X, padx=5, pady=5)

        info_parts = []
        if ctx.get("scene_count"):
            info_parts.append(f"üé¨ {ctx['scene_count']} sc√®nes")
        if ctx.get("studios"):
            studios_str = ", ".join(ctx["studios"][:10])
            if len(ctx["studios"]) > 10:
                studios_str += f" (+{len(ctx['studios']) - 10})"
            info_parts.append(f"üè¢ Studios: {studios_str}")
        if ctx.get("groups"):
            groups_str = ", ".join(ctx["groups"][:8])
            if len(ctx["groups"]) > 8:
                groups_str += f" (+{len(ctx['groups']) - 8})"
            info_parts.append(f"üìÄ Groups: {groups_str}")
        if ctx.get("collaborators"):
            top5 = [f"{c['name']} ({c['count']})" for c in ctx["collaborators"][:5]]
            info_parts.append(f"üë• Top collabs: {', '.join(top5)}")

        for part in info_parts:
            ttk.Label(frame, text=part, font=("Segoe UI", 9), wraplength=800).pack(
                anchor=tk.W, pady=1)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: DETAILS (Bio)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_details(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("details", {})
        by_source = data.get("by_source", {})
        fused = data.get("fused")
        db_bio = self.db_data.get("details") or self.db_data.get("bio") or ""

        # Contexte Stash
        self._add_stash_context(parent)

        # Valeur Stash actuelle
        if db_bio:
            stash_frame = ttk.LabelFrame(parent, text="üìã Bio actuelle (Stash)", padding=8)
            stash_frame.pack(fill=tk.X, padx=5, pady=5)
            stash_text = tk.Text(stash_frame, height=4, width=80, font=("Segoe UI", 9),
                                 wrap=tk.WORD, state=tk.DISABLED)
            stash_text.pack(fill=tk.X)
            stash_text.config(state=tk.NORMAL)
            stash_text.insert("1.0", db_bio)
            stash_text.config(state=tk.DISABLED)

        # Choix de source
        self._details_choice_frame = ttk.LabelFrame(parent, text="üîÑ Choisir la bio", padding=8)
        self._details_choice_frame.pack(fill=tk.X, padx=5, pady=5)

        options = {}
        if db_bio:
            options["stash"] = db_bio

        for source, text in by_source.items():
            options[source] = text

        if fused:
            options["_fused_"] = fused

        # Ajouter la bio IA si d√©j√† g√©n√©r√©e (persistance)
        if self.generated_bio:
            options["_ia_"] = self.generated_bio

        if not options:
            ttk.Label(self._details_choice_frame, text="Aucune bio disponible",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W)
            return

        # R√©cup√©rer la s√©lection pr√©c√©dente ou d√©faut
        prev = self.selections.get("details", {}).get("_choice")
        
        # S√©lection par d√©faut : IA si dispo, sinon Stash, sinon premi√®re source
        default = prev
        if not default:
            default = "_ia_" if self.generated_bio else ("stash" if db_bio else list(options.keys())[0])

        self._details_var = tk.StringVar(value=default)
        self._details_options = options

        for key, text in options.items():
            label = key.upper() if key != "_fused_" else "FUSION"
            length = len(text)
            ttk.Radiobutton(self._details_choice_frame, text=f"{label} ({length} car.)",
                           variable=self._details_var, value=key).pack(anchor=tk.W, padx=5, pady=2)

        # Preview
        self._details_preview = tk.Text(parent, height=8, width=80,
                                         font=("Segoe UI", 9), wrap=tk.WORD)
        self._details_preview.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)

        def update(*_):
            self._details_preview.delete("1.0", tk.END)
            self._details_preview.insert("1.0", options.get(self._details_var.get(), ""))

        self._details_var.trace_add("write", update)
        update()

        # NOUVEAU : Bouton g√©n√©ration IA
        gen_frame = ttk.LabelFrame(parent, text="‚ú® G√©n√©ration IA", padding=8)
        gen_frame.pack(fill=tk.X, padx=5, pady=5)

        self._bio_status = ttk.Label(gen_frame, text="Pr√™t (Auto)", font=("Segoe UI", 9, "italic"))
        self._bio_status.pack(side=tk.LEFT, padx=10)

        ttk.Button(
            gen_frame,
            text="üîÑ R√©g√©n√©rer (Gemini/Ollama)",
            command=self._run_bio_generation
        ).pack(side=tk.RIGHT, padx=5)

        # Lancement automatique si pas encore de bio IA
        if not self.generated_bio:
            self.after(500, self._run_bio_generation)
        else:
            self._bio_status.config(text="‚úÖ Bio IA d√©j√† disponible")

    def _run_bio_generation(self):
        """Lance la g√©n√©ration IA en thread."""
        import threading
        import copy
        self._bio_status.config(text="üöÄ G√©n√©ration auto en cours (Gemini > Ollama)...")

        def t():
            from services.bio_generator import BioGenerator
            gen = BioGenerator()

            # Utiliser les vrais champs coch√©s (Phase 1 + Phase 2 disponibles)
            all_checked = (
                self.checked_fields if self.checked_fields
                else list(self.merged_data.keys())
            )
            # Ajouter tous les champs Phase 1 potentiellement utiles.
            # Le g√©n√©rateur d√©cidera d'utiliser les specs techniques (Taille/Poids...) uniquement si les infos sont limit√©es.
            all_checked.extend(["Name", "Birthdate", "Height", "Weight", "Measurements", "Fake Tits", "Hair Color", "Eye Color", "Ethnicity", "Country", "Aliases", "Career Length"])
            
            # S'assurer que Awards et URLs sont coch√©s s'ils existent dans les donn√©es fusionn√©es
            all_checked.extend(["Awards", "URLs"])
            
            # Utiliser les donn√©es fusionn√©es, mais mettre √† jour avec les s√©lections utilisateur
            # faites dans les onglets pr√©c√©dents (puisque Details est maintenant √† la fin)
            effective_merged = copy.deepcopy(self.merged_data)
            if "awards" in self.selections:
                effective_merged.setdefault("awards", {})["merged"] = self.selections["awards"]["value"]
            
            # AJOUT : Injecter les URLs valid√©es (onglet 6) pour que l'IA connaisse les bons r√©seaux sociaux
            if "urls" in self.selections:
                effective_merged.setdefault("urls", {})["merged"] = self.selections["urls"]["value"]

            ctx = gen.build_context_from_v2(
                db_data=self.db_data,
                stash_ctx=self.stash_ctx,
                scraped_results=self.scraped_results,  # Passer les r√©sultats bruts
                merged_data=effective_merged,
                checked_fields=all_checked,
            )
            bio = gen.generate(ctx)

            def update():
                if bio:
                    self.generated_bio = bio
                    # Mise √† jour UI seulement si on est encore sur la page Details
current_key="***MASKED***"
                    if hasattr(self, "_details_options") and current_key == "details":
                        if "_ia_" not in self._details_options:
                            self._details_options["_ia_"] = bio
                            ttk.Radiobutton(
                                self._details_choice_frame,
                                text=f"ü§ñ IA GENERATED ({len(bio)} car.)",
                                variable=self._details_var,
                                value="_ia_"
                            ).pack(anchor=tk.W, padx=5, pady=2)
                        self._details_var.set("_ia_") # S√©lectionne et met √† jour la preview via trace
                    self._bio_status.config(text=f"‚úÖ Bio g√©n√©r√©e ({len(bio)} car.)")
                else:
                    self._bio_status.config(
                        text="‚ùå √âchec ‚Äî V√©rifier .gemini_key ou Ollama actif")

            self.after(0, update)

        threading.Thread(target=t, daemon=True).start()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: AWARDS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_awards(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("awards", {})
        merged = data.get("merged", [])
        sources = data.get("sources", {})

        self._add_stash_context(parent)

        if not merged:
            ttk.Label(parent, text="Aucun award trouv√©",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        # Info sources
        info_frame = ttk.LabelFrame(parent, text="üìä Sources", padding=8)
        info_frame.pack(fill=tk.X, padx=5, pady=5)
        source_info = "  ".join(f"[{s}: {len(a)}]" for s, a in sources.items() if a)
        ttk.Label(info_frame, text=source_info, font=("Segoe UI", 9)).pack(anchor=tk.W)

        # Liste cochable
        list_frame = ttk.LabelFrame(parent, text=f"üèÜ Awards trouv√©s ({len(merged)})", padding=8)
        list_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        prev = self.selections.get("awards", {}).get("_items")
        self._award_vars = []
        for i, award in enumerate(merged):
            checked = prev[i] if prev and i < len(prev) else True
            var = tk.BooleanVar(value=checked)
            self._award_vars.append((var, award))
            ttk.Checkbutton(list_frame, text=award, variable=var).pack(anchor=tk.W, padx=5)

        # Boutons tout cocher/d√©cocher
        btn_row = ttk.Frame(list_frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda: [v.set(True) for v, _ in self._award_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda: [v.set(False) for v, _ in self._award_vars]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: TRIVIA
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_trivia(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("trivia", {})
        by_source = data.get("by_source", {})

        self._add_stash_context(parent)

        if not by_source:
            ttk.Label(parent, text="Aucun trivia trouv√©",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        choice_frame = ttk.LabelFrame(parent, text="üìù Sources Trivia", padding=8)
        choice_frame.pack(fill=tk.X, padx=5, pady=5)

        prev = self.selections.get("trivia", {}).get("_choice")
        default = prev or list(by_source.keys())[0]
        self._trivia_var = tk.StringVar(value=default)
        self._trivia_sources = by_source

        for source, text in by_source.items():
            preview = text[:80] + "..." if len(text) > 80 else text
            ttk.Radiobutton(choice_frame, text=f"{source.upper()} ‚Äî {preview}",
                           variable=self._trivia_var, value=source).pack(anchor=tk.W, padx=5, pady=2)

        self._trivia_preview = tk.Text(parent, height=6, width=80,
                                        font=("Segoe UI", 9), wrap=tk.WORD)
        self._trivia_preview.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)

        def update(*_):
            self._trivia_preview.delete("1.0", tk.END)
            self._trivia_preview.insert("1.0", by_source.get(self._trivia_var.get(), ""))

        self._trivia_var.trace_add("write", update)
        update()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: TATTOOS / PIERCINGS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_body_art(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get(field_key, {})
        merged = data.get("merged", [])
        db_value = data.get("db_value", "")

        self._add_stash_context(parent)

        # Valeur actuelle Stash
        if db_value:
            stash_frame = ttk.LabelFrame(parent, text=f"üìã {field_key.title()} actuels (Stash)",
                                          padding=8)
            stash_frame.pack(fill=tk.X, padx=5, pady=5)
            ttk.Label(stash_frame, text=str(db_value), font=("Segoe UI", 9),
                      wraplength=800).pack(anchor=tk.W)

        if not merged:
            ttk.Label(parent, text=f"Aucun {field_key} trouv√©",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        list_frame = ttk.LabelFrame(parent, text=f"R√©sultat ({len(merged)} entr√©es)", padding=8)
        list_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        prev = self.selections.get(field_key, {}).get("_items")
        var_attr = f"_{field_key}_vars"
        vars_list = []
        for i, item in enumerate(merged):
            pos = item.get("position", "?")
            desc = item.get("description", "")
            label = f"{pos}" + (f" ({desc})" if desc else "")
            checked = prev[i] if prev and i < len(prev) else True
            var = tk.BooleanVar(value=checked)
            vars_list.append((var, item))
            ttk.Checkbutton(list_frame, text=label, variable=var).pack(anchor=tk.W, padx=5)

        setattr(self, var_attr, vars_list)

        btn_row = ttk.Frame(list_frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda vl=vars_list: [v.set(True) for v, _ in vl]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda vl=vars_list: [v.set(False) for v, _ in vl]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: TAGS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_tags(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("tags", {})
        merged = data.get("merged", [])
        sources = data.get("sources", {})
        stash_tags = self.db_data.get("tags", [])

        self._add_stash_context(parent)

        # Tags actuels Stash
        if stash_tags:
            stash_frame = ttk.LabelFrame(parent, text=f"üìã Tags actuels Stash ({len(stash_tags)})",
                                          padding=8)
            stash_frame.pack(fill=tk.X, padx=5, pady=5)
            ttk.Label(stash_frame, text=", ".join(stash_tags[:30]),
                      font=("Segoe UI", 9), wraplength=800).pack(anchor=tk.W)

        if not merged:
            ttk.Label(parent, text="Aucun tag scrap√©",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        # Info sources
        info = "  ".join(f"[{s}: {len(t)}]" for s, t in sources.items() if t)
        ttk.Label(parent, text=f"Union : {len(merged)} tags ‚Äî {info}",
                  font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, padx=10, pady=5)

        list_frame = ttk.LabelFrame(parent, text="üè∑Ô∏è Tags scrap√©s", padding=8)
        list_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        prev = self.selections.get("tags", {}).get("_items")
        self._tag_vars = []
        for i, tag in enumerate(merged):
            # Marquer si d√©j√† dans Stash
            in_stash = tag.lower() in [t.lower() for t in stash_tags]
            label = f"{'‚úì ' if in_stash else ''}{tag}"
            checked = prev[i] if prev and i < len(prev) else True
            var = tk.BooleanVar(value=checked)
            self._tag_vars.append((var, tag))
            ttk.Checkbutton(list_frame, text=label, variable=var).pack(anchor=tk.W, padx=5)

        btn_row = ttk.Frame(list_frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda: [v.set(True) for v, _ in self._tag_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda: [v.set(False) for v, _ in self._tag_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="Nouveaux seuls",
                   command=lambda: self._select_new_tags_only(stash_tags)).pack(side=tk.LEFT, padx=2)

    def _select_new_tags_only(self, stash_tags):
        """Cocher uniquement les tags qui ne sont PAS d√©j√† dans Stash."""
        stash_lower = {t.lower() for t in stash_tags}
        for var, tag in self._tag_vars:
            var.set(tag.lower() not in stash_lower)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: URLs
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_urls(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("urls", {})
        merged = data.get("merged", {})
        stash_urls = self.db_data.get("urls", [])

        self._add_stash_context(parent)

        # URLs actuelles Stash
        if stash_urls:
            stash_frame = ttk.LabelFrame(parent, text=f"üìã URLs actuelles Stash ({len(stash_urls)})",
                                          padding=8)
            stash_frame.pack(fill=tk.X, padx=5, pady=5)
            for url in stash_urls[:15]:
                ttk.Label(stash_frame, text=url, font=("Segoe UI", 9)).pack(anchor=tk.W)

        if not merged:
            ttk.Label(parent, text="Aucune URL scrap√©e",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        list_frame = ttk.LabelFrame(parent, text=f"üîó URLs scrap√©es ({len(merged)})", padding=8)
        list_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        prev = self.selections.get("urls", {}).get("_items")
        self._url_vars = []
        for i, (key, url) in enumerate(sorted(merged.items())):
            checked = True
            if prev:
                checked = prev.get(key, True)
            var = tk.BooleanVar(value=checked)
            self._url_vars.append((var, key, url))
            row = ttk.Frame(list_frame)
            row.pack(fill=tk.X, padx=5, pady=1)
            ttk.Checkbutton(row, variable=var).pack(side=tk.LEFT)
            ttk.Label(row, text=f"{key}:", width=12, anchor=tk.W,
                      font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
            ttk.Label(row, text=url, font=("Segoe UI", 9)).pack(side=tk.LEFT, fill=tk.X)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # SAUVEGARDE / FINITION
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _save_current(self):
        """Sauvegarder la s√©lection de la page courante."""
field_key="***MASKED***"

        if field_key == "details":
            if hasattr(self, "_details_var"):
                choice = self._details_var.get()
                text = self._details_options.get(choice, "")
                self.selections["details"] = {"_choice": choice, "value": text}

        elif field_key == "awards":
            if hasattr(self, "_award_vars"):
                items = [v.get() for v, _ in self._award_vars]
                selected = [a for v, a in self._award_vars if v.get()]
                self.selections["awards"] = {"_items": items, "value": selected}

        elif field_key == "trivia":
            if hasattr(self, "_trivia_var"):
                choice = self._trivia_var.get()
                text = self._trivia_sources.get(choice, "")
                self.selections["trivia"] = {"_choice": choice, "value": text}

        elif field_key in ("tattoos", "piercings"):
            var_attr = f"_{field_key}_vars"
            if hasattr(self, var_attr):
                vars_list = getattr(self, var_attr)
                items = [v.get() for v, _ in vars_list]
                selected = [item for v, item in vars_list if v.get()]
                self.selections[field_key] = {"_items": items, "value": selected}

        elif field_key == "tags":
            if hasattr(self, "_tag_vars"):
                items = [v.get() for v, _ in self._tag_vars]
                selected = [t for v, t in self._tag_vars if v.get()]
                self.selections["tags"] = {"_items": items, "value": selected}

        elif field_key == "urls":
            if hasattr(self, "_url_vars"):
                items = {k: v.get() for v, k, _ in self._url_vars}
                selected = {k: u for v, k, u in self._url_vars if v.get()}
                self.selections["urls"] = {"_items": items, "value": selected}

    def _finish(self):
        """Collecter toutes les s√©lections et fermer."""
        self._save_current()

        self.result = {}

        # Details
        det = self.selections.get("details", {})
        self.result["details"] = det.get("value")

        # Awards
        aw = self.selections.get("awards", {})
        self.result["awards"] = aw.get("value", [])

        # Trivia
        tr = self.selections.get("trivia", {})
        self.result["trivia"] = tr.get("value")

        # Tattoos
        tt = self.selections.get("tattoos", {})
        self.result["tattoos"] = tt.get("value", [])

        # Piercings
        pi = self.selections.get("piercings", {})
        self.result["piercings"] = pi.get("value", [])

        # Tags
        tg = self.selections.get("tags", {})
        self.result["tags"] = tg.get("value", [])

        # URLs
        ur = self.selections.get("urls", {})
        self.result["urls"] = ur.get("value", {})

        self.destroy()


============================================================
[45/124] Legacy\gui\phase2_merge_dialog.py
------------------------------------------------------------
"""
Phase2MergeDialog ‚Äî Fen√™tre modale de r√©solution des r√©sultats Phase 2.
Chaque champ a son propre mode d'affichage et de s√©lection.
"""
import tkinter as tk
from tkinter import ttk


class Phase2MergeDialog(tk.Toplevel):
    """
    Dialogue de fusion pour les champs Phase 2.
    Affiche les r√©sultats fusionn√©s et permet √† l'utilisateur
    de choisir/modifier avant application.
    """

    def __init__(self, parent, merged_data: dict):
        super().__init__(parent)
        self.title("üìã R√©sultats scraping Phase 2")
        self.merged_data = merged_data
        self.result = None  # Dict final si l'utilisateur valide

        # Configurer la fen√™tre
        self.geometry("900x700")
        self.minsize(800, 600)
        self.transient(parent)
        self.grab_set()

        # Variables de s√©lection
        self.selections = {}

        self._build_ui()
        self.wait_window()

    def _build_ui(self):
        # ‚îÄ‚îÄ Frame principal avec scroll ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        main = ttk.Frame(self, padding=10)
        main.pack(fill=tk.BOTH, expand=True)

        # Canvas + Scrollbar pour le contenu
        canvas = tk.Canvas(main, highlightthickness=0)
        scrollbar = ttk.Scrollbar(main, orient=tk.VERTICAL, command=canvas.yview)
        self.scroll_frame = ttk.Frame(canvas)

        self.scroll_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        canvas.create_window((0, 0), window=self.scroll_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        # Mousewheel scroll
        canvas.bind_all("<MouseWheel>",
                        lambda e: canvas.yview_scroll(int(-1 * (e.delta / 120)), "units"))

        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # ‚îÄ‚îÄ Sections par champ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        self._build_awards_section()
        self._build_trivia_section()
        self._build_details_section()
        self._build_body_art_section("tattoos", "üé® TATTOOS")
        self._build_body_art_section("piercings", "üíâ PIERCINGS")
        self._build_tags_section()
        self._build_urls_section()

        # ‚îÄ‚îÄ Boutons d'action ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        btn_frame = ttk.Frame(self, padding=10)
        btn_frame.pack(fill=tk.X)

        ttk.Button(btn_frame, text="‚úÖ Appliquer tout",
                   command=self._apply_all).pack(side=tk.RIGHT, padx=5)
        ttk.Button(btn_frame, text="‚ùå Annuler",
                   command=self.destroy).pack(side=tk.RIGHT, padx=5)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # AWARDS ‚Äî liste cochable
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_awards_section(self):
        data = self.merged_data.get("awards", {})
        merged = data.get("merged", [])
        sources = data.get("sources", {})

        frame = self._section_frame("üèÜ AWARDS")

        if not merged:
            ttk.Label(frame, text="Aucun award trouv√©").pack(anchor=tk.W)
            self.selections["awards"] = []
            return

        # Compteur par source
        source_info = "  ".join(f"[{s}: {len(a)}]" for s, a in sources.items() if a)
        ttk.Label(frame, text=source_info, font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, pady=(0, 5))

        # Liste cochable
        self.award_vars = []
        for award in merged:
            var = tk.BooleanVar(value=True)
            self.award_vars.append((var, award))
            ttk.Checkbutton(frame, text=award, variable=var).pack(anchor=tk.W, padx=10)

        # Boutons
        btn_row = ttk.Frame(frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout cocher",
                   command=lambda: [v.set(True) for v, _ in self.award_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Tout d√©cocher",
                   command=lambda: [v.set(False) for v, _ in self.award_vars]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TRIVIA ‚Äî s√©lecteur de source
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_trivia_section(self):
        data = self.merged_data.get("trivia", {})
        by_source = data.get("by_source", {})
        suggestion = data.get("suggestion")

        frame = self._section_frame("üìù TRIVIA")

        if not by_source:
            ttk.Label(frame, text="Aucun trivia trouv√©").pack(anchor=tk.W)
            self.selections["trivia"] = None
            return

        # Radio buttons pour chaque source
        self.trivia_var = tk.StringVar(value=list(by_source.keys())[0] if suggestion is None else
                                       next((k for k, v in by_source.items() if v == suggestion), ""))
        
        for source, text in by_source.items():
            preview = text[:100] + "..." if len(text) > 100 else text
            ttk.Radiobutton(frame, text=f"{source.upper()} ‚Äî {preview}",
                           variable=self.trivia_var, value=source).pack(anchor=tk.W, padx=10, pady=2)

        # Zone de pr√©visualisation
        self.trivia_preview = tk.Text(frame, height=4, width=80, font=("Segoe UI", 9), wrap=tk.WORD)
        self.trivia_preview.pack(fill=tk.X, padx=10, pady=5)
        
        # MAJ pr√©visualisation quand la s√©lection change
        def update_preview(*_):
            src = self.trivia_var.get()
            self.trivia_preview.delete("1.0", tk.END)
            self.trivia_preview.insert("1.0", by_source.get(src, ""))
        
        self.trivia_var.trace_add("write", update_preview)
        update_preview()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # DETAILS (Bio) ‚Äî radio source unique ou fusion
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_details_section(self):
        data = self.merged_data.get("details", {})
        by_source = data.get("by_source", {})
        fused = data.get("fused")

        frame = self._section_frame("üìñ DETAILS (Bio)")

        if not by_source:
            ttk.Label(frame, text="Aucune bio trouv√©e").pack(anchor=tk.W)
            self.selections["details"] = None
            return

        # Options
        self.details_var = tk.StringVar(value="freeones" if "freeones" in by_source else list(by_source.keys())[0])

        for source, text in by_source.items():
            length = len(text)
            ttk.Radiobutton(frame, text=f"{source.upper()} ({length} car.)",
                           variable=self.details_var, value=source).pack(anchor=tk.W, padx=10, pady=2)

        if fused:
            ttk.Radiobutton(frame, text=f"Fusion toutes sources ({len(fused)} car.)",
                           variable=self.details_var, value="_fused_").pack(anchor=tk.W, padx=10, pady=2)

        # Zone de pr√©visualisation
        self.details_preview = tk.Text(frame, height=6, width=80, font=("Segoe UI", 9), wrap=tk.WORD)
        self.details_preview.pack(fill=tk.X, padx=10, pady=5)

        def update_preview(*_):
            src = self.details_var.get()
            self.details_preview.delete("1.0", tk.END)
            if src == "_fused_":
                self.details_preview.insert("1.0", fused or "")
            else:
                self.details_preview.insert("1.0", by_source.get(src, ""))

        self.details_var.trace_add("write", update_preview)
        update_preview()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TATTOOS / PIERCINGS ‚Äî liste √©ditable
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_body_art_section(self, field: str, title: str):
        data = self.merged_data.get(field, {})
        merged = data.get("merged", [])

        frame = self._section_frame(title)

        if not merged:
            ttk.Label(frame, text=f"Aucun {field} trouv√©").pack(anchor=tk.W)
            self.selections[field] = []
            return

        # Info strat√©gie
        ttk.Label(frame, text=f"Merge auto (structur√© > flat) ‚Üí {len(merged)} entr√©es uniques",
                  font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, pady=(0, 5))

        # Liste cochable
        vars_list = []
        for item in merged:
            pos = item.get("position", "?")
            desc = item.get("description", "")
            label = f"{pos}" + (f" ({desc})" if desc else "")
            var = tk.BooleanVar(value=True)
            vars_list.append((var, item))
            ttk.Checkbutton(frame, text=label, variable=var).pack(anchor=tk.W, padx=10)

        setattr(self, f"{field}_vars", vars_list)

        # Boutons
        btn_row = ttk.Frame(frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda vl=vars_list: [v.set(True) for v, _ in vl]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda vl=vars_list: [v.set(False) for v, _ in vl]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TAGS ‚Äî liste filtrable
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_tags_section(self):
        data = self.merged_data.get("tags", {})
        merged = data.get("merged", [])
        sources = data.get("sources", {})

        frame = self._section_frame("üè∑Ô∏è TAGS")

        if not merged:
            ttk.Label(frame, text="Aucun tag trouv√©").pack(anchor=tk.W)
            self.selections["tags"] = []
            return

        # Compteur
        source_info = "  ".join(f"[{s}: {len(t)}]" for s, t in sources.items() if t)
        ttk.Label(frame, text=f"Union : {len(merged)} tags ‚Äî {source_info}",
                  font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, pady=(0, 5))

        # Filtre
        filter_frame = ttk.Frame(frame)
        filter_frame.pack(fill=tk.X, padx=10, pady=2)
        ttk.Label(filter_frame, text="Filtrer:").pack(side=tk.LEFT)
        self.tag_filter_var = tk.StringVar()
        filter_entry = ttk.Entry(filter_frame, textvariable=self.tag_filter_var, width=30)
        filter_entry.pack(side=tk.LEFT, padx=5)

        # Tags dans un frame scrollable
        tag_canvas = tk.Canvas(frame, height=150, highlightthickness=0)
        tag_scroll = ttk.Scrollbar(frame, orient=tk.VERTICAL, command=tag_canvas.yview)
        self.tag_inner = ttk.Frame(tag_canvas)

        self.tag_inner.bind("<Configure>",
                            lambda e: tag_canvas.configure(scrollregion=tag_canvas.bbox("all")))
        tag_canvas.create_window((0, 0), window=self.tag_inner, anchor="nw")
        tag_canvas.configure(yscrollcommand=tag_scroll.set)

        tag_scroll.pack(side=tk.RIGHT, fill=tk.Y)
        tag_canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=10)

        self.tag_vars = []
        for tag in merged:
            var = tk.BooleanVar(value=True)
            self.tag_vars.append((var, tag))
            ttk.Checkbutton(self.tag_inner, text=tag, variable=var).pack(anchor=tk.W)

        # Boutons
        btn_row = ttk.Frame(frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda: [v.set(True) for v, _ in self.tag_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda: [v.set(False) for v, _ in self.tag_vars]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # URLs ‚Äî tableau
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_urls_section(self):
        data = self.merged_data.get("urls", {})
        merged = data.get("merged", {})

        frame = self._section_frame("üîó URLs")

        if not merged:
            ttk.Label(frame, text="Aucune URL trouv√©e").pack(anchor=tk.W)
            self.selections["urls"] = {}
            return

        ttk.Label(frame, text=f"{len(merged)} URLs agr√©g√©es",
                  font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, pady=(0, 5))

        self.url_vars = []
        for key, url in sorted(merged.items()):
            var = tk.BooleanVar(value=True)
            self.url_vars.append((var, key, url))
            row = ttk.Frame(frame)
            row.pack(fill=tk.X, padx=10, pady=1)
            ttk.Checkbutton(row, variable=var).pack(side=tk.LEFT)
            ttk.Label(row, text=f"{key}:", width=12, anchor=tk.W,
                      font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
            ttk.Label(row, text=url, font=("Segoe UI", 9)).pack(side=tk.LEFT, fill=tk.X)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Helpers
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _section_frame(self, title: str) -> ttk.Frame:
        """Cr√©er un cadre de section avec titre."""
        sep = ttk.Separator(self.scroll_frame, orient=tk.HORIZONTAL)
        sep.pack(fill=tk.X, padx=5, pady=(10, 5))

        lbl = ttk.Label(self.scroll_frame, text=title, 
                        font=("Segoe UI", 11, "bold"))
        lbl.pack(anchor=tk.W, padx=10)

        frame = ttk.Frame(self.scroll_frame, padding=(10, 5))
        frame.pack(fill=tk.X, padx=5)
        return frame

    def _apply_all(self):
        """Collecter toutes les s√©lections et fermer."""
        self.result = {}

        # Awards
        if hasattr(self, 'award_vars'):
            self.result["awards"] = [a for v, a in self.award_vars if v.get()]
        else:
            self.result["awards"] = []

        # Trivia
        if hasattr(self, 'trivia_var'):
            src = self.trivia_var.get()
            by_source = self.merged_data.get("trivia", {}).get("by_source", {})
            self.result["trivia"] = by_source.get(src)
        else:
            self.result["trivia"] = None

        # Details
        if hasattr(self, 'details_var'):
            src = self.details_var.get()
            details_data = self.merged_data.get("details", {})
            if src == "_fused_":
                self.result["details"] = details_data.get("fused")
            else:
                self.result["details"] = details_data.get("by_source", {}).get(src)
        else:
            self.result["details"] = None

        # Tattoos
        if hasattr(self, 'tattoos_vars'):
            self.result["tattoos"] = [item for v, item in self.tattoos_vars if v.get()]
        else:
            self.result["tattoos"] = []

        # Piercings
        if hasattr(self, 'piercings_vars'):
            self.result["piercings"] = [item for v, item in self.piercings_vars if v.get()]
        else:
            self.result["piercings"] = []

        # Tags
        if hasattr(self, 'tag_vars'):
            self.result["tags"] = [t for v, t in self.tag_vars if v.get()]
        else:
            self.result["tags"] = []

        # URLs
        if hasattr(self, 'url_vars'):
            self.result["urls"] = {k: u for v, k, u in self.url_vars if v.get()}
        else:
            self.result["urls"] = {}

        self.destroy()


============================================================
[46/124] Legacy\gui\validation_window.py
------------------------------------------------------------
"""
ValidationWindow ‚Äî GUI 3/3 du flux Bio IA
==========================================
R√©capitulatif final avant injection dans Stash.

Layout :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  HEADER : performer + breadcrumb 3/3                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  R√âCAPITULATIF 40%      ‚îÇ  BIO FINALE 60%                            ‚îÇ
‚îÇ                         ‚îÇ                                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ ‚úÖ √Ä injecter ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îå‚îÄ‚îÄ ‚úçÔ∏è Biographie (√©ditable) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Trivia : ‚úì/‚úó      ‚îÇ ‚îÇ  ‚îÇ                                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Awards : N        ‚îÇ ‚îÇ  ‚îÇ  Grande zone de texte √©ditable        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Tatouages : N     ‚îÇ ‚îÇ  ‚îÇ  avec scroll                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Piercings : N     ‚îÇ ‚îÇ  ‚îÇ                                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Tags : N          ‚îÇ ‚îÇ  ‚îÇ                                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  URLs : N          ‚îÇ ‚îÇ  ‚îÇ                                       ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                         ‚îÇ  [üìã Copier] [üóë Effacer] [N chars]        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ ‚ö†Ô∏è Champs DB ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ                                            ‚îÇ
‚îÇ  ‚îÇ  Mapping injection ‚îÇ ‚îÇ                                            ‚îÇ
‚îÇ  ‚îÇ  avant/apr√®s       ‚îÇ ‚îÇ                                            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  [‚Üê Retour]  [‚úñ Annuler]              [‚úÖ INJECTER DANS STASH]       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""
import tkinter as tk
from tkinter import ttk, messagebox
import platform

# ‚îÄ‚îÄ Palette ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
P = {
    "bg":         "#13131f",
    "surface":    "#1e1e30",
    "card":       "#22223a",
    "card_hdr":   "#2c2c4a",
    "border":     "#3a3a58",
    "accent":     "#7c6af7",
    "success":    "#4caf7d",
    "success_dk": "#2a6a4a",
    "danger":     "#c05050",
    "warn":       "#d4a940",
    "text":       "#e8e8f5",
    "muted":      "#8888aa",
    "dim":        "#55557a",
    "bio_bg":     "#0d0d1e",
    "added":      "#1a3a1a",   # fond items ajout√©s
    "removed":    "#3a1a1a",   # fond items supprim√©s
}

FH1  = ("Segoe UI", 14, "bold")
FH2  = ("Segoe UI", 11, "bold")
FH3  = ("Segoe UI", 9,  "bold")
FB   = ("Segoe UI", 10)
FSM  = ("Segoe UI", 8)
FSMB = ("Segoe UI", 8,  "bold")


class ValidationWindow(tk.Toplevel):
    """
    Fen√™tre 3/3 : validation finale et injection dans Stash.
    """
    def __init__(self, parent, db_data, stash_ctx,
                 review_result, bio_result):
        super().__init__(parent)
        self.title("‚úÖ Validation & Injection ‚Äî √âtape 3/3")
        self.configure(bg=P["bg"])

        self.db_data       = db_data
        self.stash_ctx     = stash_ctx
        self.review_result = review_result   # dict depuis GUI 1
        self.bio_result    = bio_result      # dict {'bio': str} depuis GUI 2

        self.result        = None   # 'injected' | None

        _fullscreen(self)
        self.transient(parent)
        self.grab_set()

        self._build_ui()
        self._populate_summary()
        self._populate_bio()

        self.bind("<Escape>", lambda _: self._cancel())
        self.wait_window()

    # ‚îÄ‚îÄ UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_ui(self):
        self._build_header()

        body = tk.Frame(self, bg=P["bg"])
        body.pack(fill=tk.BOTH, expand=True, padx=8, pady=6)
        body.grid_columnconfigure(0, weight=38)
        body.grid_columnconfigure(1, weight=62)
        body.grid_rowconfigure(0, weight=1)

        self._build_summary_panel(body)
        self._build_bio_panel(body)

        self._build_footer()

    def _build_header(self):
        hdr = tk.Frame(self, bg=P["success"], pady=0)
        hdr.pack(fill=tk.X)
        tk.Frame(hdr, bg="#6adf9d", height=3).pack(fill=tk.X)

        row = tk.Frame(hdr, bg=P["success"], pady=7)
        row.pack(fill=tk.X, padx=12)

        tk.Label(row, text="‚úÖ", font=("Segoe UI", 18),
                 fg="white", bg=P["success"]).pack(side=tk.LEFT, padx=(0, 8))

        name = self.db_data.get("name", "Performer inconnu")
        tk.Label(row, text=f"Validation & Injection ‚Äî {name}",
                 font=FH1, fg="white", bg=P["success"]).pack(side=tk.LEFT)

        # Breadcrumb
        bc = tk.Frame(row, bg=P["success"])
        bc.pack(side=tk.RIGHT, padx=12)
        for num, lbl, active in [("1","Donn√©es",False),("2","Bio IA",False),("3","Valider",True)]:
            bg = P["success_dk"] if active else P["success"]
            fg = "white"         if active else "#aaffcc"
            tk.Label(bc, text=f" {num} ", font=FSMB, fg=fg,
                     bg=bg, padx=6, pady=3).pack(side=tk.LEFT, padx=1)
            tk.Label(bc, text=lbl, font=FSM, fg=fg,
                     bg=P["success"]).pack(side=tk.LEFT, padx=(0, 8))

    # ‚îÄ‚îÄ Panneau gauche : R√©capitulatif ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_summary_panel(self, parent):
        panel = tk.Frame(parent, bg=P["surface"])
        panel.grid(row=0, column=0, sticky="nsew", padx=(0, 5))

        _hdr(panel, "üìä R√©capitulatif des modifications")

        # Scrollable
        canvas = tk.Canvas(panel, bg=P["surface"], highlightthickness=0)
        sb     = ttk.Scrollbar(panel, orient=tk.VERTICAL, command=canvas.yview)
        self._summary_inner = tk.Frame(canvas, bg=P["surface"])

        self._summary_inner.bind("<Configure>", lambda e: canvas.configure(
            scrollregion=canvas.bbox("all")
        ))
        win = canvas.create_window((0, 0), window=self._summary_inner, anchor="nw")
        canvas.configure(yscrollcommand=sb.set)
        canvas.bind("<Configure>", lambda e: canvas.itemconfig(win, width=e.width))
        canvas.bind_all("<MouseWheel>",
                        lambda e: canvas.yview_scroll(int(-1*(e.delta/120)), "units"))

        sb.pack(side=tk.RIGHT, fill=tk.Y)
        canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

    def _populate_summary(self):
        parent = self._summary_inner
        rv = self.review_result or {}
        db = self.db_data

        def section(title, icon, items_new, items_old=None, text_mode=False):
            """Carte r√©capitulatif pour un champ."""
            card = tk.Frame(parent, bg=P["card"], pady=0)
            card.pack(fill=tk.X, padx=6, pady=4)
            tk.Frame(card, bg=P["accent"], width=3).pack(side=tk.LEFT, fill=tk.Y)

            inner = tk.Frame(card, bg=P["card"])
            inner.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

            # En-t√™te
            hf = tk.Frame(inner, bg=P["card_hdr"], pady=4)
            hf.pack(fill=tk.X)
            tk.Label(hf, text=f"  {icon}  {title}", font=FH3,
                     fg=P["text"], bg=P["card_hdr"], anchor="w").pack(side=tk.LEFT, padx=4)

            if text_mode:
                # Champ texte court
                n = len(items_new) if items_new else 0
                status = f"‚úì {n} chars" if n > 0 else "‚Äî vide"
                color  = P["success"] if n > 0 else P["muted"]
                tk.Label(hf, text=status, font=FSMB,
                         fg=color, bg=P["card_hdr"]).pack(side=tk.RIGHT, padx=8)
                if items_new:
                    preview = items_new[:120] + ("‚Ä¶" if len(items_new) > 120 else "")
                    tk.Label(inner, text=preview, font=FSM,
                             fg=P["muted"], bg=P["card"],
                             wraplength=320, justify="left",
                             anchor="w", padx=8, pady=4).pack(fill=tk.X)
            else:
                # Liste d'√©l√©ments
                n_new = len(items_new) if items_new else 0
                n_old = len(items_old) if items_old else 0
                added = n_new - n_old if n_new > n_old else 0
                status = f"+{added} ¬∑ total {n_new}" if added else f"{n_new} √©l√©ments"
                color  = P["success"] if n_new > 0 else P["muted"]
                tk.Label(hf, text=status, font=FSMB,
                         fg=color, bg=P["card_hdr"]).pack(side=tk.RIGHT, padx=8)

                body = tk.Frame(inner, bg=P["card"], padx=8, pady=4)
                body.pack(fill=tk.X)

                display = items_new[:8] if items_new else []
                for it in display:
                    label = it if isinstance(it, str) else \
                            (f"{it.get('position','')} ({it.get('description','')})"
                             if it.get("description") else it.get("position", str(it)))
                    tk.Label(body, text=f"  ‚úì {label}", font=FSM,
                             fg=P["text"], bg=P["card"], anchor="w").pack(fill=tk.X)
                if items_new and len(items_new) > 8:
                    tk.Label(body, text=f"  ‚Ä¶ +{len(items_new)-8} autres",
                             font=FSM, fg=P["muted"], bg=P["card"],
                             anchor="w").pack(fill=tk.X)
                if not items_new:
                    tk.Label(body, text="  ‚Äî aucun √©l√©ment", font=FSM,
                             fg=P["dim"], bg=P["card"], anchor="w").pack(fill=tk.X)

        # Sections
        section("Trivia",     "üìù", rv.get("trivia", ""),
                text_mode=True)
        section("Awards",     "üèÜ", rv.get("awards", []),
                items_old=db.get("awards", []))
        section("Tatouages",  "üé®", rv.get("tattoos", []),
                items_old=[])
        section("Piercings",  "üíâ", rv.get("piercings", []),
                items_old=[])
        section("Tags",       "üè∑Ô∏è", rv.get("tags", []),
                items_old=db.get("tags", []))

        # URLs : dict ‚Üí liste de strings
        urls_new = list((rv.get("urls") or {}).keys())
        urls_old = db.get("urls", [])
        section("URLs",       "üîó", urls_new, items_old=urls_old)

        # Bio
        bio = (self.bio_result or {}).get("bio", "")
        section("Biographie", "‚úçÔ∏è", bio, text_mode=True)

        # S√©parateur + info DB
        tk.Frame(parent, bg=P["border"], height=1).pack(fill=tk.X, padx=6, pady=8)

        info = tk.Frame(parent, bg=P["card"], padx=10, pady=8)
        info.pack(fill=tk.X, padx=6, pady=2)
        tk.Label(info, text="‚ÑπÔ∏è  Champs qui seront mis √† jour dans Stash :",
                 font=FH3, fg=P["muted"], bg=P["card"]).pack(anchor="w")
        fields = []
        if rv.get("trivia"):           fields.append("details (trivia)")
        if rv.get("awards"):           fields.append("custom_fields (awards)")
        if rv.get("tattoos"):          fields.append("tattoos")
        if rv.get("piercings"):        fields.append("piercings")
        if rv.get("tags"):             fields.append("tags")
        if rv.get("urls"):             fields.append("performer_urls")
        if (self.bio_result or {}).get("bio"): fields.append("details (bio)")
        for f in fields:
            tk.Label(info, text=f"  ‚Ä¢ {f}", font=FSM,
                     fg=P["text"], bg=P["card"]).pack(anchor="w")

    # ‚îÄ‚îÄ Panneau droit : Bio finale ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_bio_panel(self, parent):
        panel = tk.Frame(parent, bg=P["surface"])
        panel.grid(row=0, column=1, sticky="nsew", padx=(5, 0))

        _hdr(panel, "‚úçÔ∏è Biographie finale (√©ditable)")

        # Zone texte
        txt_f = tk.Frame(panel, bg=P["bio_bg"])
        txt_f.pack(fill=tk.BOTH, expand=True, padx=4, pady=(4, 0))

        self._txt_bio = tk.Text(
            txt_f, wrap=tk.WORD, font=("Segoe UI", 10),
            bg=P["bio_bg"], fg=P["text"],
            insertbackground=P["text"],
            relief=tk.FLAT, padx=14, pady=12,
            undo=True,
        )
        sb = ttk.Scrollbar(txt_f, command=self._txt_bio.yview)
        self._txt_bio.configure(yscrollcommand=sb.set)
        self._txt_bio.bind("<KeyRelease>", self._update_char)
        sb.pack(side=tk.RIGHT, fill=tk.Y)
        self._txt_bio.pack(fill=tk.BOTH, expand=True)

        # Barre sous l'√©diteur
        bio_bar = tk.Frame(panel, bg=P["surface"], pady=5)
        bio_bar.pack(fill=tk.X, padx=4)

        self._char_lbl = tk.Label(bio_bar, text="0 chars",
                                   font=FSM, fg=P["muted"], bg=P["surface"])
        self._char_lbl.pack(side=tk.RIGHT, padx=8)

        _mini_btn(bio_bar, "üìã Copier", self._copy_bio)
        _mini_btn(bio_bar, "üóë Effacer", self._clear_bio)
        _mini_btn(bio_bar, "‚Ü∫ Restaurer", self._restore_bio)

    def _populate_bio(self):
        bio = (self.bio_result or {}).get("bio", "")
        self._original_bio = bio
        if bio:
            self._txt_bio.insert("1.0", bio)
        else:
            self._txt_bio.insert("1.0",
                "Aucune biographie g√©n√©r√©e.\n\n"
                "Vous pouvez en saisir une manuellement ici, ou retourner √† l'√©tape 2.")
        self._update_char()

    def _update_char(self, event=None):
        n = len(self._txt_bio.get("1.0", tk.END).strip())
        color = P["success"] if 2500 <= n <= 4000 else \
                P["warn"]   if 100  <= n < 2500   else P["muted"]
        self._char_lbl.config(text=f"{n} chars", fg=color)

    def _copy_bio(self):
        text = self._txt_bio.get("1.0", tk.END).strip()
        if text:
            self.clipboard_clear()
            self.clipboard_append(text)

    def _clear_bio(self):
        if messagebox.askyesno("Effacer", "Effacer la biographie ?"):
            self._txt_bio.delete("1.0", tk.END)
            self._update_char()

    def _restore_bio(self):
        self._txt_bio.delete("1.0", tk.END)
        self._txt_bio.insert("1.0", self._original_bio or "")
        self._update_char()

    # ‚îÄ‚îÄ Footer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _build_footer(self):
        tk.Frame(self, bg=P["border"], height=1).pack(fill=tk.X)
        bar = tk.Frame(self, bg=P["surface"], pady=10)
        bar.pack(fill=tk.X, padx=12)

        _action_btn(bar, "‚Üê Retour",  P["dim"],    self._go_back, side=tk.LEFT)
        _action_btn(bar, "‚úñ Annuler", P["danger"], self._cancel,  side=tk.LEFT, padx=6)

        # Avertissement
        tk.Label(
            bar,
            text="‚ö†Ô∏è  L'injection modifie directement la base Stash ‚Äî op√©ration irr√©versible",
            font=FSM, fg=P["warn"], bg=P["surface"],
        ).pack(side=tk.LEFT, padx=20)

        _action_btn(bar, "‚úÖ  INJECTER DANS STASH",
                    P["success"], self._inject, side=tk.RIGHT, padx=8)

    # ‚îÄ‚îÄ Injection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _inject(self):
        bio = self._txt_bio.get("1.0", tk.END).strip()
        rv  = self.review_result or {}

        # R√©sum√© de confirmation
        parts = []
        if rv.get("trivia"):  parts.append(f"Trivia ({len(rv['trivia'])} chars)")
        if rv.get("awards"):  parts.append(f"{len(rv['awards'])} awards")
        if rv.get("tattoos"): parts.append(f"{len(rv['tattoos'])} tatouages")
        if rv.get("piercings"):parts.append(f"{len(rv['piercings'])} piercings")
        if rv.get("tags"):    parts.append(f"{len(rv['tags'])} tags")
        if rv.get("urls"):    parts.append(f"{len(rv['urls'])} URLs")
        if bio:               parts.append(f"Bio ({len(bio)} chars)")

        if not parts:
            messagebox.showwarning("Rien √† injecter",
                                   "Aucune donn√©e s√©lectionn√©e.")
            return

        confirm = messagebox.askyesno(
            "Confirmer l'injection",
            f"Vous allez injecter dans Stash :\n\n"
            + "\n".join(f"  ‚Ä¢ {p}" for p in parts)
            + f"\n\nPerformer : {self.db_data.get('name')}"
            + f"\nID : {self.db_data.get('id')}"
            + "\n\nCette action est irr√©versible. Continuer ?",
            icon="warning",
        )
        if not confirm:
            return

        try:
            self._do_injection(rv, bio)
            messagebox.showinfo(
                "‚úÖ Injection r√©ussie",
                f"Les donn√©es ont √©t√© inject√©es avec succ√®s dans Stash.\n\n"
                + "\n".join(f"  ‚úì {p}" for p in parts),
            )
            self.result = "injected"
            self.destroy()

        except Exception as e:
            messagebox.showerror(
                "Erreur d'injection",
                f"Une erreur est survenue lors de l'injection :\n\n{e}"
            )

    def _do_injection(self, rv: dict, bio: str):
        """Injecte les donn√©es dans la DB Stash."""
        from services.db import PerformerDB
        from utils.body_art_parser import parse_body_art

        performer_id = self.db_data.get("id")
        if not performer_id:
            raise ValueError("ID performer manquant")

        db = PerformerDB()
        try:
            updates = {}

            # ‚îÄ‚îÄ Bio / Details ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            if bio:
                updates["details"] = bio

            # ‚îÄ‚îÄ Tatouages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            tattoos = rv.get("tattoos", [])
            if tattoos:
                def _fmt_item(it):
                    pos  = it.get("position", "")
                    desc = it.get("description", "")
                    return f"{pos} ({desc})" if desc else pos
                updates["tattoos"] = "; ".join(_fmt_item(t) for t in tattoos)

            # ‚îÄ‚îÄ Piercings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            piercings = rv.get("piercings", [])
            if piercings:
                def _fmt_item(it):
                    pos  = it.get("position", "")
                    desc = it.get("description", "")
                    return f"{pos} ({desc})" if desc else pos
                updates["piercings"] = "; ".join(_fmt_item(p) for p in piercings)

            # ‚îÄ‚îÄ Appliquer updates performer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            if updates:
                db.update_performer(performer_id, updates)

            # ‚îÄ‚îÄ Tags ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            tags = rv.get("tags", [])
            if tags:
                db.update_performer_tags(performer_id, tags)

            # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            urls = rv.get("urls", {})
            if urls:
                url_list = list(urls.values())
                db.update_performer_urls(performer_id, url_list)

            # ‚îÄ‚îÄ Awards & Trivia ‚Üí custom fields ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            custom = []
            if rv.get("awards"):
                for a in rv["awards"]:
                    custom.append({"type": "award", "value": a})
            if rv.get("trivia"):
                custom.append({"type": "trivia", "value": rv["trivia"]})
            if custom:
                try:
                    from utils.customfield_utils import inject_custom_fields
                    inject_custom_fields(db, performer_id, custom)
                except Exception as e:
                    print(f"[Injection] Warning custom fields : {e}")

        finally:
            db.close()

    def _go_back(self):
        self.result = None
        self.destroy()

    def _cancel(self):
        self.result = None
        self.destroy()


# ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _fullscreen(win):
    if platform.system() == "Windows":
        win.state("zoomed")
    elif platform.system() == "Linux":
        try:
            win.attributes("-zoomed", True)
        except Exception:
            win.geometry(f"{win.winfo_screenwidth()}x{win.winfo_screenheight()}+0+0")
    else:
        win.attributes("-fullscreen", True)


def _hdr(parent, title):
    f = tk.Frame(parent, bg="#2c2c4a", pady=7)
    f.pack(fill=tk.X)
    tk.Label(f, text=f"  {title}", font=("Segoe UI", 11, "bold"),
             fg="#e8e8f5", bg="#2c2c4a", anchor="w").pack(side=tk.LEFT, padx=8)
    tk.Frame(parent, bg="#3a3a58", height=1).pack(fill=tk.X)


def _mini_btn(parent, text, cmd):
    b = tk.Button(parent, text=text, command=cmd,
                  font=("Segoe UI", 8), bg="#2c2c4a", fg="#e8e8f5",
                  relief=tk.FLAT, padx=8, pady=3, cursor="hand2",
                  activebackground="#3a3a58")
    b.pack(side=tk.LEFT, padx=3)


def _action_btn(parent, text, bg, cmd, side=tk.RIGHT, padx=6):
    b = tk.Button(parent, text=text, command=cmd,
                  font=("Segoe UI", 9, "bold"), bg=bg, fg="white",
                  relief=tk.FLAT, padx=16, pady=8, cursor="hand2",
                  activebackground=bg, activeforeground="white")
    b.pack(side=side, padx=padx)


============================================================
[47/124] Legacy\main.py
------------------------------------------------------------
from gui.launcher import start_launcher

if __name__ == "__main__":
    start_launcher()


============================================================
[48/124] Legacy\README.md
------------------------------------------------------------
# StashMaster V2

Gestion modulaire des m√©tadonn√©es performers (phase 1/phase 2).

- Phase 1 : m√©tadonn√©es usuelles (Nom, D√©sambigu√Øsation, Alias, Date de naissance, Date du d√©c√®s, Pays, Ethnicit√©, Couleur des cheveux, Couleur des yeux, Taille, Poids, Mensurations, Faux seins, Dur√©e de carri√®re)
- Phase 2 : champs avanc√©s (Bio, Trivia, Awards, Tattoos, Piercings, Tags, URLs, Details)

Structure pr√™te pour extensions (group, scene, etc.).


============================================================
[49/124] Legacy\requirements.txt
------------------------------------------------------------
# tkinter is part of stdlib, not a pip package
sv_ttk
requests
pillow
loguru
pydantic
pyyaml
lxml
beautifulsoup4
google-generativeai


============================================================
[50/124] Legacy\services\__init__.py
------------------------------------------------------------


============================================================
[51/124] Legacy\services\bio_generator.py
------------------------------------------------------------
"""
BioGenerator ‚Äî G√©n√®re une bio professionnelle via IA √† partir des donn√©es V2.
Utilise Gemini (API REST) ou Ollama (fallback local).
"""
import os
import json
import urllib.request

GEMINI_MODEL = "gemini-2.0-flash"  # Flash = moins cher, suffisant
GEMINI_API_URL="***MASKED***"

SYSTEM_PROMPT = """Tu es un r√©dacteur expert pour une base de donn√©es de films pour adultes.
Ton objectif est de r√©diger une biographie structur√©e et professionnelle en FRAN√áAIS (Qu√©bec) pour l'artiste, bas√©e sur les faits fournis.

Respecte scrupuleusement le format suivant (avec les √©mojis et le gras) :

### [Nom] : [Titre accrocheur]

**Introduction**
[Paragraphe introductif : identit√©, dates cl√©s, pseudonymes, r√©sum√© de carri√®re.]

**üìÖ Origines et Premiers Pas**
[D√©tails sur les origines, la jeunesse, et l'entr√©e dans l'industrie.]

**üèÜ Carri√®re et Filmographie**
[Parcours professionnel, studios majeurs, √©volution, sc√®nes notables.]

**üí° Faits Int√©ressants & Vie Personnelle**
[Vie hors cam√©ra, personnalit√©, anecdotes, r√©seaux sociaux.]

**üëó Apparence et Style**
[Attributs physiques, style, tatouages, piercings.]

**üèÜ Prix et Distinctions**
[R√©compenses et nominations. Si aucune, mentionner la popularit√©.]

**Conclusion rapide**
[Phrase de conclusion sur l'h√©ritage de l'artiste.]

R√®gles imp√©ratives :
- N'invente AUCUN fait. Base-toi uniquement sur les donn√©es fournies (contexte Stash, donn√©es scrap√©es).
- Si une information est manquante pour une section, r√©dige une phrase g√©n√©rale ou passe bri√®vement, mais garde la section.
- Tu DOIS inclure TOUTES les 7 sections (Introduction, Origines, Carri√®re, Faits, Apparence, Prix, Conclusion).
- Le ton doit √™tre informatif, fluide et agr√©able √† lire (style encyclop√©dique/journalistique).
- Utilise le fran√ßais standard du Qu√©bec.
"""


class BioGenerator:
    def __init__(self, gemini_key: str | None = None, ollama_url: str = "http://localhost:11434"):
        self.gemini_key = gemini_key or self._load_gemini_key()
        self.ollama_url = ollama_url

    def _load_gemini_key(self) -> str | None:
        # Chercher le .gemini_key √† la racine du projet
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
key_path="***MASKED***"

        if os.path.exists(key_path):
            with open(key_path, 'r') as f:
                return f.read().strip()
        
        # Fallback pour le chemin d'origine si n√©cessaire (peut √™tre retir√© plus tard)
        legacy_path = r"F:\V2\.gemini_key"
        if os.path.exists(legacy_path):
             with open(legacy_path, 'r') as f:
                return f.read().strip()

        return None

    def build_context_from_v2(
        self,
        db_data: dict,
        stash_ctx: dict,
        scraped_results: list[dict],
        merged_data: dict,
        checked_fields: list[str],
    ) -> dict:
        """
        Construit le contexte de g√©n√©ration depuis les donn√©es V2.
        
        Args:
            db_data:         donn√©es DB du performer (V2 PerformerDB)
            stash_ctx:       contexte Stash (sc√®nes, studios, collabs)
            scraped_results: r√©sultats bruts des extracteurs Phase 2
            merged_data:     r√©sultats fusionn√©s Phase2Merger
            checked_fields:  champs coch√©s par l'utilisateur
        """
        ctx = {
            "name":      db_data.get("name", "Unknown"),
            "birthdate": db_data.get("birthdate") if "Birthdate" in checked_fields else None,
            "details":   db_data.get("details"),
            "mini_bios": [],
            "trivia":    [],
            "scene_count":    stash_ctx.get("scene_count", 0),
            "studios":        stash_ctx.get("studios", [])[:10],
            "collaborators":  [c["name"] for c in stash_ctx.get("collaborators", [])[:5]],
            "awards":         [],
            "socials":        [],
            "career_length":  None,
        }

        # Bios scrap√©es (Details)
        if "Details" in checked_fields:
            for src, bio_text in merged_data.get("details", {}).get("by_source", {}).items():
                if bio_text and len(bio_text) > 50:
                    ctx["mini_bios"].append(f"[{src.upper()}] {bio_text[:600]}")

        # D√©tection "Infos limit√©es" : Si aucune bio textuelle n'est trouv√©e, on autorise plus de specs techniques
        limited_info_mode = (len(ctx["mini_bios"]) == 0)

        # Helper pour r√©cup√©rer une valeur (DB > Merged/Scraped)
        def get_val(key):
            v = db_data.get(key)
            if not v and isinstance(merged_data, dict):
                v = merged_data.get(key)
            
            # Fallback : Chercher dans les r√©sultats bruts du scraping
            if not v and scraped_results:
                for res in scraped_results:
                    if res.get(key):
                        v = res.get(key)
                        break
            return v

        # Career Length (Phase 1)
        if "Career Length" in checked_fields:
            val = get_val("career_length")
            if val: ctx["career_length"] = val

        # 1. Origines & Apparence de base (Toujours utile pour la narration)
        base_specs = []
        if "Ethnicity" in checked_fields:
            val = get_val("ethnicity")
            if val: base_specs.append(f"Ethnicity: {val}")
        if "Country" in checked_fields:
            val = get_val("country")
            if val: base_specs.append(f"Country: {val}")
        if "Hair Color" in checked_fields:
            val = get_val("hair_color")
            if val:
                if isinstance(val, list):
                    val = val[0] if val else ""
                val = str(val).split(",")[0].strip() # Garder couleur principale
                if val: base_specs.append(f"Hair: {val}")
        if "Eye Color" in checked_fields:
            val = get_val("eye_color")
            if val: base_specs.append(f"Eyes: {val}")
        
        if base_specs:
            ctx["trivia"].append("Appearance/Origins: " + ", ".join(base_specs))

        # 2. Specs Techniques (Seulement si infos limit√©es pour √©viter l'effet "fiche technique")
        specs = []
        tech_map = {
            "Height":       ("height", "Height"),
            "Weight":       ("weight", "Weight"),
            "Measurements": ("measurements", "Measurements"),
            "Fake Tits":    ("fake_tits", "Fake Tits"),
        }
        
        if limited_info_mode:
            for field, (db_key, label) in tech_map.items():
                if field in checked_fields and db_data.get(db_key):
                    specs.append(f"{label}: {db_data[db_key]}")
                if field in checked_fields:
                    val = get_val(db_key)
                    if val: specs.append(f"{label}: {val}")
            
            if specs:
                ctx["trivia"].append("Physical Stats (Use to flesh out bio if needed): " + ", ".join(specs))

        # Awards (depuis merged_data)
        if "Awards" in checked_fields:
            awards = merged_data.get("awards", {}).get("merged", [])
            if awards:
                ctx["awards"] = awards  # Stocker la liste compl√®te pour le prompt

        # Aliases
        if "Aliases" in checked_fields and db_data.get("aliases"):
            aliases = db_data["aliases"]
            if isinstance(aliases, list):
                aliases = ", ".join(aliases)
            ctx["trivia"].append(f"Aliases: {aliases}")

        # URLs (Socials)
        if "URLs" in checked_fields:
            urls_data = merged_data.get("urls", {}).get("merged", {})
            # Filtrer pour ne garder que les r√©seaux sociaux principaux
social_keys="***MASKED***"
            found_socials = []
            for k, v in urls_data.items():
                if any(s in k.lower() for s in social_keys):
                    # On envoie "Twitter: http..." pour que l'IA puisse extraire le handle si elle veut
                    found_socials.append(f"{k} ({v})")
            if found_socials:
                ctx["socials"] = found_socials

        return ctx

    def build_prompt(self, ctx: dict) -> str:
        """Construit le prompt utilisateur depuis le contexte."""
        parts = [f"Performer: {ctx['name']}"]

        if ctx.get("birthdate"):
            parts.append(f"Born: {ctx['birthdate']}")

        if ctx.get("career_length"):
            parts.append(f"Years Active: {ctx['career_length']}")

        if ctx.get("trivia"):
            parts.append("\nKnown facts:")
            parts.extend(f"  - {t}" for t in ctx["trivia"])

        if ctx.get("scene_count"):
            parts.append(f"\nStash scenes: {ctx['scene_count']}")

        if ctx.get("studios"):
            parts.append(f"Studios worked with: {', '.join(ctx['studios'][:8])}")

        if ctx.get("collaborators"):
            parts.append(f"Frequent collaborators: {', '.join(ctx['collaborators'])}")

        if ctx.get("awards"):
            parts.append("\nAwards & Nominations:")
            # On envoie les 20 premiers pour donner de la mati√®re √† la section 'Prix'
            parts.append("; ".join(ctx["awards"][:20]))

        if ctx.get("socials"):
            parts.append(f"\nSocial Media: {', '.join(ctx['socials'])}")

        if ctx.get("mini_bios"):
            parts.append("\nSource bios for reference:")
            for bio in ctx["mini_bios"][:3]:  # max 3 sources
                parts.append(f"  {bio}")

        if ctx.get("details"):
            parts.append(f"\nCurrent Stash bio: {ctx['details'][:400]}")

        parts.append("\nR√©dige la biographie en fran√ßais (Qu√©bec) selon le mod√®le.")
        return "\n".join(parts)

    def generate(self, ctx: dict) -> str | None:
        """G√©n√®re la bio. Essaie Gemini d'abord, Ollama en fallback."""
        prompt = self.build_prompt(ctx)

        if self.gemini_key:
            result = self._call_gemini(prompt)
            if result:
                return result.strip()

        return self._call_ollama(prompt)

    def _call_gemini(self, user_prompt: str) -> str | None:
        url = GEMINI_API_URL.format(model=GEMINI_MODEL, key=self.gemini_key)
        payload = {
            "system_instruction": {"parts": [{"text": SYSTEM_PROMPT}]},
            "contents": [{"parts": [{"text": user_prompt}]}],
            "generationConfig": {
                "temperature": 0.7,
                "maxOutputTokens": 300,
            }
        }
        try:
            data = json.dumps(payload).encode()
            req = urllib.request.Request(url, data=data,
                                         headers={"Content-Type": "application/json"})
            with urllib.request.urlopen(req, timeout=20) as resp:
                result = json.loads(resp.read())
            return result["candidates"][0]["content"]["parts"][0]["text"]
        except Exception as e:
            print(f"[BioGenerator] Gemini error: {e}")
            return None

    def _call_ollama(self, user_prompt: str, model: str = "dolphin-llama3") -> str | None:
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user",   "content": user_prompt},
            ],
            "stream": False,
            "options": {"temperature": 0.7, "num_predict": 300}
        }
        try:
            data = json.dumps(payload).encode()
            req = urllib.request.Request(
                f"{self.ollama_url}/api/chat", data=data,
                headers={"Content-Type": "application/json"}
            )
            with urllib.request.urlopen(req, timeout=30) as resp:
                result = json.loads(resp.read())
            return result["message"]["content"]
        except Exception as e:
            print(f"[BioGenerator] Ollama error: {e}")
            return None


============================================================
[52/124] Legacy\services\db.py
------------------------------------------------------------
import sqlite3

DB_PATH = r"H:\Stash\stash-go.sqlite"

class PerformerDB:
    def __init__(self, db_path=DB_PATH):
        self.conn = sqlite3.connect(db_path)
        self.conn.row_factory = sqlite3.Row

    def get_performer_by_id(self, performer_id):
        cur = self.conn.cursor()
        cur.execute("SELECT * FROM performers WHERE id=?", (performer_id,))
        row = cur.fetchone()
        if not row:
            return None
        data = dict(row)
        # Extraire les alias
        try:
            cur.execute("SELECT alias FROM performer_aliases WHERE performer_id=?", (performer_id,))
            aliases_rows = cur.fetchall()
            aliases = [r['alias'] for r in aliases_rows]
            print(f"DEBUG: Found aliases for ID {performer_id}: {aliases}")
        except Exception as e:
            print(f"DEBUG: Error fetching aliases: {e}")
            aliases = []
        data["aliases"] = aliases
        # Extraire les URLs
        try:
            cur.execute("SELECT url FROM performer_urls WHERE performer_id=?", (performer_id,))
            urls_rows = cur.fetchall()
            urls = [r['url'] for r in urls_rows]
            data["urls"] = urls
        except Exception as e:
            print(f"DEBUG: Error fetching URLs: {e}")
            data["urls"] = []

        # Extraire les tags
        try:
            # Jointure avec la table tags pour obtenir les noms
            query = """
                SELECT t.name 
                FROM tags t
                JOIN performers_tags pt ON pt.tag_id = t.id
                WHERE pt.performer_id = ?
            """
            cur.execute(query, (performer_id,))
            tags_rows = cur.fetchall()
            tags = [r['name'] for r in tags_rows]
            data["tags"] = tags
        except Exception as e:
            print(f"DEBUG: Error fetching tags: {e}")
            data["tags"] = []

        # Extraire la biographie (d√©tails)
        data["bio"] = data.get("details", "")

        return data

    def get_performer_context(self, performer_id):
        """Extraire le contexte Stash complet d'un performer."""
        cur = self.conn.cursor()
        ctx = {"groups": [], "studios": [], "collaborators": [], "scene_count": 0}

        # Nombre de sc√®nes
        try:
            cur.execute(
                "SELECT COUNT(*) as cnt FROM performers_scenes WHERE performer_id=?",
                (performer_id,),
            )
            row = cur.fetchone()
            ctx["scene_count"] = row["cnt"] if row else 0
        except Exception:
            pass

        # Groups (DVDs) via performers_scenes ‚Üí groups_scenes ‚Üí groups
        try:
            cur.execute(
                """
                SELECT DISTINCT g.name
                FROM groups g
                JOIN groups_scenes gs ON gs.group_id = g.id
                JOIN performers_scenes ps ON ps.scene_id = gs.scene_id
                WHERE ps.performer_id = ?
                ORDER BY g.name
                """,
                (performer_id,),
            )
            ctx["groups"] = [r["name"] for r in cur.fetchall()]
        except Exception:
            pass

        # Studios via scenes
        try:
            cur.execute(
                """
                SELECT DISTINCT st.name
                FROM studios st
                JOIN scenes s ON s.studio_id = st.id
                JOIN performers_scenes ps ON ps.scene_id = s.id
                WHERE ps.performer_id = ?
                ORDER BY st.name
                """,
                (performer_id,),
            )
            ctx["studios"] = [r["name"] for r in cur.fetchall()]
        except Exception:
            pass

        # Top collaborateurs
        try:
            cur.execute(
                """
                SELECT p2.name, COUNT(*) as cnt
                FROM performers_scenes ps1
                JOIN performers_scenes ps2 ON ps1.scene_id = ps2.scene_id
                JOIN performers p2 ON p2.id = ps2.performer_id
                WHERE ps1.performer_id = ? AND ps2.performer_id != ?
                GROUP BY p2.id
                ORDER BY cnt DESC
                LIMIT 20
                """,
                (performer_id, performer_id),
            )
            ctx["collaborators"] = [
                {"name": r["name"], "count": r["cnt"]} for r in cur.fetchall()
            ]
        except Exception:
            pass

        return ctx

    def get_known_performers(self):
        """Retourne une liste de tous les noms de performers connus."""
        cur = self.conn.cursor()
        cur.execute("SELECT name FROM performers ORDER BY name")
        rows = cur.fetchall()
        return [r['name'] for r in rows]

    def close(self):
        self.conn.close()

    def inject_performer_metadata(self, performer_id: int, updates: dict) -> None:
        """
        Met √† jour les champs Phase 2 d'un performer.
        G√®re : details, tattoos, piercings, awards, trivia, tags, urls.
        """
        cur = self.conn.cursor()

        # Champs texte directs
        DIRECT = {"details", "tattoos", "piercings", "trivia", "death_date", "awards"}
        direct = {k: v for k, v in updates.items() if k in DIRECT and v is not None}
        if direct:
            set_clause = ", ".join(f"{k}=?" for k in direct)
            vals = list(direct.values()) + [performer_id]
            cur.execute(
                f"UPDATE performers SET {set_clause}, updated_at=datetime('now') WHERE id=?",
                vals
            )

        # URLs ‚Üí table performer_urls
        for url in updates.get("urls", []):
            cur.execute(
                "SELECT COUNT(*) FROM performer_urls WHERE performer_id=? AND url=?",
                (performer_id, url)
            )
            if cur.fetchone()[0] == 0:
                cur.execute(
                    "SELECT COALESCE(MAX(position),-1)+1 FROM performer_urls WHERE performer_id=?",
                    (performer_id,)
                )
                pos = cur.fetchone()[0]
                cur.execute(
                    "INSERT INTO performer_urls (performer_id, position, url) VALUES (?,?,?)",
                    (performer_id, pos, url)
                )

        # Tags ‚Üí table performers_tags (get-or-create)
        for tag_name in updates.get("tags", []):
            cur.execute("SELECT id FROM tags WHERE name=?", (tag_name,))
            row = cur.fetchone()
            tag_id = row[0] if row else None
            if not tag_id:
                cur.execute(
                    "INSERT INTO tags (name, created_at, updated_at, ignore_auto_tag) "
                    "VALUES (?,datetime('now'),datetime('now'),0)",
                    (tag_name,)
                )
                tag_id = cur.lastrowid
            cur.execute(
                "SELECT COUNT(*) FROM performers_tags WHERE performer_id=? AND tag_id=?",
                (performer_id, tag_id)
            )
            if cur.fetchone()[0] == 0:
                cur.execute(
                    "INSERT INTO performers_tags (performer_id, tag_id) VALUES (?,?)",
                    (performer_id, tag_id)
                )

        self.conn.commit()

class GroupDB:
    def __init__(self, db_path=DB_PATH):
        self.conn = sqlite3.connect(db_path)
        self.conn.row_factory = sqlite3.Row

    def get_group_by_id(self, group_id):
        cur = self.conn.cursor()
        cur.execute("SELECT g.*, s.name as studio_name FROM groups g LEFT JOIN studios s ON g.studio_id = s.id WHERE g.id=?", (group_id,))
        row = cur.fetchone()
        if not row:
            return None
        data = dict(row)

        # Extraire les URLs
        try:
            cur.execute("SELECT url FROM group_urls WHERE group_id=?", (group_id,))
            urls_rows = cur.fetchall()
            urls = [r['url'] for r in urls_rows]
            data["urls"] = urls
        except Exception as e:
            print(f"DEBUG: Error fetching group URLs: {e}")
            data["urls"] = []

        # Extraire les tags (√âtiquettes)
        try:
            query = """
                SELECT t.name 
                FROM tags t
                JOIN groups_tags gt ON gt.tag_id = t.id
                WHERE gt.group_id = ?
            """
            cur.execute(query, (group_id,))
            tags_rows = cur.fetchall()
            tags = [r['name'] for r in tags_rows]
            data["tags"] = tags
        except Exception as e:
            print(f"DEBUG: Error fetching group tags: {e}")
            data["tags"] = []

        return data

    def get_group_scenes(self, group_id: int) -> list[dict]:
        """
        R√©cup√®re les sc√®nes associ√©es √† un groupe, y compris leurs URLs.
        """
        cur = self.conn.cursor()
        query = """
            SELECT
                s.id AS scene_id,
                gs.scene_index,
                s.title AS scene_title,
                GROUP_CONCAT(su.url) AS existing_urls
            FROM
                groups_scenes gs
            JOIN
                scenes s ON gs.scene_id = s.id
            LEFT JOIN
                scene_urls su ON s.id = su.scene_id
            WHERE
                gs.group_id = ?
            GROUP BY
                s.id, gs.scene_index, s.title
            ORDER BY
                gs.scene_index
        """
        cur.execute(query, (group_id,))
        rows = cur.fetchall()
        
        scenes = []
        for row in rows:
            scene = dict(row)
            scene['existing_urls'] = scene['existing_urls'].split(',') if scene['existing_urls'] else []
            scenes.append(scene)
        return scenes

    def inject_scene_urls(self, scene_urls_to_inject: list[dict]) -> None:
        """
        Injecte les URLs de sc√®nes dans la base de donn√©es.
        Args:
            scene_urls_to_inject: Liste de dicts avec {'scene_id': int, 'url': str, 'source': str}
        """
        cur = self.conn.cursor()
        for item in scene_urls_to_inject:
            scene_id = item['scene_id']
            url = item['url']
            
            # V√©rifier si l'URL existe d√©j√† pour cette sc√®ne
            cur.execute(
                "SELECT COUNT(*) FROM scene_urls WHERE scene_id=? AND url=?",
                (scene_id, url)
            )
            if cur.fetchone()[0] == 0:
                cur.execute(
                    "INSERT INTO scene_urls (scene_id, url) VALUES (?,?)",
                    (scene_id, url)
                )
        self.conn.commit()


    def close(self):
        self.conn.close()


============================================================
[53/124] Legacy\services\extractors\__init__.py
------------------------------------------------------------


============================================================
[54/124] Legacy\services\extractors\babepedia.py
------------------------------------------------------------
"""
Extracteur Babepedia ‚Äî source pour bio courte, tattoos/piercings
avec parsing am√©lior√©, et tags.
"""
import re

from services.extractors.base import BaseExtractor
from utils.body_art_parser import parse_body_art


class BabepediaExtractor(BaseExtractor):
    SOURCE_NAME = "babepedia"

    def build_url(self, performer_name: str) -> str | None:
        """Construire l'URL Babepedia depuis le nom."""
        # Babepedia utilise le nom with espaces ‚Üí underscores
        slug = self._normalize_name_underscore(performer_name)
        return f"https://www.babepedia.com/babe/{slug}"

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)

        tree = self._fetch_tree(url)
        if tree is None:
            return data

        # ‚îÄ‚îÄ Phase 1 ‚Äî M√©tadonn√©es ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["name"] = self._get_text(tree, '//h1/text()') or self._get_text(tree, '//h1//text()')
        
        aliases_text = self._get_stat(tree, "Aliases") or self._get_stat(tree, "Also known as")
        if aliases_text:
            data["aliases"] = [a.strip() for a in aliases_text.split(',') if a.strip()]

        data["birthdate"] = self._get_stat(tree, "Born") or self._get_stat(tree, "Birthday")
        data["country"] = self._get_stat(tree, "Birthplace")
        data["ethnicity"] = self._get_stat(tree, "Ethnicity")
        data["hair_color"] = self._get_stat(tree, "Hair color") or self._get_stat(tree, "Hair")
        data["eye_color"] = self._get_stat(tree, "Eye color") or self._get_stat(tree, "Eyes")
        data["height"] = self._get_stat(tree, "Height")
        data["weight"] = self._get_stat(tree, "Weight")
        data["measurements"] = self._get_stat(tree, "Measurements")
        data["fake_tits"] = self._get_stat(tree, "Boobs")

        # ‚îÄ‚îÄ Bio / Details ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        bio_xpaths = [
            '//p[@id="biotext"]/text()',
            '//div[@id="bio"]//text()',
            '//div[contains(@class,"bio")]//text()',
        ]
        for xpath in bio_xpaths:
            texts = tree.xpath(xpath)
            if texts:
                bio = " ".join(t.strip() for t in texts if t.strip())
                if len(bio) > 20:
                    data["details"] = bio
                    break

        # ‚îÄ‚îÄ Tattoos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tattoo_text = self._get_stat(tree, "Tattoos")
        data["tattoos"] = parse_body_art(tattoo_text) if tattoo_text else []

        # ‚îÄ‚îÄ Piercings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        piercing_text = self._get_stat(tree, "Piercings")
        data["piercings"] = parse_body_art(piercing_text) if piercing_text else []

# ‚îÄ‚îÄ Tags ‚Äî strat√©gie multi-fallback ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tags = []

        # Strat√©gie 1 : meta keywords (le plus fiable)
        meta_kw = tree.xpath('//meta[@name="keywords"]/@content')
        if meta_kw and meta_kw[0].strip():
            tags = [t.strip() for t in meta_kw[0].split(',')
                    if t.strip() and len(t.strip()) > 2]

        # Strat√©gie 2 : liens /tag/ ou /category/ dans la page
        if not tags:
            raw = tree.xpath(
                '//a[contains(@href,"/tag/") or contains(@href,"/category/")]/text()'
            )
            tags = [t.strip() for t in raw if t.strip() and len(t.strip()) > 2]

        # Strat√©gie 3 : balises sp√©cifiques Babepedia
        if not tags:
            raw = tree.xpath('//span[@class="tag"]/text() | //div[@class="tags"]//text()')
            tags = [t.strip() for t in raw if t.strip() and len(t.strip()) > 2]

        data["tags"] = list(dict.fromkeys(tags))[:50]  # d√©dupliquer, limiter √† 50

        # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["urls"]["babepedia"] = url

        # Extraire liens vers d'autres databases
        links = tree.xpath('//a[contains(@href, "http")]/@href')
        for link in links:
            link = link.strip()
            if "iafd.com" in link:
                data["urls"]["iafd"] = link
            elif "freeones.com" in link:
                data["urls"]["freeones"] = link
            elif "thenude" in link:
                data["urls"]["thenude"] = link
            elif "twitter.com" in link or "x.com" in link:
                data["urls"]["twitter"] = link
            elif "instagram.com" in link:
                data["urls"]["instagram"] = link

        return data

    def _get_stat(self, tree, label: str) -> str | None:
        """Extraire une stat de Babepedia (format tableau)."""
        xpaths = [
            f'//td[contains(text(), "{label}")]/following-sibling::td[1]',
            f'//span[contains(text(), "{label}")]/following-sibling::span[1]',
            f'//li[contains(., "{label}")]',
            f'//div[contains(@class,"stat")]//*[contains(text(), "{label}")]/..',
        ]
        for xpath in xpaths:
            val = self._get_text(tree, xpath)
            if val:
                # Nettoyer le label du texte si pr√©sent
                val = re.sub(rf'^{label}\s*[:]\s*', '', val, flags=re.I).strip()
                if val:
                    return val
        return None


============================================================
[55/124] Legacy\services\extractors\base.py
------------------------------------------------------------
import re
import subprocess
from abc import ABC, abstractmethod

from lxml import html as lxml_html

from services.scrape_cache import ScrapeCache


class BaseExtractor(ABC):
    """
    Base commune √† tous les extracteurs de donn√©es performer.
    Chaque sous-classe impl√©mente extract_from_url() qui retourne
    un dict unifi√© Phase 2.
    """

    SOURCE_NAME: str = "base"
    HEADERS = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept-Language": "en-US,en;q=0.9",
    }
    TIMEOUT = 15

    # ‚îÄ‚îÄ M√©thode principale ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    @abstractmethod
    def extract_from_url(self, url: str) -> dict:
        """
        Extraire les donn√©es Phase 1 + Phase 2 depuis une URL.
        
        Retourne un dict unifi√© avec champs Phase 1 (m√©tadonn√©es)
        et Phase 2 (awards, trivia, details, tattoos, piercings, tags, urls).
        """
        ...

    def build_url(self, performer_name: str) -> str | None:
        """
        Construire l'URL d'un performer √† partir de son nom.
        √Ä surcharger dans les sous-classes.
        Retourne None si pas de logique de construction disponible.
        """
        return None

    # ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _empty_result(self, url: str) -> dict:
        """Retourner un r√©sultat vide avec les m√©tadonn√©es source."""
        return {
            "_source": self.SOURCE_NAME,
            "_url": url,
            # Phase 1 ‚Äî m√©tadonn√©es
            "name": None,
            "aliases": [],
            "birthdate": None,
            "death_date": None,
            "country": None,
            "ethnicity": None,
            "hair_color": None,
            "eye_color": None,
            "height": None,
            "weight": None,
            "measurements": None,
            "fake_tits": None,
            "career_length": None,
            # Phase 2 ‚Äî champs avanc√©s
            "awards": [],
            "trivia": None,
            "details": None,
            "tattoos": [],
            "piercings": [],
            "tags": [],
            "urls": {},
        }

    def _fetch_tree(self, url: str) -> lxml_html.HtmlElement | None:
        """
        T√©l√©charger une page HTML et retourner l'arbre lxml.
        Utilise le ScrapeCache si disponible.
        """
        try:
            command = ['curl.exe', '-L', '-s', '-A', self.HEADERS['User-Agent'], url]
            # Ex√©cuter curl et capturer la sortie binaire brute
            result = subprocess.run(command, capture_output=True, check=True, timeout=self.TIMEOUT)
            
            # Laisser lxml analyser le contenu binaire, il auto-d√©tectera l'encodage
            return lxml_html.fromstring(result.stdout)
        except subprocess.CalledProcessError as e:
            # D√©coder stderr pour l'impression, avec un fallback s√ªr
            error_message = e.stderr.decode('utf-8', errors='replace') if e.stderr else ''
            print(f"[{self.SOURCE_NAME}] Erreur fetch {url}: {error_message}")
            return None
        except Exception as e:
            print(f"[{self.SOURCE_NAME}] Erreur fetch {url}: {e}")
            return None

    def _get_text(self, tree: lxml_html.HtmlElement, xpath: str) -> str | None:
        """Extraire le texte d'un n≈ìud via XPath."""
        nodes = tree.xpath(xpath)
        if nodes:
            if isinstance(nodes[0], str):
                return nodes[0].strip() or None
            text = nodes[0].text_content().strip()
            return text if text else None
        return None

    def _get_texts(self, tree: lxml_html.HtmlElement, xpath: str) -> list[str]:
        """Extraire une liste de textes via XPath."""
        nodes = tree.xpath(xpath)
        result = []
        for n in nodes:
            if isinstance(n, str):
                t = n.strip()
            else:
                t = n.text_content().strip()
            if t:
                result.append(t)
        return result

    @staticmethod
    def _normalize_name(name: str) -> str:
        """Normaliser un nom pour construction d'URL (espaces ‚Üí tirets, lowercase)."""
        name = name.strip().lower()
        name = re.sub(r'[^a-z0-9\s-]', '', name)
        name = re.sub(r'\s+', '-', name)
        return name

    @staticmethod
    def _normalize_name_plus(name: str) -> str:
        """Normaliser un nom pour URL FreeOnes (espaces ‚Üí +)."""
        return name.strip().replace(' ', '+')

    @staticmethod
    def _normalize_name_underscore(name: str) -> str:
        """Normaliser un nom pour URL (espaces ‚Üí _)."""
        return name.strip().replace(' ', '_')


============================================================
[56/124] Legacy\services\extractors\dvd\__init__.py
------------------------------------------------------------


============================================================
[57/124] Legacy\services\extractors\dvd\adultempire_dvd.py
------------------------------------------------------------
"""
AdultEmpireDVDExtractor ‚Äî Source secondaire (P2).
Extrait aussi les URLs de clips/sc√®nes pour la Phase 2.
"""
import re
from services.extractors.dvd.base_dvd import BaseExtractorDVD


class AdultEmpireDVDExtractor(BaseExtractorDVD):
    SOURCE_NAME = "adultdvdempire"

    # Cookie requis pour acc√®s
    COOKIES = "ageConfirmed=true"

    def _fetch_tree(self, url: str):
        """Override pour injecter le cookie age."""
        cached = self.cache.get(url)
        if cached:
            from lxml import html as lxml_html
            return lxml_html.fromstring(cached)
        try:
            import subprocess
            from lxml import html as lxml_html
            result = subprocess.run(
                ["curl", "-sL", "--max-time", "15",
                 "-H", f"Cookie: {self.COOKIES}", url],
                capture_output=True, timeout=20
            )
            html = result.stdout.decode("utf-8", errors="replace")
            if html and len(html) > 500:
                self.cache.set(url, html)
                return lxml_html.fromstring(html)
        except Exception as e:
            print(f"[adultdvdempire] Fetch error: {e}")
        return None

    def search_urls(self, title: str, studio: str = "") -> list[str]:
        slug = re.sub(r'[^a-z0-9]+', '-', title.lower()).strip('-')
        return [
            f"https://www.adultdvdempire.com/{slug}.html",
            f"https://www.adultdvdempire.com/dvd/{slug}/",
        ]

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)
        tree = self._fetch_tree(url)
        if tree is None:
            return data

        data["title"] = (self._get_text(tree, '//h1[@class="title"]/text()') or 
                        self._get_text(tree, '//h1/text()'))

        data["studio"] = self._get_text(tree, '//a[contains(@href,"/studio/")]/text()')

        # Date ‚Äî format "Sep 22 2008"
        data["date"] = (self._get_text(tree, '//li[contains(text(),"Released")]/text()') or 
                       self._get_text(tree, '//*[contains(@class,"release-date")]/text()'))

        # Duration ‚Äî format "1 hrs. 55 mins."
        data["duration_raw"] = (self._get_text(tree, '//li[contains(text(),"Run Time")]/text()') or 
                                self._get_text(tree, '//*[contains(@class,"run-time")]/text()'))

        data["director"] = self._get_text(tree, '//a[contains(@href,"/director/")]/text()')

        # Synopsis ‚Äî strip "Show More"
        desc_nodes = tree.xpath('//*[contains(@class,"synopsis")]//text()')
        if desc_nodes:
            desc = " ".join(t.strip() for t in desc_nodes if t.strip()
                            and "show more" not in t.lower())
            data["description"] = desc[:2000] or None

        # Covers
        data["front_cover_url"] = (self._get_text(tree, '//img[contains(@class,"cover-front")]/@src') or 
                                   self._get_text(tree, '//div[@id="front-cover"]//img/@src'))
        data["back_cover_url"]  = (self._get_text(tree, '//img[contains(@class,"cover-back")]/@src') or 
                                   self._get_text(tree, '//div[@id="back-cover"]//img/@src'))

        # ‚îÄ‚îÄ PHASE 2 : extraction URLs sc√®nes via liens /clip/ ‚îÄ‚îÄ‚îÄ
        clip_links = tree.xpath('//a[contains(@href, "/clip/")]/@href')
        seen = set()
        for i, href in enumerate(clip_links):
            if href in seen:
                continue
            seen.add(href)
            scene_url = href if href.startswith("http") else f"https://www.adultdvdempire.com{href}"
            data["scenes"].append({
                "index": i + 1,
                "title": None,
                "url_adultdvdempire": scene_url,
            })

        return data


============================================================
[58/124] Legacy\services\extractors\dvd\base_dvd.py
------------------------------------------------------------
"""
BaseExtractorDVD ‚Äî Base commune pour tous les extracteurs de groupes/DVD.
Similaire √† BaseExtractor (performer) mais adapt√© aux m√©tadonn√©es group.
"""
from abc import ABC, abstractmethod
import re
import subprocess
from lxml import html as lxml_html
from services.scrape_cache import ScrapeCache


class BaseExtractorDVD(ABC):
    SOURCE_NAME = "unknown"

    def __init__(self):
        self.cache = ScrapeCache()

    def _fetch_tree(self, url: str):
        """Fetch HTML avec cache. Retourne lxml tree ou None."""
        cached = self.cache.get(url)
        if cached:
            return lxml_html.fromstring(cached)
        try:
            result = subprocess.run(
                ["curl", "-sL", "--max-time", "15", url],
                capture_output=True, timeout=20
            )
            html = result.stdout.decode("utf-8", errors="replace")
            if html and len(html) > 500:
                self.cache.set(url, html)
                return lxml_html.fromstring(html)
        except Exception as e:
            print(f"[{self.SOURCE_NAME}] Fetch error: {e}")
        return None

    def _get_text(self, tree, xpath: str) -> str | None:
        nodes = tree.xpath(xpath)
        if nodes:
            text = nodes[0] if isinstance(nodes[0], str) else nodes[0].text_content()
            return text.strip() or None
        return None

    def _empty_result(self, url: str) -> dict:
        """Retourne un dict vide standard pour un group."""
        return {
            "_source": self.SOURCE_NAME,
            "_url": url,
            # M√©tadonn√©es group (Phase 1)
            "title": None,
            "aliases": None,
            "date": None,
            "studio": None,
            "director": None,
            "duration_raw": None,   # cha√Æne brute ‚Äî conversion dans merger
            "description": None,
            "tags": [],
            "front_cover_url": None,
            "back_cover_url": None,
            # Sc√®nes extraites du DVD (Phase 2)
            "scenes": [],   # list[dict] avec cl√©s : index, title, url_source
        }

    @abstractmethod
    def extract_from_url(self, url: str) -> dict:
        """Scraper une URL de group et retourner le dict unifi√©."""
        ...

    def search_urls(self, title: str, studio: str = "") -> list[str]:
        """
        Recherche de l'URL du group sur la source.
        √Ä impl√©menter dans chaque sous-classe.
        Retourne une liste d'URLs candidates (max 5).
        """
        return []


============================================================
[59/124] Legacy\services\extractors\dvd\data18_dvd.py
------------------------------------------------------------
"""
Data18DVDExtractor ‚Äî Source prioritaire (P1) pour les m√©tadonn√©es group.
Extrait aussi les URLs de sc√®nes individuelles pour la Phase 2.
"""
import re
from services.extractors.dvd.base_dvd import BaseExtractorDVD


class Data18DVDExtractor(BaseExtractorDVD):
    SOURCE_NAME = "data18"

    def search_urls(self, title: str, studio: str = "") -> list[str]:
        """G√©n√®re des URLs candidates data18 pour un titre DVD."""
        slug = re.sub(r'[^a-z0-9]+', '-', title.lower()).strip('-')
        candidates = [
            f"https://www.data18.com/movies/{slug}",
            f"https://www.data18.com/content/{slug}",
            f"https://www.data18.com/movies/{slug}-dvd",
        ]
        
        # RESTAURATION DE LA RECHERCHE ACTIVE
        # Le site data18.com a un endpoint de recherche qui peut retourner une liste de r√©sultats
        try:
            search_query = title.strip().replace(' ', '+')
            search_url = f"https://www.data18.com/search?k={search_query}"
            
            tree = self._fetch_tree(search_url)
            if tree is not None:
                # R√©cup√©rer tous les liens qui ressemblent √† des films/DVDs
                # Pattern: /movies/ID-titre ou /content/ID-titre
                links = tree.xpath('//a[contains(@href, "/movies/")]/@href')
                links += tree.xpath('//a[contains(@href, "/content/")]/@href')
                
                # Aussi chercher dans les blocs gen11/gen12 typiques de data18
                links += tree.xpath('//div[contains(@class,"gen")]//a/@href')

                for link in links:
                    # Filtrer les faux positifs (scenes, tags, sites, etc.)
                    if any(x in link for x in ["/scenes/", "/tags/", "/sites/", "/pornstars/", "/studios/"]):
                        continue
                        
                    full_link = link if link.startswith("http") else f"https://www.data18.com{link}"
                    
                    # √âviter les doublons
                    if full_link not in candidates:
                        # Priorit√© : ins√©rer au d√©but si le titre est tr√®s proche (TODO)
                        # Pour l'instant on ajoute √† la fin
                        candidates.append(full_link)
        except Exception as e:
            print(f"[Data18] Search error: {e}")

        return candidates

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)
        tree = self._fetch_tree(url)
        if tree is None:
            return data

        # Titre
        data["title"] = self._get_text(tree, '//h1[@itemprop="name"]/text()') or \
                        self._get_text(tree, '//h1/text()')

        # Date ‚Äî plusieurs fallbacks
        date_raw = self._get_text(tree, '//span[contains(text(),"Release")]/following-sibling::text()[1]')
        if not date_raw:
            date_raw = self._get_text(tree, '//div[@class="gen-wrap"]//span[contains(@class,"date")]/text()')
        data["date"] = date_raw

        # Studio
        data["studio"] = self._get_text(tree, '//a[@itemprop="publisher"]/text()') or \
                         self._get_text(tree, '//span[@itemprop="publisher"]/text()')

        # Director
        data["director"] = self._get_text(tree, '//span[@itemprop="director"]/text()')

        # Duration ‚Äî format [HH:MM:SS]
        dur = self._get_text(tree, '//span[@itemprop="duration"]/text()') or \
              self._get_text(tree, '//*[contains(text(),"Run Time")]/following-sibling::text()[1]')
        data["duration_raw"] = dur

        # Description / Synopsis
        desc_nodes = tree.xpath('//div[@class="boxdesc"]//text()')
        if desc_nodes:
            desc = " ".join(t.strip() for t in desc_nodes if t.strip())
            # Couper avant <ul> ou <br><br>
            data["description"] = desc[:2000] if desc else None

        # Covers
        data["front_cover_url"] = self._get_text(tree, '//img[@id="frontbox"]/@src') or \
                                   self._get_text(tree, '//img[contains(@alt,"front")]/@src')
        data["back_cover_url"]  = self._get_text(tree, '//img[@id="backbox"]/@src') or \
                                   self._get_text(tree, '//img[contains(@alt,"back")]/@src')

        # Tags ‚Äî 3 listes : categories, themes, genres
        tags = []
        for xpath in [
            '//a[contains(@href,"/categories/")]/text()',
            '//a[contains(@href,"/themes/")]/text()',
            '//a[contains(@href,"/genres/")]/text()',
        ]:
            tags += [t.strip() for t in tree.xpath(xpath) if t.strip()]
        data["tags"] = list(dict.fromkeys(tags))  # d√©dupliqu√©, ordre pr√©serv√©

        # ‚îÄ‚îÄ PHASE 2 : extraction URLs sc√®nes individuelles ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # data18 liste les sc√®nes avec des liens /content/SCENE_ID
        scene_links = tree.xpath('//div[contains(@class,"scene")]//a[contains(@href,"/content/")]/@href')
        if not scene_links:
            scene_links = tree.xpath('//a[contains(@href,"/content/") and contains(@href,"/scene")]/@href')

        for i, href in enumerate(scene_links):
            scene_url = href if href.startswith("http") else f"https://www.data18.com{href}"
            data["scenes"].append({
                "index": i + 1,
                "title": None,      # sera enrichi si besoin
                "url_data18": scene_url,
            })

        return data


============================================================
[60/124] Legacy\services\extractors\dvd\iafd_dvd.py
------------------------------------------------------------
"""
IafdDVDExtractor ‚Äî Extracteur IAFD pour les titres DVDs/Groups.
Source P2 (prioritaire pour DVDs classiques US, cast fiable, dates pr√©cises).
"""
import re
from services.extractors.dvd.base_dvd import BaseExtractorDVD
from utils.duration import parse_duration_to_seconds


class IafdDVDExtractor(BaseExtractorDVD):
    SOURCE_NAME = "iafd_dvd"
    BASE_URL = "https://www.iafd.com/title.rme/title="

    def build_url_from_title(self, title: str, year: str | None = None) -> str:
        """Construire l'URL de recherche IAFD depuis un titre."""
        slug = re.sub(r'[^a-z0-9]+', '-', title.lower()).strip('-')
        url = f"{self.BASE_URL}{slug}"
        if year:
            url += f"/year={year}"
        return url

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)

        tree = self._fetch_tree(url)
        if tree is None:
            return data

        # ‚îÄ‚îÄ Titre ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["title"] = self._get_text(tree, '//h1[@itemprop="name"]/text()')
        if not data["title"]:
            data["title"] = self._get_text(tree, '//h1/text()')

        # ‚îÄ‚îÄ Date / Ann√©e ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["date"] = (self._get_stat(tree, "Release Date") or 
                       self._get_stat(tree, "Year"))

        # ‚îÄ‚îÄ Studio ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["studio"] = (self._get_text(
            tree, '//p[@class="subheading"]/a[1]/text()'
        ) or self._get_stat(tree, "Studio"))

        # ‚îÄ‚îÄ R√©alisateur ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["director"] = (self._get_text(
            tree, '//p[b[contains(text(),"Director")]]/a/text()'
        ) or self._get_stat(tree, "Director"))

        # ‚îÄ‚îÄ Dur√©e ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        raw_duration = (self._get_stat(tree, "Running Time") or 
                       self._get_stat(tree, "Duration"))
        if raw_duration:
            data["duration"] = raw_duration
            secs = parse_duration_to_seconds(raw_duration)
            if secs:
                data["_duration_seconds"] = secs

        # ‚îÄ‚îÄ Description / Synopsis ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["description"] = (self._get_text(
            tree, '//div[@class="synopsis"]//text()'
        ) or self._get_text(
            tree, '//div[contains(@class,"description")]//text()'
        ))

        # ‚îÄ‚îÄ Aliases / Titres alternatifs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        aliases_text = (self._get_stat(tree, "Alternate Titles") or 
                       self._get_stat(tree, "AKA"))
        if aliases_text:
            data["aliases"] = [a.strip() for a in aliases_text.split(',') if a.strip()]

        # ‚îÄ‚îÄ Tags / Cat√©gories ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tags = tree.xpath('//div[@id="genres"]//a/text()')
        if not tags:
            tags = tree.xpath('//a[contains(@href,"/genre/")]/text()')
        data["tags"] = [t.strip() for t in tags if t.strip()]

        # ‚îÄ‚îÄ Sc√®nes (liste index√©e) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["scenes"] = self._extract_scenes(tree)

        # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["urls"] = {"iafd_dvd": url}

        return data

    def _extract_scenes(self, tree) -> list[dict]:
        """
        Extraire la liste des sc√®nes du DVD depuis la page IAFD.
        Retourne : [{"index": int, "title": str | None, "performers": [str]}]
        """
        scenes = []

        # IAFD liste les sc√®nes dans un tableau ou une liste ordonn√©e
        # Structure typique : <div id="sceneinfo"> ou <table class="w100">
        scene_rows = tree.xpath(
            '//div[contains(@id,"scene")] | //tr[contains(@class,"scene")]'
        )

        for i, row in enumerate(scene_rows, start=1):
            scene = {"index": i, "title": None, "performers": [], "url": None}

            # Titre de sc√®ne
            title_nodes = row.xpath('.//b/text() | .//strong/text()')
            if title_nodes:
                scene["title"] = title_nodes[0].strip()

            # Performers
            perf_links = row.xpath('.//a[contains(@href,"/person.rme/")]/text()')
            scene["performers"] = [p.strip() for p in perf_links if p.strip()]

            # URL directe de la sc√®ne (si disponible)
            scene_link = row.xpath('.//a[contains(@href,"/scene.rme/")]/@href')
            if scene_link:
                scene["url"] = "https://www.iafd.com" + scene_link[0]

            if scene["title"] or scene["performers"]:
                scenes.append(scene)

        return scenes

    def _get_stat(self, tree, label: str) -> str | None:
        """Extraire une valeur depuis les tableaux info IAFD."""
        xpaths = [
            f'//td[contains(text(),"{label}")]/following-sibling::td[1]',
            f'//b[contains(text(),"{label}")]/parent::*/following-sibling::*[1]',
            f'//p[contains(text(),"{label}")]/following-sibling::p[1]',
            f'//span[normalize-space(text())="{label}"]/following-sibling::span[1]',
        ]
        for xpath in xpaths:
            val = self._get_text(tree, xpath)
            if val:
                return val.strip()
        return None


============================================================
[61/124] Legacy\services\extractors\dvd\jeedoo_dvd.py
------------------------------------------------------------
"""
JedeeDVDExtractor ‚Äî Extracteur Jeedoo pour productions europ√©ennes.
Source P4 (priorit√© basse ‚Äî sp√©cialis√© europe).
"""
from services.extractors.dvd.base_dvd import BaseExtractorDVD


class JedeeDVDExtractor(BaseExtractorDVD):
    SOURCE_NAME = "jeedoo"
    BASE_URL = "https://www.jeedoo.com/"

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)
        tree = self._fetch_tree(url)
        if tree is None:
            return data

        data["title"] = self._get_text(tree, '//h1/text()')
        data["date"] = self._get_text(tree,
            '//span[contains(@class,"date")]/text() | '
            '//p[contains(text(),"Ann√©e")]/following-sibling::p[1]/text()')
        data["studio"] = self._get_text(tree,
            '//a[contains(@href,"/studio/")]/text()')
        data["director"] = self._get_text(tree,
            '//p[contains(text(),"R√©alisateur")]/following-sibling::p[1]/text()')
        data["duration"] = self._get_text(tree,
            '//p[contains(text(),"Dur√©e")]/following-sibling::p[1]/text()')
        data["description"] = self._get_text(tree,
            '//div[contains(@class,"description")]//text()')

        tags = tree.xpath('//a[contains(@href,"/categorie/")]/text()')
        data["tags"] = [t.strip() for t in tags if t.strip()]

        data["scenes"] = []  # Jeedoo ne liste pas les sc√®nes individuellement
        data["urls"] = {"jeedoo": url}

        return data


============================================================
[62/124] Legacy\services\extractors\freeones.py
------------------------------------------------------------
"""
Extracteur FreeOnes ‚Äî source primaire pour trivia et bio.
S√©pare details / trivia / awards contrairement au V1.
"""
import re
import html as html_std

from services.extractors.base import BaseExtractor
from utils.body_art_parser import parse_body_art


# Regex multi-ceremonies pour extraire les awards du texte
AWARD_PATTERN = re.compile(
    r'((?:\d{4}\s+)?(?:AVN|XBIZ|XRCO|NightMoves|XCritic|AEBN|AdultFilmDatabase'
    r'|Fans of Adult|TEA|GayVN|Grabby|Urban X|CAVR|Inked)[^\n.;]*)',
    re.I
)


class FreeonesExtractor(BaseExtractor):
    SOURCE_NAME = "freeones"

    def build_url(self, performer_name: str) -> str | None:
        """Construire l'URL FreeOnes depuis le nom."""
        slug = self._normalize_name(performer_name)
        return f"https://www.freeones.com/{slug}/bio"

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)

        tree = self._fetch_tree(url)
        if tree is None:
            return data

        # ‚îÄ‚îÄ Phase 1 ‚Äî M√©tadonn√©es ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["name"] = self._get_text(tree, '//h1/text()') or self._get_text(tree, '//h1//text()')
        
        aliases_el = self._get_stat(tree, "Aliases") or self._get_stat(tree, "Also Known As")
        if aliases_el:
            data["aliases"] = [a.strip() for a in aliases_el.split(',') if a.strip()]

        data["birthdate"] = self._get_stat(tree, "Date of Birth") or self._get_stat(tree, "Birthday")
        data["country"] = self._get_stat(tree, "Place of Birth") or self._get_stat(tree, "Birthplace")
        data["ethnicity"] = self._get_stat(tree, "Ethnicity")
        data["hair_color"] = self._get_stat(tree, "Hair Color") or self._get_stat(tree, "Hair")
        data["eye_color"] = self._get_stat(tree, "Eye Color") or self._get_stat(tree, "Eyes")
        data["height"] = self._get_stat(tree, "Height")
        data["weight"] = self._get_stat(tree, "Weight")

        # Ethnie : Essayer de r√©cup√©rer via les liens si le texte simple √©choue
        if not data["ethnicity"]:
            eth_links = tree.xpath('//a[contains(@href, "/ethnicity/")]/text()')
            if eth_links:
                data["ethnicity"] = eth_links[0].strip()

        data["measurements"] = self._get_stat(tree, "Measurements")
        
        # Si pas de measurements, essayer de construire depuis Bust/Waist/Hip
        if not data["measurements"]:
            bust = self._get_stat(tree, "Bust")
            waist = self._get_stat(tree, "Waist")
            hip = self._get_stat(tree, "Hip")
            if bust and waist and hip:
                data["measurements"] = f"{bust}-{waist}-{hip}"
        
        # Normalisation Mensurations (Conversion CM -> Pouces si n√©cessaire)
        if data["measurements"]:
            try:
                # Si les valeurs semblent √™tre en cm (toutes > 50), on convertit
                parts = re.findall(r'\d+', data["measurements"])
                if len(parts) == 3 and all(int(x) > 50 for x in parts):
                    imperial = [str(round(int(x) / 2.54)) for x in parts]
                    data["measurements"] = "-".join(imperial)
            except Exception:
                pass

        data["fake_tits"] = self._get_stat(tree, "Boobs") or self._get_stat(tree, "Enhanced")
        
        # Career Length : Combiner Start/End si Years Active est vide
        data["career_length"] = self._get_stat(tree, "Career Start") or self._get_stat(tree, "Years Active")
        if not data["career_length"]:
            start = self._get_stat(tree, "Career Start") or self._get_stat(tree, "Started")
            end = self._get_stat(tree, "Career End") or self._get_stat(tree, "Ended") or "Present"
            if start:
                data["career_length"] = f"{start}-{end}"

        # ‚îÄ‚îÄ Bio / Details ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        bio_xpaths = [
            '//div[@data-test="biography"]//text()[normalize-space()]',
            '//div[contains(@class,"biography")]//text()[normalize-space()]',
            '//div[@class="bio"]//text()[normalize-space()]',
        ]
        for xpath in bio_xpaths:
            bio_texts = tree.xpath(xpath)
            if bio_texts:
                bio = " ".join(t.strip() for t in bio_texts if t.strip())
                if len(bio) > 20:
                    data["details"] = bio
                    break

        # ‚îÄ‚îÄ Trivia / Additional Information ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        trivia_xpaths = [
            (
                "//p[normalize-space(text())='Additional Information']"
                "/following-sibling::div[contains(@class,'hide-on-edit')]"
                "//text()[normalize-space()]"
            ),
            (
                "//h3[contains(text(),'Additional')]"
                "/following-sibling::div//text()[normalize-space()]"
            ),
            (
                "//div[contains(@class,'additional')]"
                "//text()[normalize-space()]"
            ),
        ]
        for xpath in trivia_xpaths:
            trivia_texts = tree.xpath(xpath)
            if trivia_texts:
                raw = " ".join(t.strip() for t in trivia_texts if t.strip())
                if len(raw) > 10:
                    data["trivia"] = html_std.unescape(raw)
                    break

        # ‚îÄ‚îÄ Awards (regex depuis texte bio + trivia) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        combined = (data.get("details") or "") + " " + (data.get("trivia") or "")
        if combined.strip():
            matches = AWARD_PATTERN.findall(combined)
            data["awards"] = [m.strip() for m in matches if m.strip()]

        # ‚îÄ‚îÄ Tattoos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tattoo_text = self._get_stat(tree, "Tattoos")
        data["tattoos"] = parse_body_art(tattoo_text) if tattoo_text else []

        # ‚îÄ‚îÄ Piercings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        piercing_text = self._get_stat(tree, "Piercings")
        data["piercings"] = parse_body_art(piercing_text) if piercing_text else []

        # ‚îÄ‚îÄ Tags (cat√©gories) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tag_xpaths = [
            "//a[contains(@href,'/category/')]//text()",
            "//a[contains(@href,'/tag/')]//text()",
            "//div[contains(@class,'tag')]//a/text()",
        ]
        for xpath in tag_xpaths:
            tags_raw = self._get_texts(tree, xpath)
            if tags_raw:
                data["tags"] = [t.strip() for t in tags_raw if t.strip() and len(t.strip()) > 1]
                break

        # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["urls"]["freeones"] = url

        # Extraire les liens vers databases ext√©rieures
        links = tree.xpath('//a[contains(@class,"link") or contains(@class,"database")]/@href')
        for link in links:
            link = link.strip()
            if "iafd.com" in link:
                data["urls"]["iafd"] = link
            elif "babepedia.com" in link:
                data["urls"]["babepedia"] = link
            elif "thenude" in link:
                data["urls"]["thenude"] = link
            elif "twitter.com" in link or "x.com" in link:
                data["urls"]["twitter"] = link
            elif "instagram.com" in link:
                data["urls"]["instagram"] = link

        return data

    def _get_stat(self, tree, label: str) -> str | None:
        """Extraire une stat depuis la page FreeOnes."""
        label_slug = label.lower().replace(" ", "-")
        xpaths = [
            f'//span[contains(text(), "{label}")]/following-sibling::span[1]',
            f'//p[contains(@data-test, "{label.lower()}")]',
            f'//p[contains(@data-test, "{label_slug}")]',
            f'//div[contains(@data-test, "{label.lower()}")]',
            f'//div[contains(@data-test, "{label_slug}")]',
            f'//a[contains(@data-test, "{label_slug}")]',
            f'//td[contains(text(), "{label}")]/following-sibling::td[1]',
            f'//*[contains(text(), "{label}")]/following-sibling::*[1]',
        ]
        for xpath in xpaths:
            val = self._get_text(tree, xpath)
            if val:
                return val
        return None


============================================================
[63/124] Legacy\services\extractors\iafd.py
------------------------------------------------------------
"""
Extracteur IAFD ‚Äî source primaire pour les awards.
"""
import re

from services.extractors.base import BaseExtractor
from utils.body_art_parser import parse_body_art


# Lignes parasites √† ignorer dans les awards
AWARD_SKIP = re.compile(r'^([-‚Äì‚Äî]+|nomin[√©e]e?d?|winner|won|year|award)$', re.I)

# Regex √©tendue multi-ceremonies pour fallback texte
AWARD_PATTERN = re.compile(
    r'(\d{4}\s+)?(AVN|XBIZ|XRCO|NightMoves|XCritic|AEBN|AdultFilmDatabase'
    r'|Fans of Adult Media|TEA|GayVN|Grabby|Urban X|CAVR|Inked)[^.\n]*',
    re.I
)


class IafdExtractor(BaseExtractor):
    SOURCE_NAME = "iafd"

    def build_url(self, performer_name: str) -> str | None:
        """Construire l'URL IAFD depuis le nom."""
        slug = self._normalize_name(performer_name)
        return f"https://www.iafd.com/person.rme/perfid={slug}/gender=f/{slug}.htm"

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)

        tree = self._fetch_tree(url)
        if tree is None:
            return data

        # ‚îÄ‚îÄ Phase 1 ‚Äî M√©tadonn√©es ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["name"] = self._get_text(tree, '//h1/text()')
        
        aliases_text = self._get_stat(tree, "Aliases") or self._get_stat(tree, "AKA")
        if aliases_text:
            data["aliases"] = [a.strip() for a in aliases_text.split(',') if a.strip()]

        data["birthdate"] = self._get_stat(tree, "Birthday") or self._get_stat(tree, "Date of Birth")
        data["death_date"] = self._get_stat(tree, "Death")
        data["ethnicity"] = self._get_stat(tree, "Ethnicity") or self._get_stat(tree, "Race")
        data["country"] = self._get_stat(tree, "Birthplace") or self._get_stat(tree, "Nationality")
        data["hair_color"] = self._get_stat(tree, "Hair Color")
        data["eye_color"] = self._get_stat(tree, "Eye Color")
        
        height_text = self._get_stat(tree, "Height")
        if height_text:
            data["height"] = height_text
        
        weight_text = self._get_stat(tree, "Weight")
        if weight_text:
            data["weight"] = weight_text
            
        data["measurements"] = self._get_stat(tree, "Measurements")
        data["fake_tits"] = self._get_stat(tree, "Boobs") or self._get_stat(tree, "Breast")
        
        career_years = self._get_stat(tree, "Years Active")
        if career_years:
            data["career_length"] = career_years
        else:
            start = self._get_stat(tree, "Start") or self._get_stat(tree, "Career Start")
            end = self._get_stat(tree, "End") or self._get_stat(tree, "Career End") or "Present"
            if start:
                data["career_length"] = f"{start} - {end}"

        # ‚îÄ‚îÄ Phase 2 ‚Äî Awards ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["awards"] = self._extract_awards(tree)

        # ‚îÄ‚îÄ Bio / Details ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        bio_text = self._get_text(tree, '//div[@id="bio"]')
        if not bio_text:
            bio_text = self._get_text(tree, '//div[contains(@class,"biodata")]')
        data["details"] = bio_text

        # ‚îÄ‚îÄ Tattoos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tattoo_text = self._get_stat(tree, "Tattoos")
        data["tattoos"] = parse_body_art(tattoo_text) if tattoo_text else []

        # ‚îÄ‚îÄ Piercings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        piercing_text = self._get_stat(tree, "Piercings")
        data["piercings"] = parse_body_art(piercing_text) if piercing_text else []

        # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["urls"]["iafd"] = url

        # Extraire les liens vers d'autres databases
        db_links = tree.xpath('//a[contains(@href, "http")]/@href')
        for link in db_links:
            link = link.strip()
            if "freeones.com" in link:
                data["urls"]["freeones"] = link
            elif "babepedia.com" in link:
                data["urls"]["babepedia"] = link
            elif "thenude.com" in link or "thenude.eu" in link:
                data["urls"]["thenude"] = link

        return data

    def _extract_awards(self, tree) -> list[str]:
        """Parser les awards depuis div#awards de IAFD."""
        awards_divs = tree.xpath("//div[@id='awards']")
        if not awards_divs:
            # Essayer un s√©lecteur alternatif
            awards_divs = tree.xpath("//div[contains(@class,'award')]")
        
        if not awards_divs:
            return []

        div = awards_divs[0]
        # Ins√©rer des sauts de ligne avant chaque <br>
        for br in div.xpath(".//br"):
            br.tail = "\n" + (br.tail or "")

        awards_text = div.text_content()
        raw_lines = [line.strip() for line in awards_text.splitlines() if line.strip()]
        
        # Filtrer les lignes parasites
        result = []
        current_year = ""

        for line in raw_lines:
            # D√©tection de l'ann√©e (ex: 2012) pour l'associer √† l'award
            if re.match(r'^\d{4}$', line):
                current_year = line
                continue

            if AWARD_SKIP.match(line):
                continue
            if len(line) < 4:
                continue
            
            # Ajouter l'ann√©e devant la ligne si elle n'y est pas d√©j√†
            if current_year and not line.startswith(current_year):
                result.append(f"{current_year} {line}")
            else:
                result.append(line)

        return result

    def _get_stat(self, tree, label: str) -> str | None:
        """Extraire une stat depuis le tableau IAFD (label ‚Üí valeur)."""
        # Essayer plusieurs formats de tableau bio IAFD
        xpaths = [
            f'//p[contains(text(), "{label}")]/following-sibling::p[1]',
            f'//td[contains(text(), "{label}")]/following-sibling::td[1]',
            f'//b[contains(text(), "{label}")]/parent::*/following-sibling::*[1]',
            f'//span[contains(text(), "{label}")]/following-sibling::span[1]',
        ]
        for xpath in xpaths:
            val = self._get_text(tree, xpath)
            if val:
                return val
        return None


============================================================
[64/124] Legacy\services\extractors\thenude.py
------------------------------------------------------------
"""
Extracteur TheNude ‚Äî source pour bios studio contextualis√©es.
"""
import re

from services.extractors.base import BaseExtractor
from utils.body_art_parser import parse_body_art


class ThenudeExtractor(BaseExtractor):
    SOURCE_NAME = "thenude"

    def build_url(self, performer_name: str) -> str | None:
        """Construire l'URL TheNude depuis le nom."""
        slug = self._normalize_name_underscore(performer_name)
        return f"https://www.thenude.com/models/{slug}.htm"

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)

        tree = self._fetch_tree(url)
        if tree is None:
            return data

        # ‚îÄ‚îÄ Phase 1 ‚Äî M√©tadonn√©es ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["name"] = self._get_text(tree, '//h1/text()')
        
        aliases_text = self._get_stat(tree, "Aliases") or self._get_stat(tree, "AKA")
        if aliases_text:
            data["aliases"] = [a.strip() for a in aliases_text.split(',') if a.strip()]

        data["birthdate"] = self._get_stat(tree, "Born") or self._get_stat(tree, "Date of Birth")
        data["country"] = self._get_stat(tree, "Birthplace") or self._get_stat(tree, "Country")
        data["ethnicity"] = self._get_stat(tree, "Ethnicity")
        data["hair_color"] = self._get_stat(tree, "Hair")
        data["eye_color"] = self._get_stat(tree, "Eyes")
        data["height"] = self._get_stat(tree, "Height")
        data["weight"] = self._get_stat(tree, "Weight")
        data["measurements"] = self._get_stat(tree, "Measurements")
        data["fake_tits"] = self._get_stat(tree, "Boobs")

        # ‚îÄ‚îÄ Bio principale ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        bio_xpaths = [
            '//p[@class="description"]/text()',
            '//div[@class="description"]//text()',
            '//div[contains(@class,"bio")]//text()',
        ]
        for xpath in bio_xpaths:
            texts = tree.xpath(xpath)
            if texts:
                bio = " ".join(t.strip() for t in texts if t.strip())
                if len(bio) > 20:
                    data["details"] = bio
                    break

        # ‚îÄ‚îÄ Bios studio (trivia) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        studio_bios = self._extract_studio_bios(tree)
        if studio_bios:
            data["trivia"] = "\n\n".join(studio_bios)

        # ‚îÄ‚îÄ Tattoos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tattoo_text = self._get_stat(tree, "Tattoos")
        data["tattoos"] = parse_body_art(tattoo_text) if tattoo_text else []

        # ‚îÄ‚îÄ Piercings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        piercing_text = self._get_stat(tree, "Piercings")
        data["piercings"] = parse_body_art(piercing_text) if piercing_text else []

        # ‚îÄ‚îÄ Tags (keywords) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tag_xpaths = [
            '//div[@class="keywords"]//a/text()',
            '//meta[@name="keywords"]/@content',
            '//div[contains(@class,"tag")]//a/text()',
        ]
        for xpath in tag_xpaths:
            tags_raw = tree.xpath(xpath)
            if tags_raw:
                # Si c'est un meta keywords, splitter par virgule
                if len(tags_raw) == 1 and ',' in tags_raw[0]:
                    tags_raw = [t.strip() for t in tags_raw[0].split(',')]
                data["tags"] = [t.strip() for t in tags_raw 
                                if t.strip() and len(t.strip()) > 1]
                break

        # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["urls"]["thenude"] = url

        # Extraire les liens vers databases
        links = tree.xpath('//a[contains(@href, "http")]/@href')
        for link in links:
            link = link.strip()
            if "iafd.com" in link:
                data["urls"]["iafd"] = link
            elif "freeones.com" in link:
                data["urls"]["freeones"] = link
            elif "babepedia.com" in link:
                data["urls"]["babepedia"] = link

        return data

    def _extract_studio_bios(self, tree) -> list[str]:
        """Extraire les bios par studio (ex: 'BRAZZERS biography')."""
        studio_bios = []
        
        # Chercher les headers contenant "biography"
        bio_headers = tree.xpath(
            '//*[contains(translate(text(),"BIOGRAPHY","biography"), "biography")]'
        )
        
        for h in bio_headers:
            title = h.text_content().strip()
            if title.lower() == "biography":
                continue

            # Le contenu suit le header
            content_node = h.getnext()
            if content_node is not None:
                content = content_node.text_content().strip()
                if content and len(content) > 20:
                    studio_bios.append(f"[{title}]\n{content}")

        return studio_bios

    def _get_stat(self, tree, label: str) -> str | None:
        """Extraire une stat depuis la page TheNude."""
        xpaths = [
            f'//span[contains(text(), "{label}")]/following-sibling::span[1]',
            f'//td[contains(text(), "{label}")]/following-sibling::td[1]',
            f'//li[contains(., "{label}")]',
            f'//div[contains(@class,"stat")]//*[contains(text(), "{label}")]/..',
        ]
        for xpath in xpaths:
            val = self._get_text(tree, xpath)
            if val:
                # Nettoyer le label du texte si pr√©sent
                val = re.sub(rf'^{label}\s*[:]\s*', '', val, flags=re.I).strip()
                if val:
                    return val
        return None


============================================================
[65/124] Legacy\services\group_phase1_merger.py
------------------------------------------------------------
"""
GroupPhase1Merger ‚Äî Fusionne les m√©tadonn√©es DVD/Group scrap√©es avec la DB.
"""

GROUP_FIELDS = {
    "Title": "name",
    "Aliases": "aliases",
    "Date": "date",
    "Studio": "studio",
    "Director": "director",
    "Duration": "duration",
    "Description": "description",
    "Tags": "tags",
    "URLs": "urls",
}

class GroupPhase1Merger:
    def merge(self, db_data: dict, scraped_results: list[dict], checked_fields: list[str]) -> dict:
        result = {}
        for field in checked_fields:
db_key="***MASKED***"
            if not db_key: continue
            
            db_val = db_data.get(db_key)
            if field == "Studio" and db_data.get("studio_name"):
                db_val = db_data["studio_name"]

            scraped_vals = {}
            for r in scraped_results:
                val = r.get(db_key.replace("name", "title") if db_key == "name" else db_key)
                if val:
                    scraped_vals[r["_source"]] = val
            
            if not scraped_vals:
                status = "empty"
                suggestion = db_val
            elif not db_val:
                status = "new"
                suggestion = self._pick_best(scraped_vals)
            else:
                # Priorit√© Data18 : si data18 est pr√©sent et diff√®re de DB, c'est un conflit (m√™me si d'autres sources matchent)
                data18_val = scraped_vals.get("data18")
                if data18_val and str(data18_val).lower().strip() != str(db_val).lower().strip():
                    status = "conflict"
                    suggestion = data18_val
                elif self._matches(db_val, scraped_vals):
                    status = "confirmed"
                    suggestion = db_val
                else:
                    status = "conflict"
                    suggestion = self._pick_best(scraped_vals)

            result[field] = {
                "status": status,
                "db_value": db_val,
                "scraped_values": scraped_vals,
                "suggestion": suggestion
            }
        return result

    def _matches(self, db_val, scraped_vals):
        db_s = str(db_val).lower().strip()
        for v in scraped_vals.values():
            if str(v).lower().strip() == db_s:
                return True
        return False

    def _pick_best(self, scraped_vals):
        # Priorit√© simple : data18 > iafd_dvd > adultdvdempire
        for src in ["data18", "iafd_dvd", "adultdvdempire"]:
            if src in scraped_vals:
                return scraped_vals[src]
        return list(scraped_vals.values())[0]


============================================================
[66/124] Legacy\services\group_phase1_scraper.py
------------------------------------------------------------
"""
GroupPhase1ScraperService ‚Äî Orchestre le scraping des m√©tadonn√©es DVD/Group.
"""
from services.extractors.dvd.data18_dvd import Data18DVDExtractor
from services.extractors.dvd.adultempire_dvd import AdultEmpireDVDExtractor
from services.extractors.dvd.iafd_dvd import IafdDVDExtractor
from services.extractors.dvd.jeedoo_dvd import JedeeDVDExtractor

class GroupPhase1ScraperService:
    def __init__(self):
        self.extractors = {
            "data18": Data18DVDExtractor(),
            "adultdvdempire": AdultEmpireDVDExtractor(),
            "iafd_dvd": IafdDVDExtractor(),
            "jeedoo": JedeeDVDExtractor(),
        }

    def scrape(self, title: str, year: str = None, known_urls: list = None, progress_callback=None) -> list[dict]:
        results = []
        known_urls = known_urls or []

        # 1. Utiliser les URLs connues
        for url in known_urls:
            extractor = self._find_extractor_for_url(url)
            if extractor:
                if progress_callback:
                    progress_callback(extractor.SOURCE_NAME, "Fetching from known URL...")
                res = extractor.extract_from_url(url)
                if res and res.get("title"):
                    results.append(res)

        # 2. Si Data18 n'est pas trouv√© dans les URLs connues, essayer de deviner l'URL
        has_data18 = any(r.get("_source") == "data18" for r in results)
        if not has_data18:
            data18 = self.extractors["data18"]
            candidates = data18.search_urls(title)
            # Pas de boucle ici si search_urls ne fait que des guesses simples qui ont peu de chance de marcher
            # Mais on essaie quand m√™me les guesses simples
            for url in candidates:
                if progress_callback:
                    progress_callback("data18", f"Trying candidate: {url}...")
                res = data18.extract_from_url(url)
                if res and res.get("title"):
                    results.append(res)
                    break # Found one valid Data18 page

        # 3. Si pas de r√©sultat ou pour compl√©ter, essayer IAFD par titre
        has_iafd = any(r.get("_source") == "iafd_dvd" for r in results)
        if not has_iafd:
            # Essayer IAFD par titre
            iafd = self.extractors["iafd_dvd"]
            url = iafd.build_url_from_title(title, year)
            if progress_callback:
                progress_callback("iafd_dvd", f"Searching for {title}...")
            res = iafd.extract_from_url(url)
            if res and res.get("title"):
                results.append(res)

        return results

    def _find_extractor_for_url(self, url: str):
        if "data18.com" in url: return self.extractors["data18"]
        if "adultdvdempire.com" in url: return self.extractors["adultdvdempire"]
        if "iafd.com" in url: return self.extractors["iafd_dvd"]
        if "jeedoo.com" in url: return self.extractors["jeedoo"]
        return None


============================================================
[67/124] Legacy\services\group_phase2_merger.py
------------------------------------------------------------
"""
GroupPhase2Merger ‚Äî Associe les URLs scrap√©es aux sc√®nes Stash par index.

Strat√©gie de matching :
  1. Exact sur scene_index (num√©ro de sc√®ne dans le DVD)
  2. Fallback : comparaison de titre normalis√©
"""


class GroupPhase2Merger:

    def merge(
        self,
        stash_scenes: list[dict],
        scraped_scene_urls: dict[int, dict[str, str]]
    ) -> list[dict]:
        """
        Associe les URLs scrap√©es √† chaque sc√®ne Stash.

        Args:
            stash_scenes:       liste de sc√®nes DB (scene_id, scene_index, scene_title, existing_urls)
            scraped_scene_urls: {scene_index: {"data18": url, "adultdvdempire": url}}

        Returns:
            Liste de dicts enrichis avec "new_urls" et "status"
        """
        result = []

        for scene in stash_scenes:
            scene_id    = scene["scene_id"]
            scene_index = scene.get("scene_index")
            scene_title = scene.get("scene_title", f"Sc√®ne {scene_id}")
            existing    = scene.get("existing_urls", [])

            # Chercher par index exact
            new_urls = scraped_scene_urls.get(scene_index, {})

            # Filtrer les URLs d√©j√† pr√©sentes
            truly_new = {
                src: url for src, url in new_urls.items()
                if url and url not in existing
            }

            # D√©terminer le statut
            if not new_urls:
                status = "no_match"
            elif not truly_new:
                status = "already_present"
            elif existing:
                status = "partial"
            else:
                status = "new"

            result.append({
                "scene_id":      scene_id,
                "scene_index":   scene_index,
                "scene_title":   scene_title,
                "existing_urls": existing,
                "new_urls":      truly_new,
                "status":        status,
            })

        return result


============================================================
[68/124] Legacy\services\group_phase2_scraper.py
------------------------------------------------------------
"""
GroupPhase2ScraperService ‚Äî Scraping des URLs de sc√®nes individuelles pour un Group.
"""
from services.extractors.dvd.data18_dvd import Data18DVDExtractor
from services.extractors.dvd.adultempire_dvd import AdultEmpireDVDExtractor

class GroupPhase2ScraperService:
    def __init__(self):
        self.extractors = {
            "data18": Data18DVDExtractor(),
            "adultdvdempire": AdultEmpireDVDExtractor(),
        }

    def scrape(self, group_data: dict, progress_callback=None) -> dict[int, dict[str, str]]:
        """
        Retourne : { scene_index: { "source": "url" } }
        """
        all_scene_urls = {} # {index: {source: url}}

        urls = group_data.get("urls", [])
        for url in urls:
            extractor = None
            if "data18.com" in url: extractor = self.extractors["data18"]
            elif "adultdvdempire.com" in url: extractor = self.extractors["adultdvdempire"]
            
            if extractor:
                if progress_callback:
                    progress_callback(extractor.SOURCE_NAME, f"Scraping scene URLs...")
                
                res = extractor.extract_from_url(url)
                scenes = res.get("scenes", [])
                for s in scenes:
                    idx = s.get("index")
                    s_url = s.get("url")
                    if idx and s_url:
                        if idx not in all_scene_urls:
                            all_scene_urls[idx] = {}
                        all_scene_urls[idx][extractor.SOURCE_NAME] = s_url

        return all_scene_urls


============================================================
[69/124] Legacy\services\phase1_merger.py
------------------------------------------------------------
"""
Phase1Merger ‚Äî Compare les donn√©es scrap√©es avec la DB pour les champs Phase 1.
Identifie les confirmations et les conflits.
G√®re la fusion et la normalisation des ALIAS.
"""

# Ordre de priorit√© des sources pour les champs simples
SOURCE_PRIORITY = ["iafd", "freeones", "babepedia", "thenude"]

# Champs Phase 1 avec leur cl√© DB
PHASE1_FIELDS = {
    "Name": "name",
    "Aliases": "aliases",
    "Birthdate": "birthdate",
    "Deathdate": "death_date",
    "Country": "country",
    "Ethnicity": "ethnicity",
    "Hair Color": "hair_color",
    "Eye Color": "eye_color",
    "Height": "height",
    "Weight": "weight",
    "Measurements": "measurements",
    "Fake Tits": "fake_tits",
    "Career Length": "career_length",
}


class Phase1Merger:
    def merge(self, db_data: dict, scraped_results: list[dict], 
              checked_fields: list[str]) -> dict:
        """
        Fusionner les donn√©es pour les champs coch√©s.
        Logique sp√©ciale pour les ALIASES.
        """
        result = {}
        
        for field_name in checked_fields:
db_key="***MASKED***"
            if not db_key:
                continue

            # --- Logique sp√©ciale pour les ALIASES ---
            if db_key == 'aliases':
                result[field_name] = self._merge_aliases(db_data, scraped_results)
                continue

            # --- Logique g√©n√©rale pour les autres champs ---
            db_value = self._normalize_value(db_data.get(db_key))
            
            scraped_values = {}
            for r in scraped_results:
                source = r.get("_source", "?")
                val = self._normalize_value(r.get(db_key))
                if val:
                    scraped_values[source] = val
            
            if not scraped_values:
                result[field_name] = self._build_result("empty", db_value, scraped_values, db_value)
            elif not db_value:
                suggestion = self._pick_best(scraped_values)
                result[field_name] = self._build_result("new", None, scraped_values, suggestion)
            elif self._matches_any(db_value, scraped_values):
                result[field_name] = self._build_result("confirmed", db_value, scraped_values, db_value)
            else:
                suggestion = self._pick_best(scraped_values)
                result[field_name] = self._build_result("conflict", db_value, scraped_values, suggestion)
        
        return result

    def _normalize_alias(self, alias: str) -> str | None:
        """Normalise une cha√Æne d'alias (lowercase, strip)."""
        if not isinstance(alias, str):
            return None
        normalized = alias.lower().strip()
        return normalized if normalized else None

    def _merge_aliases(self, db_data: dict, scraped_results: list[dict]) -> dict:
        """Fusionne, normalise et d√©doublonne les alias de toutes les sources."""
db_key="***MASKED***"
        db_aliases = db_data.get(db_key) or []
        
        final_aliases = set()
        
        # 1. Ajouter les alias de la DB
        for alias in db_aliases:
            norm_alias = self._normalize_alias(alias)
            if norm_alias:
                final_aliases.add(norm_alias)
        
        # 2. Ajouter les alias des sources scrap√©es
        scraped_values_dict = {}
        for r in scraped_results:
            source_name = r.get('_source', '?')
            source_aliases = r.get(db_key)
            if source_aliases:
                scraped_values_dict[source_name] = source_aliases
                for alias in source_aliases:
                    norm_alias = self._normalize_alias(alias)
                    if norm_alias:
                        final_aliases.add(norm_alias)
        
        # 3. D√©terminer le statut de la fusion
        db_set = {self._normalize_alias(a) for a in db_aliases if self._normalize_alias(a)}
        status = "empty"
        if final_aliases:
            if not db_set:
                status = "new"
            elif db_set == final_aliases:
                status = "confirmed"
            else:
                status = "conflict"  # 'conflict' indique qu'il y a des changements/ajouts

        # 4. Construire le r√©sultat final pour l'UI
        return self._build_result(
            status,
            db_value=", ".join(sorted(list(db_set))),
            scraped_values={k: ", ".join(v) for k, v in scraped_values_dict.items()},
            suggestion=", ".join(sorted(list(final_aliases)))
        )

    def _build_result(self, status, db_value, scraped_values, suggestion):
        """Helper pour construire le dictionnaire de r√©sultat."""
        return {
            "status": status,
            "db_value": db_value,
            "scraped_values": scraped_values,
            "suggestion": suggestion,
        }

    def _normalize_value(self, val) -> str | None:
        """Normaliser une valeur simple pour comparaison."""
        if val is None:
            return None
        # Ne g√®re plus les listes ici, elles sont trait√©es par _merge_aliases
        val = str(val).strip()
        return val if val else None

    def _matches_any(self, db_value: str, scraped_values: dict) -> bool:
        """V√©rifier si la valeur DB correspond √† au moins une source."""
        db_lower = db_value.lower().strip()
        for val in scraped_values.values():
            if val and val.lower().strip() == db_lower:
                return True
        return False

    def _pick_best(self, scraped_values: dict) -> str:
        """Choisir la meilleure valeur selon la priorit√© des sources."""
        for source in SOURCE_PRIORITY:
            if source in scraped_values and scraped_values[source]:
                return scraped_values[source]
        # Fallback : premi√®re valeur non-None
        for val in scraped_values.values():
            if val:
                return val
        return ""


============================================================
[70/124] Legacy\services\phase2_merger.py
------------------------------------------------------------
"""
Phase2Merger ‚Äî fusionne les r√©sultats de scraping Phase 2
en donn√©es pr√™tes pour l'interface de r√©solution.
"""


class Phase2Merger:
    """
    Fusionne les r√©sultats de scraping Phase 2 en donn√©es pr√™tes
    pour l'interface Phase2MergeDialog.
    
    Chaque champ a sa propre strat√©gie de fusion.
    """

    def merge(self, db_data: dict, scraped_results: list[dict]) -> dict:
        """
        Fusionner les donn√©es DB + scraping pour tous les champs Phase 2.
        
        Args:
            db_data: Donn√©es actuelles du performer depuis Stash DB
            scraped_results: Liste de dicts retourn√©s par les extracteurs
            
        Returns:
            Dict avec les r√©sultats fusionn√©s par champ
        """
        return {
            "awards":    self._merge_awards(db_data, scraped_results),
            "trivia":    self._merge_trivia(db_data, scraped_results),
            "details":   self._merge_details(db_data, scraped_results),
            "tattoos":   self._merge_body_art("tattoos", db_data, scraped_results),
            "piercings": self._merge_body_art("piercings", db_data, scraped_results),
            "tags":      self._merge_tags(db_data, scraped_results),
            "urls":      self._merge_urls(db_data, scraped_results),
        }

    # ‚îÄ‚îÄ AWARDS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _merge_awards(self, db, scraped):
        """
        IAFD = source primaire (liste structur√©e).
        Autres sources = regex fallback depuis bio/trivia.
        R√©sultat : union d√©dupliqu√©e, IAFD en premier.
        """
        seen = set()
        result = []

        # 1. IAFD en premier (plus fiable)
        for r in scraped:
            if r.get("_source") == "iafd":
                for award in (r.get("awards") or []):
key="***MASKED***"
                    if key not in seen:
                        seen.add(key)
                        result.append(award)

        # 2. Autres sources en d√©duplication
        for r in scraped:
            if r.get("_source") != "iafd":
                for award in (r.get("awards") or []):
key="***MASKED***"
                    if key not in seen:
                        seen.add(key)
                        result.append(award)

        return {
            "db_value":  db.get("awards", []),
            "merged":    result,
            "sources":   {r["_source"]: r.get("awards", []) for r in scraped},
            "strategy":  "union_iafd_first"
        }

    # ‚îÄ‚îÄ TRIVIA ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _merge_trivia(self, db, scraped):
        """
        FreeOnes "Additional Information" est la meilleure source.
        TheNude bios studio = contexte suppl√©mentaire.
        Babepedia peut avoir quelques lignes.
        ‚Üí Proposer les 3 s√©par√©ment pour que l'utilisateur choisisse/combine.
        """
        by_source = {}
        for r in scraped:
            trivia = r.get("trivia")
            if trivia:
                by_source[r["_source"]] = trivia

        # S√©lection automatique par priorit√©
        best = None
        TRIVIA_PRIORITY = ["freeones", "thenude", "babepedia"]
        for src in TRIVIA_PRIORITY:
            if src in by_source:
                best = by_source[src]
                break

        return {
            "db_value":   db.get("trivia"),
            "by_source":  by_source,
            "suggestion": best,
            "strategy":   "best_source_priority"
        }

    # ‚îÄ‚îÄ BIO / DETAILS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _merge_details(self, db, scraped):
        """
        FreeOnes = bio narrative la plus compl√®te.
        TheNude = bios studio (contexte professionnel).
        Babepedia = court texte.
        ‚Üí Option 1 : FreeOnes seule
        ‚Üí Option 2 : Toutes sources concat√©n√©es (s√©parateurs clairs)
        ‚Üí Option 3 : DB actuelle
        """
        by_source = {}
        for r in scraped:
            detail = r.get("details")
            if detail and len(detail) > 50:
                by_source[r["_source"]] = detail

        # Construire option "fusion" ordonn√©e
        DETAIL_ORDER = ["freeones", "babepedia", "thenude", "boobpedia"]
        fused_parts = []
        for src in DETAIL_ORDER:
            if src in by_source:
                fused_parts.append(f"[Source: {src.upper()}]\n{by_source[src]}")

        fused = "\n\n---\n\n".join(fused_parts) if fused_parts else None

        return {
            "db_value":  db.get("details"),
            "by_source": by_source,
            "fused":     fused,
            "strategy":  "user_choice"
        }

    # ‚îÄ‚îÄ TATTOOS / PIERCINGS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _merge_body_art(self, field: str, db, scraped):
        """
        Hi√©rarchie : IAFD/FreeOnes (structur√©) > Babepedia/TheNude (flat).
        Union d√©dupliqu√©e par (position, description).
        Les entr√©es position="multiple" ne sont gard√©es que si aucune
        entr√©e structur√©e n'existe pour ce champ.
        """
        structured = []
        flat = []
        seen = set()

        QUALITY_ORDER = ["iafd", "freeones", "thenude", "babepedia"]
        for source in QUALITY_ORDER:
            for r in scraped:
                if r.get("_source") != source:
                    continue
                for item in (r.get(field) or []):
                    pos  = (item.get("position") or "").lower().strip()
                    desc = (item.get("description") or "").lower().strip()
key="***MASKED***"
                    if key in seen:
                        continue
                    seen.add(key)
                    if pos == "multiple":
                        flat.append(item)
                    else:
                        structured.append(item)

        # Utiliser flat uniquement si aucune entr√©e structur√©e
        merged = structured if structured else flat

        return {
            "db_value":  db.get(field, ""),
            "merged":    merged,
            "sources":   {r["_source"]: r.get(field, []) for r in scraped},
            "strategy":  "structured_priority"
        }

    # ‚îÄ‚îÄ TAGS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _merge_tags(self, db, scraped):
        """
        Union de toutes les sources, d√©duplification insensible √† la casse.
        """
        seen = set()
        merged = []
        for r in scraped:
            for tag in (r.get("tags") or []):
key="***MASKED***"
                if key and key not in seen:
                    seen.add(key)
                    merged.append(tag.strip().title())
        merged.sort()
        return {
            "db_value": db.get("tags", []),
            "merged":   merged,
            "sources":  {r["_source"]: r.get("tags", []) for r in scraped},
            "strategy": "union_all"
        }

    # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _merge_urls(self, db, scraped):
        """
        Agr√©ger databases + social_media de toutes les sources.
        Priorit√© : freeones > iafd > babepedia > thenude (pour ordre d'affichage).
        """
        URL_PRIORITY = ["freeones", "iafd", "babepedia", "thenude", "boobpedia"]
        merged = {}
        for source in reversed(URL_PRIORITY):
            for r in scraped:
                if r.get("_source") != source:
                    continue
                for key, url in (r.get("urls") or {}).items():
                    if url:
                        merged[key] = url
        return {
            "db_value": db.get("urls", []),
            "merged":   merged,
            "strategy": "priority_merge"
        }


============================================================
[71/124] Legacy\services\phase2_scraper.py
------------------------------------------------------------
"""
Phase2ScraperService ‚Äî orchestre les 4 extracteurs et g√®re le cache.
"""
from services.scrape_cache import ScrapeCache
from services.extractors.iafd import IafdExtractor
from services.extractors.freeones import FreeonesExtractor
from services.extractors.thenude import ThenudeExtractor
from services.extractors.babepedia import BabepediaExtractor


class Phase2ScraperService:
    """
    Lance le scraping Phase 2 sur toutes les sources disponibles.
    
    Utilise le ScrapeCache pour √©viter le double scraping.
    Construit les URLs automatiquement si non disponibles.
    """

    def __init__(self):
        self.extractors = [
            IafdExtractor(),
            FreeonesExtractor(),
            ThenudeExtractor(),
            BabepediaExtractor(),
        ]

    def scrape(
        self,
        performer_name: str,
        known_urls: list[str] | None = None,
        progress_callback=None,
    ) -> list[dict]:
        """
        Scraper toutes les sources pour un performer.
        
        Args:
            performer_name: Nom du performer
            known_urls: URLs d√©j√† connues (depuis DB Stash)
            progress_callback: Callback(source_name, status) pour le progr√®s
            
        Returns:
            Liste de dicts Phase 2 (un par source r√©ussie)
        """
        results = []
        known_urls = known_urls or []

        # Mapper les URLs connues par source
        url_map = self._map_urls_to_sources(known_urls)

        for i, extractor in enumerate(self.extractors):
            source = extractor.SOURCE_NAME
            
            if progress_callback:
                progress_callback(source, f"Scraping {source}...")

            # D√©terminer l'URL √† utiliser
            url = url_map.get(source)
            if not url:
                url = extractor.build_url(performer_name)
            
            if not url:
                print(f"[Phase2Scraper] Pas d'URL pour {source}, skip")
                continue

            try:
                # V√©rifier le cache d'abord
                cached = ScrapeCache.get(url)
                if cached:
                    print(f"[Phase2Scraper] Cache hit pour {source}: {url}")
                    results.append(cached)
                    continue

                # Scraper
                print(f"[Phase2Scraper] Scraping {source}: {url}")
                data = extractor.extract_from_url(url)
                
                if data:
                    # Stocker en cache
                    ScrapeCache.set(url, data)
                    results.append(data)
                    print(f"[Phase2Scraper] {source} OK ‚Äî "
                          f"awards:{len(data.get('awards',[]))}, "
                          f"tags:{len(data.get('tags',[]))}, "
                          f"tattoos:{len(data.get('tattoos',[]))}")
                          
            except Exception as e:
                print(f"[Phase2Scraper] Erreur {source}: {e}")
                if progress_callback:
                    progress_callback(source, f"Erreur: {e}")

        if progress_callback:
            progress_callback("done", f"Scraping termin√© ‚Äî {len(results)} sources")

        return results

    def _map_urls_to_sources(self, urls: list[str]) -> dict[str, str]:
        """Mapper les URLs connues aux noms de source."""
        url_map = {}
        for url in urls:
            url_lower = url.lower()
            if "iafd.com" in url_lower:
                url_map["iafd"] = url
            elif "freeones.com" in url_lower:
                url_map["freeones"] = url
            elif "thenude.com" in url_lower or "thenude.eu" in url_lower:
                url_map["thenude"] = url
            elif "babepedia.com" in url_lower:
                url_map["babepedia"] = url
        return url_map


============================================================
[72/124] Legacy\services\scrape_cache.py
------------------------------------------------------------
"""
Cache en m√©moire des r√©sultats de scraping pour √©viter le double
appel r√©seau entre Phase 1 et Phase 2.
"""


class ScrapeCache:
    """
    Cache class-level partag√© entre toutes les instances.
    Stocke les r√©sultats de scraping par URL.
    """
    _data: dict[str, dict] = {}

    @classmethod
    def set(cls, url: str, data: dict):
        """Stocker le r√©sultat de scraping pour une URL."""
        cls._data[url] = data

    @classmethod
    def get(cls, url: str) -> dict | None:
        """R√©cup√©rer le r√©sultat de scraping pour une URL, ou None."""
        return cls._data.get(url)

    @classmethod
    def has(cls, url: str) -> bool:
        """V√©rifier si une URL est en cache."""
        return url in cls._data

    @classmethod
    def clear(cls):
        """Vider tout le cache."""
        cls._data.clear()

    @classmethod
    def size(cls) -> int:
        """Nombre d'entr√©es en cache."""
        return len(cls._data)


============================================================
[73/124] Legacy\start.bat
------------------------------------------------------------
@echo off
REM Optimized launcher: checks venv, dependencies, installs if missing, generates a report, and then runs main.py

REM Set venv directory name
set VENV_DIR=.venv

REM Check if venv exists and is valid
if not exist %VENV_DIR%\Scripts\activate.bat (
    echo Creating virtual environment...
    python -m venv %VENV_DIR%
    if %errorlevel% neq 0 (
        echo ERROR: Failed to create virtual environment. Please ensure Python is installed and in your PATH.
        pause
        exit /b 1
    )
)

REM Activate venv
call %VENV_DIR%\Scripts\activate.bat

REM Check and install dependencies if needed
echo Upgrading pip and installing requirements from requirements.txt...
python -m pip install --upgrade pip >nul
pip install -r requirements.txt

REM --- System Hardware and AI Report ---

REM Verify for NVIDIA GPU and Ollama, and generate a report
echo Generating system report...
for /f %%i in ('powershell -Command "Get-Date -format 'yyyyMMdd-HHmmss'"') do set TIMESTAMP=%%i
set REPORT_FILE=rapport_Ollama_%TIMESTAMP%.txt

(
    echo Report generated on %DATE% at %TIME%
    echo.
) > %REPORT_FILE%

REM Check for NVIDIA GPU
nvidia-smi >nul 2>&1
if %errorlevel% neq 0 (
    echo WARNING: NVIDIA GPU not found or nvidia-smi is not in your PATH.
    (
        echo === NVIDIA GPU Status ===
        echo NVIDIA GPU not found or nvidia-smi command failed.
    ) >> %REPORT_FILE%
) else (
    echo NVIDIA GPU detected.
    (
        echo === NVIDIA GPU Status ===
        nvidia-smi
    ) >> %REPORT_FILE%
)

REM Check for Ollama
where ollama >nul 2>&1
if %errorlevel% neq 0 (
    echo WARNING: Ollama not found. Please ensure it is installed and in your PATH.
    (
        echo.
        echo === Ollama Status ===
        echo Ollama not found.
    ) >> %REPORT_FILE%
) else (
    echo Ollama detected.
    (
        echo.
        echo === Ollama Models List ===
        ollama list
    ) >> %REPORT_FILE%
)

echo Report saved to %REPORT_FILE%
echo.

REM --- Launching Application ---
echo Starting the main application...
python main.py

pause


============================================================
[74/124] Legacy\tests\__init__.py
------------------------------------------------------------


============================================================
[75/124] Legacy\tests\test_db.py
------------------------------------------------------------
import unittest
from services.db import PerformerDB

class TestDB(unittest.TestCase):
    def setUp(self):
        self.db = PerformerDB()

    def tearDown(self):
        self.db.close()

    def test_get_known_performers(self):
        performers = self.db.get_known_performers()
        self.assertIsInstance(performers, list)
        if performers:
            self.assertIsInstance(performers[0], str)

if __name__ == '__main__':
    unittest.main()


============================================================
[76/124] Legacy\tests\test_performer_fields.py
------------------------------------------------------------
import unittest
from gui.performer_frame import PerformerFrame
import tkinter as tk

class TestPerformerFields(unittest.TestCase):
    def setUp(self):
        self.root = tk.Tk()
        # On passe un stash_id de test
        self.frame = PerformerFrame(self.root, "1")

    def tearDown(self):
        self.root.destroy()

    def test_phase1_fields(self):
        # Le PerformerFrame d√©marre en Phase 1
        phase1_frame = self.frame.current_frame
        
        # V√©rifier que les champs de la Phase 1 sont bien cr√©√©s
        for field in phase1_frame.fields_list:
            self.assertIn(field, phase1_frame.fields)
            self.assertIsNotNone(phase1_frame.fields[field])

    def test_phase2_fields(self):
        # Naviguer vers la Phase 2
        self.frame.goto_phase2()
        phase2_frame = self.frame.current_frame

        # V√©rifier que les champs de la Phase 2 sont bien cr√©√©s
        for field in phase2_frame.fields_list:
            self.assertIn(field, phase2_frame.fields)
            self.assertIsNotNone(phase2_frame.fields[field])

if __name__ == "__main__":
    unittest.main()


============================================================
[77/124] Legacy\utils\__init__.py
------------------------------------------------------------


============================================================
[78/124] Legacy\utils\audit_markers.csv
------------------------------------------------------------
Ôªøid,title,duration,markers,points,tags,overlaps,short,long,exact_dupes,penalty
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",966.67,6,0,2,0,0,0,0,0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",800.35,6,0,2,0,0,1,0,1
179,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc5",915.47,5,0,2,0,0,0,0,0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",1138.83,8,0,2,0,0,0,0,0
181,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc1",955.18,2,0,1,0,0,0,0,0
182,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc5",336.28,1,0,1,0,0,1,0,1
183,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc3,986.29,3,0,1,0,0,0,0,0
186,"Sai Tai Tiger in Frauen Knast, Teufelsbrut Hinter Gittern! Sc2",1004.31,1,0,1,0,0,0,0,0
187,Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc1,669.9,1,0,1,0,0,1,0,1
188,Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc4,523.91,2,0,1,0,0,0,0,0
190,"Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc7",444.5,3,0,2,0,0,1,0,1
191,Aderes Quin in StepMom Gets Double Dick,2797.73,11,0,2,0,0,0,0,0
192,Alejandra Rico in Que Rico!,1346.01,4,0,2,0,0,0,0,0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",1367.91,5,0,2,0,0,0,0,0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",2422.35,7,0,2,0,0,0,0,0
195,"Athenea Rose in Safe Cracked, Holes Filled",2387.01,6,0,4,0,0,1,0,1
196,Ava Devine in Milf Asian Cummouth Facial,1971.46,8,0,2,0,0,0,0,0
197,"Barbie Sins in Anal Domination, 6on1 DAP",2676.75,11,0,4,0,0,0,0,0
198,Barbie Sins in DP Bandits! Sc4,2951.67,8,0,1,0,0,0,0,0
199,Blanche Bradburry in 10 Guy Anal Showdown,5188.42,16,0,3,0,0,0,0,0
200,Blanche Bradburry in Gangbang Anal Blitz,2098.1,6,0,1,0,0,0,0,0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,4003.86,10,0,4,0,0,0,0,0
202,Blanche Bradburry in Rough DAP Gangbang,3178.32,6,0,2,0,0,0,0,0
203,Blanche Bradburry in Triple Penetration Madness,3097.07,8,0,2,0,0,0,0,0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3392.67,5,0,1,0,0,0,0,0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3460.38,7,0,2,0,0,0,0,0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3478.7,7,0,2,0,0,0,0,0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",4584.67,6,0,3,0,0,0,0,0
208,Cherry Kiss in DP Bandits! Sc2,2380.63,5,0,1,0,0,0,0,0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3294.08,17,0,6,0,0,0,0,0
210,Destiny Mira in Put My Back Into It,1176.67,2,0,1,0,0,0,0,0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",6115.7,16,0,4,0,0,0,0,0
212,Jolee Love in Craving For Cocks,3458.3,7,0,3,0,0,0,0,0
213,Jolee Love in DP Bandits! 2 Sc3,3020.64,8,0,3,0,0,0,0,0
214,Jolee Love in Hardcore DAP Creampie,2874.07,5,0,2,0,0,0,0,0
215,,2020.04,3,0,1,0,0,0,0,0
216,,1843.83,2,0,1,0,0,0,0,0
217,,2426.39,4,0,2,0,0,0,0,0
218,,2378.98,2,0,1,0,0,0,0,0
219,,2209.99,12,0,3,0,0,0,0,0
220,,2484.12,3,0,1,0,0,0,0,0
221,,1027.97,6,0,3,0,0,0,0,0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",3097.41,18,0,3,0,0,0,0,0
223,,2142.37,14,0,4,0,0,0,0,0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2381.19,10,0,3,0,0,0,0,0
225,,2856.41,9,0,2,0,0,0,0,0
226,,2213.08,14,0,4,0,0,0,0,0
227,Adriana Chechik in Horny Housewives 6 Sc2,2073.71,8,0,2,0,0,0,0,0
228,Adira Allure in Airtight Diva Sc4,1848.07,4,0,2,0,0,0,0,0
229,Alexis Tae in Gangbang Sluts Sc1,2470.0,15,0,3,0,0,0,0,0
230,Amirah Adara in DP Bandits! Sc1,3162.37,8,0,1,0,0,0,0,0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,1838.44,11,0,4,0,0,0,0,0
232,Anai Loves in My stepmom,1847.97,4,0,2,0,0,0,0,0
233,Angela White in Angela's Airtight DP,2710.14,7,0,3,0,0,1,0,1
234,Anissa Kate in Love Everything About Her,2003.57,4,0,2,0,0,0,0,0
235,Anissa Kate in Sizziling Double Penetration Delight,1963.0,7,0,3,0,0,0,0,0
236,"Anissa Kate, Olivia Del Rio in Personal Guide Chapter 1",1572.27,1,0,1,0,0,0,0,0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",2276.54,6,0,2,0,0,0,0,0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,2911.74,11,0,3,0,0,0,0,0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",1808.53,10,0,2,0,0,0,0,0
240,Assh Lee in All Over That Cock,1501.5,2,0,1,0,0,0,0,0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",1811.95,5,0,1,0,0,0,0,0
242,Baby Gemini in Ricky's Room Blowbang,930.94,7,0,1,0,0,0,0,0
243,Barbie Sins in DAP with Creampie,2322.6,5,0,1,0,0,0,0,0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4231.12,21,0,2,0,0,0,0,0
245,Belinha Baracho in Intense 5on1 Gangbang,3285.92,14,0,3,0,0,0,0,0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3969.33,8,0,1,0,0,0,0,0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",4620.67,6,0,3,0,0,0,0,0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",3686.05,6,0,3,0,0,0,0,0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",2988.67,8,0,3,0,0,0,0,0
250,,7096.8,15,0,3,0,0,0,0,0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",3115.78,7,0,1,0,0,0,0,0
252,Bonny Bon in Sexual Rage 2 Sc3,2024.6,4,0,1,0,0,1,0,1
253,Cali Caliente in The Gangbang Part IV,2273.1,12,0,3,0,0,0,0,0
254,Carla Morelli in Gangbang with 4 Cocks,2008.38,6,0,2,0,0,1,0,1
255,Carla Morelli in Hot for Teacher,1324.61,3,0,2,0,0,0,0,0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4683.35,16,0,4,0,0,0,0,0
257,Chloe Amour in Mon Amour Sc1,2140.95,14,0,3,0,0,0,0,0
258,Chloe Amour in Mon Amour Sc2,1705.6,4,0,2,0,0,0,0,0
259,Chloe Amour in Mon Amour Sc3,2999.48,12,0,4,0,0,0,0,0
260,"Chloe Amour, Jennifer White in Mon Amour Sc4",2322.44,4,0,2,0,0,0,0,0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,4135.34,17,0,4,0,0,0,0,0
262,"Cindy Starfall, Gaia in Swappers Sc1",1883.39,5,0,3,0,0,0,0,0
263,Cookie Cream in Asian 1st BBG Threesome & DP,1959.48,6,0,2,0,0,0,0,0
265,"Danielle Renee, MarsFoxxx in Group Bang",2928.73,16,0,4,0,0,0,0,0
266,Daria Glower in Die Haremsw√§chterin des √ñl Scheichs Sc6,872.11,2,0,1,0,0,1,0,1
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,3879.57,12,0,3,0,0,0,0,0
268,Anissa Kate in Les Retrouvailles Sc5,1655.17,10,0,2,0,0,0,0,0
269,Emmanuelle Noire in Busty Ebony Beauty,2046.87,10,0,3,0,0,0,0,0
270,Francesca Le in Lewood Gangbang Battle of the MILFs Sc1,313.95,1,0,1,0,0,0,0,0
271,Francesca Le in Lewood Gangbang Battle of the MILFs Sc2,322.69,2,0,1,0,0,0,0,0
272,Francesca Le in Lewood Gangbang Battle of the MILFs Sc3,351.55,1,0,1,0,0,1,0,1
273,Francesca Le in Lewood Gangbang Battle of the MILFs Sc4,353.15,2,0,1,0,0,0,0,0
274,Francesca Le in Lewood Gangbang Battle of the MILFs Sc5,372.77,2,0,2,0,0,0,0,0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,2935.1,18,0,3,0,0,0,0,0
276,"Gaia in Screw My Wife, Please 76 Sc2",970.13,1,0,1,0,0,0,0,0
277,Gaia in Throated 39 Sc5,1687.53,9,0,3,0,0,0,0,0
278,Hannah Jo in Thick Dick Threesome,2248.52,10,0,1,0,0,0,0,0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,4474.28,23,0,3,0,0,0,0,0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",1356.44,8,0,3,0,0,0,0,0
281,Jada Fire in Assault That Ass 8 Sc3,1772.08,6,0,3,0,0,0,0,0
282,Jada Fire in Throat Yogurt 2 Sc1,825.17,2,0,2,0,0,1,0,1
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,1741.89,5,0,1,0,0,0,0,0
284,Jasminy Villar in The Stepfather And His Four Friends,3601.9,16,0,4,0,0,0,0,0
285,Jena LaRose in Blacks On Blondes,2051.49,6,0,1,0,0,0,0,0
286,Jennifer White in Jennifer White Overload Sc1,3271.9,19,0,3,0,0,0,0,0
287,Jennifer White in Jennifer White Overload Sc2,3398.01,14,0,4,0,0,1,0,1
288,Jennifer White in Jennifer White Overload Sc3,2525.56,17,0,4,0,0,0,0,0
289,Jenny Pretinha in Fucked by 3 Huge Cocks,1525.5,2,0,2,0,0,0,0,0
290,Juelz Ventura in POV BBC Airtight Gangbang,2889.31,15,0,2,0,0,0,0,0
291,"Jureka Del Mar, Maylee Fun in Asian Hotties Work to Cure",927.16,1,0,1,0,0,0,0,0
292,Katalina Kyle in Ass Worship 18 Sc3,2497.1,12,0,3,0,0,0,0,0
293,Katalina Kyle in Takes Every Inch Of Manuel,1619.48,7,0,2,0,0,0,0,0
294,Katia Belinii in Swallowing 5 Big Loads,1602.46,8,0,3,0,0,0,0,0
295,Kayla Carera in Bride Bangers Sc1,1466.64,1,0,1,0,0,0,0,0
296,Kayla Carrera in Anal Integrity Sc1,2701.5,3,0,3,0,0,0,0,0
297,Kaylani Lei in Asian Fuck Machines Sc5,2666.77,5,0,3,0,0,0,0,0
298,Kazumi Squirts in BBC Orgy Room,4253.97,18,0,3,0,0,0,0,0
299,Kazumi Squirts in Gangbang With Piss and DP,3392.33,15,0,5,0,0,0,0,0
301,Kelly Oliveira in Assfucked 4on1 with DP,2982.93,6,0,2,0,0,0,0,0
302,Kelly Oliveira in First DP for Brazilian Teen,1941.68,4,0,2,0,0,0,0,0
303,Kelly Oliveira in Sexy Latina DAP 3on1,2731.42,5,0,3,0,0,0,0,0
304,"Kelly Oliveira, Veronica Leal in 5on2 orgy with DP",3354.17,5,0,3,0,0,0,0,0
305,Keri Sable in Cum Filled Asshole Overload 2 Sc1,2796.03,3,0,2,0,0,1,0,1
306,Kim XXX in Manga Total Vollgespritzt Sc1,1921.96,7,0,3,0,0,0,0,0
307,Kim XXX in Tiny Whore Kim XXX Stuffed,2071.48,5,0,2,0,0,0,0,0
308,Kira Thorn in Balls Deep 5on2,2984.69,7,0,2,0,0,0,0,0
309,Kitana Montana in Birthday Threeway,1767.27,6,0,2,0,0,0,0,0
310,Kitana Montana in Post,2356.53,10,0,3,0,0,0,0,0
311,Laura Fiorentino in 6on1 Swallow,3877.5,14,0,6,0,0,0,0,0
312,Lela Star in Assparade 54 Sc2,719.83,2,0,2,0,0,0,0,0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,2180.24,5,0,3,0,0,0,0,0
314,Lolly Ink in True Gonzo Sc5,1580.37,4,0,3,0,0,1,0,1
315,Luna Star in Double Stuffed,2003.93,8,0,2,0,0,0,0,0
316,Luna Star in Why She's A Pornstar,2068.02,7,0,3,0,0,0,0,0
317,Marilyn Johnson in Airtight Diva Sc1,1617.07,2,0,1,0,0,0,0,0
318,"Mariska, Tekohas in Cum Cum Gangbang Orgy",996.07,6,0,2,0,0,0,0,0
319,Maxine X in I'm Here for the Gang Bang! Sc1,2883.66,9,0,4,0,0,0,0,0
320,Maxine X in I'm Here for the Gang Bang! Sc2,3520.19,11,0,2,0,0,0,0,0
321,Megan Rain in 10 Cock Blowbang!,1505.33,2,0,2,0,0,1,0,1
322,Melissa Hot in Fucked by 4 Big Cocks,2871.21,8,0,3,0,0,0,0,0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",3110.92,7,0,1,0,0,0,0,0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1472.38,5,0,2,0,0,0,0,0
325,Mercedes Carrera in MILF Cumsluts Sc4,2974.8,10,0,3,0,0,0,0,0
326,Mia Trejsi in 100% Hell,3432.54,14,0,3,0,0,0,0,0
327,"Mia Trejsi, Ria Sunn in Angels Of Hardcore 3 Sc1",368.17,2,0,2,0,0,0,0,0
328,Mih Ninfetinha in 4on1 with DP,2697.69,12,0,3,0,0,0,0,0
329,Miss Teela in First Time 10 Gangbang,1702.4,9,0,2,0,0,0,0,0
330,Monika Fox in DP Fantasies 11 Sc3,2549.07,6,0,3,0,0,1,0,1
331,Natasha Teen in Pussy DAPTAP,2914.04,6,0,2,0,0,0,0,0
332,Nia Nacci in Cum Bang 15 Sc3,2274.57,10,0,3,0,0,1,0,1
333,Nia Nacci in We Fuck Black Girls 14 Sc5,1953.01,2,0,2,0,0,0,0,0
334,Nia Nacci in White Out 9 Sc2,3971.75,17,0,4,0,0,0,0,0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",2217.92,11,0,2,0,0,0,0,0
336,Nina Elle in Big Wet Milf Asses Sc2,1890.05,3,0,1,0,0,0,0,0
337,Nina Elle in Gang Bang Addiction Sc4,2890.12,7,0,2,0,0,0,0,0
338,Nina Elle in MILF Cumsluts Sc3,2316.06,2,0,2,0,0,1,0,1
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1654.47,5,0,4,0,0,0,0,0
340,Phoenix Marie in Ass Worship 13 Sc4,2550.74,8,0,1,0,0,0,0,0
341,Rachele Richey in Gangbang Audition,2679.13,8,0,2,0,0,0,0,0
342,Rose Lynn in Airtight Diva Sc3,1585.07,5,0,3,0,0,0,0,0
343,Sadie Summers in Gangbang Sluts Sc2,3098.0,19,0,4,0,0,0,0,0
344,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc2,692.67,4,0,1,0,0,0,0,0
345,"Sai Tai Tiger, Daria Glower in Die Haremsw√§chterin des √ñl Scheichs Sc5",912.0,1,0,1,0,0,0,0,0
346,"Sai Tai Tiger, Daria Glower, Valerie Hilton in Die Haremsw√§chterin des √ñl Scheichs Sc1",1574.35,1,0,1,0,0,1,0,1
347,Sandra Parker in 1st at GB Junkies,1725.71,8,0,2,0,0,0,0,0
348,Sandra Parker in Anal Driller 9 Sc3,1411.03,6,0,1,0,0,0,0,0
349,Sandra Parker in Analizator Sc4,1873.37,2,0,2,0,0,1,0,1
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1852.02,4,0,1,0,0,0,0,0
351,Sandra Parker in Double Stuffed 8 Sc1,1689.02,5,0,1,0,0,0,0,0
352,Sara Retali in BBC Piss Gangbang,2231.14,13,0,2,0,0,0,0,0
353,Sara Retali in Slut Cant Get Enough Gangbang,1841.15,9,0,4,0,0,0,0,0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1207.62,6,0,2,0,0,0,0,0
355,"Sara Retali, Sapphire Astrea in Spa Day Gone Wild",930.08,2,0,2,0,0,0,0,0
356,Sarai Minx in Big Tit Slut Milks Cock,1562.59,5,0,3,0,0,0,0,0
357,"Sheila Ortega, Sara Retali, Anny Star in 3 Latinas by the Pool",2443.6,5,0,3,0,0,0,0,0
358,Shyla Stylez in Anal Integrity Sc2,2097.2,4,0,3,0,0,0,0,0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,3149.11,14,0,2,0,0,0,0,0
360,Skin Diamond in Rump Raiders Sc3,1820.97,3,0,1,0,0,0,0,0
361,,18562.53,50,0,4,0,0,0,0,0
362,Summer Day in America Bukkake Live,1495.04,2,0,2,0,0,1,0,1
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1893.47,11,0,3,0,0,0,0,0
364,Summer Vixen in Gangbang Sluts Sc3,2318.0,14,0,3,0,0,0,0,0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,3808.68,15,0,4,0,0,0,0,0
366,Tekohas in Ass & BigTits,1987.33,6,0,3,0,0,0,0,0
367,Tekohas in Bareback Party in Stuttgart,3021.03,15,0,3,0,0,0,0,0
368,Thai Suzy in WeLoveBukkake 4,993.54,2,0,2,0,0,1,0,1
369,Tia Maria in Cum On Melon Tits,2922.37,8,0,2,0,0,0,0,0
370,Tia Maria in DPd By Two BWCs,1677.93,4,0,2,0,0,0,0,0
371,Tyra Ride in First BBC  DP Gangbang,2411.49,12,0,4,0,0,0,0,0
372,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc10,363.0,1,0,1,0,0,0,0,0
373,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc11,377.98,1,0,1,0,0,0,0,0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,3230.76,17,0,4,0,0,0,0,0
376,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc8,287.95,1,0,1,0,0,0,0,0
378,Veronica Leal in Domination Gangbang,3676.9,18,0,4,0,0,0,0,0
379,Vittoria Devine in DP Pee 5on1,4787.33,17,0,3,0,0,0,0,0
380,Vittoria Devine in Domination Gangbang,2075.61,8,0,4,0,0,0,0,0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,2631.01,4,0,1,0,0,0,0,0
382,Vit√≥ria Beatriz in Edjunior VideoGuru,892.0,2,0,1,0,0,0,0,0
383,Willow Ryder in I Love Anal 3 Sc3,2177.1,11,0,2,0,0,1,0,1
384,Yasmina Khan in Birthday Gangbang,1583.33,6,0,3,0,0,0,0,0
385,Yasmina Khan in Play with 4 Cocks at Once!,2459.47,11,0,1,0,0,0,0,0
386,AJ Applegate in Gangbang Me Sc1,3184.69,4,0,2,0,0,0,0,0
388,Adira Allure in Interracial Blowbang 24 Sc1,2018.72,8,0,4,0,0,1,0,1
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",1200.63,6,0,2,0,0,0,0,0
392,"Adriana Chechik, Gaia in Grease XXX A Parody Sc5",1245.2,2,0,1,0,0,1,0,1
394,Adrianna Luna in Praise The Load 7 Sc1,1590.98,6,0,4,0,0,1,0,1
395,Adrianna Luna in Slut Puppies 5 Sc5,1963.99,7,0,1,0,0,0,0,0
396,Aidra Fox in Gangbanged 7 Sc1,3703.02,7,0,3,0,0,0,0,0
397,Alena Croft in Blacks on Cougars 17 Sc1,1905.91,4,0,2,0,0,1,0,1
398,Alena Croft in Feeding Frenzy 12 Sc2,1668.84,4,0,3,0,0,1,0,1
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2074.42,8,0,3,0,0,0,0,0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",3823.94,9,0,3,0,0,0,0,0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2108.36,9,0,2,0,0,1,0,1
402,Alexis Ford in Gang Bang Addiction Sc3,3074.11,17,0,3,0,0,0,0,0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2406.19,8,0,3,0,0,2,0,2
404,Alexis Monroe in Gang Bang Addiction Sc5,3167.07,7,0,4,0,0,0,0,0
405,Alexis Texas in Gang Bang Addiction Sc1,1871.1,9,0,2,0,0,0,0,0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2099.5,12,0,2,0,0,0,0,0
407,Alina in Annegret Zugekleistert Sc4,1086.13,4,0,2,0,0,0,0,0
409,Alina Lopez in No Going Back Sc1,720.81,1,0,1,0,0,0,0,0
410,Alina Lopez in Perfectly Natural 19 Sc4,1335.45,2,0,1,0,0,0,0,0
411,Alina Lopez in Pussy is The Best Medicine 9 Sc5,643.23,1,0,1,0,0,0,0,0
413,Alina Lopez in Wet Food 9 Sc1,3173.84,17,0,2,0,0,1,0,1
415,"Alyssa Divine, Kiki Minaj, Victoria Pure, Victoria Summers in Backstage Bangers Sc1",1698.25,5,0,2,0,0,0,0,0
416,Amara Romani in Gangbang Auditions 31 Sc3,3535.03,12,0,4,0,0,0,0,0
418,Amber Deen in Dogfart Invades Europe! 4 Sc1,2030.69,4,0,2,0,0,1,0,1
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2825.64,11,0,3,0,0,0,0,0
420,Amelia Sadaat in White Dicks in Black Chics 3 Sc4,631.71,1,0,1,0,0,0,0,0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2221.59,9,0,2,0,0,1,0,1
422,Andi Anderson in Young Harlots Gang Bang Sc2,4059.46,23,0,4,0,0,0,0,0
423,"Angel Eyes, Jada Fire in Freak Nasty Sc1",681.83,3,0,2,0,0,0,0,0
424,Angela White in Going All Out with a Gangbang 2 Sc4,2827.82,8,0,4,0,0,0,0,0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3376.44,15,0,3,0,0,0,0,0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",1079.8,6,0,3,0,0,1,0,1
428,Alejandra Rico in Intense Latin Gangbang,1350.14,5,0,2,0,0,0,0,0
429,Alejandra Rico in Tons of Cum,1581.57,5,0,2,0,0,1,0,1
430,Alex Grey in A Dirty Submissive Slut For Cock,1393.96,3,0,1,0,0,1,0,1
431,Alexa Nova in GangBang Creampie 246,2005.99,7,0,1,0,0,0,0,0
432,Alexa Payne in Stepmom Seduction on the Kitchen Counter,2150.67,5,0,2,0,0,0,0,0
433,"Alexia Vendome, Sothy Hiko, Yasmine Lafitte in No Limits Anal and Oral Orgy",1317.99,4,0,1,0,0,0,0,0
434,Alexis Kay in GangBang Creampie 417,782.58,4,0,3,0,0,0,0,0
435,Alicia Ribeiro in Anal and Oral with 3 Big Black Cocks,2701.5,4,0,2,0,0,0,0,0
436,Alicia Trece in Rough Gangbang and Pee Play,2822.9,8,0,3,0,0,0,0,0
437,Aliyah Taylor in Gang Bang All Her Holes,1805.91,7,0,3,0,0,0,0,0
438,Allatra Hot in MILF Craving Hardcore Attention,1370.7,4,0,1,0,0,0,0,0
439,Alura Jenson in GangBang Creampie 240,2719.32,10,0,3,0,0,0,0,0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",3930.19,11,0,3,0,0,0,0,0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2893.35,5,0,3,0,0,0,0,0
442,Amari Anne in You Wanna Cheat Again,2416.03,6,0,2,0,0,0,0,0
443,Amirah Adara in Rough Gangbang Session,3034.87,2,0,1,0,0,0,0,0
444,Amy Reid in AllOut Blowbang Session,1026.51,2,0,2,0,0,1,0,1
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,3231.2,7,0,3,0,0,0,0,0
446,Ana J√∫lia in DP com a Mulata Cavala,2450.12,7,0,3,0,0,0,0,0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",1562.35,3,0,2,0,0,0,0,0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",5115.56,4,0,2,0,0,0,0,0
449,Angel Lima in Big Butt Airtight Show,2837.54,5,0,2,0,0,0,0,0
450,Angel Lima in Hardcore Brazilian Double Anal,3478.13,7,0,3,0,0,0,0,0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3114.73,8,0,2,0,0,0,0,0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",1476.81,6,0,2,0,0,0,0,0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3114.77,11,0,2,0,0,0,0,0
454,Angel Sins in Short Stuff Big Stuff,2863.71,2,0,1,0,0,0,0,0
455,"Angel Smalls, Anna De Ville, Barbie Sins, Jureka Del Mar, May Thai, Nathaly Cherie, Selvaggia in Messy Facial Compilation",479.91,1,0,1,0,0,1,0,1
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",4638.91,13,0,4,0,0,0,0,0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,2835.8,8,0,1,0,0,0,0,0
459,Ania Kinski in Kinky DP Session At The Clinic,2079.32,1,0,1,0,0,1,0,1
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,2198.79,8,0,2,0,0,0,0,0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",4163.26,11,0,3,0,0,0,0,0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",1977.47,3,0,2,0,0,0,0,0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",2405.01,4,0,1,0,0,0,0,0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3407.16,7,0,2,0,0,0,0,0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",1761.96,8,0,2,0,0,0,0,0
468,Anissa Kate in A Hot Surfer Threesome,1927.33,6,0,2,0,0,0,0,0
469,Anissa Kate in Hardcore Business Meeting,2364.29,6,0,3,0,0,0,0,0
470,Anissa Kate in Poolside DP with BBC and Cumshot Finish,3007.15,6,0,2,0,0,0,0,0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3045.74,8,0,3,0,0,0,0,0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3016.48,7,0,2,0,0,0,0,0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",2188.8,8,0,3,0,0,0,0,0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",2800.07,7,0,3,0,0,1,0,1
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",913.45,7,0,2,0,0,0,0,0
477,Anni Star in Lingerie Pleasure Premi√®re,841.89,2,0,2,0,0,0,0,0
479,April Snow in GangBang Creampie 232,2387.31,11,0,3,0,0,0,0,0
480,"Ariel Pure Magic, Zoey Reyes in Dominican Oil Twins",997.95,1,0,1,0,0,0,0,0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,2230.09,4,0,1,0,0,0,0,0
482,"Ashby Winter in Vogue 2, Part 5",2819.84,6,0,3,0,0,0,0,0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,945.82,5,0,2,0,0,0,0,0
484,Ashley Cumstar in Gangbang Party,685.52,4,0,2,0,0,0,0,0
485,Athenea Rose in 5on1 Hardcore Gangbang,3371.07,15,0,3,0,0,0,0,0
486,Athenea Rose in 7on1 DAP Gangbang,3957.23,12,0,5,0,0,0,0,0
487,Athenea Rose in Airtight 6on1 Destruction,3391.51,16,0,3,0,0,0,0,0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,3491.85,11,0,3,0,0,0,0,0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5211.6,22,0,5,0,0,0,0,0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,4281.95,14,0,5,0,0,0,0,0
491,Athenea Rose in Hardcore Interracial DAP,3057.12,2,0,1,0,0,0,0,0
492,Athenea Rose in Hecho en Medelln,3900.67,6,0,3,0,0,0,0,0
493,Athenea Rose in Intense Anal Destruction,1839.23,2,0,1,0,0,0,0,0
494,Athenea Rose in Loves Public Anal,2607.95,2,0,1,0,0,0,0,0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,1953.07,1,0,1,0,0,0,0,0
496,Athenea Rose in Playing with 3 BBC,2441.44,5,0,1,0,0,0,0,0
497,Athenea Rose in PremiumBukkake #1,1263.3,5,0,2,0,0,1,0,1
498,Athenea Rose in PremiumBukkake #2,1064.0,2,0,2,0,0,2,0,2
499,Athenea Rose in PremiumBukkake #3,2329.59,4,0,2,0,0,1,0,1
500,Athenea Rose in Sex Crazed Slut 4on1,2765.94,8,0,4,0,0,0,0,0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,3828.96,15,0,5,0,0,0,0,0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",3745.63,3,0,1,0,0,0,0,0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",4161.3,10,0,3,0,0,0,0,0
504,Aubrey Black in GangBang Creampie 225,1779.39,8,0,3,0,0,0,0,0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",4394.87,12,0,4,0,0,0,0,0
506,Avery Jane in Brutal 7on1 DAP Birthday,3513.58,10,0,5,0,0,0,0,0
507,Avery Jane in Milking Mike Adriano,2301.68,4,0,3,0,0,0,0,0
508,Avery Jane in Piss Soaked Backdoor Debut,4138.75,16,0,5,0,0,0,0,0
509,Avi Love in GangBang Creampie 216,2673.71,11,0,4,0,0,0,0,0
511,Baby Gemini in All About The Booty,1607.66,1,0,1,0,0,0,0,0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,2648.0,10,0,3,0,0,1,0,1
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",3220.68,6,0,2,0,0,0,0,0
514,Barbie Esm in Rough 4on1 DAP Fantasy,3077.02,12,0,3,0,0,0,0,0
515,Barbie Sins in Barbie Gets wet with 2 BBC,2173.76,6,0,2,0,0,0,0,0
517,"Barbie Sins in DAP, Piss and Power Play",3215.13,8,0,3,0,0,0,0,0
518,Barbie Sins in No Holes Barred Gonzo Assault,3180.83,14,0,4,0,0,0,0,0
519,Barbie Sins in Rough DAP & Swallow Madness,2896.44,12,0,4,0,0,0,0,0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",1603.11,4,0,2,0,0,0,0,0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",3022.73,9,0,1,0,0,0,0,0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",2763.36,12,0,2,0,0,0,0,0
931,"Sarai Minx, Savanah Storm in Messy Manicure Threesome",1798.08,6,0,2,0,0,0,0,0
1008,"Sai Tai Tiger, Salma De Nora in Die Haremsw√§chterin des √ñl Scheichs Sc4",1003.34,3,0,1,0,0,0,0,0
1020,,2733.12,8,0,1,0,0,0,0,0
1023,Sai Tai Tiger in Die Haremsw√§chterin die Verbannung Sc3,568.01,1,0,1,0,0,0,0,0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",1826.8,7,0,2,0,0,0,0,0
1030,Sai Tai Tiger in Die Haremsw√§chterin die Verbannung Sc2,973.26,4,0,1,0,0,0,0,0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",2183.45,6,0,1,0,0,0,0,0
1038,Alina Li in Asian Fuck Faces 3 Sc6,1208.25,2,0,2,0,0,2,0,2
1043,"Anni Star, CJ Miles in Glamorous Double Penetration",1859.5,4,0,2,0,0,0,0,0
1044,"Ania Kinski, Anissa Kate in Real Estate Gets Real Dirty",1707.04,4,0,2,0,0,1,0,1
1047,Ania Kinski in Home Alone Double Penetration,2581.06,10,0,3,0,0,0,0,0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",2046.91,7,0,3,0,0,0,0,0
1062,Six Bodies In Motion,4574.0,8,0,2,0,0,0,0,0
1063,,1618.58,5,0,2,0,0,0,0,0
1065,,2630.13,8,0,4,0,0,0,0,0
1068,,1027.87,5,0,3,0,0,0,0,0


============================================================
[79/124] Legacy\utils\body_art_parser.py
------------------------------------------------------------
"""
Utilitaire partag√© pour parser les tattoos/piercings depuis n'importe quelle source.
G√®re les formats structur√©s ("wrist (tribal)") et flat ("multiple tattoos").
"""
import re


def parse_body_art(raw_text: str) -> list[dict]:
    """
    Parse un texte brut de tattoos/piercings en liste structur√©e.
    
    Retourne: [{"position": str, "description": str | None}]
    
    Exemples d'entr√©e:
        "left wrist (tribal); right arm (sleeve)" ‚Üí 2 entr√©es structur√©es
        "Yes - multiple tattoos" ‚Üí 1 entr√©e flat
        "None" ‚Üí []
    """
    items = []
    if not raw_text or raw_text.strip().lower() in ('unknown', 'no', 'n/a', 'none', ''):
        return items

    # Retirer le pr√©fixe "Yes - " ou "Yes," courant
    cleaned = re.sub(r'^Yes\s*[-,]?\s*', '', raw_text, flags=re.I).strip()
    if not cleaned:
        return items

    # S√©parer par point-virgule d'abord (plus fiable), sinon virgule
    if ';' in cleaned:
        parts = cleaned.split(';')
    else:
        parts = cleaned.split(',')

    for part in parts:
        part = part.strip()
        if not part or part.lower() in ('unknown', 'no', 'n/a', 'none'):
            continue

        # Tenter de parser "position (description)"
        m = re.match(r'(.+?)\s*\((.+?)\)\s*$', part)
        if m:
            items.append({
                "position": m.group(1).strip(),
                "description": m.group(2).strip()
            })
        else:
            # Tenter "position - description" ou "position : description"
            m2 = re.match(r'(.+?)\s*[-‚Äì:]\s+(.+)', part)
            if m2 and len(m2.group(1)) < 30:
                items.append({
                    "position": m2.group(1).strip(),
                    "description": m2.group(2).strip()
                })
            else:
                items.append({
                    "position": part,
                    "description": None
                })

    return items


============================================================
[80/124] Legacy\utils\cleanup_all.py
------------------------------------------------------------
from __future__ import annotations
import sqlite3
from collections import defaultdict, OrderedDict
from typing import Dict, List, Set, Tuple, Optional, Any

DB_PATH = r"H:\Stash\stash-go.sqlite"

# ============================================================
# CONFIGURATION DU NETTOYAGE
# ============================================================

# Dur√©e minimale globale : tout marqueur <= cette valeur (s) sera supprim√©
# Mettre √† 0.0 pour d√©sactiver.
GLOBAL_MIN_DURATION: float = 60.0

# R√®gles cibl√©es par tag (ind√©pendantes de la r√®gle globale)
# Format : (tag, dur√©e_max_ou_None_pour_tous)
TARGETED_DELETE_RULES: List[Tuple[str, Optional[float]]] = [
    ("69", None),  # Supprimer TOUS les marqueurs "69" (quelle que soit la dur√©e)
]

# ============================================================
conn = sqlite3.connect(DB_PATH)
cur = conn.cursor()

print("=== M√âNAGE COMPLET DES MARQUEURS ===\n")

# ---- 1. Chargement de tous les marqueurs ----
cur.execute("""
    SELECT m.id, m.seconds, m.end_seconds, GROUP_CONCAT(t.name), m.title
    FROM scene_markers m
    LEFT JOIN scene_markers_tags mt ON m.id = mt.scene_marker_id
    LEFT JOIN tags t ON mt.tag_id = t.id
    GROUP BY m.id
    ORDER BY m.scene_id, m.seconds
""")
all_markers: List[Any] = cur.fetchall()

# Construire un dict id -> marker data
marker_data: Dict[int, Dict[str, Any]] = {}
for row in all_markers:
    mid: int = int(row[0])
    start: float = float(row[1])
    end: float = float(row[2]) if row[2] else 0.0
    tags_str: str = str(row[3]) if row[3] else ""
    title: str = str(row[4]) if row[4] else ""
    tags: Set[str] = set(tags_str.split(",")) if tags_str else set()
    if title:
        tags.add(title)
    marker_data[mid] = {"start": start, "end": end, "tags": tags}

# ---- 2. Grouper les IDs par sc√®ne (ordonn√©s par seconds) ----
cur.execute("SELECT scene_id, id FROM scene_markers ORDER BY scene_id, seconds")
scene_groups: Dict[int, List[int]] = OrderedDict()
for row in cur.fetchall():
    sid: int = int(row[0])
    mid: int = int(row[1])
    if sid not in scene_groups:
        scene_groups[sid] = []
    scene_groups[sid].append(mid)

# ---- 3. R√®gles FUSION + CONTENANCE ----
markers_to_delete: Set[int] = set()
markers_to_update: Dict[int, float] = {}
fusions_count: int = 0
contained_count: int = 0

for sid, mids in scene_groups.items():
    for i in range(len(mids)):
        id1: int = mids[i]
        if id1 not in marker_data:
            continue
        d1 = marker_data[id1]
        start1: float = d1["start"]
        end1: float = d1["end"]
        tags1: Set[str] = d1["tags"]

        for j in range(i + 1, len(mids)):
            id2: int = mids[j]
            if id2 not in marker_data or id2 in markers_to_delete:
                continue
            d2 = marker_data[id2]
            start2: float = d2["start"]
            end2: float = d2["end"]
            tags2: Set[str] = d2["tags"]

            # Doit avoir le m√™me tag unique
            if tags1 != tags2 or len(tags1) != 1:
                if start2 >= end1:
                    break
                continue

            gap: float = start2 - end1

            # R√®gle FUSION : gap < 10s
            if gap < 10:
                new_end: float = max(end1, end2 if end2 else start2)
                markers_to_update[id1] = new_end
                markers_to_delete.add(id2)
                end1 = new_end
                d1["end"] = new_end
                fusions_count += 1
                continue

            # Arr√™t si hors zone
            if start2 >= end1:
                break

            # R√®gle CONTENANCE
            if start1 < end2 and start2 < end1:
                if start1 <= start2 and end1 >= end2 and end2 > 0:
                    markers_to_delete.add(id2)
                    contained_count += 1
                elif start2 <= start1 and end2 >= end1 and end1 > 0:
                    markers_to_delete.add(id1)
                    contained_count += 1

# ---- 4. R√®gle GLOBALE : dur√©e minimale ----
global_short_count: int = 0
if GLOBAL_MIN_DURATION > 0:
    for mid, d in marker_data.items():
        if mid in markers_to_delete:
            continue
        m_dur: float = d["end"] - d["start"] if d["end"] > d["start"] else 0.0
        if m_dur <= GLOBAL_MIN_DURATION:
            markers_to_delete.add(mid)
            global_short_count += 1
            # Annuler la fusion si ce marqueur √©tait source d'une mise √† jour
            if mid in markers_to_update:
                del markers_to_update[mid]

# ---- 5. R√®gles CIBL√âES par tag ----
targeted_deletes: Dict[str, int] = {}
for mid, d in marker_data.items():
    if mid in markers_to_delete:
        continue
    m_dur = d["end"] - d["start"] if d["end"] > d["start"] else 0.0
    m_tags: Set[str] = d["tags"]
    for tag_rule, max_dur in TARGETED_DELETE_RULES:
        if tag_rule in m_tags:
            if max_dur is None or m_dur <= max_dur:
                markers_to_delete.add(mid)
                rule_key: str = f"{tag_rule} (<= {max_dur}s)" if max_dur else tag_rule
                targeted_deletes[rule_key] = targeted_deletes.get(rule_key, 0) + 1
                break

# ---- 6. Bilan ----
total_updates: int = len(markers_to_update)
total_deletes: int = len(markers_to_delete)

print(f"Bilan du nettoyage identifi√© :\n")
print(f"  [FUSION]     {fusions_count} marqueurs fusionn√©s")
print(f"  [CONTENANCE] {contained_count} marqueurs contenus dans un autre")
if GLOBAL_MIN_DURATION > 0:
    print(f"  [DUR√âE ‚â§{int(GLOBAL_MIN_DURATION)}s] {global_short_count} marqueurs trop courts (toutes cat√©gories)")
for rule, count in targeted_deletes.items():
    print(f"  [CIBL√â]      {count} marqueurs '{rule}'")
print(f"\n  TOTAL : {total_updates} UPDATE(s), {total_deletes} DELETE(s)\n")

if not markers_to_delete and not markers_to_update:
    print("[OK] Aucune action n√©cessaire. Base d√©j√† propre !")
    conn.close()
    exit()

confirm: str = input("Voulez-vous appliquer tous ces changements ? (OUI/non) : ")

if confirm == "OUI":
    try:
        for mid, new_end in markers_to_update.items():
            cur.execute("UPDATE scene_markers SET end_seconds = ? WHERE id = ?", (new_end, mid))

        ids_to_del: List[Tuple[int]] = [(mid,) for mid in markers_to_delete]
        cur.executemany("DELETE FROM scene_markers_tags WHERE scene_marker_id = ?", ids_to_del)
        cur.executemany("DELETE FROM scene_markers WHERE id = ?", ids_to_del)

        conn.commit()
        print(f"\n[OK] {total_updates} mise(s) √† jour et {total_deletes} suppression(s) appliqu√©es avec succ√®s.")
    except Exception as e:
        print(f"\n[ERREUR] √âchec : {e}")
        conn.rollback()
else:
    print("[INFO] Nettoyage annul√©.")

conn.close()


============================================================
[81/124] Legacy\utils\cleanup_specific.py
------------------------------------------------------------
import sqlite3

DB_PATH = r"H:\Stash\stash-go.sqlite"

def main():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()

    print("=== NETTOYAGE CIBL√â DES MARQUEURS ===\n")

    # 1. Identifier les marqueurs "69" (via Tag ou Titre)
    cur.execute("""
        SELECT DISTINCT m.id 
        FROM scene_markers m
        LEFT JOIN scene_markers_tags mt ON m.id = mt.scene_marker_id
        LEFT JOIN tags t ON mt.tag_id = t.id
        WHERE m.title = '69' OR t.name = '69'
    """)
    ids_69 = [r[0] for r in cur.fetchall()]

    # 2. Identifier les marqueurs "Anal" <= 60s (via Tag ou Titre)
    cur.execute("""
        SELECT DISTINCT m.id 
        FROM scene_markers m
        LEFT JOIN scene_markers_tags mt ON m.id = mt.scene_marker_id
        LEFT JOIN tags t ON mt.tag_id = t.id
        WHERE (m.title = 'Anal' OR t.name = 'Anal')
          AND ((m.end_seconds - m.seconds) <= 60 
               OR m.end_seconds IS NULL 
               OR m.end_seconds = 0)
    """)
    ids_anal_short = [r[0] for r in cur.fetchall()]

    total_ids = set(ids_69) | set(ids_anal_short)

    print(f"Bilan avant suppression :")
    print(f"  - Marqueurs '69' trouv√©s : {len(ids_69)}")
    print(f"  - Marqueurs 'Anal' (<= 60s) trouv√©s : {len(ids_anal_short)}")
    print(f"  - Total unique √† supprimer : {len(total_ids)}")

    if not total_ids:
        print("\n[!] Aucun marqueur ne correspond aux crit√®res. Fin du script.")
        conn.close()
        return

    confirm = input("\n√ätes-vous s√ªr de vouloir SUPPRIMER ces marqueurs d√©finitivement ? (OUI/non) : ")
    
    if confirm == "OUI":
        try:
            ids_tuple = [(int(mid),) for mid in total_ids]
            
            # Supprimer les liens tags
            cur.executemany("DELETE FROM scene_markers_tags WHERE scene_marker_id = ?", ids_tuple)
            # Supprimer les marqueurs
            cur.executemany("DELETE FROM scene_markers WHERE id = ?", ids_tuple)
            
            conn.commit()
            print(f"\n[OK] {len(total_ids)} marqueurs ont √©t√© supprim√©s avec succ√®s.")
        except Exception as e:
            print(f"\n[ERREUR] √âchec de la suppression : {e}")
            conn.rollback()
    else:
        print("\n[INFO] Op√©ration annul√©e.")

    conn.close()

if __name__ == "__main__":
    main()


============================================================
[82/124] Legacy\utils\customfield_utils.py
------------------------------------------------------------
# Utilitaire pour injecter des customfields performer

def inject_customfields(db, performer_id, customfields):
    """
    Injecte une liste de customfields pour un performer.
    customfields = [
        {"type": "award", "value": "..."},
        {"type": "trivia", "value": "..."},
        {"type": "tattoo", "value": "..."},
        {"type": "piercing", "value": "..."},
    ]
    """
    cur = db.conn.cursor()
    for cf in customfields:
        cur.execute(
            "INSERT INTO performer_customfields (performer_id, type, value) VALUES (?, ?, ?)",
            (performer_id, cf["type"], cf["value"])
        )
    db.conn.commit()


============================================================
[83/124] Legacy\utils\duration.py
------------------------------------------------------------
"""
Utilitaire de conversion de dur√©es pour les DVDs.
G√®re tous les formats rencontr√©s dans les sources DVD.
"""
import re
from typing import Optional


def parse_duration_to_seconds(raw: str) -> Optional[int]:
    """
    Convertit n'importe quelle repr√©sentation de dur√©e en secondes (INTEGER).

    Formats support√©s :
        '[01:55:32]'  ‚Üí data18
        '01:55:00'    ‚Üí adultdvdempire (apr√®s conversion interne)
        '115'         ‚Üí iafd (minutes brutes)
        '240 minutes' ‚Üí jeedoo
        '1 hrs. 55 mins.' ‚Üí adultdvdempire brut
    """
    if not raw:
        return None
    raw = str(raw).strip().strip("[]")

    # HH:MM:SS
    m = re.match(r'^(\d+):(\d{2}):(\d{2})$', raw)
    if m:
        return int(m.group(1)) * 3600 + int(m.group(2)) * 60 + int(m.group(3))

    # MM:SS
    m = re.match(r'^(\d+):(\d{2})$', raw)
    if m:
        return int(m.group(1)) * 60 + int(m.group(2))

    # "1 hrs. 55 mins." ou "1 hr 55 min"
    m = re.search(r'(\d+)\s*hr', raw, re.I)
    if m:
        hours = int(m.group(1))
        mins_m = re.search(r'(\d+)\s*min', raw, re.I)
        mins = int(mins_m.group(1)) if mins_m else 0
        return hours * 3600 + mins * 60

    # "240 minutes" ou "240 mins"
    m = re.search(r'(\d+)\s*min', raw, re.I)
    if m:
        return int(m.group(1)) * 60

    # Nombre seul ‚Üí consid√©r√© comme minutes (iafd)
    m = re.match(r'^(\d+)$', raw)
    if m:
        return int(m.group(1)) * 60

    return None


def format_duration(seconds: int) -> str:
    """Formate des secondes en HH:MM:SS pour affichage."""
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    return f"{h:02d}:{m:02d}:{s:02d}"


============================================================
[84/124] Legacy\utils\list_short_markers.py
------------------------------------------------------------
import sqlite3
import csv

DB_PATH = r"H:\Stash\stash-go.sqlite"
OUTPUT_CSV = "short_markers.csv"

def main():
    print(f"Extraction des marqueurs <= 60s...")
    
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    
    query = """
    SELECT 
        s.id as scene_id, 
        s.title as scene_title, 
        m.id as marker_id, 
        m.title as marker_title, 
        m.seconds as start, 
        m.end_seconds as end
    FROM scene_markers m
    JOIN scenes s ON m.scene_id = s.id
    WHERE (m.end_seconds - m.seconds) <= 60 
       OR m.end_seconds IS NULL 
       OR m.end_seconds = 0
    ORDER BY s.id, m.seconds
    """
    
    cur.execute(query)
    rows = cur.fetchall()
    
    results = []
    for r in rows:
        scene_id, scene_title, m_id, m_title, start, end = r
        duration = (end - start) if (end and end > start) else 0
        results.append({
            "scene_id": scene_id,
            "scene_title": scene_title,
            "marker_id": m_id,
            "marker_title": m_title,
            "start": round(start, 2),
            "end": round(end, 2) if end else 0,
            "duration": round(duration, 2)
        })
    
    if results:
        with open(OUTPUT_CSV, mode='w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=results[0].keys())
            writer.writeheader()
            writer.writerows(results)
        print(f"[OK] {len(results)} marqueurs extraits dans {OUTPUT_CSV}")
    else:
        print("[!] Aucun marqueur de moins de 60s trouv√©.")
        
    conn.close()

if __name__ == "__main__":
    main()


============================================================
[85/124] Legacy\utils\marker.py
------------------------------------------------------------
from __future__ import annotations
import sqlite3
import csv
from collections import defaultdict
from typing import Dict, List, Set, Tuple, Optional, Any

DB_PATH = r"H:\Stash\stash-go.sqlite"
CSV_REPORT = "audit_markers.csv"

conn = sqlite3.connect(DB_PATH)
cur = conn.cursor()

print("=== AUDIT AVANCE DES MARKERS ===\n")

# R√©cup√©rer toutes sc√®nes
cur.execute("""
    SELECT s.id, s.title, v.duration 
    FROM scenes s
    LEFT JOIN scenes_files sf ON s.id = sf.scene_id
    LEFT JOIN video_files v ON sf.file_id = v.file_id
    WHERE sf."primary" = 1 OR sf."primary" IS NULL
    GROUP BY s.id
""")
scenes: List[Any] = cur.fetchall()

tag_stats: Dict[str, Dict[str, int]] = defaultdict(lambda: {
    "total": 0,
    "overlaps": 0,
    "short": 0,
    "long": 0,
    "exact_dupes": 0
})

scene_results: List[Dict[str, Any]] = []
scene_scores: List[Tuple[Optional[str], int]] = []
markers_to_delete: Set[int] = set()
markers_to_update: Dict[int, float] = {}

for scene_row in scenes:
    scene_id: int = int(scene_row[0])
    title: Optional[str] = str(scene_row[1]) if scene_row[1] else None
    duration: float = float(scene_row[2]) if scene_row[2] else 0.0

    cur.execute("""
    SELECT m.id, m.title, m.seconds, m.end_seconds,
           GROUP_CONCAT(t.name)
    FROM scene_markers m
    LEFT JOIN scene_markers_tags mt ON m.id = mt.scene_marker_id
    LEFT JOIN tags t ON mt.tag_id = t.id
    WHERE m.scene_id = ?
    GROUP BY m.id
    """, (scene_id,))

    markers: List[Any] = cur.fetchall()
    if not markers:
        continue

    # Stats par sc√®ne
    markers_count: int = len(markers)
    unique_tags: Set[str] = set()
    scene_overlaps: int = 0
    scene_short: int = 0
    scene_long: int = 0
    scene_exact_dupes: int = 0
    scene_points: int = 0

    overlaps_list: List[Any] = []
    seen_ranges: Dict[Tuple[float, float], List[Set[str]]] = {}

    markers_sorted: List[Any] = sorted(markers, key=lambda x: x[2])

    for i in range(len(markers_sorted)):
        _row1 = markers_sorted[i]
        id1: int = int(_row1[0])
        m_title1: str = str(_row1[1]) if _row1[1] else ""
        start1: float = float(_row1[2])
        end1: float = float(_row1[3]) if _row1[3] else 0.0
        tags1_str: str = str(_row1[4]) if _row1[4] else ""

        tags1: Set[str] = set(tags1_str.split(",")) if tags1_str else set()
        if m_title1:
            tags1.add(m_title1)

        unique_tags.update(tags1)
        duration_marker: float = end1 - start1 if end1 else 0.0

        # Detection points sans dur√©e
        if not end1 or end1 <= start1:
            scene_points += 1

        # Detection doublons exacts (m√™me plage)
        time_range: Tuple[float, float] = (round(start1, 2), round(end1, 2) if end1 else 0.0)
        if time_range in seen_ranges:
            for prev_tags in seen_ranges[time_range]:
                common: Set[str] = tags1.intersection(prev_tags)
                if common:
                    scene_exact_dupes += len(common)
                    for t in common:
                        tag_stats[t]["exact_dupes"] += 1

        if time_range not in seen_ranges:
            seen_ranges[time_range] = []
        seen_ranges[time_range].append(tags1)

        # Stats tags
        for tag in tags1:
            tag_stats[tag]["total"] += 1
            if duration_marker > 0 and duration_marker < 3:
                tag_stats[tag]["short"] += 1
                scene_short += 1
            if duration and duration_marker > duration * 0.2:
                tag_stats[tag]["long"] += 1
                scene_long += 1

        # Check overlaps / Fusion / Redondance
        for j in range(i + 1, len(markers_sorted)):
            _row2 = markers_sorted[j]
            id2: int = int(_row2[0])
            m_title2: str = str(_row2[1]) if _row2[1] else ""
            start2: float = float(_row2[2])
            end2: float = float(_row2[3]) if _row2[3] else 0.0
            tags2_str: str = str(_row2[4]) if _row2[4] else ""

            tags2: Set[str] = set(tags2_str.split(",")) if tags2_str else set()
            if m_title2:
                tags2.add(m_title2)

            # --- LOGIQUE DE FUSION (GAP < 10s) ---
            if tags1 == tags2 and len(tags1) == 1:
                gap: float = start2 - (end1 if end1 else start1)
                if gap < 10:
                    new_end: float = max(end1 if end1 else start1, end2 if end2 else start2)
                    markers_to_update[id1] = new_end
                    markers_to_delete.add(id2)
                    end1 = new_end

            # Arr√™t de la boucle j si on d√©passe la zone de collision
            if start2 >= (end1 if end1 else start1):
                break

            # --- LOGIQUE DE CHEVAUCHEMENT / OVERLAP ---
            if start1 < (end2 if end2 else start2) and start2 < (end1 if end1 else start1):
                common2: Set[str] = tags1.intersection(tags2)
                if common2:
                    scene_overlaps += len(common2)
                    overlaps_list.append((id1, id2, start1, end1, start2, end2, list(common2)))
                    for tag in common2:
                        tag_stats[tag]["overlaps"] += 1

                    # --- LOGIQUE DE SUPPRESSION (CONTENU DANS) ---
                    if tags1 == tags2 and len(tags1) == 1:
                        # B est dans A
                        if start1 <= start2 and (end1 >= end2 if end1 and end2 else False):
                            markers_to_delete.add(id2)
                        # A est dans B
                        elif start2 <= start1 and (end2 >= end1 if end1 and end2 else False):
                            markers_to_delete.add(id1)

    scene_penalty: int = (scene_overlaps * 2) + scene_short + scene_long + (scene_exact_dupes * 5) + scene_points

    res: Dict[str, Any] = {
        "id": scene_id,
        "title": title,
        "duration": duration,
        "markers": markers_count,
        "points": scene_points,
        "tags": len(unique_tags),
        "overlaps": scene_overlaps,
        "short": scene_short,
        "long": scene_long,
        "exact_dupes": scene_exact_dupes,
        "penalty": scene_penalty
    }
    scene_results.append(res)
    scene_scores.append((title, scene_penalty))

    if scene_penalty > 0:
        display_title: str = title[:60] if title else "Sans titre"
        print(f"SCENE: {display_title}...")
        print(f"  - Dur√©e: {duration}s | Marqueurs: {markers_count} (Points: {scene_points}) | Tags: {len(unique_tags)}")
        print(f"  - Probl√®mes: Overlaps={scene_overlaps}, Courts={scene_short}, Longs={scene_long}, Doublons Plage={scene_exact_dupes}")
        print(f"  - Score P√©nalit√©: {scene_penalty}")
        print("-" * 30)

# Export CSV
with open(CSV_REPORT, mode='w', newline='', encoding='utf-8-sig') as f:
    if scene_results:
        writer = csv.DictWriter(f, fieldnames=list(scene_results[0].keys()))
        writer.writeheader()
        writer.writerows(scene_results)

print(f"\n[OK] Rapport d√©taill√© export√© dans: {CSV_REPORT}")

# Classement sc√®nes
print("\n=== TOP 20 SCENES PROBLEMATIQUES ===\n")
scene_scores.sort(key=lambda x: x[1], reverse=True)
top20: List[Tuple[Optional[str], int]] = scene_scores[:20]
for s in top20:
    if s[1] > 0:
        print(f"{s[1]:>4} pts | {s[0]}")

# Audit Tags
print("\n=== SCORE DE COHERENCE PAR TAG ===\n")
tag_audit: List[Tuple[str, int, Dict[str, int]]] = []
for tag, data in tag_stats.items():
    score: int = max(0, 100 - (data["overlaps"] * 3) - data["short"] - data["long"] - (data["exact_dupes"] * 10))
    tag_audit.append((tag, score, data))

tag_audit.sort(key=lambda x: x[1])
top_tags: List[Tuple[str, int, Dict[str, int]]] = tag_audit[:15]
for tag, score, data in top_tags:
    print(f"{tag[:20]:<20} | Score: {score:>3}/100 | {data['total']} total, {data['overlaps']} over, {data['exact_dupes']} dupes")

# --- SUPPRESSION / MISE A JOUR DES DOUBLONS ET FUSIONS ---
if markers_to_delete or markers_to_update:
    print(f"\n[!] ALERT: Travail de nettoyage identifi√© :")
    if markers_to_delete:
        print(f"  - {len(markers_to_delete)} marqueurs √† SUPPRIMER (doublons/fusions)")
    if markers_to_update:
        print(f"  - {len(markers_to_update)} marqueurs √† METTRE √Ä JOUR (fusions de dur√©e)")

    confirm: str = input(f"\nVoulez-vous appliquer ces changements √† la base de donn√©es ? (oui/non) : ")

    if confirm.lower() in ['oui', 'y', 'yes', 'o']:
        try:
            # 1. Mises √† jour (Fusions)
            for mid, new_end in markers_to_update.items():
                cur.execute("UPDATE scene_markers SET end_seconds = ? WHERE id = ?", (new_end, mid))

            # 2. Suppressions
            ids_to_del: List[Tuple[int]] = [(mid,) for mid in markers_to_delete]
            cur.executemany("DELETE FROM scene_markers WHERE id = ?", ids_to_del)
            cur.executemany("DELETE FROM scene_markers_tags WHERE scene_marker_id = ?", ids_to_del)

            conn.commit()
            print(f"[OK] Modifications appliqu√©es avec succ√®s.")
        except Exception as e:
            print(f"[ERREUR] √âchec des modifications : {e}")
            conn.rollback()
    else:
        print("[INFO] Nettoyage annul√©.")

conn.close()

============================================================
[86/124] Legacy\utils\meta_tag_utils.py
------------------------------------------------------------
# Utilitaires pour la normalisation et la propagation des tags de m√©tadonn√©es

COLORED_HAIR_TAGS = {
    "Blue": "BlueHair",
    "Green": "GreenHair",
    "Grey": "GreyHair",
    "Pink": "PinkHair",
    "Purple": "PurpleHair",
    "Red": "RedHair",
    "White": "WhiteHair"
}
NATURAL_HAIR_TAGS = {
    "Black": "BlackHair",
    "Blond": "BlondHair",
    "Brown": "BrownHair",
    "Redhead": "RedHead"
}
HAIR_TAGS = {**COLORED_HAIR_TAGS, **NATURAL_HAIR_TAGS}

NATIONALITY_TAGS = {
    "Cubaine": "Cuban",
    "Dominicaine": "Dominican",
    "Colombienne": "Colombian",
    "Thai": "Thai",
    "V√©n√©zu√©lienne": "Venezuelan",
    "Brasilian": "Brazilian",
    "Mexicaine": "Mexican"
}

ETHNY_TAGS = {
    "Asian": "Asian",
    "Ebony": "Ebony",
    "Latina": "Latina"
}


def normalize_tag(value):
    """Normalise une valeur de m√©tadonn√©e pour correspondre √† un tag."""
    v = value.strip().capitalize()
    return v


def meta_to_tags(meta):
    """Transforme les m√©tadonn√©es en tags selon les r√®gles."""
    tags = []
    # Couleur de cheveux
    hair = meta.get("Hair Color", "")
    for color in [c.strip() for c in hair.split(",") if c.strip()]:
        tag = HAIR_TAGS.get(normalize_tag(color))
        if tag:
            tags.append(tag)
    # Nationalit√©
    nat = meta.get("Country", "")
    tag = NATIONALITY_TAGS.get(normalize_tag(nat))
    if tag:
        tags.append(tag)
    # Ethnie
    ethny = meta.get("Ethnicity", "")
    tag = ETHNY_TAGS.get(normalize_tag(ethny))
    if tag:
        tags.append(tag)
    # MILF
    import datetime
    birthdate = meta.get("Birthdate", "")
    if birthdate:
        try:
            birth = datetime.datetime.strptime(birthdate, "%Y-%m-%d")
            age = (datetime.datetime.now() - birth).days // 365
            if age >= 35:
                tags.append("MILF")
        except Exception:
            pass
    # BigTits
    measurements = meta.get("Measurements", "")
    bust = None
    hips = None
    if measurements:
        # Format attendu: 36-24-38
        parts = [p.strip() for p in measurements.split("-")]
        if len(parts) == 3:
            try:
                bust = int(parts[0])
            except Exception:
                pass
            try:
                hips = int(parts[2])
            except Exception:
                pass
    # BigTits
    if bust is not None and bust >= 36:
        tags.append("BigTits")
    # BigButt
    if hips is not None and hips >= 38:
        tags.append("BigButt")
    # Bimbo
    if "BigTits" in tags and "BigButt" in tags:
        tags.append("Bimbo")
    return tags


def propagate_tags_to_scenes(db, performer_id, tags):
    """Ajoute/supprime les tags de couleur de cheveux, nationalit√©, ethnie sur toutes les sc√®nes de l'artiste."""
    # R√©cup√©rer toutes les sc√®nes de l'artiste
    cur = db.conn.cursor()
    cur.execute("SELECT scene_id FROM performers_scenes WHERE performer_id=?", (performer_id,))
    scene_ids = [r[0] for r in cur.fetchall()]
    for scene_id in scene_ids:
        # R√©cup√©rer les tags actuels
        cur.execute("SELECT t.name FROM tags t JOIN scenes_tags st ON st.tag_id = t.id WHERE st.scene_id=?", (scene_id,))
        scene_tags = [r[0] for r in cur.fetchall()]
        # Ajouter les tags manquants
        for tag in tags:
            if tag not in scene_tags:
                # Ajout du tag √† la sc√®ne
                cur.execute("SELECT id FROM tags WHERE name=?", (tag,))
                row = cur.fetchone()
                if row:
                    tag_id = row[0]
                    cur.execute("INSERT INTO scenes_tags (scene_id, tag_id) VALUES (?,?)", (scene_id, tag_id))
        # Supprimer les tags erron√©s
        valid_tags = set(HAIR_TAGS.values()) | set(NATIONALITY_TAGS.values()) | set(ETHNY_TAGS.values()) | {"MILF", "BigTits", "BigButt", "Bimbo"}
        for tag in scene_tags:
            if tag in valid_tags and tag not in tags:
                cur.execute("SELECT id FROM tags WHERE name=?", (tag,))
                row = cur.fetchone()
                if row:
                    tag_id = row[0]
                    cur.execute("DELETE FROM scenes_tags WHERE scene_id=? AND tag_id=?", (scene_id, tag_id))
    db.conn.commit()


============================================================
[87/124] Legacy\utils\short_markers.csv
------------------------------------------------------------
Ôªøscene_id,scene_title,marker_id,marker_title,start,end,duration
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",23,BlowJob,16.0,28.0,12.0
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",24,BlowJob,246.0,266.0,20.0
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",28,Cumshot,382.0,402.0,20.0
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",30,Cumshot,590.0,612.0,22.0
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",31,Cumshot,718.0,750.0,32.0
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",26,BlowJob,816.0,838.0,22.0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",42,Cumshot,134.0,170.0,36.0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",36,BlowJob,276.0,286.0,10.0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",37,BlowJob,338.0,372.0,34.0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",40,BlowJob,476.0,526.0,50.0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",39,BlowJob,720.0,732.0,12.0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",44,Cumshot,778.0,796.0,18.0
179,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc5",68,BlowJob,176.0,218.0,42.0
179,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc5",69,BlowJob,358.0,370.0,12.0
179,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc5",70,BlowJob,656.0,666.0,10.0
179,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc5",71,BlowJob,718.0,744.0,26.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",52,BlowJob,184.0,188.0,4.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",47,BlowJob,416.0,444.0,28.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",48,BlowJob,504.0,540.0,36.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",54,BlowJob,582.0,590.0,8.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",60,Cumshot,622.0,634.0,12.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",61,Cumshot,658.0,664.0,6.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",49,BlowJob,814.0,832.0,18.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",50,BlowJob,878.0,902.0,24.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",58,Cumshot,888.0,910.0,22.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",51,BlowJob,972.0,1002.0,30.0
181,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc1",64,BlowJob,104.0,124.0,20.0
181,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc1",62,BlowJob,144.0,156.0,12.0
181,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc1",63,BlowJob,206.0,236.0,30.0
181,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc1",65,BlowJob,876.0,922.0,46.0
181,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc1",66,Cumshot,896.0,932.0,36.0
182,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc5",77,Cumshot,280.0,306.0,26.0
183,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc3,82,BlowJob,92.0,142.0,50.0
183,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc3,83,BlowJob,196.0,208.0,12.0
183,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc3,86,Anal,696.0,724.0,28.0
183,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc3,84,BlowJob,848.0,860.0,12.0
186,"Sai Tai Tiger in Frauen Knast, Teufelsbrut Hinter Gittern! Sc2",80,Anal,346.0,370.0,24.0
186,"Sai Tai Tiger in Frauen Knast, Teufelsbrut Hinter Gittern! Sc2",81,Anal,608.0,620.0,12.0
188,Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc4,93,Cumshot,140.0,150.0,10.0
188,Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc4,94,Cumshot,208.0,234.0,26.0
190,"Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc7",103,Cumshot,160.0,194.0,34.0
190,"Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc7",104,Cumshot,246.0,258.0,12.0
191,Aderes Quin in StepMom Gets Double Dick,110,BlowJob,744.0,756.0,12.0
191,Aderes Quin in StepMom Gets Double Dick,112,BlowJob,1092.0,1102.0,10.0
191,Aderes Quin in StepMom Gets Double Dick,113,BlowJob,1144.0,1168.0,24.0
191,Aderes Quin in StepMom Gets Double Dick,114,BlowJob,1212.0,1264.0,52.0
191,Aderes Quin in StepMom Gets Double Dick,117,BlowJob,1878.0,1918.0,40.0
191,Aderes Quin in StepMom Gets Double Dick,122,Grabbing Boobs,2572.0,2592.0,20.0
191,Aderes Quin in StepMom Gets Double Dick,123,Grabbing Boobs,2692.0,2746.0,54.0
192,Alejandra Rico in Que Rico!,1950,BlowJob,154.0,210.0,56.0
192,Alejandra Rico in Que Rico!,1952,BlowJob,836.0,876.0,40.0
192,Alejandra Rico in Que Rico!,1957,Cumshot,1310.0,1346.0,36.0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",2434,Gangbang,146.0,160.0,14.0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",2427,BlowJob,550.0,564.0,14.0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",2428,BlowJob,1118.0,1158.0,40.0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",2429,BlowJob,1228.0,1258.0,30.0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",2436,Grabbing Boobs,1310.0,1328.0,18.0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",2433,BlowJob,1340.0,1354.0,14.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1869,Grabbing Boobs,418.0,444.0,26.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1870,Grabbing Boobs,488.0,524.0,36.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1871,Grabbing Boobs,972.0,992.0,20.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1878,Anal,1480.0,1506.0,26.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1879,Anal,1556.0,1570.0,14.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1880,Anal,1636.0,1692.0,56.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1881,Anal,1980.0,2010.0,30.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1875,BlowJob,2046.0,2068.0,22.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1882,Anal,2230.0,2268.0,38.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1883,Cumshot,2358.0,2398.0,40.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1877,BlowJob,2360.0,2402.0,42.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1884,Grabbing Boobs,464.0,502.0,38.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1885,Grabbing Boobs,542.0,570.0,28.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1886,Grabbing Boobs,740.0,774.0,34.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1892,Gangbang,968.0,984.0,16.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1887,Grabbing Boobs,1706.0,1752.0,46.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1894,Anal,1832.0,1842.0,10.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1896,Anal,2248.0,2280.0,32.0
196,Ava Devine in Milf Asian Cummouth Facial,3663,Grabbing Boobs,6.0,26.0,20.0
196,Ava Devine in Milf Asian Cummouth Facial,3665,BlowJob,418.0,466.0,48.0
196,Ava Devine in Milf Asian Cummouth Facial,3670,BlowJob,508.0,520.0,12.0
196,Ava Devine in Milf Asian Cummouth Facial,3666,BlowJob,810.0,834.0,24.0
196,Ava Devine in Milf Asian Cummouth Facial,3667,BlowJob,1030.0,1054.0,24.0
196,Ava Devine in Milf Asian Cummouth Facial,3668,BlowJob,1222.0,1242.0,20.0
196,Ava Devine in Milf Asian Cummouth Facial,3671,Anal,1374.0,1414.0,40.0
196,Ava Devine in Milf Asian Cummouth Facial,3669,BlowJob,1424.0,1436.0,12.0
196,Ava Devine in Milf Asian Cummouth Facial,3672,Anal,1612.0,1666.0,54.0
196,Ava Devine in Milf Asian Cummouth Facial,3673,Anal,1764.0,1806.0,42.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3644,BlowJob,204.0,246.0,42.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3649,Pissing,318.0,324.0,6.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3657,Gangbang,700.0,726.0,26.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3646,BlowJob,794.0,820.0,26.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3658,Gangbang,1004.0,1022.0,18.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3659,Gangbang,1176.0,1186.0,10.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3647,BlowJob,1370.0,1400.0,30.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3660,Gangbang,1508.0,1546.0,38.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3652,Anal,1818.0,1858.0,40.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3653,Anal,1938.0,1952.0,14.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3654,Anal,2032.0,2054.0,22.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3655,Anal,2110.0,2134.0,24.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3656,Anal,2218.0,2268.0,50.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3661,Cumshot,2470.0,2528.0,58.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3662,Cumshot,2604.0,2620.0,16.0
198,Barbie Sins in DP Bandits! Sc4,3701,BlowJob,1008.0,1016.0,8.0
198,Barbie Sins in DP Bandits! Sc4,3705,Anal,1554.0,1564.0,10.0
198,Barbie Sins in DP Bandits! Sc4,3697,BlowJob,1964.0,1994.0,30.0
198,Barbie Sins in DP Bandits! Sc4,3707,Anal,2146.0,2164.0,18.0
198,Barbie Sins in DP Bandits! Sc4,3698,BlowJob,2612.0,2632.0,20.0
198,Barbie Sins in DP Bandits! Sc4,3709,Cumshot,2798.0,2822.0,24.0
198,Barbie Sins in DP Bandits! Sc4,3700,BlowJob,2816.0,2852.0,36.0
198,Barbie Sins in DP Bandits! Sc4,3710,Cumshot,2892.0,2924.0,32.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3621,BlowJob,584.0,630.0,46.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3600,Anal,680.0,698.0,18.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3601,Anal,742.0,780.0,38.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3629,BlowJob,816.0,862.0,46.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3603,Anal,1258.0,1278.0,20.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3630,Cumshot,1344.0,1378.0,34.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3605,Anal,1722.0,1744.0,22.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3606,Anal,1800.0,1858.0,58.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3607,Anal,1924.0,1944.0,20.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3609,Anal,2728.0,2780.0,52.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3610,Anal,2862.0,2900.0,38.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3635,Cumshot,2912.0,2922.0,10.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3611,Anal,2940.0,2974.0,34.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3612,Anal,3030.0,3064.0,34.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3624,BlowJob,3158.0,3172.0,14.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3636,Cumshot,3168.0,3174.0,6.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3625,BlowJob,3454.0,3476.0,22.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3613,Anal,3496.0,3548.0,52.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3639,Gangbang,3530.0,3570.0,40.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3626,BlowJob,3544.0,3558.0,14.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3614,Anal,3604.0,3618.0,14.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3640,Gangbang,3678.0,3692.0,14.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3627,BlowJob,3682.0,3694.0,12.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3615,Anal,3764.0,3796.0,32.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3616,Anal,4008.0,4024.0,16.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3617,Anal,4168.0,4224.0,56.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3643,Gangbang,4306.0,4336.0,30.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3618,Anal,4332.0,4344.0,12.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3619,Anal,4382.0,4400.0,18.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3628,BlowJob,4560.0,4602.0,42.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3632,Cumshot,4784.0,4796.0,12.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3637,Cumshot,4816.0,4822.0,6.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3633,Cumshot,4888.0,4896.0,8.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3620,Anal,4922.0,4948.0,26.0
200,Blanche Bradburry in Gangbang Anal Blitz,3588,BlowJob,126.0,154.0,28.0
200,Blanche Bradburry in Gangbang Anal Blitz,3595,69,274.0,300.0,26.0
200,Blanche Bradburry in Gangbang Anal Blitz,3589,BlowJob,346.0,372.0,26.0
200,Blanche Bradburry in Gangbang Anal Blitz,3590,BlowJob,468.0,494.0,26.0
200,Blanche Bradburry in Gangbang Anal Blitz,3596,Anal,742.0,758.0,16.0
200,Blanche Bradburry in Gangbang Anal Blitz,3591,BlowJob,780.0,792.0,12.0
200,Blanche Bradburry in Gangbang Anal Blitz,3597,Anal,792.0,812.0,20.0
200,Blanche Bradburry in Gangbang Anal Blitz,3592,BlowJob,994.0,1010.0,16.0
200,Blanche Bradburry in Gangbang Anal Blitz,3593,BlowJob,1868.0,1882.0,14.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3714,Anal,1066.0,1080.0,14.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3715,Anal,1158.0,1176.0,18.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3725,BlowJob,1282.0,1296.0,14.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3726,BlowJob,1340.0,1368.0,28.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3727,BlowJob,1514.0,1530.0,16.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3729,BlowJob,1782.0,1792.0,10.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3731,Gangbang,2082.0,2098.0,16.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3717,Anal,2116.0,2154.0,38.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3732,Gangbang,2144.0,2160.0,16.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3718,Anal,2206.0,2246.0,40.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3733,Gangbang,2214.0,2226.0,12.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3719,Anal,2282.0,2292.0,10.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3720,Anal,2450.0,2480.0,30.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3721,Anal,2828.0,2860.0,32.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3730,BlowJob,2982.0,3012.0,30.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3723,Anal,3106.0,3122.0,16.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3734,Cumshot,3832.0,3854.0,22.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3735,Cumshot,3890.0,3928.0,38.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3738,Cumshot,3958.0,3992.0,34.0
202,Blanche Bradburry in Rough DAP Gangbang,3800,Grabbing Boobs,4.0,34.0,30.0
202,Blanche Bradburry in Rough DAP Gangbang,3802,BlowJob,1230.0,1246.0,16.0
202,Blanche Bradburry in Rough DAP Gangbang,3806,Anal,1296.0,1308.0,12.0
202,Blanche Bradburry in Rough DAP Gangbang,3807,Anal,1340.0,1362.0,22.0
202,Blanche Bradburry in Rough DAP Gangbang,3803,BlowJob,1376.0,1418.0,42.0
202,Blanche Bradburry in Rough DAP Gangbang,3808,Anal,1514.0,1562.0,48.0
202,Blanche Bradburry in Rough DAP Gangbang,3805,BlowJob,3020.0,3034.0,14.0
202,Blanche Bradburry in Rough DAP Gangbang,3812,Anal,3122.0,3150.0,28.0
203,Blanche Bradburry in Triple Penetration Madness,3739,Grabbing Boobs,130.0,190.0,60.0
203,Blanche Bradburry in Triple Penetration Madness,3740,Grabbing Boobs,276.0,298.0,22.0
203,Blanche Bradburry in Triple Penetration Madness,3741,Grabbing Boobs,384.0,404.0,20.0
203,Blanche Bradburry in Triple Penetration Madness,3742,Grabbing Boobs,602.0,622.0,20.0
203,Blanche Bradburry in Triple Penetration Madness,3745,Anal,922.0,956.0,34.0
203,Blanche Bradburry in Triple Penetration Madness,3751,BlowJob,1486.0,1516.0,30.0
203,Blanche Bradburry in Triple Penetration Madness,3752,BlowJob,2220.0,2250.0,30.0
203,Blanche Bradburry in Triple Penetration Madness,3748,Anal,2804.0,2818.0,14.0
203,Blanche Bradburry in Triple Penetration Madness,3743,Grabbing Boobs,2830.0,2848.0,18.0
203,Blanche Bradburry in Triple Penetration Madness,3749,Anal,2888.0,2912.0,24.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3674,Anal,354.0,370.0,16.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3675,Anal,448.0,460.0,12.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3676,Anal,582.0,602.0,20.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3677,Anal,662.0,674.0,12.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3688,BlowJob,1382.0,1442.0,60.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3679,Anal,1546.0,1558.0,12.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3690,BlowJob,1640.0,1666.0,26.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3678,Anal,1692.0,1740.0,48.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3691,BlowJob,1836.0,1870.0,34.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3692,BlowJob,2214.0,2230.0,16.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3681,Anal,2618.0,2642.0,24.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3693,BlowJob,2672.0,2706.0,34.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3682,Anal,2686.0,2710.0,24.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3683,Anal,2798.0,2812.0,14.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3684,Anal,2858.0,2904.0,46.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3685,Anal,2938.0,2960.0,22.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3686,Anal,3208.0,3226.0,18.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3687,Anal,3284.0,3310.0,26.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3753,Grabbing Boobs,422.0,482.0,60.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3754,Anal,1222.0,1244.0,22.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3755,Anal,1286.0,1346.0,60.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3757,Anal,1356.0,1390.0,34.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3758,Anal,1482.0,1498.0,16.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3765,BlowJob,1822.0,1838.0,16.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3762,BlowJob,2180.0,2192.0,12.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3763,BlowJob,2368.0,2386.0,18.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3764,BlowJob,3150.0,3160.0,10.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3813,Grabbing Boobs,16.0,50.0,34.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3814,Anal,382.0,392.0,10.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3824,Anal,970.0,980.0,10.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3825,Anal,1196.0,1206.0,10.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3826,BlowJob,1412.0,1422.0,10.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3827,BlowJob,1500.0,1554.0,54.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3828,BlowJob,1988.0,2006.0,18.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3815,Anal,2186.0,2204.0,18.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3829,BlowJob,2304.0,2314.0,10.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3816,Anal,2384.0,2416.0,32.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3817,Anal,2520.0,2552.0,32.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3818,Anal,2622.0,2682.0,60.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3819,Anal,2716.0,2730.0,14.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3830,BlowJob,2744.0,2760.0,16.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3820,Anal,2798.0,2826.0,28.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3821,Anal,2924.0,2938.0,14.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3822,Anal,2972.0,3018.0,46.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3823,Anal,3166.0,3178.0,12.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3831,BlowJob,3418.0,3442.0,24.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3878,Grabbing Boobs,98.0,136.0,38.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3879,Anal,164.0,188.0,24.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3880,Anal,646.0,676.0,30.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3881,Anal,708.0,720.0,12.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3884,BlowJob,1926.0,1964.0,38.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3885,BlowJob,4178.0,4192.0,14.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3886,BlowJob,4296.0,4324.0,28.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3887,Cumshot,4512.0,4526.0,14.0
208,Cherry Kiss in DP Bandits! Sc2,3867,BlowJob,518.0,568.0,50.0
208,Cherry Kiss in DP Bandits! Sc2,3873,Anal,696.0,750.0,54.0
208,Cherry Kiss in DP Bandits! Sc2,3868,BlowJob,698.0,732.0,34.0
208,Cherry Kiss in DP Bandits! Sc2,3869,BlowJob,798.0,812.0,14.0
208,Cherry Kiss in DP Bandits! Sc2,3870,BlowJob,852.0,874.0,22.0
208,Cherry Kiss in DP Bandits! Sc2,3875,Anal,1120.0,1142.0,22.0
208,Cherry Kiss in DP Bandits! Sc2,3871,BlowJob,1936.0,1968.0,32.0
208,Cherry Kiss in DP Bandits! Sc2,3872,BlowJob,2342.0,2358.0,16.0
208,Cherry Kiss in DP Bandits! Sc2,3877,Cumshot,2346.0,2362.0,16.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3768,Grabbing Boobs,6.0,56.0,50.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3772,Gangbang,156.0,168.0,12.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3780,Anal,172.0,228.0,56.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3790,BlowJob,256.0,286.0,30.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3781,Anal,276.0,302.0,26.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3791,BlowJob,494.0,506.0,12.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3792,BlowJob,626.0,658.0,32.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3774,Gangbang,782.0,816.0,34.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3782,Anal,882.0,900.0,18.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3775,Gangbang,888.0,916.0,28.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3776,Gangbang,956.0,978.0,22.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3777,Gangbang,1178.0,1192.0,14.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3793,BlowJob,1232.0,1274.0,42.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3784,Anal,1276.0,1292.0,16.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3796,Pissing,1332.0,1370.0,38.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3794,BlowJob,1394.0,1406.0,12.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3778,Gangbang,1406.0,1424.0,18.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3769,Grabbing Boobs,1574.0,1602.0,28.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3798,Titjob,1590.0,1606.0,16.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3785,Anal,1718.0,1734.0,16.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3779,Gangbang,1762.0,1804.0,42.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3787,Anal,2226.0,2242.0,16.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3797,Pissing,2296.0,2334.0,38.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3770,Grabbing Boobs,2442.0,2478.0,36.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3771,Grabbing Boobs,3176.0,3214.0,38.0
210,Destiny Mira in Put My Back Into It,1899,BlowJob,288.0,312.0,24.0
210,Destiny Mira in Put My Back Into It,1900,BlowJob,612.0,662.0,50.0
210,Destiny Mira in Put My Back Into It,1901,Anal,960.0,1004.0,44.0
210,Destiny Mira in Put My Back Into It,1902,Anal,1048.0,1058.0,10.0
210,Destiny Mira in Put My Back Into It,1903,Anal,1114.0,1146.0,32.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3832,BlowJob,1516.0,1572.0,56.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3834,BlowJob,1826.0,1856.0,30.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3843,Gangbang,1978.0,1992.0,14.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3836,BlowJob,2228.0,2264.0,36.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3837,BlowJob,2522.0,2540.0,18.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3849,Pissing,2804.0,2816.0,12.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3838,BlowJob,2834.0,2844.0,10.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3852,Anal,3442.0,3452.0,10.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3839,BlowJob,3472.0,3484.0,12.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3853,Anal,3600.0,3618.0,18.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3854,Anal,3660.0,3710.0,50.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3855,Anal,3746.0,3762.0,16.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3857,Anal,4076.0,4088.0,12.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3850,Pissing,4250.0,4268.0,18.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3847,Gangbang,4724.0,4744.0,20.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3848,Gangbang,4776.0,4816.0,40.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3859,Anal,4820.0,4842.0,22.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3840,BlowJob,4828.0,4876.0,48.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3862,Cumshot,5800.0,5814.0,14.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3863,Cumshot,5856.0,5906.0,50.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3864,Cumshot,5950.0,5994.0,44.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3865,Cumshot,6046.0,6076.0,30.0
212,Jolee Love in Craving For Cocks,1905,BlowJob,972.0,1000.0,28.0
212,Jolee Love in Craving For Cocks,1906,BlowJob,1134.0,1144.0,10.0
212,Jolee Love in Craving For Cocks,1907,BlowJob,1206.0,1230.0,24.0
212,Jolee Love in Craving For Cocks,1912,Anal,1804.0,1820.0,16.0
212,Jolee Love in Craving For Cocks,1908,BlowJob,2194.0,2212.0,18.0
212,Jolee Love in Craving For Cocks,1915,Anal,2540.0,2590.0,50.0
212,Jolee Love in Craving For Cocks,1909,BlowJob,2746.0,2798.0,52.0
212,Jolee Love in Craving For Cocks,1917,DP,3182.0,3186.0,4.0
213,Jolee Love in DP Bandits! 2 Sc3,1918,Grabbing Boobs,1030.0,1050.0,20.0
213,Jolee Love in DP Bandits! 2 Sc3,1929,Anal,1470.0,1508.0,38.0
213,Jolee Love in DP Bandits! 2 Sc3,1920,BlowJob,1526.0,1544.0,18.0
213,Jolee Love in DP Bandits! 2 Sc3,1930,Anal,1606.0,1640.0,34.0
213,Jolee Love in DP Bandits! 2 Sc3,1921,BlowJob,1820.0,1848.0,28.0
213,Jolee Love in DP Bandits! 2 Sc3,1926,BlowJob,1914.0,1948.0,34.0
213,Jolee Love in DP Bandits! 2 Sc3,1935,Cumshot,1952.0,1958.0,6.0
213,Jolee Love in DP Bandits! 2 Sc3,1923,BlowJob,2360.0,2416.0,56.0
213,Jolee Love in DP Bandits! 2 Sc3,1924,BlowJob,2452.0,2468.0,16.0
213,Jolee Love in DP Bandits! 2 Sc3,1933,Anal,2654.0,2690.0,36.0
213,Jolee Love in DP Bandits! 2 Sc3,1934,Anal,2758.0,2788.0,30.0
213,Jolee Love in DP Bandits! 2 Sc3,1925,BlowJob,2890.0,2928.0,38.0
214,Jolee Love in Hardcore DAP Creampie,1941,BlowJob,370.0,382.0,12.0
214,Jolee Love in Hardcore DAP Creampie,1944,BlowJob,1780.0,1794.0,14.0
214,Jolee Love in Hardcore DAP Creampie,1945,BlowJob,2056.0,2078.0,22.0
214,Jolee Love in Hardcore DAP Creampie,1947,Pissing,2628.0,2656.0,28.0
215,,2140,Anal,188.0,208.0,20.0
215,,2141,Anal,722.0,742.0,20.0
215,,2144,BlowJob,818.0,834.0,16.0
215,,2145,BlowJob,894.0,932.0,38.0
215,,2143,Anal,1452.0,1464.0,12.0
215,,2146,BlowJob,1640.0,1652.0,12.0
216,,2150,Anal,162.0,188.0,26.0
216,,2151,Anal,222.0,260.0,38.0
216,,2152,Anal,388.0,408.0,20.0
216,,2153,Anal,602.0,620.0,18.0
216,,2154,Cumshot,1830.0,1844.0,14.0
217,,2159,BlowJob,692.0,740.0,48.0
217,,2157,Anal,1220.0,1238.0,18.0
217,,2160,BlowJob,1800.0,1852.0,52.0
217,,2161,BlowJob,2262.0,2278.0,16.0
218,,2162,BlowJob,486.0,504.0,18.0
218,,2165,Anal,788.0,842.0,54.0
218,,2166,Anal,1494.0,1536.0,42.0
218,,2167,Anal,2144.0,2170.0,26.0
218,,2168,Anal,2206.0,2236.0,30.0
218,,2163,BlowJob,2268.0,2280.0,12.0
218,,2169,Cumshot,2300.0,2330.0,30.0
218,,2164,BlowJob,2306.0,2312.0,6.0
219,,2171,BlowJob,66.0,86.0,20.0
219,,2181,BlowJob,444.0,468.0,24.0
219,,2186,Cumshot,450.0,496.0,46.0
219,,2189,Cumshot,528.0,534.0,6.0
219,,2172,BlowJob,584.0,638.0,54.0
219,,2173,BlowJob,682.0,706.0,24.0
219,,2182,BlowJob,1032.0,1038.0,6.0
219,,2190,Cumshot,1074.0,1112.0,38.0
219,,2174,BlowJob,1086.0,1120.0,34.0
219,,2191,Cumshot,1312.0,1324.0,12.0
219,,2176,BlowJob,1382.0,1428.0,46.0
219,,2194,Gangbang,1408.0,1422.0,14.0
219,,2177,BlowJob,1512.0,1532.0,20.0
219,,2178,BlowJob,1634.0,1690.0,56.0
219,,2179,BlowJob,1726.0,1742.0,16.0
219,,2183,BlowJob,1778.0,1820.0,42.0
219,,2192,Cumshot,1820.0,1832.0,12.0
219,,2180,BlowJob,2050.0,2090.0,40.0
219,,2184,BlowJob,2140.0,2172.0,32.0
219,,2193,Cumshot,2176.0,2186.0,10.0
220,,2196,BlowJob,662.0,678.0,16.0
220,,2198,BlowJob,688.0,698.0,10.0
220,,2197,BlowJob,716.0,726.0,10.0
220,,2199,BlowJob,1334.0,1356.0,22.0
220,,2201,Cumshot,2468.0,2478.0,10.0
221,,2202,BlowJob,114.0,158.0,44.0
221,,2203,BlowJob,332.0,382.0,50.0
221,,2205,Gangbang,388.0,398.0,10.0
221,,2206,Cumshot,670.0,696.0,26.0
221,,2204,BlowJob,732.0,772.0,40.0
221,,2210,Cumshot,806.0,834.0,28.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2017,Gangbang,218.0,274.0,56.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2058,BlowJob,256.0,272.0,16.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2018,Gangbang,358.0,376.0,18.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2020,Gangbang,576.0,634.0,58.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2063,Anal,586.0,600.0,14.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2059,BlowJob,592.0,602.0,10.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2064,Anal,746.0,772.0,26.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2065,Anal,822.0,860.0,38.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2021,Gangbang,982.0,994.0,12.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2030,Anal,1046.0,1080.0,34.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2052,Gangbang,1062.0,1116.0,54.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2031,Anal,1140.0,1150.0,10.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2067,Anal,1230.0,1242.0,12.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2023,Gangbang,1506.0,1542.0,36.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2024,Gangbang,1630.0,1678.0,48.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2069,Cumshot,1806.0,1834.0,28.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2034,BlowJob,1808.0,1838.0,30.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2032,Anal,2002.0,2016.0,14.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2055,Gangbang,2076.0,2086.0,10.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2057,Gangbang,2504.0,2534.0,30.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2073,Cumshot,2662.0,2704.0,42.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2074,Cumshot,2770.0,2780.0,10.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2036,BlowJob,2940.0,2988.0,48.0
223,,2212,Cumshot,112.0,122.0,10.0
223,,2215,BlowJob,208.0,218.0,10.0
223,,2218,BlowJob,258.0,274.0,16.0
223,,2216,BlowJob,352.0,406.0,54.0
223,,2220,BlowJob,466.0,488.0,22.0
223,,2231,Gangbang,800.0,812.0,12.0
223,,2222,BlowJob,868.0,884.0,16.0
223,,2228,Grabbing Boobs,1150.0,1178.0,28.0
223,,2223,BlowJob,1164.0,1174.0,10.0
223,,2229,Grabbing Boobs,1250.0,1298.0,48.0
223,,2232,Gangbang,1400.0,1436.0,36.0
223,,2233,Anal,1488.0,1504.0,16.0
223,,2225,BlowJob,1606.0,1616.0,10.0
223,,2230,Grabbing Boobs,2106.0,2130.0,24.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2350,BlowJob,106.0,112.0,6.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2357,BlowJob,310.0,332.0,22.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2235,BlowJob,408.0,444.0,36.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2236,BlowJob,770.0,818.0,48.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2241,BlowJob,886.0,898.0,12.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2244,Cumshot,888.0,894.0,6.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2363,Anal,1358.0,1394.0,36.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2245,Anal,1426.0,1438.0,12.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2353,BlowJob,1610.0,1618.0,8.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2246,Anal,1696.0,1716.0,20.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2243,Grabbing Boobs,1880.0,1928.0,48.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2242,BlowJob,2224.0,2232.0,8.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2355,BlowJob,2332.0,2340.0,8.0
225,,2247,Grabbing Boobs,26.0,56.0,30.0
225,,2250,BlowJob,746.0,758.0,12.0
225,,2251,BlowJob,806.0,822.0,16.0
225,,2256,Anal,1120.0,1132.0,12.0
225,,2257,Anal,1202.0,1216.0,14.0
225,,2258,Anal,1256.0,1284.0,28.0
225,,2252,BlowJob,1336.0,1356.0,20.0
225,,2259,Anal,1416.0,1430.0,14.0
225,,2260,Anal,1522.0,1544.0,22.0
225,,2249,Grabbing Boobs,1618.0,1646.0,28.0
225,,2253,BlowJob,1754.0,1796.0,42.0
225,,2261,Anal,1918.0,1950.0,32.0
225,,2263,Anal,2238.0,2268.0,30.0
225,,2254,BlowJob,2382.0,2406.0,24.0
225,,2264,Anal,2630.0,2656.0,26.0
225,,2265,Anal,2706.0,2756.0,50.0
225,,2255,BlowJob,2766.0,2814.0,48.0
226,,2266,Grabbing Boobs,26.0,44.0,18.0
226,,2267,Grabbing Boobs,236.0,264.0,28.0
226,,2268,Grabbing Boobs,300.0,320.0,20.0
226,,2281,Titjob,308.0,318.0,10.0
226,,2272,BlowJob,788.0,798.0,10.0
226,,2269,Grabbing Boobs,912.0,940.0,28.0
226,,2273,BlowJob,1074.0,1108.0,34.0
226,,2274,BlowJob,1448.0,1458.0,10.0
226,,2288,Anal,1476.0,1496.0,20.0
226,,2275,BlowJob,1502.0,1542.0,40.0
226,,2283,Cumshot,1600.0,1632.0,32.0
226,,2276,BlowJob,1636.0,1660.0,24.0
226,,2289,Anal,1694.0,1734.0,40.0
226,,2277,BlowJob,1736.0,1778.0,42.0
226,,2278,BlowJob,1878.0,1906.0,28.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2295,Gangbang,638.0,652.0,14.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2377,BlowJob,996.0,1010.0,14.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2298,Anal,1048.0,1064.0,16.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2388,Cumshot,1112.0,1118.0,6.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2382,Anal,1142.0,1160.0,18.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2378,BlowJob,1154.0,1174.0,20.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2379,BlowJob,1220.0,1256.0,36.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2299,Anal,1232.0,1278.0,46.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2385,Anal,1808.0,1818.0,10.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2293,BlowJob,1830.0,1844.0,14.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2386,Anal,1924.0,1936.0,12.0
228,Adira Allure in Airtight Diva Sc4,2332,Grabbing Boobs,178.0,218.0,40.0
228,Adira Allure in Airtight Diva Sc4,2339,Cumshot,798.0,802.0,4.0
228,Adira Allure in Airtight Diva Sc4,2340,Cumshot,854.0,880.0,26.0
228,Adira Allure in Airtight Diva Sc4,2334,BlowJob,872.0,924.0,52.0
228,Adira Allure in Airtight Diva Sc4,2335,BlowJob,1108.0,1148.0,40.0
228,Adira Allure in Airtight Diva Sc4,2336,BlowJob,1184.0,1210.0,26.0
228,Adira Allure in Airtight Diva Sc4,2342,Cumshot,1808.0,1832.0,24.0
228,Adira Allure in Airtight Diva Sc4,2338,BlowJob,1824.0,1834.0,10.0
229,Alexis Tae in Gangbang Sluts Sc1,2533,BlowJob,796.0,828.0,32.0
229,Alexis Tae in Gangbang Sluts Sc1,2534,BlowJob,888.0,898.0,10.0
229,Alexis Tae in Gangbang Sluts Sc1,2524,Gangbang,912.0,936.0,24.0
229,Alexis Tae in Gangbang Sluts Sc1,2535,BlowJob,936.0,972.0,36.0
229,Alexis Tae in Gangbang Sluts Sc1,2536,BlowJob,1050.0,1102.0,52.0
229,Alexis Tae in Gangbang Sluts Sc1,2526,Gangbang,1266.0,1290.0,24.0
229,Alexis Tae in Gangbang Sluts Sc1,2527,Gangbang,1344.0,1384.0,40.0
229,Alexis Tae in Gangbang Sluts Sc1,2528,Gangbang,1538.0,1548.0,10.0
229,Alexis Tae in Gangbang Sluts Sc1,2529,Gangbang,1600.0,1656.0,56.0
229,Alexis Tae in Gangbang Sluts Sc1,2537,BlowJob,1696.0,1708.0,12.0
229,Alexis Tae in Gangbang Sluts Sc1,2530,Gangbang,2018.0,2072.0,54.0
229,Alexis Tae in Gangbang Sluts Sc1,2531,Gangbang,2134.0,2170.0,36.0
229,Alexis Tae in Gangbang Sluts Sc1,2539,Cumshot,2218.0,2256.0,38.0
230,Amirah Adara in DP Bandits! Sc1,2814,BlowJob,1308.0,1354.0,46.0
230,Amirah Adara in DP Bandits! Sc1,2823,Anal,1510.0,1528.0,18.0
230,Amirah Adara in DP Bandits! Sc1,2815,BlowJob,1786.0,1844.0,58.0
230,Amirah Adara in DP Bandits! Sc1,2825,Anal,1856.0,1870.0,14.0
230,Amirah Adara in DP Bandits! Sc1,2816,BlowJob,1914.0,1946.0,32.0
230,Amirah Adara in DP Bandits! Sc1,2817,BlowJob,2078.0,2104.0,26.0
230,Amirah Adara in DP Bandits! Sc1,5061,Anal,2086.0,2132.0,46.0
230,Amirah Adara in DP Bandits! Sc1,5062,Anal,2230.0,2284.0,54.0
230,Amirah Adara in DP Bandits! Sc1,2818,BlowJob,2560.0,2590.0,30.0
230,Amirah Adara in DP Bandits! Sc1,5065,Anal,2592.0,2606.0,14.0
230,Amirah Adara in DP Bandits! Sc1,2819,BlowJob,2840.0,2896.0,56.0
230,Amirah Adara in DP Bandits! Sc1,2827,Anal,2932.0,2948.0,16.0
230,Amirah Adara in DP Bandits! Sc1,2822,BlowJob,3002.0,3022.0,20.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2892,Grabbing Boobs,86.0,110.0,24.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2900,Gangbang,230.0,254.0,24.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2895,BlowJob,744.0,774.0,30.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2901,Gangbang,834.0,878.0,44.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2896,BlowJob,904.0,930.0,26.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2902,Gangbang,1280.0,1332.0,52.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2898,BlowJob,1282.0,1334.0,52.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2893,Grabbing Boobs,1370.0,1410.0,40.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2899,BlowJob,1474.0,1486.0,12.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2904,Cumshot,1630.0,1634.0,4.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2905,Cumshot,1728.0,1752.0,24.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2906,Cumshot,1822.0,1832.0,10.0
232,Anai Loves in My stepmom,2908,Grabbing Boobs,668.0,710.0,42.0
232,Anai Loves in My stepmom,2916,Titjob,710.0,736.0,26.0
232,Anai Loves in My stepmom,2914,BlowJob,726.0,780.0,54.0
232,Anai Loves in My stepmom,2915,BlowJob,796.0,802.0,6.0
232,Anai Loves in My stepmom,2917,Anal,850.0,908.0,58.0
232,Anai Loves in My stepmom,2918,Anal,1060.0,1098.0,38.0
232,Anai Loves in My stepmom,2910,Grabbing Boobs,1208.0,1240.0,32.0
232,Anai Loves in My stepmom,2919,Anal,1446.0,1460.0,14.0
233,Angela White in Angela's Airtight DP,3062,Anal,488.0,516.0,28.0
233,Angela White in Angela's Airtight DP,3063,Anal,648.0,702.0,54.0
233,Angela White in Angela's Airtight DP,3069,Grabbing Boobs,660.0,686.0,26.0
233,Angela White in Angela's Airtight DP,3064,Anal,750.0,770.0,20.0
233,Angela White in Angela's Airtight DP,3065,Anal,1052.0,1094.0,42.0
233,Angela White in Angela's Airtight DP,3060,BlowJob,1104.0,1140.0,36.0
233,Angela White in Angela's Airtight DP,3070,Grabbing Boobs,1494.0,1516.0,22.0
233,Angela White in Angela's Airtight DP,3067,Anal,2158.0,2170.0,12.0
233,Angela White in Angela's Airtight DP,3068,Anal,2244.0,2264.0,20.0
233,Angela White in Angela's Airtight DP,3071,Cumshot,2480.0,2508.0,28.0
233,Angela White in Angela's Airtight DP,3072,Cumshot,2578.0,2612.0,34.0
233,Angela White in Angela's Airtight DP,3073,Cumshot,2656.0,2674.0,18.0
234,Anissa Kate in Love Everything About Her,3314,BlowJob,192.0,226.0,34.0
234,Anissa Kate in Love Everything About Her,3318,BlowJob,322.0,346.0,24.0
234,Anissa Kate in Love Everything About Her,3316,BlowJob,942.0,954.0,12.0
234,Anissa Kate in Love Everything About Her,3317,BlowJob,1358.0,1368.0,10.0
234,Anissa Kate in Love Everything About Her,3322,Anal,1676.0,1720.0,44.0
235,Anissa Kate in Sizziling Double Penetration Delight,3334,Grabbing Boobs,170.0,212.0,42.0
235,Anissa Kate in Sizziling Double Penetration Delight,3336,BlowJob,330.0,382.0,52.0
235,Anissa Kate in Sizziling Double Penetration Delight,3335,Grabbing Boobs,466.0,518.0,52.0
235,Anissa Kate in Sizziling Double Penetration Delight,3339,BlowJob,470.0,516.0,46.0
235,Anissa Kate in Sizziling Double Penetration Delight,3343,Anal,1102.0,1138.0,36.0
235,Anissa Kate in Sizziling Double Penetration Delight,3341,BlowJob,1192.0,1226.0,34.0
235,Anissa Kate in Sizziling Double Penetration Delight,3342,BlowJob,1536.0,1546.0,10.0
235,Anissa Kate in Sizziling Double Penetration Delight,3344,Anal,1564.0,1596.0,32.0
235,Anissa Kate in Sizziling Double Penetration Delight,3345,Anal,1658.0,1690.0,32.0
235,Anissa Kate in Sizziling Double Penetration Delight,3346,Cumshot,1876.0,1916.0,40.0
236,"Anissa Kate, Olivia Del Rio in Personal Guide Chapter 1",3406,Grabbing Boobs,328.0,386.0,58.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3432,BlowJob,468.0,518.0,50.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3433,BlowJob,784.0,798.0,14.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3438,Anal,884.0,928.0,44.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3439,Anal,964.0,988.0,24.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3440,Anal,1254.0,1274.0,20.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3441,Anal,1540.0,1550.0,10.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3442,Anal,1594.0,1606.0,12.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3436,BlowJob,1632.0,1644.0,12.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3437,BlowJob,2168.0,2194.0,26.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3434,BlowJob,2204.0,2256.0,52.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3444,Cumshot,2264.0,2274.0,10.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3888,Gangbang,64.0,98.0,34.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3891,BlowJob,556.0,588.0,32.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3892,BlowJob,814.0,858.0,44.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3893,BlowJob,906.0,956.0,50.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3894,BlowJob,1034.0,1052.0,18.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3895,BlowJob,1104.0,1152.0,48.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3896,BlowJob,1354.0,1374.0,20.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3899,Anal,2380.0,2414.0,34.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3900,Anal,2458.0,2476.0,18.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3901,Anal,2578.0,2610.0,32.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3902,Cumshot,2780.0,2812.0,32.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3903,Cumshot,2864.0,2898.0,34.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3905,BlowJob,114.0,132.0,18.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3906,BlowJob,368.0,412.0,44.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3912,Cumshot,376.0,402.0,26.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3908,BlowJob,620.0,654.0,34.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3909,BlowJob,734.0,754.0,20.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3910,BlowJob,870.0,906.0,36.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3911,BlowJob,1108.0,1116.0,8.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3914,Cumshot,1128.0,1136.0,8.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3917,Cumshot,1460.0,1492.0,32.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3918,Cumshot,1574.0,1598.0,24.0
240,Assh Lee in All Over That Cock,3964,Anal,114.0,130.0,16.0
240,Assh Lee in All Over That Cock,3966,BlowJob,790.0,830.0,40.0
240,Assh Lee in All Over That Cock,3969,Cumshot,1436.0,1440.0,4.0
240,Assh Lee in All Over That Cock,3967,BlowJob,1470.0,1480.0,10.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3922,BlowJob,276.0,286.0,10.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3924,BlowJob,364.0,422.0,58.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3928,Anal,570.0,582.0,12.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3925,BlowJob,1124.0,1154.0,30.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3926,BlowJob,1298.0,1308.0,10.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3932,Anal,1314.0,1330.0,16.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3929,Anal,1372.0,1424.0,52.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3927,BlowJob,1434.0,1458.0,24.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3930,Anal,1568.0,1584.0,16.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3934,Anal,1610.0,1626.0,16.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3935,Anal,1706.0,1746.0,40.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3936,Cumshot,1768.0,1810.0,42.0
242,Baby Gemini in Ricky's Room Blowbang,3937,Gangbang,62.0,84.0,22.0
242,Baby Gemini in Ricky's Room Blowbang,3938,Gangbang,124.0,140.0,16.0
242,Baby Gemini in Ricky's Room Blowbang,3939,Gangbang,238.0,254.0,16.0
242,Baby Gemini in Ricky's Room Blowbang,3940,Gangbang,300.0,350.0,50.0
242,Baby Gemini in Ricky's Room Blowbang,3941,Gangbang,594.0,638.0,44.0
242,Baby Gemini in Ricky's Room Blowbang,3942,Gangbang,694.0,722.0,28.0
242,Baby Gemini in Ricky's Room Blowbang,3943,Gangbang,764.0,806.0,42.0
242,Baby Gemini in Ricky's Room Blowbang,3944,Gangbang,864.0,898.0,34.0
243,Barbie Sins in DAP with Creampie,3977,Anal,652.0,672.0,20.0
243,Barbie Sins in DAP with Creampie,3978,Anal,704.0,722.0,18.0
243,Barbie Sins in DAP with Creampie,3971,BlowJob,770.0,782.0,12.0
243,Barbie Sins in DAP with Creampie,3972,BlowJob,946.0,980.0,34.0
243,Barbie Sins in DAP with Creampie,3973,BlowJob,1390.0,1440.0,50.0
243,Barbie Sins in DAP with Creampie,3981,Anal,1498.0,1542.0,44.0
243,Barbie Sins in DAP with Creampie,3982,Anal,1624.0,1648.0,24.0
243,Barbie Sins in DAP with Creampie,3984,Anal,1888.0,1926.0,38.0
243,Barbie Sins in DAP with Creampie,3985,Anal,1968.0,2002.0,34.0
243,Barbie Sins in DAP with Creampie,3974,BlowJob,1970.0,2012.0,42.0
243,Barbie Sins in DAP with Creampie,3986,Anal,2172.0,2216.0,44.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4014,Gangbang,180.0,220.0,40.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4015,Gangbang,374.0,398.0,24.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4024,Anal,532.0,572.0,40.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4025,Anal,666.0,676.0,10.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4003,BlowJob,882.0,896.0,14.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4027,Anal,1158.0,1214.0,56.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4004,BlowJob,1216.0,1226.0,10.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4038,Cumshot,1312.0,1320.0,8.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4040,Cumshot,1334.0,1344.0,10.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4005,BlowJob,1360.0,1394.0,34.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4017,Gangbang,1488.0,1502.0,14.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4028,Anal,1642.0,1676.0,34.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4007,BlowJob,1678.0,1692.0,14.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4018,Gangbang,1792.0,1802.0,10.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4019,Gangbang,1878.0,1904.0,26.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4030,Anal,2288.0,2334.0,46.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4008,BlowJob,2304.0,2330.0,26.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4031,Anal,2514.0,2536.0,22.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4021,Gangbang,2638.0,2666.0,28.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4009,BlowJob,2642.0,2684.0,42.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4032,Anal,2646.0,2686.0,40.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4022,Gangbang,2812.0,2828.0,16.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4034,Anal,2848.0,2862.0,14.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4013,BlowJob,3046.0,3056.0,10.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4035,Anal,3554.0,3590.0,36.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4036,Anal,3672.0,3700.0,28.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4037,Anal,3978.0,3990.0,12.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4012,BlowJob,4124.0,4138.0,14.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4039,Cumshot,4200.0,4204.0,4.0
245,Belinha Baracho in Intense 5on1 Gangbang,4041,Grabbing Boobs,6.0,28.0,22.0
245,Belinha Baracho in Intense 5on1 Gangbang,4042,Grabbing Boobs,142.0,176.0,34.0
245,Belinha Baracho in Intense 5on1 Gangbang,4050,Anal,482.0,520.0,38.0
245,Belinha Baracho in Intense 5on1 Gangbang,4057,Gangbang,608.0,632.0,24.0
245,Belinha Baracho in Intense 5on1 Gangbang,4051,Anal,610.0,662.0,52.0
245,Belinha Baracho in Intense 5on1 Gangbang,4044,BlowJob,624.0,656.0,32.0
245,Belinha Baracho in Intense 5on1 Gangbang,4058,Gangbang,720.0,758.0,38.0
245,Belinha Baracho in Intense 5on1 Gangbang,4045,BlowJob,746.0,772.0,26.0
245,Belinha Baracho in Intense 5on1 Gangbang,4052,Anal,758.0,796.0,38.0
245,Belinha Baracho in Intense 5on1 Gangbang,4059,Gangbang,932.0,984.0,52.0
245,Belinha Baracho in Intense 5on1 Gangbang,4060,Gangbang,1212.0,1232.0,20.0
245,Belinha Baracho in Intense 5on1 Gangbang,4061,Gangbang,1362.0,1380.0,18.0
245,Belinha Baracho in Intense 5on1 Gangbang,4046,BlowJob,1686.0,1722.0,36.0
245,Belinha Baracho in Intense 5on1 Gangbang,4048,BlowJob,1992.0,2004.0,12.0
245,Belinha Baracho in Intense 5on1 Gangbang,4049,BlowJob,2044.0,2058.0,14.0
245,Belinha Baracho in Intense 5on1 Gangbang,4063,Gangbang,2282.0,2304.0,22.0
245,Belinha Baracho in Intense 5on1 Gangbang,4064,Gangbang,2350.0,2372.0,22.0
245,Belinha Baracho in Intense 5on1 Gangbang,4054,Anal,2858.0,2878.0,20.0
245,Belinha Baracho in Intense 5on1 Gangbang,4055,Anal,2994.0,3028.0,34.0
245,Belinha Baracho in Intense 5on1 Gangbang,4056,Anal,3184.0,3194.0,10.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3946,Anal,1076.0,1128.0,52.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3956,BlowJob,1256.0,1286.0,30.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3947,Anal,1294.0,1336.0,42.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3949,Anal,1996.0,2038.0,42.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3951,Anal,2370.0,2428.0,58.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3952,Anal,2478.0,2508.0,30.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3957,BlowJob,2996.0,3028.0,32.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3953,Anal,3056.0,3080.0,24.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3958,BlowJob,3092.0,3112.0,20.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3954,Anal,3256.0,3274.0,18.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3959,BlowJob,3312.0,3352.0,40.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3960,BlowJob,3394.0,3418.0,24.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3961,BlowJob,3638.0,3656.0,18.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3962,BlowJob,3816.0,3858.0,42.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3987,Cumshot,1354.0,1374.0,20.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3988,Gangbang,1708.0,1720.0,12.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3991,BlowJob,1926.0,1958.0,32.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3994,Anal,2090.0,2120.0,30.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3995,Anal,2276.0,2292.0,16.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3996,Anal,2328.0,2376.0,48.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3989,Gangbang,2706.0,2720.0,14.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3998,Anal,3830.0,3846.0,16.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3992,BlowJob,3834.0,3848.0,14.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3999,Anal,3878.0,3916.0,38.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3990,Gangbang,3894.0,3908.0,14.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",4000,Anal,4006.0,4020.0,14.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",4001,Anal,4136.0,4146.0,10.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4065,Grabbing Boobs,26.0,44.0,18.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4066,Anal,232.0,264.0,32.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4067,Anal,478.0,502.0,24.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4068,Anal,966.0,1006.0,40.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4078,BlowJob,1160.0,1216.0,56.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4082,Cumshot,1596.0,1618.0,22.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4070,Anal,2034.0,2072.0,38.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4071,Anal,2138.0,2170.0,32.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4072,Anal,2210.0,2266.0,56.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4079,BlowJob,2484.0,2542.0,58.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4080,BlowJob,2690.0,2706.0,16.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4081,BlowJob,2864.0,2874.0,10.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4074,Anal,3228.0,3262.0,34.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4075,Anal,3312.0,3348.0,36.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4076,Anal,3396.0,3416.0,20.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4077,Anal,3456.0,3484.0,28.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4084,BlowJob,696.0,738.0,42.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4083,Grabbing Boobs,886.0,924.0,38.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4085,BlowJob,1008.0,1018.0,10.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4086,BlowJob,1050.0,1060.0,10.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4087,BlowJob,1206.0,1218.0,12.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4089,BlowJob,1932.0,1952.0,20.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4090,BlowJob,2688.0,2708.0,20.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4091,Cumshot,2928.0,2978.0,50.0
250,,2302,Gangbang,368.0,380.0,12.0
250,,2303,Gangbang,426.0,456.0,30.0
250,,2304,Gangbang,588.0,620.0,32.0
250,,2306,Gangbang,1028.0,1058.0,30.0
250,,2324,Grabbing Boobs,1182.0,1200.0,18.0
250,,2326,Anal,1224.0,1256.0,32.0
250,,2307,Gangbang,1230.0,1252.0,22.0
250,,2308,Gangbang,1308.0,1354.0,46.0
250,,2309,Gangbang,1602.0,1614.0,12.0
250,,2310,Gangbang,1646.0,1696.0,50.0
250,,2311,Gangbang,1736.0,1760.0,24.0
250,,2327,Anal,1802.0,1836.0,34.0
250,,2312,Gangbang,1906.0,1922.0,16.0
250,,2328,Anal,2064.0,2084.0,20.0
250,,2313,Gangbang,2158.0,2188.0,30.0
250,,2329,Anal,3432.0,3450.0,18.0
250,,2330,Anal,3514.0,3532.0,18.0
250,,2319,BlowJob,5992.0,6014.0,22.0
250,,2320,BlowJob,6062.0,6098.0,36.0
250,,2325,Grabbing Boobs,6140.0,6184.0,44.0
250,,2321,BlowJob,6160.0,6186.0,26.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4094,BlowJob,976.0,996.0,20.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4101,Anal,1360.0,1398.0,38.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4102,Anal,1552.0,1596.0,44.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4095,BlowJob,1756.0,1778.0,22.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4103,Anal,1782.0,1830.0,48.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4104,Anal,1934.0,1980.0,46.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4096,BlowJob,2416.0,2436.0,20.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4097,BlowJob,2642.0,2680.0,38.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4098,BlowJob,2714.0,2726.0,12.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4107,Anal,2748.0,2764.0,16.0
252,Bonny Bon in Sexual Rage 2 Sc3,4109,Anal,570.0,610.0,40.0
252,Bonny Bon in Sexual Rage 2 Sc3,4110,Anal,724.0,758.0,34.0
252,Bonny Bon in Sexual Rage 2 Sc3,4111,Anal,930.0,952.0,22.0
252,Bonny Bon in Sexual Rage 2 Sc3,4112,Anal,986.0,1018.0,32.0
252,Bonny Bon in Sexual Rage 2 Sc3,4113,Anal,1112.0,1152.0,40.0
252,Bonny Bon in Sexual Rage 2 Sc3,4117,BlowJob,1120.0,1156.0,36.0
252,Bonny Bon in Sexual Rage 2 Sc3,4114,Anal,1450.0,1474.0,24.0
252,Bonny Bon in Sexual Rage 2 Sc3,4120,Cumshot,1952.0,1960.0,8.0
252,Bonny Bon in Sexual Rage 2 Sc3,4119,BlowJob,1962.0,1994.0,32.0
253,Cali Caliente in The Gangbang Part IV,4121,BlowJob,172.0,214.0,42.0
253,Cali Caliente in The Gangbang Part IV,4129,Gangbang,832.0,878.0,46.0
253,Cali Caliente in The Gangbang Part IV,4133,Anal,1014.0,1036.0,22.0
253,Cali Caliente in The Gangbang Part IV,4124,BlowJob,1214.0,1246.0,32.0
253,Cali Caliente in The Gangbang Part IV,4134,Anal,1342.0,1376.0,34.0
253,Cali Caliente in The Gangbang Part IV,4135,Anal,1460.0,1502.0,42.0
253,Cali Caliente in The Gangbang Part IV,4136,Anal,1562.0,1602.0,40.0
253,Cali Caliente in The Gangbang Part IV,4137,Anal,1668.0,1690.0,22.0
253,Cali Caliente in The Gangbang Part IV,4131,Gangbang,1756.0,1788.0,32.0
253,Cali Caliente in The Gangbang Part IV,4126,BlowJob,1844.0,1886.0,42.0
253,Cali Caliente in The Gangbang Part IV,4138,Anal,1922.0,1934.0,12.0
253,Cali Caliente in The Gangbang Part IV,4127,BlowJob,1956.0,1970.0,14.0
253,Cali Caliente in The Gangbang Part IV,4128,BlowJob,2048.0,2060.0,12.0
253,Cali Caliente in The Gangbang Part IV,4139,Cumshot,2118.0,2128.0,10.0
253,Cali Caliente in The Gangbang Part IV,4140,Cumshot,2206.0,2222.0,16.0
254,Carla Morelli in Gangbang with 4 Cocks,4142,BlowJob,862.0,902.0,40.0
254,Carla Morelli in Gangbang with 4 Cocks,4145,BlowJob,1556.0,1600.0,44.0
254,Carla Morelli in Gangbang with 4 Cocks,4143,BlowJob,1646.0,1668.0,22.0
254,Carla Morelli in Gangbang with 4 Cocks,4144,BlowJob,1722.0,1734.0,12.0
254,Carla Morelli in Gangbang with 4 Cocks,4146,Cumshot,1760.0,1796.0,36.0
255,Carla Morelli in Hot for Teacher,1936,Grabbing Boobs,98.0,140.0,42.0
255,Carla Morelli in Hot for Teacher,1937,BlowJob,226.0,274.0,48.0
255,Carla Morelli in Hot for Teacher,1939,Cumshot,1294.0,1308.0,14.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4152,Grabbing Boobs,552.0,594.0,42.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4154,BlowJob,752.0,766.0,14.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4164,Anal,1074.0,1104.0,30.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4156,BlowJob,1244.0,1258.0,14.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4165,Anal,1448.0,1470.0,22.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4166,Anal,1754.0,1782.0,28.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4157,BlowJob,1822.0,1880.0,58.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4167,Anal,2000.0,2022.0,22.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4158,BlowJob,2108.0,2150.0,42.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4173,Gangbang,2260.0,2274.0,14.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4160,BlowJob,2406.0,2454.0,48.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4169,Anal,3170.0,3206.0,36.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4174,Gangbang,3178.0,3188.0,10.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4153,Grabbing Boobs,3536.0,3568.0,32.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4155,BlowJob,3668.0,3686.0,18.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4162,BlowJob,3772.0,3812.0,40.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4171,Anal,3908.0,3922.0,14.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4163,BlowJob,3952.0,3962.0,10.0
257,Chloe Amour in Mon Amour Sc1,4181,BlowJob,292.0,302.0,10.0
257,Chloe Amour in Mon Amour Sc1,4182,BlowJob,504.0,546.0,42.0
257,Chloe Amour in Mon Amour Sc1,4183,BlowJob,618.0,650.0,32.0
257,Chloe Amour in Mon Amour Sc1,4189,BlowJob,906.0,920.0,14.0
257,Chloe Amour in Mon Amour Sc1,4185,BlowJob,1020.0,1054.0,34.0
257,Chloe Amour in Mon Amour Sc1,4190,DP,1100.0,1110.0,10.0
257,Chloe Amour in Mon Amour Sc1,4186,BlowJob,1110.0,1122.0,12.0
257,Chloe Amour in Mon Amour Sc1,4191,DP,1220.0,1248.0,28.0
257,Chloe Amour in Mon Amour Sc1,4192,DP,1292.0,1310.0,18.0
257,Chloe Amour in Mon Amour Sc1,4188,BlowJob,1568.0,1608.0,40.0
257,Chloe Amour in Mon Amour Sc1,4193,DP,1848.0,1868.0,20.0
257,Chloe Amour in Mon Amour Sc1,4179,Anal,1958.0,1974.0,16.0
257,Chloe Amour in Mon Amour Sc1,4198,Gangbang,2004.0,2028.0,24.0
257,Chloe Amour in Mon Amour Sc1,4196,Cumshot,2132.0,2140.0,8.0
258,Chloe Amour in Mon Amour Sc2,4200,BlowJob,372.0,382.0,10.0
258,Chloe Amour in Mon Amour Sc2,4201,BlowJob,614.0,644.0,30.0
258,Chloe Amour in Mon Amour Sc2,4202,BlowJob,738.0,794.0,56.0
259,Chloe Amour in Mon Amour Sc3,4205,Gangbang,164.0,190.0,26.0
259,Chloe Amour in Mon Amour Sc3,4206,Gangbang,242.0,260.0,18.0
259,Chloe Amour in Mon Amour Sc3,4207,Gangbang,442.0,456.0,14.0
259,Chloe Amour in Mon Amour Sc3,4208,Gangbang,548.0,578.0,30.0
259,Chloe Amour in Mon Amour Sc3,4215,BlowJob,918.0,974.0,56.0
259,Chloe Amour in Mon Amour Sc3,4209,Gangbang,944.0,980.0,36.0
259,Chloe Amour in Mon Amour Sc3,4210,Gangbang,1352.0,1372.0,20.0
259,Chloe Amour in Mon Amour Sc3,4228,DP,1392.0,1420.0,28.0
259,Chloe Amour in Mon Amour Sc3,4211,Gangbang,2184.0,2202.0,18.0
259,Chloe Amour in Mon Amour Sc3,4212,Gangbang,2468.0,2492.0,24.0
259,Chloe Amour in Mon Amour Sc3,4213,Gangbang,2804.0,2828.0,24.0
259,Chloe Amour in Mon Amour Sc3,4226,Cumshot,2960.0,2988.0,28.0
260,"Chloe Amour, Jennifer White in Mon Amour Sc4",151,BlowJob,230.0,244.0,14.0
260,"Chloe Amour, Jennifer White in Mon Amour Sc4",153,BlowJob,412.0,432.0,20.0
260,"Chloe Amour, Jennifer White in Mon Amour Sc4",157,Cumshot,2310.0,2324.0,14.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,159,Grabbing Boobs,308.0,348.0,40.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,161,BlowJob,784.0,820.0,36.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,172,BlowJob,872.0,908.0,36.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,174,Gangbang,994.0,1008.0,14.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,164,BlowJob,1610.0,1646.0,36.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,165,BlowJob,2016.0,2060.0,44.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,166,BlowJob,2132.0,2144.0,12.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,168,BlowJob,2498.0,2514.0,16.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,178,Anal,2508.0,2556.0,48.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,173,BlowJob,2654.0,2660.0,6.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,176,Gangbang,3258.0,3308.0,50.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,169,BlowJob,3414.0,3426.0,12.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,170,BlowJob,3466.0,3500.0,34.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,180,Anal,3720.0,3730.0,10.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,181,Anal,3814.0,3864.0,50.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,171,BlowJob,3856.0,3914.0,58.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,182,Cumshot,3944.0,3990.0,46.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,184,Cumshot,4024.0,4072.0,48.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,185,Cumshot,4110.0,4132.0,22.0
262,"Cindy Starfall, Gaia in Swappers Sc1",213,Grabbing Boobs,200.0,242.0,42.0
262,"Cindy Starfall, Gaia in Swappers Sc1",215,BlowJob,842.0,852.0,10.0
262,"Cindy Starfall, Gaia in Swappers Sc1",218,69,876.0,906.0,30.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,186,Grabbing Boobs,116.0,146.0,30.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,188,BlowJob,214.0,252.0,38.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,189,BlowJob,332.0,348.0,16.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,195,Anal,1414.0,1466.0,52.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,194,BlowJob,1860.0,1868.0,8.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,187,Grabbing Boobs,1868.0,1914.0,46.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,196,Cumshot,1950.0,1956.0,6.0
265,"Danielle Renee, MarsFoxxx in Group Bang",235,Gangbang,230.0,262.0,32.0
265,"Danielle Renee, MarsFoxxx in Group Bang",224,BlowJob,380.0,392.0,12.0
265,"Danielle Renee, MarsFoxxx in Group Bang",236,Gangbang,396.0,428.0,32.0
265,"Danielle Renee, MarsFoxxx in Group Bang",243,Anal,416.0,426.0,10.0
265,"Danielle Renee, MarsFoxxx in Group Bang",226,BlowJob,648.0,700.0,52.0
265,"Danielle Renee, MarsFoxxx in Group Bang",238,Gangbang,684.0,706.0,22.0
265,"Danielle Renee, MarsFoxxx in Group Bang",244,Anal,814.0,842.0,28.0
265,"Danielle Renee, MarsFoxxx in Group Bang",245,Anal,920.0,930.0,10.0
265,"Danielle Renee, MarsFoxxx in Group Bang",228,BlowJob,942.0,968.0,26.0
265,"Danielle Renee, MarsFoxxx in Group Bang",246,Cumshot,1072.0,1092.0,20.0
265,"Danielle Renee, MarsFoxxx in Group Bang",229,BlowJob,1160.0,1206.0,46.0
265,"Danielle Renee, MarsFoxxx in Group Bang",241,Grabbing Boobs,1252.0,1282.0,30.0
265,"Danielle Renee, MarsFoxxx in Group Bang",230,BlowJob,1326.0,1354.0,28.0
265,"Danielle Renee, MarsFoxxx in Group Bang",239,Gangbang,1330.0,1350.0,20.0
265,"Danielle Renee, MarsFoxxx in Group Bang",247,Cumshot,1494.0,1504.0,10.0
265,"Danielle Renee, MarsFoxxx in Group Bang",248,Cumshot,1588.0,1594.0,6.0
265,"Danielle Renee, MarsFoxxx in Group Bang",231,BlowJob,1662.0,1704.0,42.0
265,"Danielle Renee, MarsFoxxx in Group Bang",232,BlowJob,1874.0,1916.0,42.0
265,"Danielle Renee, MarsFoxxx in Group Bang",242,Grabbing Boobs,1908.0,1942.0,34.0
265,"Danielle Renee, MarsFoxxx in Group Bang",233,BlowJob,2442.0,2464.0,22.0
265,"Danielle Renee, MarsFoxxx in Group Bang",240,Gangbang,2482.0,2508.0,26.0
266,Daria Glower in Die Haremsw√§chterin des √ñl Scheichs Sc6,250,BlowJob,782.0,804.0,22.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,251,Grabbing Boobs,84.0,114.0,30.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,254,BlowJob,590.0,614.0,24.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,265,Cumshot,1372.0,1376.0,4.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,257,BlowJob,1412.0,1428.0,16.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,252,Grabbing Boobs,1458.0,1512.0,54.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,269,Anal,2194.0,2208.0,14.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,260,BlowJob,2660.0,2676.0,16.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,261,BlowJob,2716.0,2730.0,14.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,271,Anal,2746.0,2798.0,52.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,262,BlowJob,3350.0,3404.0,54.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,263,BlowJob,3520.0,3530.0,10.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,264,BlowJob,3658.0,3694.0,36.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,266,Cumshot,3696.0,3718.0,22.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1858,BlowJob,176.0,214.0,38.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1859,BlowJob,382.0,410.0,28.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1860,BlowJob,460.0,476.0,16.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1861,BlowJob,560.0,592.0,32.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1856,Grabbing Boobs,638.0,666.0,28.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1862,BlowJob,650.0,680.0,30.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1863,BlowJob,890.0,902.0,12.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1857,Grabbing Boobs,1016.0,1074.0,58.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1864,BlowJob,1064.0,1094.0,30.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1866,BlowJob,1310.0,1316.0,6.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1868,Cumshot,1572.0,1592.0,20.0
269,Emmanuelle Noire in Busty Ebony Beauty,132,Grabbing Boobs,214.0,236.0,22.0
269,Emmanuelle Noire in Busty Ebony Beauty,140,BlowJob,342.0,354.0,12.0
269,Emmanuelle Noire in Busty Ebony Beauty,141,BlowJob,482.0,486.0,4.0
269,Emmanuelle Noire in Busty Ebony Beauty,134,BlowJob,546.0,564.0,18.0
269,Emmanuelle Noire in Busty Ebony Beauty,133,Grabbing Boobs,574.0,608.0,34.0
269,Emmanuelle Noire in Busty Ebony Beauty,135,BlowJob,924.0,936.0,12.0
269,Emmanuelle Noire in Busty Ebony Beauty,143,Anal,1124.0,1184.0,60.0
269,Emmanuelle Noire in Busty Ebony Beauty,136,BlowJob,1172.0,1204.0,32.0
269,Emmanuelle Noire in Busty Ebony Beauty,144,Anal,1216.0,1234.0,18.0
269,Emmanuelle Noire in Busty Ebony Beauty,137,BlowJob,1242.0,1260.0,18.0
269,Emmanuelle Noire in Busty Ebony Beauty,145,Anal,1420.0,1430.0,10.0
269,Emmanuelle Noire in Busty Ebony Beauty,138,BlowJob,1614.0,1636.0,22.0
269,Emmanuelle Noire in Busty Ebony Beauty,139,BlowJob,1688.0,1700.0,12.0
269,Emmanuelle Noire in Busty Ebony Beauty,142,BlowJob,1726.0,1730.0,4.0
269,Emmanuelle Noire in Busty Ebony Beauty,146,Cumshot,1976.0,2002.0,26.0
269,Emmanuelle Noire in Busty Ebony Beauty,148,Cumshot,2040.0,2048.0,8.0
270,Francesca Le in Lewood Gangbang Battle of the MILFs Sc1,149,BlowJob,162.0,210.0,48.0
271,Francesca Le in Lewood Gangbang Battle of the MILFs Sc2,274,BlowJob,52.0,72.0,20.0
271,Francesca Le in Lewood Gangbang Battle of the MILFs Sc2,275,BlowJob,214.0,228.0,14.0
271,Francesca Le in Lewood Gangbang Battle of the MILFs Sc2,276,Anal,278.0,296.0,18.0
272,Francesca Le in Lewood Gangbang Battle of the MILFs Sc3,278,69,136.0,148.0,12.0
273,Francesca Le in Lewood Gangbang Battle of the MILFs Sc4,279,BlowJob,16.0,48.0,32.0
273,Francesca Le in Lewood Gangbang Battle of the MILFs Sc4,282,Anal,130.0,140.0,10.0
273,Francesca Le in Lewood Gangbang Battle of the MILFs Sc4,280,BlowJob,178.0,202.0,24.0
273,Francesca Le in Lewood Gangbang Battle of the MILFs Sc4,281,BlowJob,330.0,342.0,12.0
274,Francesca Le in Lewood Gangbang Battle of the MILFs Sc5,283,BlowJob,136.0,160.0,24.0
274,Francesca Le in Lewood Gangbang Battle of the MILFs Sc5,284,Grabbing Boobs,224.0,252.0,28.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,300,Gangbang,162.0,190.0,28.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,285,BlowJob,166.0,188.0,22.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,286,BlowJob,222.0,238.0,16.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,312,Cumshot,230.0,236.0,6.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,287,BlowJob,274.0,296.0,22.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,288,BlowJob,342.0,364.0,22.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,302,Gangbang,510.0,562.0,52.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,289,BlowJob,512.0,534.0,22.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,303,Gangbang,660.0,692.0,32.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,291,BlowJob,982.0,994.0,12.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,306,Gangbang,1230.0,1256.0,26.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,316,Anal,1468.0,1508.0,40.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,307,Gangbang,1480.0,1528.0,48.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,292,BlowJob,1514.0,1540.0,26.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,317,Anal,1560.0,1580.0,20.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,293,BlowJob,1582.0,1612.0,30.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,294,BlowJob,1670.0,1684.0,14.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,318,Anal,1684.0,1712.0,28.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,319,Anal,1764.0,1814.0,50.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,308,Gangbang,1814.0,1836.0,22.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,295,BlowJob,1920.0,1944.0,24.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,322,DP,2300.0,2324.0,24.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,310,Gangbang,2318.0,2350.0,32.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,311,Gangbang,2692.0,2706.0,14.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,299,BlowJob,2744.0,2750.0,6.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,298,BlowJob,2852.0,2872.0,20.0
276,"Gaia in Screw My Wife, Please 76 Sc2",323,BlowJob,184.0,230.0,46.0
277,Gaia in Throated 39 Sc5,324,Grabbing Boobs,254.0,298.0,44.0
277,Gaia in Throated 39 Sc5,326,BlowJob,350.0,362.0,12.0
277,Gaia in Throated 39 Sc5,332,BlowJob,378.0,406.0,28.0
277,Gaia in Throated 39 Sc5,335,Cumshot,586.0,596.0,10.0
277,Gaia in Throated 39 Sc5,334,BlowJob,702.0,752.0,50.0
277,Gaia in Throated 39 Sc5,329,BlowJob,1080.0,1140.0,60.0
277,Gaia in Throated 39 Sc5,330,BlowJob,1302.0,1356.0,54.0
277,Gaia in Throated 39 Sc5,325,Grabbing Boobs,1346.0,1382.0,36.0
277,Gaia in Throated 39 Sc5,331,BlowJob,1530.0,1568.0,38.0
278,Hannah Jo in Thick Dick Threesome,340,BlowJob,968.0,978.0,10.0
278,Hannah Jo in Thick Dick Threesome,341,BlowJob,1036.0,1046.0,10.0
278,Hannah Jo in Thick Dick Threesome,342,BlowJob,1140.0,1174.0,34.0
278,Hannah Jo in Thick Dick Threesome,343,BlowJob,1212.0,1230.0,18.0
278,Hannah Jo in Thick Dick Threesome,344,BlowJob,1364.0,1374.0,10.0
278,Hannah Jo in Thick Dick Threesome,345,BlowJob,1892.0,1910.0,18.0
278,Hannah Jo in Thick Dick Threesome,346,BlowJob,2038.0,2094.0,56.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,348,Grabbing Boobs,50.0,80.0,30.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,349,Grabbing Boobs,378.0,402.0,24.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,350,Grabbing Boobs,638.0,658.0,20.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,353,BlowJob,916.0,932.0,16.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,354,BlowJob,1060.0,1074.0,14.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,370,Anal,1084.0,1116.0,32.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,355,BlowJob,1124.0,1166.0,42.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,356,BlowJob,1414.0,1430.0,16.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,357,BlowJob,1514.0,1524.0,10.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,358,BlowJob,1726.0,1780.0,54.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,360,BlowJob,2150.0,2180.0,30.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,376,Gangbang,2574.0,2620.0,46.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,351,Grabbing Boobs,2706.0,2738.0,32.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,371,Anal,2750.0,2760.0,10.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,372,Anal,2814.0,2836.0,22.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,373,Anal,2892.0,2934.0,42.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,352,Grabbing Boobs,2916.0,2954.0,38.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,374,Anal,3048.0,3102.0,54.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,364,BlowJob,3074.0,3114.0,40.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,366,BlowJob,3646.0,3690.0,44.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,369,BlowJob,4148.0,4180.0,32.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,377,Cumshot,4236.0,4294.0,58.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,378,Cumshot,4354.0,4362.0,8.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,379,Cumshot,4424.0,4466.0,42.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",380,BlowJob,266.0,294.0,28.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",385,BlowJob,312.0,318.0,6.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",386,Gangbang,336.0,346.0,10.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",382,BlowJob,442.0,474.0,32.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",387,Gangbang,488.0,514.0,26.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",5011,BlowJob,576.0,586.0,10.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",5012,BlowJob,744.0,792.0,48.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",383,BlowJob,834.0,846.0,12.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",388,Gangbang,864.0,894.0,30.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",5014,BlowJob,900.0,922.0,22.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",393,Cumshot,1224.0,1248.0,24.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",5015,BlowJob,1260.0,1300.0,40.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",400,Cumshot,1280.0,1302.0,22.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",401,Grabbing Boobs,1308.0,1332.0,24.0
281,Jada Fire in Assault That Ass 8 Sc3,404,BlowJob,338.0,372.0,34.0
281,Jada Fire in Assault That Ass 8 Sc3,405,BlowJob,416.0,470.0,54.0
281,Jada Fire in Assault That Ass 8 Sc3,408,Cumshot,472.0,476.0,4.0
281,Jada Fire in Assault That Ass 8 Sc3,406,BlowJob,912.0,942.0,30.0
281,Jada Fire in Assault That Ass 8 Sc3,407,BlowJob,1082.0,1124.0,42.0
281,Jada Fire in Assault That Ass 8 Sc3,409,Cumshot,1650.0,1654.0,4.0
282,Jada Fire in Throat Yogurt 2 Sc1,412,Cumshot,126.0,182.0,56.0
282,Jada Fire in Throat Yogurt 2 Sc1,413,Cumshot,788.0,826.0,38.0
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,417,BlowJob,886.0,928.0,42.0
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,418,BlowJob,980.0,1022.0,42.0
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,419,BlowJob,1116.0,1136.0,20.0
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,420,BlowJob,1350.0,1382.0,32.0
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,421,Anal,1522.0,1538.0,16.0
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,422,Cumshot,1696.0,1738.0,42.0
284,Jasminy Villar in The Stepfather And His Four Friends,423,Anal,194.0,230.0,36.0
284,Jasminy Villar in The Stepfather And His Four Friends,434,BlowJob,386.0,390.0,4.0
284,Jasminy Villar in The Stepfather And His Four Friends,438,Gangbang,690.0,722.0,32.0
284,Jasminy Villar in The Stepfather And His Four Friends,439,Gangbang,756.0,786.0,30.0
284,Jasminy Villar in The Stepfather And His Four Friends,430,BlowJob,1148.0,1166.0,18.0
284,Jasminy Villar in The Stepfather And His Four Friends,425,Anal,1174.0,1196.0,22.0
284,Jasminy Villar in The Stepfather And His Four Friends,440,Gangbang,1350.0,1388.0,38.0
284,Jasminy Villar in The Stepfather And His Four Friends,447,DP,1648.0,1670.0,22.0
284,Jasminy Villar in The Stepfather And His Four Friends,448,DP,1840.0,1876.0,36.0
284,Jasminy Villar in The Stepfather And His Four Friends,441,Gangbang,1910.0,1944.0,34.0
284,Jasminy Villar in The Stepfather And His Four Friends,442,Gangbang,2006.0,2016.0,10.0
284,Jasminy Villar in The Stepfather And His Four Friends,444,Gangbang,2330.0,2382.0,52.0
284,Jasminy Villar in The Stepfather And His Four Friends,427,Anal,2368.0,2380.0,12.0
284,Jasminy Villar in The Stepfather And His Four Friends,431,BlowJob,2520.0,2532.0,12.0
284,Jasminy Villar in The Stepfather And His Four Friends,445,Gangbang,2532.0,2552.0,20.0
284,Jasminy Villar in The Stepfather And His Four Friends,449,Cumshot,3312.0,3366.0,54.0
284,Jasminy Villar in The Stepfather And His Four Friends,452,Cumshot,3384.0,3394.0,10.0
284,Jasminy Villar in The Stepfather And His Four Friends,433,BlowJob,3406.0,3458.0,52.0
284,Jasminy Villar in The Stepfather And His Four Friends,450,Cumshot,3422.0,3432.0,10.0
284,Jasminy Villar in The Stepfather And His Four Friends,451,Cumshot,3468.0,3492.0,24.0
285,Jena LaRose in Blacks On Blondes,453,BlowJob,206.0,246.0,40.0
285,Jena LaRose in Blacks On Blondes,461,Anal,492.0,548.0,56.0
285,Jena LaRose in Blacks On Blondes,462,Anal,602.0,626.0,24.0
285,Jena LaRose in Blacks On Blondes,463,Anal,702.0,716.0,14.0
285,Jena LaRose in Blacks On Blondes,458,BlowJob,750.0,782.0,32.0
285,Jena LaRose in Blacks On Blondes,464,Anal,764.0,800.0,36.0
285,Jena LaRose in Blacks On Blondes,465,Anal,892.0,934.0,42.0
285,Jena LaRose in Blacks On Blondes,454,BlowJob,968.0,990.0,22.0
285,Jena LaRose in Blacks On Blondes,459,BlowJob,1250.0,1276.0,26.0
285,Jena LaRose in Blacks On Blondes,456,BlowJob,1622.0,1660.0,38.0
285,Jena LaRose in Blacks On Blondes,457,BlowJob,1778.0,1804.0,26.0
285,Jena LaRose in Blacks On Blondes,467,Cumshot,2034.0,2052.0,18.0
286,Jennifer White in Jennifer White Overload Sc1,469,Grabbing Boobs,212.0,244.0,32.0
286,Jennifer White in Jennifer White Overload Sc1,470,Gangbang,414.0,446.0,32.0
286,Jennifer White in Jennifer White Overload Sc1,488,BlowJob,598.0,622.0,24.0
286,Jennifer White in Jennifer White Overload Sc1,472,Gangbang,656.0,672.0,16.0
286,Jennifer White in Jennifer White Overload Sc1,473,Gangbang,754.0,772.0,18.0
286,Jennifer White in Jennifer White Overload Sc1,489,BlowJob,816.0,838.0,22.0
286,Jennifer White in Jennifer White Overload Sc1,475,Gangbang,1224.0,1244.0,20.0
286,Jennifer White in Jennifer White Overload Sc1,490,BlowJob,1640.0,1664.0,24.0
286,Jennifer White in Jennifer White Overload Sc1,492,BlowJob,1826.0,1860.0,34.0
286,Jennifer White in Jennifer White Overload Sc1,478,Gangbang,1990.0,2026.0,36.0
286,Jennifer White in Jennifer White Overload Sc1,479,Gangbang,2094.0,2114.0,20.0
286,Jennifer White in Jennifer White Overload Sc1,481,Gangbang,2312.0,2342.0,30.0
286,Jennifer White in Jennifer White Overload Sc1,482,Gangbang,2394.0,2416.0,22.0
286,Jennifer White in Jennifer White Overload Sc1,494,Anal,2550.0,2570.0,20.0
286,Jennifer White in Jennifer White Overload Sc1,495,Anal,2608.0,2620.0,12.0
286,Jennifer White in Jennifer White Overload Sc1,484,Gangbang,2676.0,2696.0,20.0
286,Jennifer White in Jennifer White Overload Sc1,485,Gangbang,2732.0,2746.0,14.0
286,Jennifer White in Jennifer White Overload Sc1,486,Gangbang,2780.0,2798.0,18.0
286,Jennifer White in Jennifer White Overload Sc1,497,Anal,2934.0,2950.0,16.0
287,Jennifer White in Jennifer White Overload Sc2,498,Grabbing Boobs,84.0,114.0,30.0
287,Jennifer White in Jennifer White Overload Sc2,499,Grabbing Boobs,196.0,220.0,24.0
287,Jennifer White in Jennifer White Overload Sc2,508,Gangbang,712.0,752.0,40.0
287,Jennifer White in Jennifer White Overload Sc2,511,Cumshot,846.0,878.0,32.0
287,Jennifer White in Jennifer White Overload Sc2,517,Anal,1004.0,1042.0,38.0
287,Jennifer White in Jennifer White Overload Sc2,518,Anal,1186.0,1242.0,56.0
287,Jennifer White in Jennifer White Overload Sc2,519,Anal,1342.0,1364.0,22.0
287,Jennifer White in Jennifer White Overload Sc2,501,BlowJob,1644.0,1692.0,48.0
287,Jennifer White in Jennifer White Overload Sc2,502,BlowJob,1792.0,1812.0,20.0
287,Jennifer White in Jennifer White Overload Sc2,520,Anal,1970.0,2026.0,56.0
287,Jennifer White in Jennifer White Overload Sc2,521,Anal,2102.0,2162.0,60.0
287,Jennifer White in Jennifer White Overload Sc2,522,Anal,2270.0,2330.0,60.0
287,Jennifer White in Jennifer White Overload Sc2,504,BlowJob,2274.0,2312.0,38.0
287,Jennifer White in Jennifer White Overload Sc2,505,BlowJob,2588.0,2646.0,58.0
287,Jennifer White in Jennifer White Overload Sc2,509,Gangbang,2626.0,2640.0,14.0
287,Jennifer White in Jennifer White Overload Sc2,524,Anal,2722.0,2766.0,44.0
287,Jennifer White in Jennifer White Overload Sc2,506,BlowJob,2876.0,2918.0,42.0
287,Jennifer White in Jennifer White Overload Sc2,510,Gangbang,3104.0,3138.0,34.0
287,Jennifer White in Jennifer White Overload Sc2,514,Cumshot,3106.0,3140.0,34.0
287,Jennifer White in Jennifer White Overload Sc2,515,Cumshot,3196.0,3252.0,56.0
287,Jennifer White in Jennifer White Overload Sc2,513,Cumshot,3282.0,3294.0,12.0
288,Jennifer White in Jennifer White Overload Sc3,525,Grabbing Boobs,58.0,100.0,42.0
288,Jennifer White in Jennifer White Overload Sc3,539,BlowJob,130.0,156.0,26.0
288,Jennifer White in Jennifer White Overload Sc3,540,BlowJob,324.0,358.0,34.0
288,Jennifer White in Jennifer White Overload Sc3,541,BlowJob,400.0,428.0,28.0
288,Jennifer White in Jennifer White Overload Sc3,528,Gangbang,408.0,426.0,18.0
288,Jennifer White in Jennifer White Overload Sc3,542,BlowJob,470.0,484.0,14.0
288,Jennifer White in Jennifer White Overload Sc3,543,BlowJob,568.0,586.0,18.0
288,Jennifer White in Jennifer White Overload Sc3,545,BlowJob,756.0,768.0,12.0
288,Jennifer White in Jennifer White Overload Sc3,526,Grabbing Boobs,784.0,826.0,42.0
288,Jennifer White in Jennifer White Overload Sc3,530,Gangbang,810.0,862.0,52.0
288,Jennifer White in Jennifer White Overload Sc3,531,Gangbang,920.0,944.0,24.0
288,Jennifer White in Jennifer White Overload Sc3,552,Anal,934.0,990.0,56.0
288,Jennifer White in Jennifer White Overload Sc3,551,BlowJob,938.0,950.0,12.0
288,Jennifer White in Jennifer White Overload Sc3,547,BlowJob,1000.0,1010.0,10.0
288,Jennifer White in Jennifer White Overload Sc3,549,BlowJob,1268.0,1282.0,14.0
288,Jennifer White in Jennifer White Overload Sc3,554,Anal,1660.0,1692.0,32.0
288,Jennifer White in Jennifer White Overload Sc3,534,Gangbang,1738.0,1750.0,12.0
288,Jennifer White in Jennifer White Overload Sc3,535,Gangbang,1794.0,1838.0,44.0
288,Jennifer White in Jennifer White Overload Sc3,555,Anal,1840.0,1856.0,16.0
288,Jennifer White in Jennifer White Overload Sc3,550,BlowJob,1952.0,1982.0,30.0
288,Jennifer White in Jennifer White Overload Sc3,536,Gangbang,2042.0,2074.0,32.0
288,Jennifer White in Jennifer White Overload Sc3,557,Cumshot,2368.0,2388.0,20.0
288,Jennifer White in Jennifer White Overload Sc3,558,Cumshot,2438.0,2478.0,40.0
288,Jennifer White in Jennifer White Overload Sc3,538,Gangbang,2440.0,2464.0,24.0
289,Jenny Pretinha in Fucked by 3 Huge Cocks,560,Anal,374.0,408.0,34.0
289,Jenny Pretinha in Fucked by 3 Huge Cocks,559,BlowJob,450.0,466.0,16.0
289,Jenny Pretinha in Fucked by 3 Huge Cocks,561,Anal,696.0,720.0,24.0
289,Jenny Pretinha in Fucked by 3 Huge Cocks,562,Anal,858.0,868.0,10.0
289,Jenny Pretinha in Fucked by 3 Huge Cocks,563,Anal,1088.0,1100.0,12.0
290,Juelz Ventura in POV BBC Airtight Gangbang,565,BlowJob,2.0,18.0,16.0
290,Juelz Ventura in POV BBC Airtight Gangbang,581,Anal,52.0,70.0,18.0
290,Juelz Ventura in POV BBC Airtight Gangbang,582,Anal,192.0,202.0,10.0
290,Juelz Ventura in POV BBC Airtight Gangbang,583,Anal,238.0,282.0,44.0
290,Juelz Ventura in POV BBC Airtight Gangbang,567,BlowJob,376.0,410.0,34.0
290,Juelz Ventura in POV BBC Airtight Gangbang,568,BlowJob,472.0,514.0,42.0
290,Juelz Ventura in POV BBC Airtight Gangbang,569,BlowJob,628.0,642.0,14.0
290,Juelz Ventura in POV BBC Airtight Gangbang,570,BlowJob,674.0,692.0,18.0
290,Juelz Ventura in POV BBC Airtight Gangbang,572,BlowJob,1130.0,1148.0,18.0
290,Juelz Ventura in POV BBC Airtight Gangbang,580,BlowJob,1166.0,1190.0,24.0
290,Juelz Ventura in POV BBC Airtight Gangbang,588,Cumshot,1222.0,1226.0,4.0
290,Juelz Ventura in POV BBC Airtight Gangbang,589,Cumshot,1342.0,1354.0,12.0
290,Juelz Ventura in POV BBC Airtight Gangbang,573,BlowJob,1344.0,1358.0,14.0
290,Juelz Ventura in POV BBC Airtight Gangbang,585,Anal,1402.0,1416.0,14.0
290,Juelz Ventura in POV BBC Airtight Gangbang,590,Cumshot,1512.0,1522.0,10.0
290,Juelz Ventura in POV BBC Airtight Gangbang,594,Cumshot,1628.0,1640.0,12.0
290,Juelz Ventura in POV BBC Airtight Gangbang,575,BlowJob,1702.0,1738.0,36.0
290,Juelz Ventura in POV BBC Airtight Gangbang,576,BlowJob,1774.0,1790.0,16.0
290,Juelz Ventura in POV BBC Airtight Gangbang,586,Anal,1812.0,1846.0,34.0
290,Juelz Ventura in POV BBC Airtight Gangbang,587,Anal,1882.0,1912.0,30.0
290,Juelz Ventura in POV BBC Airtight Gangbang,577,BlowJob,2074.0,2088.0,14.0
290,Juelz Ventura in POV BBC Airtight Gangbang,592,Cumshot,2430.0,2468.0,38.0
290,Juelz Ventura in POV BBC Airtight Gangbang,578,BlowJob,2446.0,2478.0,32.0
290,Juelz Ventura in POV BBC Airtight Gangbang,595,Cumshot,2850.0,2884.0,34.0
290,Juelz Ventura in POV BBC Airtight Gangbang,579,BlowJob,2854.0,2868.0,14.0
291,"Jureka Del Mar, Maylee Fun in Asian Hotties Work to Cure",596,BlowJob,112.0,118.0,6.0
291,"Jureka Del Mar, Maylee Fun in Asian Hotties Work to Cure",597,BlowJob,130.0,186.0,56.0
292,Katalina Kyle in Ass Worship 18 Sc3,603,BlowJob,608.0,620.0,12.0
292,Katalina Kyle in Ass Worship 18 Sc3,604,BlowJob,674.0,684.0,10.0
292,Katalina Kyle in Ass Worship 18 Sc3,605,BlowJob,766.0,776.0,10.0
292,Katalina Kyle in Ass Worship 18 Sc3,606,BlowJob,824.0,834.0,10.0
292,Katalina Kyle in Ass Worship 18 Sc3,600,Grabbing Boobs,942.0,980.0,38.0
292,Katalina Kyle in Ass Worship 18 Sc3,607,BlowJob,1264.0,1300.0,36.0
292,Katalina Kyle in Ass Worship 18 Sc3,614,Anal,1310.0,1342.0,32.0
292,Katalina Kyle in Ass Worship 18 Sc3,615,Anal,1464.0,1480.0,16.0
292,Katalina Kyle in Ass Worship 18 Sc3,619,DP,1712.0,1740.0,28.0
292,Katalina Kyle in Ass Worship 18 Sc3,601,Grabbing Boobs,1898.0,1938.0,40.0
292,Katalina Kyle in Ass Worship 18 Sc3,620,DP,2018.0,2030.0,12.0
292,Katalina Kyle in Ass Worship 18 Sc3,613,BlowJob,2064.0,2112.0,48.0
292,Katalina Kyle in Ass Worship 18 Sc3,609,BlowJob,2174.0,2186.0,12.0
292,Katalina Kyle in Ass Worship 18 Sc3,610,BlowJob,2252.0,2276.0,24.0
292,Katalina Kyle in Ass Worship 18 Sc3,602,Grabbing Boobs,2328.0,2346.0,18.0
292,Katalina Kyle in Ass Worship 18 Sc3,611,BlowJob,2354.0,2396.0,42.0
292,Katalina Kyle in Ass Worship 18 Sc3,612,BlowJob,2436.0,2474.0,38.0
293,Katalina Kyle in Takes Every Inch Of Manuel,630,BlowJob,440.0,468.0,28.0
293,Katalina Kyle in Takes Every Inch Of Manuel,625,Grabbing Boobs,506.0,562.0,56.0
293,Katalina Kyle in Takes Every Inch Of Manuel,626,Grabbing Boobs,746.0,790.0,44.0
293,Katalina Kyle in Takes Every Inch Of Manuel,631,BlowJob,908.0,936.0,28.0
293,Katalina Kyle in Takes Every Inch Of Manuel,635,Anal,1028.0,1040.0,12.0
293,Katalina Kyle in Takes Every Inch Of Manuel,632,BlowJob,1124.0,1140.0,16.0
293,Katalina Kyle in Takes Every Inch Of Manuel,627,Grabbing Boobs,1156.0,1176.0,20.0
293,Katalina Kyle in Takes Every Inch Of Manuel,628,Grabbing Boobs,1484.0,1520.0,36.0
293,Katalina Kyle in Takes Every Inch Of Manuel,634,BlowJob,1544.0,1570.0,26.0
293,Katalina Kyle in Takes Every Inch Of Manuel,636,Cumshot,1550.0,1594.0,44.0
293,Katalina Kyle in Takes Every Inch Of Manuel,629,Grabbing Boobs,1578.0,1614.0,36.0
294,Katia Belinii in Swallowing 5 Big Loads,637,Grabbing Boobs,18.0,48.0,30.0
294,Katia Belinii in Swallowing 5 Big Loads,638,Grabbing Boobs,338.0,368.0,30.0
294,Katia Belinii in Swallowing 5 Big Loads,645,Anal,482.0,514.0,32.0
294,Katia Belinii in Swallowing 5 Big Loads,641,BlowJob,574.0,588.0,14.0
294,Katia Belinii in Swallowing 5 Big Loads,639,Grabbing Boobs,728.0,746.0,18.0
294,Katia Belinii in Swallowing 5 Big Loads,648,Cumshot,740.0,750.0,10.0
294,Katia Belinii in Swallowing 5 Big Loads,642,BlowJob,752.0,784.0,32.0
294,Katia Belinii in Swallowing 5 Big Loads,647,Anal,1100.0,1114.0,14.0
294,Katia Belinii in Swallowing 5 Big Loads,643,BlowJob,1132.0,1172.0,40.0
294,Katia Belinii in Swallowing 5 Big Loads,649,Cumshot,1208.0,1238.0,30.0
294,Katia Belinii in Swallowing 5 Big Loads,644,BlowJob,1262.0,1298.0,36.0
294,Katia Belinii in Swallowing 5 Big Loads,650,Cumshot,1520.0,1538.0,18.0
296,Kayla Carrera in Anal Integrity Sc1,653,Grabbing Boobs,108.0,154.0,46.0
296,Kayla Carrera in Anal Integrity Sc1,654,BlowJob,528.0,570.0,42.0
296,Kayla Carrera in Anal Integrity Sc1,657,Anal,1716.0,1770.0,54.0
296,Kayla Carrera in Anal Integrity Sc1,658,Anal,1848.0,1882.0,34.0
296,Kayla Carrera in Anal Integrity Sc1,660,Anal,2394.0,2404.0,10.0
296,Kayla Carrera in Anal Integrity Sc1,661,Anal,2516.0,2560.0,44.0
297,Kaylani Lei in Asian Fuck Machines Sc5,664,DP,912.0,918.0,6.0
297,Kaylani Lei in Asian Fuck Machines Sc5,666,BlowJob,1622.0,1650.0,28.0
297,Kaylani Lei in Asian Fuck Machines Sc5,667,BlowJob,1696.0,1728.0,32.0
297,Kaylani Lei in Asian Fuck Machines Sc5,665,DP,2096.0,2132.0,36.0
298,Kazumi Squirts in BBC Orgy Room,684,Cumshot,174.0,220.0,46.0
298,Kazumi Squirts in BBC Orgy Room,692,Gangbang,184.0,208.0,24.0
298,Kazumi Squirts in BBC Orgy Room,693,Gangbang,240.0,268.0,28.0
298,Kazumi Squirts in BBC Orgy Room,685,Cumshot,398.0,428.0,30.0
298,Kazumi Squirts in BBC Orgy Room,671,BlowJob,1230.0,1252.0,22.0
298,Kazumi Squirts in BBC Orgy Room,672,BlowJob,1380.0,1408.0,28.0
298,Kazumi Squirts in BBC Orgy Room,695,Gangbang,1398.0,1424.0,26.0
298,Kazumi Squirts in BBC Orgy Room,673,BlowJob,1526.0,1540.0,14.0
298,Kazumi Squirts in BBC Orgy Room,696,Gangbang,2264.0,2292.0,28.0
298,Kazumi Squirts in BBC Orgy Room,674,BlowJob,2386.0,2432.0,46.0
298,Kazumi Squirts in BBC Orgy Room,683,BlowJob,2838.0,2842.0,4.0
298,Kazumi Squirts in BBC Orgy Room,676,BlowJob,3054.0,3064.0,10.0
298,Kazumi Squirts in BBC Orgy Room,677,BlowJob,3234.0,3244.0,10.0
298,Kazumi Squirts in BBC Orgy Room,678,BlowJob,3346.0,3356.0,10.0
298,Kazumi Squirts in BBC Orgy Room,688,Cumshot,3490.0,3510.0,20.0
298,Kazumi Squirts in BBC Orgy Room,689,Cumshot,3560.0,3612.0,52.0
298,Kazumi Squirts in BBC Orgy Room,686,Cumshot,3644.0,3652.0,8.0
298,Kazumi Squirts in BBC Orgy Room,690,Cumshot,3696.0,3706.0,10.0
298,Kazumi Squirts in BBC Orgy Room,687,Cumshot,3830.0,3836.0,6.0
298,Kazumi Squirts in BBC Orgy Room,679,BlowJob,4166.0,4204.0,38.0
298,Kazumi Squirts in BBC Orgy Room,691,Cumshot,4204.0,4228.0,24.0
299,Kazumi Squirts in Gangbang With Piss and DP,704,Pissing,26.0,36.0,10.0
299,Kazumi Squirts in Gangbang With Piss and DP,708,BlowJob,208.0,246.0,38.0
299,Kazumi Squirts in Gangbang With Piss and DP,716,Cumshot,494.0,516.0,22.0
299,Kazumi Squirts in Gangbang With Piss and DP,717,Cumshot,914.0,918.0,4.0
299,Kazumi Squirts in Gangbang With Piss and DP,705,Pissing,940.0,990.0,50.0
299,Kazumi Squirts in Gangbang With Piss and DP,725,Anal,1512.0,1534.0,22.0
299,Kazumi Squirts in Gangbang With Piss and DP,700,Gangbang,1608.0,1620.0,12.0
299,Kazumi Squirts in Gangbang With Piss and DP,726,Anal,1622.0,1636.0,14.0
299,Kazumi Squirts in Gangbang With Piss and DP,701,Gangbang,1808.0,1818.0,10.0
299,Kazumi Squirts in Gangbang With Piss and DP,702,Gangbang,1866.0,1886.0,20.0
299,Kazumi Squirts in Gangbang With Piss and DP,714,BlowJob,1876.0,1894.0,18.0
299,Kazumi Squirts in Gangbang With Piss and DP,711,BlowJob,1962.0,1986.0,24.0
299,Kazumi Squirts in Gangbang With Piss and DP,727,Anal,2016.0,2046.0,30.0
299,Kazumi Squirts in Gangbang With Piss and DP,729,DP,2028.0,2048.0,20.0
299,Kazumi Squirts in Gangbang With Piss and DP,728,Anal,2218.0,2252.0,34.0
299,Kazumi Squirts in Gangbang With Piss and DP,703,Gangbang,2386.0,2402.0,16.0
299,Kazumi Squirts in Gangbang With Piss and DP,712,BlowJob,2418.0,2470.0,52.0
299,Kazumi Squirts in Gangbang With Piss and DP,715,BlowJob,2596.0,2614.0,18.0
299,Kazumi Squirts in Gangbang With Piss and DP,706,Pissing,2934.0,2968.0,34.0
299,Kazumi Squirts in Gangbang With Piss and DP,713,BlowJob,3258.0,3268.0,10.0
301,Kelly Oliveira in Assfucked 4on1 with DP,748,BlowJob,560.0,578.0,18.0
301,Kelly Oliveira in Assfucked 4on1 with DP,749,BlowJob,682.0,710.0,28.0
301,Kelly Oliveira in Assfucked 4on1 with DP,750,BlowJob,758.0,778.0,20.0
301,Kelly Oliveira in Assfucked 4on1 with DP,753,Titjob,760.0,776.0,16.0
301,Kelly Oliveira in Assfucked 4on1 with DP,756,Anal,1420.0,1450.0,30.0
301,Kelly Oliveira in Assfucked 4on1 with DP,757,Anal,1608.0,1636.0,28.0
301,Kelly Oliveira in Assfucked 4on1 with DP,752,BlowJob,1722.0,1754.0,32.0
301,Kelly Oliveira in Assfucked 4on1 with DP,759,Anal,2014.0,2040.0,26.0
301,Kelly Oliveira in Assfucked 4on1 with DP,760,Anal,2174.0,2184.0,10.0
301,Kelly Oliveira in Assfucked 4on1 with DP,761,Anal,2236.0,2292.0,56.0
301,Kelly Oliveira in Assfucked 4on1 with DP,763,Cumshot,2918.0,2922.0,4.0
301,Kelly Oliveira in Assfucked 4on1 with DP,764,Cumshot,2934.0,2970.0,36.0
301,Kelly Oliveira in Assfucked 4on1 with DP,745,Grabbing Boobs,2956.0,2976.0,20.0
302,Kelly Oliveira in First DP for Brazilian Teen,765,BlowJob,160.0,190.0,30.0
302,Kelly Oliveira in First DP for Brazilian Teen,766,BlowJob,270.0,312.0,42.0
302,Kelly Oliveira in First DP for Brazilian Teen,772,Anal,610.0,668.0,58.0
302,Kelly Oliveira in First DP for Brazilian Teen,773,Anal,702.0,724.0,22.0
302,Kelly Oliveira in First DP for Brazilian Teen,774,Anal,796.0,810.0,14.0
302,Kelly Oliveira in First DP for Brazilian Teen,775,Anal,978.0,990.0,12.0
302,Kelly Oliveira in First DP for Brazilian Teen,778,Grabbing Boobs,1762.0,1788.0,26.0
302,Kelly Oliveira in First DP for Brazilian Teen,770,BlowJob,1874.0,1900.0,26.0
302,Kelly Oliveira in First DP for Brazilian Teen,779,Cumshot,1902.0,1912.0,10.0
303,Kelly Oliveira in Sexy Latina DAP 3on1,782,Anal,766.0,812.0,46.0
303,Kelly Oliveira in Sexy Latina DAP 3on1,786,BlowJob,1590.0,1620.0,30.0
303,Kelly Oliveira in Sexy Latina DAP 3on1,781,Grabbing Boobs,2224.0,2250.0,26.0
303,Kelly Oliveira in Sexy Latina DAP 3on1,784,Anal,2538.0,2596.0,58.0
303,Kelly Oliveira in Sexy Latina DAP 3on1,787,Cumshot,2638.0,2698.0,60.0
304,"Kelly Oliveira, Veronica Leal in 5on2 orgy with DP",791,Anal,322.0,332.0,10.0
304,"Kelly Oliveira, Veronica Leal in 5on2 orgy with DP",793,BlowJob,960.0,990.0,30.0
304,"Kelly Oliveira, Veronica Leal in 5on2 orgy with DP",796,Cumshot,3154.0,3196.0,42.0
304,"Kelly Oliveira, Veronica Leal in 5on2 orgy with DP",797,Cumshot,3234.0,3274.0,40.0
305,Keri Sable in Cum Filled Asshole Overload 2 Sc1,806,Grabbing Boobs,2128.0,2154.0,26.0
305,Keri Sable in Cum Filled Asshole Overload 2 Sc1,804,Anal,2324.0,2384.0,60.0
305,Keri Sable in Cum Filled Asshole Overload 2 Sc1,802,BlowJob,2412.0,2428.0,16.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,807,Cumshot,236.0,252.0,16.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,809,Cumshot,266.0,270.0,4.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,813,BlowJob,670.0,680.0,10.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,814,BlowJob,848.0,874.0,26.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,816,Anal,1000.0,1044.0,44.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,817,Anal,1124.0,1134.0,10.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,818,Pissing,1224.0,1238.0,14.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,812,Grabbing Boobs,1616.0,1668.0,52.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,815,BlowJob,1646.0,1700.0,54.0
307,Kim XXX in Tiny Whore Kim XXX Stuffed,819,Grabbing Boobs,144.0,164.0,20.0
307,Kim XXX in Tiny Whore Kim XXX Stuffed,820,BlowJob,184.0,238.0,54.0
307,Kim XXX in Tiny Whore Kim XXX Stuffed,821,BlowJob,316.0,332.0,16.0
307,Kim XXX in Tiny Whore Kim XXX Stuffed,822,BlowJob,672.0,686.0,14.0
307,Kim XXX in Tiny Whore Kim XXX Stuffed,823,BlowJob,1044.0,1096.0,52.0
308,Kira Thorn in Balls Deep 5on2,830,BlowJob,200.0,226.0,26.0
308,Kira Thorn in Balls Deep 5on2,831,BlowJob,508.0,526.0,18.0
308,Kira Thorn in Balls Deep 5on2,839,DP,790.0,794.0,4.0
308,Kira Thorn in Balls Deep 5on2,840,DP,958.0,976.0,18.0
308,Kira Thorn in Balls Deep 5on2,832,BlowJob,1032.0,1088.0,56.0
308,Kira Thorn in Balls Deep 5on2,841,Gangbang,1126.0,1142.0,16.0
308,Kira Thorn in Balls Deep 5on2,833,BlowJob,1128.0,1140.0,12.0
308,Kira Thorn in Balls Deep 5on2,834,BlowJob,1192.0,1206.0,14.0
308,Kira Thorn in Balls Deep 5on2,838,Anal,2728.0,2774.0,46.0
309,Kitana Montana in Birthday Threeway,849,Cumshot,116.0,126.0,10.0
309,Kitana Montana in Birthday Threeway,850,Cumshot,218.0,248.0,30.0
309,Kitana Montana in Birthday Threeway,843,BlowJob,260.0,272.0,12.0
309,Kitana Montana in Birthday Threeway,853,Anal,530.0,570.0,40.0
309,Kitana Montana in Birthday Threeway,854,Anal,916.0,930.0,14.0
309,Kitana Montana in Birthday Threeway,855,Anal,984.0,1004.0,20.0
309,Kitana Montana in Birthday Threeway,845,BlowJob,1032.0,1060.0,28.0
309,Kitana Montana in Birthday Threeway,856,Anal,1202.0,1226.0,24.0
309,Kitana Montana in Birthday Threeway,851,Cumshot,1672.0,1688.0,16.0
309,Kitana Montana in Birthday Threeway,848,BlowJob,1694.0,1706.0,12.0
310,Kitana Montana in Post,1836,BlowJob,176.0,198.0,22.0
310,Kitana Montana in Post,1842,Cumshot,290.0,294.0,4.0
310,Kitana Montana in Post,1838,BlowJob,630.0,640.0,10.0
310,Kitana Montana in Post,1852,DP,1036.0,1040.0,4.0
310,Kitana Montana in Post,1845,Anal,1058.0,1086.0,28.0
310,Kitana Montana in Post,1839,BlowJob,1212.0,1234.0,22.0
310,Kitana Montana in Post,1846,Anal,1422.0,1452.0,30.0
310,Kitana Montana in Post,1853,DP,1740.0,1772.0,32.0
310,Kitana Montana in Post,1848,Anal,1858.0,1904.0,46.0
310,Kitana Montana in Post,1854,DP,1886.0,1892.0,6.0
310,Kitana Montana in Post,1855,DP,1946.0,2006.0,60.0
310,Kitana Montana in Post,1849,Anal,1950.0,1960.0,10.0
310,Kitana Montana in Post,1850,Anal,2000.0,2028.0,28.0
310,Kitana Montana in Post,1851,Anal,2124.0,2162.0,38.0
310,Kitana Montana in Post,1841,BlowJob,2270.0,2274.0,4.0
310,Kitana Montana in Post,1843,Cumshot,2306.0,2350.0,44.0
311,Laura Fiorentino in 6on1 Swallow,861,BlowJob,290.0,320.0,30.0
311,Laura Fiorentino in 6on1 Swallow,870,Gangbang,704.0,748.0,44.0
311,Laura Fiorentino in 6on1 Swallow,874,Cumshot,968.0,972.0,4.0
311,Laura Fiorentino in 6on1 Swallow,880,Pissing,974.0,990.0,16.0
311,Laura Fiorentino in 6on1 Swallow,875,Cumshot,1042.0,1088.0,46.0
311,Laura Fiorentino in 6on1 Swallow,862,BlowJob,1084.0,1120.0,36.0
311,Laura Fiorentino in 6on1 Swallow,883,DP,2126.0,2130.0,4.0
311,Laura Fiorentino in 6on1 Swallow,873,Gangbang,2244.0,2270.0,26.0
311,Laura Fiorentino in 6on1 Swallow,881,Pissing,2322.0,2360.0,38.0
311,Laura Fiorentino in 6on1 Swallow,882,Pissing,2414.0,2442.0,28.0
311,Laura Fiorentino in 6on1 Swallow,876,Cumshot,2474.0,2496.0,22.0
311,Laura Fiorentino in 6on1 Swallow,864,Anal,2772.0,2794.0,22.0
311,Laura Fiorentino in 6on1 Swallow,866,Anal,2974.0,3004.0,30.0
311,Laura Fiorentino in 6on1 Swallow,867,Anal,3048.0,3106.0,58.0
311,Laura Fiorentino in 6on1 Swallow,868,Anal,3194.0,3254.0,60.0
311,Laura Fiorentino in 6on1 Swallow,869,Anal,3360.0,3406.0,46.0
311,Laura Fiorentino in 6on1 Swallow,858,Grabbing Boobs,3558.0,3582.0,24.0
311,Laura Fiorentino in 6on1 Swallow,877,Cumshot,3622.0,3632.0,10.0
311,Laura Fiorentino in 6on1 Swallow,879,Cumshot,3852.0,3856.0,4.0
312,Lela Star in Assparade 54 Sc2,884,Anal,94.0,112.0,18.0
312,Lela Star in Assparade 54 Sc2,885,BlowJob,166.0,178.0,12.0
312,Lela Star in Assparade 54 Sc2,887,Titjob,182.0,216.0,34.0
312,Lela Star in Assparade 54 Sc2,886,BlowJob,368.0,404.0,36.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,890,BlowJob,88.0,148.0,60.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,894,Grabbing Boobs,156.0,180.0,24.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,891,BlowJob,238.0,256.0,18.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,895,DP,980.0,992.0,12.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,892,BlowJob,1030.0,1040.0,10.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,896,Anal,1102.0,1144.0,42.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,897,Anal,1356.0,1396.0,40.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,898,Anal,1438.0,1450.0,12.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,899,Cumshot,2064.0,2094.0,30.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,893,BlowJob,2112.0,2166.0,54.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,900,Cumshot,2136.0,2182.0,46.0
314,Lolly Ink in True Gonzo Sc5,907,Anal,994.0,1014.0,20.0
314,Lolly Ink in True Gonzo Sc5,903,Grabbing Boobs,1180.0,1200.0,20.0
314,Lolly Ink in True Gonzo Sc5,908,Cumshot,1486.0,1504.0,18.0
315,Luna Star in Double Stuffed,909,Grabbing Boobs,18.0,72.0,54.0
315,Luna Star in Double Stuffed,910,BlowJob,156.0,186.0,30.0
315,Luna Star in Double Stuffed,913,BlowJob,986.0,1010.0,24.0
315,Luna Star in Double Stuffed,917,Anal,1338.0,1356.0,18.0
315,Luna Star in Double Stuffed,914,BlowJob,1512.0,1534.0,22.0
315,Luna Star in Double Stuffed,919,Anal,1558.0,1574.0,16.0
315,Luna Star in Double Stuffed,915,BlowJob,1632.0,1684.0,52.0
316,Luna Star in Why She's A Pornstar,920,Grabbing Boobs,212.0,234.0,22.0
316,Luna Star in Why She's A Pornstar,921,Grabbing Boobs,314.0,340.0,26.0
316,Luna Star in Why She's A Pornstar,922,BlowJob,450.0,462.0,12.0
316,Luna Star in Why She's A Pornstar,923,BlowJob,502.0,536.0,34.0
316,Luna Star in Why She's A Pornstar,924,BlowJob,778.0,798.0,20.0
316,Luna Star in Why She's A Pornstar,925,BlowJob,1172.0,1186.0,14.0
316,Luna Star in Why She's A Pornstar,926,BlowJob,1686.0,1700.0,14.0
316,Luna Star in Why She's A Pornstar,927,BlowJob,1956.0,1976.0,20.0
317,Marilyn Johnson in Airtight Diva Sc1,930,BlowJob,578.0,590.0,12.0
317,Marilyn Johnson in Airtight Diva Sc1,935,BlowJob,616.0,632.0,16.0
317,Marilyn Johnson in Airtight Diva Sc1,931,BlowJob,668.0,682.0,14.0
317,Marilyn Johnson in Airtight Diva Sc1,932,BlowJob,878.0,890.0,12.0
317,Marilyn Johnson in Airtight Diva Sc1,933,BlowJob,1446.0,1456.0,10.0
317,Marilyn Johnson in Airtight Diva Sc1,937,Cumshot,1470.0,1474.0,4.0
317,Marilyn Johnson in Airtight Diva Sc1,939,Cumshot,1578.0,1602.0,24.0
318,"Mariska, Tekohas in Cum Cum Gangbang Orgy",940,BlowJob,172.0,218.0,46.0
318,"Mariska, Tekohas in Cum Cum Gangbang Orgy",946,Cumshot,502.0,510.0,8.0
318,"Mariska, Tekohas in Cum Cum Gangbang Orgy",942,BlowJob,592.0,610.0,18.0
318,"Mariska, Tekohas in Cum Cum Gangbang Orgy",943,BlowJob,856.0,866.0,10.0
318,"Mariska, Tekohas in Cum Cum Gangbang Orgy",944,BlowJob,904.0,960.0,56.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,950,BlowJob,164.0,182.0,18.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,953,BlowJob,2002.0,2038.0,36.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,955,Cumshot,2194.0,2236.0,42.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,949,Grabbing Boobs,2216.0,2234.0,18.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,956,Cumshot,2588.0,2598.0,10.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,958,Cumshot,2642.0,2648.0,6.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,959,Cumshot,2704.0,2730.0,26.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,960,Cumshot,2804.0,2844.0,40.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,961,Cumshot,2876.0,2882.0,6.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,962,BlowJob,208.0,240.0,32.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,963,BlowJob,470.0,480.0,10.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,973,Anal,566.0,608.0,42.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,975,Anal,884.0,898.0,14.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,976,Anal,938.0,964.0,26.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,977,Anal,1006.0,1038.0,32.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,964,BlowJob,1064.0,1078.0,14.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,978,Anal,1314.0,1336.0,22.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,965,BlowJob,1392.0,1414.0,22.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,979,Anal,1566.0,1602.0,36.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,980,Anal,1648.0,1678.0,30.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,966,BlowJob,1810.0,1828.0,18.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,981,Anal,1860.0,1902.0,42.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,967,BlowJob,1990.0,2010.0,20.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,970,BlowJob,2090.0,2106.0,16.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,971,BlowJob,2168.0,2190.0,22.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,972,BlowJob,2262.0,2272.0,10.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,968,BlowJob,2898.0,2912.0,14.0
321,Megan Rain in 10 Cock Blowbang!,986,Gangbang,548.0,578.0,30.0
321,Megan Rain in 10 Cock Blowbang!,987,Cumshot,1490.0,1506.0,16.0
322,Melissa Hot in Fucked by 4 Big Cocks,991,BlowJob,76.0,86.0,10.0
322,Melissa Hot in Fucked by 4 Big Cocks,998,Gangbang,94.0,126.0,32.0
322,Melissa Hot in Fucked by 4 Big Cocks,989,Anal,736.0,756.0,20.0
322,Melissa Hot in Fucked by 4 Big Cocks,992,BlowJob,954.0,1000.0,46.0
322,Melissa Hot in Fucked by 4 Big Cocks,993,BlowJob,1032.0,1042.0,10.0
322,Melissa Hot in Fucked by 4 Big Cocks,999,Gangbang,1726.0,1746.0,20.0
322,Melissa Hot in Fucked by 4 Big Cocks,1000,Gangbang,2142.0,2168.0,26.0
322,Melissa Hot in Fucked by 4 Big Cocks,1001,Gangbang,2204.0,2222.0,18.0
322,Melissa Hot in Fucked by 4 Big Cocks,1002,Gangbang,2254.0,2274.0,20.0
322,Melissa Hot in Fucked by 4 Big Cocks,1003,Gangbang,2350.0,2364.0,14.0
322,Melissa Hot in Fucked by 4 Big Cocks,997,BlowJob,2414.0,2432.0,18.0
322,Melissa Hot in Fucked by 4 Big Cocks,1004,Cumshot,2436.0,2448.0,12.0
322,Melissa Hot in Fucked by 4 Big Cocks,1005,Cumshot,2526.0,2538.0,12.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1015,Anal,782.0,792.0,10.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1014,BlowJob,1052.0,1090.0,38.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1016,Anal,1154.0,1176.0,22.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1006,BlowJob,1580.0,1596.0,16.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1007,BlowJob,1684.0,1696.0,12.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1008,BlowJob,1944.0,1954.0,10.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1009,BlowJob,2166.0,2190.0,24.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1010,BlowJob,2484.0,2514.0,30.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1019,Anal,2540.0,2568.0,28.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1011,BlowJob,2630.0,2662.0,32.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1012,BlowJob,2982.0,2998.0,16.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1013,BlowJob,3040.0,3072.0,32.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1021,Cumshot,3064.0,3108.0,44.0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1028,Anal,350.0,380.0,30.0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1024,BlowJob,862.0,904.0,42.0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1029,Grabbing Boobs,1058.0,1084.0,26.0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1030,Grabbing Boobs,1256.0,1284.0,28.0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1025,BlowJob,1374.0,1410.0,36.0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1027,BlowJob,1438.0,1446.0,8.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1031,Grabbing Boobs,182.0,218.0,36.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1041,Cumshot,684.0,698.0,14.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1033,BlowJob,962.0,976.0,14.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1034,BlowJob,1360.0,1392.0,32.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1036,BlowJob,2384.0,2412.0,28.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1037,BlowJob,2464.0,2476.0,12.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1038,BlowJob,2512.0,2558.0,46.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1039,BlowJob,2614.0,2632.0,18.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1042,Cumshot,2882.0,2892.0,10.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1044,Cumshot,2918.0,2960.0,42.0
326,Mia Trejsi in 100% Hell,1050,Gangbang,94.0,126.0,32.0
326,Mia Trejsi in 100% Hell,1059,BlowJob,464.0,480.0,16.0
326,Mia Trejsi in 100% Hell,1051,Gangbang,674.0,688.0,14.0
326,Mia Trejsi in 100% Hell,1061,BlowJob,1318.0,1340.0,22.0
326,Mia Trejsi in 100% Hell,1047,Grabbing Boobs,1324.0,1370.0,46.0
326,Mia Trejsi in 100% Hell,1052,Gangbang,1332.0,1372.0,40.0
326,Mia Trejsi in 100% Hell,1053,Gangbang,1420.0,1434.0,14.0
326,Mia Trejsi in 100% Hell,1054,Gangbang,1626.0,1652.0,26.0
326,Mia Trejsi in 100% Hell,1062,BlowJob,1880.0,1892.0,12.0
326,Mia Trejsi in 100% Hell,1063,BlowJob,2308.0,2326.0,18.0
326,Mia Trejsi in 100% Hell,1055,Gangbang,2496.0,2510.0,14.0
326,Mia Trejsi in 100% Hell,1056,Gangbang,2762.0,2788.0,26.0
326,Mia Trejsi in 100% Hell,1057,Gangbang,2926.0,2938.0,12.0
326,Mia Trejsi in 100% Hell,1058,Gangbang,2972.0,2992.0,20.0
326,Mia Trejsi in 100% Hell,1048,Grabbing Boobs,3280.0,3338.0,58.0
326,Mia Trejsi in 100% Hell,1064,BlowJob,3318.0,3324.0,6.0
326,Mia Trejsi in 100% Hell,1049,Grabbing Boobs,3384.0,3426.0,42.0
326,Mia Trejsi in 100% Hell,1066,Cumshot,3416.0,3424.0,8.0
327,"Mia Trejsi, Ria Sunn in Angels Of Hardcore 3 Sc1",1067,Grabbing Boobs,0.0,20.0,20.0
327,"Mia Trejsi, Ria Sunn in Angels Of Hardcore 3 Sc1",1068,Gangbang,60.0,94.0,34.0
327,"Mia Trejsi, Ria Sunn in Angels Of Hardcore 3 Sc1",1070,Anal,192.0,230.0,38.0
327,"Mia Trejsi, Ria Sunn in Angels Of Hardcore 3 Sc1",1069,Gangbang,314.0,362.0,48.0
328,Mih Ninfetinha in 4on1 with DP,1074,BlowJob,946.0,988.0,42.0
328,Mih Ninfetinha in 4on1 with DP,1079,BlowJob,1024.0,1054.0,30.0
328,Mih Ninfetinha in 4on1 with DP,1080,BlowJob,1542.0,1550.0,8.0
328,Mih Ninfetinha in 4on1 with DP,1076,BlowJob,1848.0,1880.0,32.0
328,Mih Ninfetinha in 4on1 with DP,1084,Gangbang,1852.0,1912.0,60.0
328,Mih Ninfetinha in 4on1 with DP,1081,BlowJob,2004.0,2008.0,4.0
328,Mih Ninfetinha in 4on1 with DP,1077,BlowJob,2288.0,2332.0,44.0
328,Mih Ninfetinha in 4on1 with DP,1085,Cumshot,2516.0,2524.0,8.0
329,Miss Teela in First Time 10 Gangbang,1086,Gangbang,124.0,168.0,44.0
329,Miss Teela in First Time 10 Gangbang,1092,BlowJob,884.0,914.0,30.0
329,Miss Teela in First Time 10 Gangbang,1093,BlowJob,954.0,994.0,40.0
329,Miss Teela in First Time 10 Gangbang,1089,Gangbang,1180.0,1208.0,28.0
329,Miss Teela in First Time 10 Gangbang,1094,BlowJob,1214.0,1266.0,52.0
329,Miss Teela in First Time 10 Gangbang,1095,BlowJob,1314.0,1342.0,28.0
329,Miss Teela in First Time 10 Gangbang,1096,BlowJob,1422.0,1448.0,26.0
329,Miss Teela in First Time 10 Gangbang,1098,Cumshot,1608.0,1636.0,28.0
329,Miss Teela in First Time 10 Gangbang,1099,Cumshot,1658.0,1702.0,44.0
330,Monika Fox in DP Fantasies 11 Sc3,1101,Grabbing Boobs,172.0,204.0,32.0
330,Monika Fox in DP Fantasies 11 Sc3,1102,Grabbing Boobs,430.0,458.0,28.0
330,Monika Fox in DP Fantasies 11 Sc3,1103,Grabbing Boobs,952.0,978.0,26.0
330,Monika Fox in DP Fantasies 11 Sc3,1108,Cumshot,1428.0,1438.0,10.0
330,Monika Fox in DP Fantasies 11 Sc3,1110,Anal,2074.0,2104.0,30.0
330,Monika Fox in DP Fantasies 11 Sc3,1105,BlowJob,2330.0,2348.0,18.0
330,Monika Fox in DP Fantasies 11 Sc3,1112,Anal,2474.0,2490.0,16.0
330,Monika Fox in DP Fantasies 11 Sc3,1109,Cumshot,2520.0,2538.0,18.0
331,Natasha Teen in Pussy DAPTAP,1115,BlowJob,606.0,620.0,14.0
331,Natasha Teen in Pussy DAPTAP,1116,BlowJob,1064.0,1112.0,48.0
331,Natasha Teen in Pussy DAPTAP,1119,Gangbang,1176.0,1200.0,24.0
331,Natasha Teen in Pussy DAPTAP,1117,BlowJob,1178.0,1210.0,32.0
331,Natasha Teen in Pussy DAPTAP,1120,DP,1366.0,1396.0,30.0
331,Natasha Teen in Pussy DAPTAP,1118,BlowJob,2744.0,2772.0,28.0
332,Nia Nacci in Cum Bang 15 Sc3,1132,Cumshot,1060.0,1082.0,22.0
332,Nia Nacci in Cum Bang 15 Sc3,1123,BlowJob,1106.0,1152.0,46.0
332,Nia Nacci in Cum Bang 15 Sc3,1124,BlowJob,1234.0,1284.0,50.0
332,Nia Nacci in Cum Bang 15 Sc3,1126,BlowJob,1532.0,1566.0,34.0
332,Nia Nacci in Cum Bang 15 Sc3,1127,BlowJob,1598.0,1614.0,16.0
332,Nia Nacci in Cum Bang 15 Sc3,1128,BlowJob,1666.0,1696.0,30.0
332,Nia Nacci in Cum Bang 15 Sc3,1129,BlowJob,1742.0,1778.0,36.0
332,Nia Nacci in Cum Bang 15 Sc3,1130,BlowJob,1920.0,1930.0,10.0
333,Nia Nacci in We Fuck Black Girls 14 Sc5,1137,Grabbing Boobs,640.0,680.0,40.0
333,Nia Nacci in We Fuck Black Girls 14 Sc5,1138,Anal,1474.0,1532.0,58.0
333,Nia Nacci in We Fuck Black Girls 14 Sc5,1139,Anal,1606.0,1632.0,26.0
333,Nia Nacci in We Fuck Black Girls 14 Sc5,1140,Cumshot,1924.0,1952.0,28.0
334,Nia Nacci in White Out 9 Sc2,1142,BlowJob,84.0,134.0,50.0
334,Nia Nacci in White Out 9 Sc2,1155,Grabbing Boobs,1052.0,1104.0,52.0
334,Nia Nacci in White Out 9 Sc2,1144,BlowJob,1302.0,1354.0,52.0
334,Nia Nacci in White Out 9 Sc2,1145,BlowJob,1386.0,1402.0,16.0
334,Nia Nacci in White Out 9 Sc2,1161,Anal,1504.0,1530.0,26.0
334,Nia Nacci in White Out 9 Sc2,1156,Grabbing Boobs,1940.0,1978.0,38.0
334,Nia Nacci in White Out 9 Sc2,1152,Gangbang,1954.0,1976.0,22.0
334,Nia Nacci in White Out 9 Sc2,1158,Grabbing Boobs,2526.0,2558.0,32.0
334,Nia Nacci in White Out 9 Sc2,1147,BlowJob,2690.0,2718.0,28.0
334,Nia Nacci in White Out 9 Sc2,1148,BlowJob,2922.0,2938.0,16.0
334,Nia Nacci in White Out 9 Sc2,1159,Grabbing Boobs,3076.0,3118.0,42.0
334,Nia Nacci in White Out 9 Sc2,1160,Grabbing Boobs,3200.0,3224.0,24.0
334,Nia Nacci in White Out 9 Sc2,1149,BlowJob,3330.0,3358.0,28.0
334,Nia Nacci in White Out 9 Sc2,1162,Anal,3578.0,3594.0,16.0
334,Nia Nacci in White Out 9 Sc2,1153,Gangbang,3582.0,3620.0,38.0
334,Nia Nacci in White Out 9 Sc2,1163,Anal,3662.0,3680.0,18.0
334,Nia Nacci in White Out 9 Sc2,1151,BlowJob,3784.0,3792.0,8.0
334,Nia Nacci in White Out 9 Sc2,1164,Cumshot,3850.0,3860.0,10.0
334,Nia Nacci in White Out 9 Sc2,1165,Cumshot,3878.0,3890.0,12.0
334,Nia Nacci in White Out 9 Sc2,1166,Cumshot,3944.0,3970.0,26.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1167,BlowJob,264.0,314.0,50.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1177,Cumshot,334.0,370.0,36.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1168,BlowJob,468.0,488.0,20.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1175,BlowJob,686.0,696.0,10.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1169,BlowJob,742.0,764.0,22.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1170,BlowJob,810.0,846.0,36.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1180,Cumshot,876.0,892.0,16.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1171,BlowJob,934.0,988.0,54.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1181,Cumshot,978.0,996.0,18.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1187,Cumshot,1382.0,1388.0,6.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1173,BlowJob,1434.0,1444.0,10.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1182,Cumshot,1436.0,1446.0,10.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1174,BlowJob,2022.0,2062.0,40.0
336,Nina Elle in Big Wet Milf Asses Sc2,1194,Anal,612.0,656.0,44.0
336,Nina Elle in Big Wet Milf Asses Sc2,1191,BlowJob,676.0,696.0,20.0
336,Nina Elle in Big Wet Milf Asses Sc2,1192,BlowJob,1496.0,1526.0,30.0
336,Nina Elle in Big Wet Milf Asses Sc2,1196,Cumshot,1842.0,1846.0,4.0
336,Nina Elle in Big Wet Milf Asses Sc2,1197,Cumshot,1862.0,1878.0,16.0
336,Nina Elle in Big Wet Milf Asses Sc2,1193,Grabbing Boobs,1870.0,1890.0,20.0
337,Nina Elle in Gang Bang Addiction Sc4,1199,BlowJob,762.0,790.0,28.0
337,Nina Elle in Gang Bang Addiction Sc4,1200,BlowJob,888.0,908.0,20.0
337,Nina Elle in Gang Bang Addiction Sc4,1206,Anal,908.0,938.0,30.0
337,Nina Elle in Gang Bang Addiction Sc4,1209,Gangbang,1070.0,1100.0,30.0
337,Nina Elle in Gang Bang Addiction Sc4,1201,BlowJob,1356.0,1370.0,14.0
337,Nina Elle in Gang Bang Addiction Sc4,1203,BlowJob,2248.0,2266.0,18.0
337,Nina Elle in Gang Bang Addiction Sc4,1208,Anal,2266.0,2284.0,18.0
337,Nina Elle in Gang Bang Addiction Sc4,1204,BlowJob,2300.0,2322.0,22.0
337,Nina Elle in Gang Bang Addiction Sc4,1205,BlowJob,2844.0,2866.0,22.0
338,Nina Elle in MILF Cumsluts Sc3,1212,Anal,494.0,510.0,16.0
338,Nina Elle in MILF Cumsluts Sc3,1213,Anal,730.0,752.0,22.0
338,Nina Elle in MILF Cumsluts Sc3,1214,Anal,990.0,1002.0,12.0
338,Nina Elle in MILF Cumsluts Sc3,1211,Grabbing Boobs,1058.0,1080.0,22.0
338,Nina Elle in MILF Cumsluts Sc3,1222,Titjob,1076.0,1100.0,24.0
338,Nina Elle in MILF Cumsluts Sc3,1223,Cumshot,1090.0,1110.0,20.0
338,Nina Elle in MILF Cumsluts Sc3,1226,Cumshot,1146.0,1154.0,8.0
338,Nina Elle in MILF Cumsluts Sc3,1215,Anal,1286.0,1312.0,26.0
338,Nina Elle in MILF Cumsluts Sc3,1219,Anal,1988.0,2022.0,34.0
338,Nina Elle in MILF Cumsluts Sc3,1224,Cumshot,2232.0,2240.0,8.0
338,Nina Elle in MILF Cumsluts Sc3,1228,Cumshot,2262.0,2314.0,52.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1232,BlowJob,170.0,184.0,14.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1229,Grabbing Boobs,254.0,286.0,32.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1235,BlowJob,644.0,666.0,22.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1241,Gangbang,662.0,682.0,20.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1230,Grabbing Boobs,1016.0,1038.0,22.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1239,Anal,1070.0,1094.0,24.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1240,Anal,1238.0,1266.0,28.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1236,BlowJob,1530.0,1558.0,28.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1237,BlowJob,1602.0,1624.0,22.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1231,Grabbing Boobs,1610.0,1644.0,34.0
340,Phoenix Marie in Ass Worship 13 Sc4,1244,Anal,552.0,604.0,52.0
340,Phoenix Marie in Ass Worship 13 Sc4,1254,BlowJob,1034.0,1074.0,40.0
340,Phoenix Marie in Ass Worship 13 Sc4,1245,Anal,1036.0,1054.0,18.0
340,Phoenix Marie in Ass Worship 13 Sc4,1255,BlowJob,1142.0,1158.0,16.0
340,Phoenix Marie in Ass Worship 13 Sc4,1246,Anal,1332.0,1380.0,48.0
340,Phoenix Marie in Ass Worship 13 Sc4,1247,Anal,1466.0,1514.0,48.0
340,Phoenix Marie in Ass Worship 13 Sc4,1248,Anal,1556.0,1594.0,38.0
340,Phoenix Marie in Ass Worship 13 Sc4,1258,BlowJob,1674.0,1684.0,10.0
340,Phoenix Marie in Ass Worship 13 Sc4,1259,BlowJob,1748.0,1764.0,16.0
340,Phoenix Marie in Ass Worship 13 Sc4,1250,Anal,1894.0,1914.0,20.0
340,Phoenix Marie in Ass Worship 13 Sc4,1251,Anal,1952.0,1990.0,38.0
340,Phoenix Marie in Ass Worship 13 Sc4,1261,BlowJob,2038.0,2080.0,42.0
341,Rachele Richey in Gangbang Audition,1262,Grabbing Boobs,162.0,192.0,30.0
341,Rachele Richey in Gangbang Audition,1263,BlowJob,666.0,690.0,24.0
341,Rachele Richey in Gangbang Audition,1266,BlowJob,700.0,760.0,60.0
341,Rachele Richey in Gangbang Audition,1265,BlowJob,798.0,848.0,50.0
341,Rachele Richey in Gangbang Audition,1268,BlowJob,904.0,918.0,14.0
341,Rachele Richey in Gangbang Audition,1274,Anal,1064.0,1082.0,18.0
341,Rachele Richey in Gangbang Audition,1269,BlowJob,1110.0,1128.0,18.0
341,Rachele Richey in Gangbang Audition,1270,BlowJob,1290.0,1322.0,32.0
341,Rachele Richey in Gangbang Audition,1276,Anal,1428.0,1462.0,34.0
341,Rachele Richey in Gangbang Audition,1277,Anal,1508.0,1540.0,32.0
341,Rachele Richey in Gangbang Audition,1278,Anal,1584.0,1596.0,12.0
341,Rachele Richey in Gangbang Audition,1279,Anal,1654.0,1708.0,54.0
341,Rachele Richey in Gangbang Audition,1271,BlowJob,1700.0,1722.0,22.0
341,Rachele Richey in Gangbang Audition,1272,BlowJob,2534.0,2544.0,10.0
341,Rachele Richey in Gangbang Audition,1273,BlowJob,2584.0,2604.0,20.0
341,Rachele Richey in Gangbang Audition,1282,Cumshot,2666.0,2676.0,10.0
342,Rose Lynn in Airtight Diva Sc3,1283,Grabbing Boobs,212.0,268.0,56.0
342,Rose Lynn in Airtight Diva Sc3,1284,BlowJob,362.0,390.0,28.0
342,Rose Lynn in Airtight Diva Sc3,1285,BlowJob,428.0,464.0,36.0
342,Rose Lynn in Airtight Diva Sc3,1286,BlowJob,948.0,988.0,40.0
343,Sadie Summers in Gangbang Sluts Sc2,1309,Cumshot,542.0,586.0,44.0
343,Sadie Summers in Gangbang Sluts Sc2,1295,Gangbang,1166.0,1206.0,40.0
343,Sadie Summers in Gangbang Sluts Sc2,1296,Gangbang,1250.0,1292.0,42.0
343,Sadie Summers in Gangbang Sluts Sc2,1290,Grabbing Boobs,1608.0,1628.0,20.0
343,Sadie Summers in Gangbang Sluts Sc2,1297,Gangbang,1610.0,1644.0,34.0
343,Sadie Summers in Gangbang Sluts Sc2,1307,BlowJob,1688.0,1702.0,14.0
343,Sadie Summers in Gangbang Sluts Sc2,1298,Gangbang,1692.0,1712.0,20.0
343,Sadie Summers in Gangbang Sluts Sc2,1299,Gangbang,1780.0,1800.0,20.0
343,Sadie Summers in Gangbang Sluts Sc2,1300,Gangbang,1848.0,1900.0,52.0
343,Sadie Summers in Gangbang Sluts Sc2,1306,BlowJob,2132.0,2180.0,48.0
343,Sadie Summers in Gangbang Sluts Sc2,1302,Gangbang,2318.0,2338.0,20.0
343,Sadie Summers in Gangbang Sluts Sc2,1303,Gangbang,2378.0,2392.0,14.0
343,Sadie Summers in Gangbang Sluts Sc2,1292,Grabbing Boobs,2560.0,2602.0,42.0
343,Sadie Summers in Gangbang Sluts Sc2,1311,Cumshot,2782.0,2786.0,4.0
343,Sadie Summers in Gangbang Sluts Sc2,1312,Cumshot,2838.0,2842.0,4.0
343,Sadie Summers in Gangbang Sluts Sc2,1316,Cumshot,3022.0,3056.0,34.0
343,Sadie Summers in Gangbang Sluts Sc2,1317,Cumshot,3088.0,3098.0,10.0
344,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc2,1319,BlowJob,220.0,230.0,10.0
344,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc2,1320,BlowJob,302.0,316.0,14.0
344,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc2,1321,BlowJob,370.0,398.0,28.0
344,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc2,1323,BlowJob,636.0,662.0,26.0
344,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc2,1324,Grabbing Boobs,642.0,676.0,34.0
345,"Sai Tai Tiger, Daria Glower in Die Haremsw√§chterin des √ñl Scheichs Sc5",1325,69,46.0,64.0,18.0
345,"Sai Tai Tiger, Daria Glower in Die Haremsw√§chterin des √ñl Scheichs Sc5",1326,69,292.0,318.0,26.0
346,"Sai Tai Tiger, Daria Glower, Valerie Hilton in Die Haremsw√§chterin des √ñl Scheichs Sc1",1332,Cumshot,1534.0,1568.0,34.0
347,Sandra Parker in 1st at GB Junkies,1335,BlowJob,402.0,414.0,12.0
347,Sandra Parker in 1st at GB Junkies,1336,BlowJob,656.0,668.0,12.0
347,Sandra Parker in 1st at GB Junkies,1342,Anal,674.0,722.0,48.0
347,Sandra Parker in 1st at GB Junkies,1344,Anal,960.0,1006.0,46.0
347,Sandra Parker in 1st at GB Junkies,1338,BlowJob,1040.0,1056.0,16.0
347,Sandra Parker in 1st at GB Junkies,1339,BlowJob,1316.0,1330.0,14.0
347,Sandra Parker in 1st at GB Junkies,1346,Anal,1374.0,1430.0,56.0
347,Sandra Parker in 1st at GB Junkies,1340,BlowJob,1518.0,1528.0,10.0
347,Sandra Parker in 1st at GB Junkies,1347,Anal,1536.0,1554.0,18.0
347,Sandra Parker in 1st at GB Junkies,1348,Cumshot,1650.0,1658.0,8.0
348,Sandra Parker in Anal Driller 9 Sc3,1351,BlowJob,830.0,852.0,22.0
348,Sandra Parker in Anal Driller 9 Sc3,1352,BlowJob,942.0,998.0,56.0
348,Sandra Parker in Anal Driller 9 Sc3,1353,BlowJob,1040.0,1070.0,30.0
348,Sandra Parker in Anal Driller 9 Sc3,1354,BlowJob,1140.0,1198.0,58.0
348,Sandra Parker in Anal Driller 9 Sc3,1355,BlowJob,1320.0,1348.0,28.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1363,Anal,276.0,298.0,22.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1372,BlowJob,598.0,610.0,12.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1364,Anal,650.0,694.0,44.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1375,BlowJob,664.0,674.0,10.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1365,Anal,812.0,838.0,26.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1373,BlowJob,850.0,876.0,26.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1366,Anal,976.0,998.0,22.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1367,Anal,1034.0,1068.0,34.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1368,Anal,1192.0,1236.0,44.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1369,Anal,1306.0,1330.0,24.0
351,Sandra Parker in Double Stuffed 8 Sc1,1377,BlowJob,632.0,662.0,30.0
351,Sandra Parker in Double Stuffed 8 Sc1,1383,Anal,692.0,702.0,10.0
351,Sandra Parker in Double Stuffed 8 Sc1,1384,Anal,764.0,796.0,32.0
351,Sandra Parker in Double Stuffed 8 Sc1,1378,BlowJob,888.0,916.0,28.0
351,Sandra Parker in Double Stuffed 8 Sc1,1385,Anal,958.0,980.0,22.0
351,Sandra Parker in Double Stuffed 8 Sc1,1379,BlowJob,974.0,996.0,22.0
351,Sandra Parker in Double Stuffed 8 Sc1,1380,BlowJob,1240.0,1290.0,50.0
351,Sandra Parker in Double Stuffed 8 Sc1,1387,Anal,1314.0,1346.0,32.0
351,Sandra Parker in Double Stuffed 8 Sc1,1381,BlowJob,1600.0,1658.0,58.0
351,Sandra Parker in Double Stuffed 8 Sc1,1389,Cumshot,1608.0,1612.0,4.0
351,Sandra Parker in Double Stuffed 8 Sc1,1390,Cumshot,1634.0,1686.0,52.0
352,Sara Retali in BBC Piss Gangbang,1391,Gangbang,40.0,92.0,52.0
352,Sara Retali in BBC Piss Gangbang,1398,BlowJob,110.0,136.0,26.0
352,Sara Retali in BBC Piss Gangbang,1392,Gangbang,192.0,204.0,12.0
352,Sara Retali in BBC Piss Gangbang,1399,BlowJob,194.0,214.0,20.0
352,Sara Retali in BBC Piss Gangbang,1400,BlowJob,360.0,406.0,46.0
352,Sara Retali in BBC Piss Gangbang,1393,Gangbang,610.0,656.0,46.0
352,Sara Retali in BBC Piss Gangbang,1409,Anal,748.0,792.0,44.0
352,Sara Retali in BBC Piss Gangbang,1394,Gangbang,792.0,818.0,26.0
352,Sara Retali in BBC Piss Gangbang,1402,BlowJob,830.0,868.0,38.0
352,Sara Retali in BBC Piss Gangbang,1404,BlowJob,1192.0,1216.0,24.0
352,Sara Retali in BBC Piss Gangbang,1395,Gangbang,1222.0,1258.0,36.0
352,Sara Retali in BBC Piss Gangbang,1396,Gangbang,1290.0,1318.0,28.0
352,Sara Retali in BBC Piss Gangbang,1405,BlowJob,1488.0,1500.0,12.0
352,Sara Retali in BBC Piss Gangbang,1410,Anal,1684.0,1718.0,34.0
352,Sara Retali in BBC Piss Gangbang,1397,Gangbang,1686.0,1732.0,46.0
352,Sara Retali in BBC Piss Gangbang,1406,BlowJob,1858.0,1910.0,52.0
352,Sara Retali in BBC Piss Gangbang,1411,Cumshot,2130.0,2142.0,12.0
352,Sara Retali in BBC Piss Gangbang,1413,Cumshot,2152.0,2180.0,28.0
352,Sara Retali in BBC Piss Gangbang,1412,Cumshot,2190.0,2196.0,6.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1414,BlowJob,122.0,158.0,36.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1418,Gangbang,136.0,174.0,38.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1424,Grabbing Boobs,300.0,326.0,26.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1419,Gangbang,420.0,432.0,12.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1420,Gangbang,804.0,838.0,34.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1425,Anal,1124.0,1178.0,54.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1421,Gangbang,1222.0,1248.0,26.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1426,Anal,1262.0,1302.0,40.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1416,BlowJob,1308.0,1360.0,52.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1422,Gangbang,1336.0,1350.0,14.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1427,Anal,1340.0,1366.0,26.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1423,Gangbang,1548.0,1598.0,50.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1428,Anal,1566.0,1582.0,16.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1429,Cumshot,1754.0,1766.0,12.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1430,Gangbang,48.0,62.0,14.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1431,Gangbang,206.0,226.0,20.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1436,BlowJob,284.0,308.0,24.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1433,Gangbang,400.0,414.0,14.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1437,BlowJob,402.0,426.0,24.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1439,Anal,416.0,428.0,12.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1434,Gangbang,474.0,492.0,18.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1440,Anal,626.0,652.0,26.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1441,Anal,710.0,736.0,26.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1438,BlowJob,964.0,986.0,22.0
355,"Sara Retali, Sapphire Astrea in Spa Day Gone Wild",1442,Grabbing Boobs,252.0,280.0,28.0
355,"Sara Retali, Sapphire Astrea in Spa Day Gone Wild",1443,BlowJob,778.0,802.0,24.0
356,Sarai Minx in Big Tit Slut Milks Cock,1445,Grabbing Boobs,278.0,334.0,56.0
356,Sarai Minx in Big Tit Slut Milks Cock,1447,BlowJob,508.0,564.0,56.0
356,Sarai Minx in Big Tit Slut Milks Cock,1449,BlowJob,584.0,626.0,42.0
356,Sarai Minx in Big Tit Slut Milks Cock,1446,Grabbing Boobs,652.0,684.0,32.0
356,Sarai Minx in Big Tit Slut Milks Cock,1451,Titjob,1472.0,1496.0,24.0
356,Sarai Minx in Big Tit Slut Milks Cock,1452,Cumshot,1476.0,1490.0,14.0
356,Sarai Minx in Big Tit Slut Milks Cock,1453,Cumshot,1504.0,1546.0,42.0
357,"Sheila Ortega, Sara Retali, Anny Star in 3 Latinas by the Pool",1460,Titjob,904.0,916.0,12.0
357,"Sheila Ortega, Sara Retali, Anny Star in 3 Latinas by the Pool",1461,Anal,1180.0,1236.0,56.0
357,"Sheila Ortega, Sara Retali, Anny Star in 3 Latinas by the Pool",1458,BlowJob,2230.0,2234.0,4.0
357,"Sheila Ortega, Sara Retali, Anny Star in 3 Latinas by the Pool",1459,BlowJob,2322.0,2330.0,8.0
358,Shyla Stylez in Anal Integrity Sc2,1470,Anal,1116.0,1158.0,42.0
358,Shyla Stylez in Anal Integrity Sc2,1466,BlowJob,1230.0,1266.0,36.0
358,Shyla Stylez in Anal Integrity Sc2,1467,Grabbing Boobs,1272.0,1294.0,22.0
358,Shyla Stylez in Anal Integrity Sc2,1471,Titjob,1284.0,1300.0,16.0
358,Shyla Stylez in Anal Integrity Sc2,1464,BlowJob,1470.0,1480.0,10.0
358,Shyla Stylez in Anal Integrity Sc2,1468,Grabbing Boobs,1502.0,1548.0,46.0
358,Shyla Stylez in Anal Integrity Sc2,1465,BlowJob,2014.0,2032.0,18.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1473,BlowJob,190.0,212.0,22.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1474,BlowJob,246.0,302.0,56.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1475,BlowJob,480.0,492.0,12.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1476,BlowJob,540.0,578.0,38.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1477,BlowJob,892.0,922.0,30.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1478,BlowJob,1258.0,1286.0,28.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1480,BlowJob,1788.0,1804.0,16.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1487,Anal,1840.0,1854.0,14.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1481,BlowJob,1912.0,1940.0,28.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1488,Anal,1954.0,1988.0,34.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1482,BlowJob,2140.0,2172.0,32.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1484,BlowJob,2488.0,2520.0,32.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1489,Anal,2542.0,2562.0,20.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1485,BlowJob,2642.0,2680.0,38.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1486,BlowJob,2800.0,2820.0,20.0
360,Skin Diamond in Rump Raiders Sc3,1492,BlowJob,774.0,786.0,12.0
360,Skin Diamond in Rump Raiders Sc3,1493,BlowJob,826.0,868.0,42.0
360,Skin Diamond in Rump Raiders Sc3,1494,Anal,906.0,958.0,52.0
360,Skin Diamond in Rump Raiders Sc3,1495,Anal,1014.0,1062.0,48.0
361,,3538,Cumshot,1358.0,1362.0,4.0
361,,3564,Cumshot,1764.0,1778.0,14.0
361,,3453,Grabbing Boobs,2804.0,2838.0,34.0
361,,3540,Cumshot,2820.0,2852.0,32.0
361,,3541,Cumshot,3052.0,3104.0,52.0
361,,3454,Grabbing Boobs,3754.0,3810.0,56.0
361,,3518,BlowJob,3892.0,3902.0,10.0
361,,3455,Grabbing Boobs,4208.0,4230.0,22.0
361,,3456,Grabbing Boobs,4730.0,4772.0,42.0
361,,3457,Grabbing Boobs,4926.0,4974.0,48.0
361,,3582,Titjob,4930.0,4966.0,36.0
361,,3583,Titjob,5410.0,5424.0,14.0
361,,3570,Cumshot,5454.0,5510.0,56.0
361,,3459,Grabbing Boobs,5666.0,5714.0,48.0
361,,3571,Cumshot,6348.0,6366.0,18.0
361,,3547,Cumshot,6568.0,6576.0,8.0
361,,3573,Cumshot,7966.0,8012.0,46.0
361,,3460,Grabbing Boobs,8028.0,8078.0,50.0
361,,3584,Titjob,9490.0,9522.0,32.0
361,,3550,Cumshot,10168.0,10172.0,4.0
361,,3577,Cumshot,11414.0,11420.0,6.0
361,,3463,Grabbing Boobs,11842.0,11884.0,42.0
361,,3585,Titjob,11878.0,11898.0,20.0
361,,3523,BlowJob,11900.0,11912.0,12.0
361,,3464,Grabbing Boobs,11920.0,11942.0,22.0
361,,3586,Titjob,11936.0,11952.0,16.0
361,,3508,BlowJob,12116.0,12130.0,14.0
361,,3525,BlowJob,12148.0,12194.0,46.0
361,,3465,Grabbing Boobs,12158.0,12178.0,20.0
361,,3526,BlowJob,12236.0,12274.0,38.0
361,,3527,BlowJob,12324.0,12372.0,48.0
361,,3578,Cumshot,12354.0,12396.0,42.0
361,,3466,Grabbing Boobs,12384.0,12424.0,40.0
361,,3467,Grabbing Boobs,12642.0,12688.0,46.0
361,,3529,BlowJob,12704.0,12718.0,14.0
361,,3468,Grabbing Boobs,13206.0,13226.0,20.0
361,,3469,Grabbing Boobs,13326.0,13346.0,20.0
361,,3554,Cumshot,14380.0,14384.0,4.0
361,,3556,Cumshot,14534.0,14556.0,22.0
361,,3557,Cumshot,14758.0,14770.0,12.0
361,,3471,Grabbing Boobs,15488.0,15526.0,38.0
361,,3558,Cumshot,15776.0,15824.0,48.0
361,,3559,Cumshot,16422.0,16442.0,20.0
361,,3563,Cumshot,18522.0,18532.0,10.0
362,Summer Day in America Bukkake Live,1497,Gangbang,914.0,938.0,24.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1508,Gangbang,294.0,308.0,14.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1509,Gangbang,382.0,392.0,10.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1514,Cumshot,384.0,388.0,4.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1501,BlowJob,520.0,532.0,12.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1502,BlowJob,572.0,606.0,34.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1510,Gangbang,708.0,730.0,22.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1503,BlowJob,762.0,798.0,36.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1511,Gangbang,848.0,864.0,16.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1518,Anal,850.0,862.0,12.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1519,Anal,906.0,958.0,52.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1512,Gangbang,1014.0,1030.0,16.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1504,BlowJob,1106.0,1122.0,16.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1505,BlowJob,1310.0,1352.0,42.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1521,Anal,1388.0,1404.0,16.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1507,BlowJob,1592.0,1638.0,46.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1513,Gangbang,1646.0,1682.0,36.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1517,Cumshot,1698.0,1716.0,18.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1515,Cumshot,1750.0,1766.0,16.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1516,Cumshot,1854.0,1886.0,32.0
364,Summer Vixen in Gangbang Sluts Sc3,1522,Gangbang,86.0,116.0,30.0
364,Summer Vixen in Gangbang Sluts Sc3,1523,Gangbang,158.0,198.0,40.0
364,Summer Vixen in Gangbang Sluts Sc3,1535,BlowJob,222.0,242.0,20.0
364,Summer Vixen in Gangbang Sluts Sc3,1538,Anal,230.0,252.0,22.0
364,Summer Vixen in Gangbang Sluts Sc3,1524,Gangbang,364.0,374.0,10.0
364,Summer Vixen in Gangbang Sluts Sc3,1536,BlowJob,368.0,424.0,56.0
364,Summer Vixen in Gangbang Sluts Sc3,1539,Anal,398.0,444.0,46.0
364,Summer Vixen in Gangbang Sluts Sc3,1525,Gangbang,424.0,448.0,24.0
364,Summer Vixen in Gangbang Sluts Sc3,1540,Anal,602.0,612.0,10.0
364,Summer Vixen in Gangbang Sluts Sc3,1526,Gangbang,688.0,706.0,18.0
364,Summer Vixen in Gangbang Sluts Sc3,1541,Anal,710.0,724.0,14.0
364,Summer Vixen in Gangbang Sluts Sc3,1542,Anal,858.0,900.0,42.0
364,Summer Vixen in Gangbang Sluts Sc3,1528,Gangbang,956.0,1014.0,58.0
364,Summer Vixen in Gangbang Sluts Sc3,1537,BlowJob,1206.0,1218.0,12.0
364,Summer Vixen in Gangbang Sluts Sc3,1529,Gangbang,1288.0,1338.0,50.0
364,Summer Vixen in Gangbang Sluts Sc3,1544,Anal,1378.0,1396.0,18.0
364,Summer Vixen in Gangbang Sluts Sc3,1530,Gangbang,1386.0,1400.0,14.0
364,Summer Vixen in Gangbang Sluts Sc3,1531,Gangbang,1462.0,1498.0,36.0
364,Summer Vixen in Gangbang Sluts Sc3,1548,DP,1484.0,1500.0,16.0
364,Summer Vixen in Gangbang Sluts Sc3,1545,Anal,1486.0,1504.0,18.0
364,Summer Vixen in Gangbang Sluts Sc3,1532,Gangbang,1692.0,1722.0,30.0
364,Summer Vixen in Gangbang Sluts Sc3,1533,Gangbang,1798.0,1826.0,28.0
364,Summer Vixen in Gangbang Sluts Sc3,1534,Gangbang,1874.0,1886.0,12.0
364,Summer Vixen in Gangbang Sluts Sc3,1549,Cumshot,2290.0,2318.0,28.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1550,Grabbing Boobs,266.0,290.0,24.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1551,Grabbing Boobs,462.0,500.0,38.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1554,BlowJob,996.0,1008.0,12.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1560,BlowJob,1086.0,1098.0,12.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1562,BlowJob,1182.0,1200.0,18.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1563,Gangbang,1402.0,1416.0,14.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1555,BlowJob,1468.0,1478.0,10.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1556,BlowJob,1700.0,1748.0,48.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1557,BlowJob,1908.0,1944.0,36.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1565,Anal,2236.0,2270.0,34.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1558,BlowJob,2356.0,2376.0,20.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1566,Anal,2420.0,2442.0,22.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1552,Grabbing Boobs,2472.0,2518.0,46.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1567,Anal,2738.0,2792.0,54.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1564,Gangbang,2874.0,2884.0,10.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1568,Anal,3006.0,3042.0,36.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1569,Anal,3112.0,3140.0,28.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1570,Anal,3284.0,3344.0,60.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1571,Anal,3388.0,3422.0,34.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1561,BlowJob,3620.0,3628.0,8.0
366,Tekohas in Ass & BigTits,1573,BlowJob,206.0,234.0,28.0
366,Tekohas in Ass & BigTits,1574,BlowJob,344.0,404.0,60.0
366,Tekohas in Ass & BigTits,1576,Grabbing Boobs,432.0,452.0,20.0
366,Tekohas in Ass & BigTits,1577,Anal,658.0,690.0,32.0
366,Tekohas in Ass & BigTits,1575,BlowJob,972.0,982.0,10.0
366,Tekohas in Ass & BigTits,1578,Cumshot,1070.0,1080.0,10.0
367,Tekohas in Bareback Party in Stuttgart,1585,BlowJob,434.0,458.0,24.0
367,Tekohas in Bareback Party in Stuttgart,1581,Grabbing Boobs,850.0,876.0,26.0
367,Tekohas in Bareback Party in Stuttgart,1586,BlowJob,898.0,910.0,12.0
367,Tekohas in Bareback Party in Stuttgart,1587,BlowJob,1028.0,1040.0,12.0
367,Tekohas in Bareback Party in Stuttgart,1588,BlowJob,1090.0,1104.0,14.0
367,Tekohas in Bareback Party in Stuttgart,1582,Grabbing Boobs,1092.0,1114.0,22.0
367,Tekohas in Bareback Party in Stuttgart,1589,BlowJob,1184.0,1244.0,60.0
367,Tekohas in Bareback Party in Stuttgart,1596,Cumshot,1234.0,1240.0,6.0
367,Tekohas in Bareback Party in Stuttgart,1600,Cumshot,1250.0,1300.0,50.0
367,Tekohas in Bareback Party in Stuttgart,1590,BlowJob,1338.0,1348.0,10.0
367,Tekohas in Bareback Party in Stuttgart,1607,Anal,1460.0,1476.0,16.0
367,Tekohas in Bareback Party in Stuttgart,1591,BlowJob,2142.0,2162.0,20.0
367,Tekohas in Bareback Party in Stuttgart,1601,Cumshot,2232.0,2266.0,34.0
367,Tekohas in Bareback Party in Stuttgart,1602,Cumshot,2324.0,2360.0,36.0
367,Tekohas in Bareback Party in Stuttgart,1603,Cumshot,2450.0,2466.0,16.0
367,Tekohas in Bareback Party in Stuttgart,1592,BlowJob,2530.0,2548.0,18.0
367,Tekohas in Bareback Party in Stuttgart,1593,BlowJob,2656.0,2668.0,12.0
367,Tekohas in Bareback Party in Stuttgart,1595,BlowJob,2766.0,2792.0,26.0
367,Tekohas in Bareback Party in Stuttgart,1598,Cumshot,2780.0,2806.0,26.0
367,Tekohas in Bareback Party in Stuttgart,1605,Cumshot,2868.0,2872.0,4.0
368,Thai Suzy in WeLoveBukkake 4,1611,Cumshot,90.0,110.0,20.0
368,Thai Suzy in WeLoveBukkake 4,1612,Cumshot,974.0,994.0,20.0
369,Tia Maria in Cum On Melon Tits,1613,BlowJob,244.0,270.0,26.0
369,Tia Maria in Cum On Melon Tits,1614,BlowJob,338.0,348.0,10.0
369,Tia Maria in Cum On Melon Tits,1620,Grabbing Boobs,900.0,932.0,32.0
369,Tia Maria in Cum On Melon Tits,1615,BlowJob,1152.0,1186.0,34.0
369,Tia Maria in Cum On Melon Tits,1616,BlowJob,1248.0,1272.0,24.0
369,Tia Maria in Cum On Melon Tits,1621,Grabbing Boobs,1294.0,1320.0,26.0
369,Tia Maria in Cum On Melon Tits,1622,Grabbing Boobs,1702.0,1740.0,38.0
369,Tia Maria in Cum On Melon Tits,1623,Grabbing Boobs,2648.0,2708.0,60.0
369,Tia Maria in Cum On Melon Tits,1624,Cumshot,2896.0,2918.0,22.0
370,Tia Maria in DPd By Two BWCs,1626,BlowJob,236.0,254.0,18.0
370,Tia Maria in DPd By Two BWCs,1630,Cumshot,660.0,680.0,20.0
370,Tia Maria in DPd By Two BWCs,1633,Anal,920.0,962.0,42.0
370,Tia Maria in DPd By Two BWCs,1634,Anal,1034.0,1076.0,42.0
370,Tia Maria in DPd By Two BWCs,1628,BlowJob,1336.0,1350.0,14.0
370,Tia Maria in DPd By Two BWCs,1629,BlowJob,1384.0,1432.0,48.0
370,Tia Maria in DPd By Two BWCs,1636,Anal,1482.0,1500.0,18.0
370,Tia Maria in DPd By Two BWCs,1631,Cumshot,1628.0,1650.0,22.0
370,Tia Maria in DPd By Two BWCs,1632,Cumshot,1660.0,1668.0,8.0
371,Tyra Ride in First BBC  DP Gangbang,1651,Pissing,366.0,370.0,4.0
371,Tyra Ride in First BBC  DP Gangbang,1644,BlowJob,378.0,414.0,36.0
371,Tyra Ride in First BBC  DP Gangbang,1645,BlowJob,468.0,478.0,10.0
371,Tyra Ride in First BBC  DP Gangbang,1646,BlowJob,538.0,580.0,42.0
371,Tyra Ride in First BBC  DP Gangbang,1638,Gangbang,584.0,598.0,14.0
371,Tyra Ride in First BBC  DP Gangbang,1647,BlowJob,674.0,724.0,50.0
371,Tyra Ride in First BBC  DP Gangbang,1639,Gangbang,682.0,696.0,14.0
371,Tyra Ride in First BBC  DP Gangbang,1648,BlowJob,944.0,970.0,26.0
371,Tyra Ride in First BBC  DP Gangbang,1640,Gangbang,1012.0,1040.0,28.0
371,Tyra Ride in First BBC  DP Gangbang,1652,Pissing,1200.0,1204.0,4.0
371,Tyra Ride in First BBC  DP Gangbang,1654,Anal,1344.0,1356.0,12.0
371,Tyra Ride in First BBC  DP Gangbang,1655,Anal,1390.0,1412.0,22.0
371,Tyra Ride in First BBC  DP Gangbang,1657,Anal,1626.0,1662.0,36.0
371,Tyra Ride in First BBC  DP Gangbang,1649,BlowJob,1748.0,1766.0,18.0
371,Tyra Ride in First BBC  DP Gangbang,1658,Anal,1750.0,1782.0,32.0
371,Tyra Ride in First BBC  DP Gangbang,1642,Gangbang,1908.0,1936.0,28.0
371,Tyra Ride in First BBC  DP Gangbang,1659,Anal,1970.0,2016.0,46.0
371,Tyra Ride in First BBC  DP Gangbang,1660,Cumshot,2274.0,2320.0,46.0
371,Tyra Ride in First BBC  DP Gangbang,1661,Cumshot,2348.0,2372.0,24.0
371,Tyra Ride in First BBC  DP Gangbang,1650,BlowJob,2354.0,2364.0,10.0
372,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc10,1662,BlowJob,104.0,140.0,36.0
373,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc11,1663,BlowJob,40.0,54.0,14.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1678,Gangbang,192.0,202.0,10.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1679,Gangbang,434.0,474.0,40.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1665,BlowJob,552.0,582.0,30.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1680,Gangbang,560.0,576.0,16.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1666,BlowJob,732.0,752.0,20.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1681,Gangbang,884.0,914.0,30.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1667,BlowJob,888.0,898.0,10.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1682,Gangbang,1042.0,1052.0,10.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1668,BlowJob,1102.0,1128.0,26.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1683,Grabbing Boobs,1174.0,1202.0,28.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1684,Anal,1308.0,1318.0,10.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1670,BlowJob,1428.0,1462.0,34.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1685,Anal,1474.0,1488.0,14.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1686,Anal,1578.0,1608.0,30.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1687,Anal,1678.0,1694.0,16.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1671,BlowJob,1694.0,1706.0,12.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1688,Anal,1792.0,1802.0,10.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1689,Anal,1852.0,1866.0,14.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1692,DP,1904.0,1922.0,18.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1673,BlowJob,2238.0,2256.0,18.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1675,BlowJob,2756.0,2788.0,32.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1691,Anal,2924.0,2936.0,12.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1676,BlowJob,2932.0,2956.0,24.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1693,Cumshot,2976.0,2984.0,8.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1694,Cumshot,3016.0,3074.0,58.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1695,Cumshot,3166.0,3222.0,56.0
376,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc8,1696,Grabbing Boobs,204.0,232.0,28.0
377,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc9,1697,BlowJob,218.0,234.0,16.0
378,Veronica Leal in Domination Gangbang,1698,BlowJob,76.0,94.0,18.0
378,Veronica Leal in Domination Gangbang,1699,BlowJob,148.0,184.0,36.0
378,Veronica Leal in Domination Gangbang,1709,Gangbang,236.0,252.0,16.0
378,Veronica Leal in Domination Gangbang,1710,Gangbang,292.0,324.0,32.0
378,Veronica Leal in Domination Gangbang,1711,Gangbang,570.0,580.0,10.0
378,Veronica Leal in Domination Gangbang,1712,Gangbang,640.0,652.0,12.0
378,Veronica Leal in Domination Gangbang,1724,Pissing,1232.0,1250.0,18.0
378,Veronica Leal in Domination Gangbang,1701,BlowJob,1280.0,1308.0,28.0
378,Veronica Leal in Domination Gangbang,1713,Gangbang,1330.0,1364.0,34.0
378,Veronica Leal in Domination Gangbang,1702,BlowJob,1342.0,1368.0,26.0
378,Veronica Leal in Domination Gangbang,1714,Gangbang,1430.0,1452.0,22.0
378,Veronica Leal in Domination Gangbang,1715,Gangbang,1498.0,1520.0,22.0
378,Veronica Leal in Domination Gangbang,1716,Gangbang,1964.0,1996.0,32.0
378,Veronica Leal in Domination Gangbang,1717,Gangbang,2178.0,2196.0,18.0
378,Veronica Leal in Domination Gangbang,1718,Gangbang,2254.0,2270.0,16.0
378,Veronica Leal in Domination Gangbang,1703,BlowJob,2398.0,2432.0,34.0
378,Veronica Leal in Domination Gangbang,1725,Pissing,2484.0,2512.0,28.0
378,Veronica Leal in Domination Gangbang,1719,Gangbang,2548.0,2578.0,30.0
378,Veronica Leal in Domination Gangbang,1704,BlowJob,2550.0,2562.0,12.0
378,Veronica Leal in Domination Gangbang,1705,BlowJob,2700.0,2726.0,26.0
378,Veronica Leal in Domination Gangbang,1706,BlowJob,2786.0,2812.0,26.0
378,Veronica Leal in Domination Gangbang,1720,Gangbang,2832.0,2856.0,24.0
378,Veronica Leal in Domination Gangbang,1707,BlowJob,2844.0,2858.0,14.0
378,Veronica Leal in Domination Gangbang,1721,Gangbang,2954.0,2970.0,16.0
378,Veronica Leal in Domination Gangbang,1723,Grabbing Boobs,3206.0,3238.0,32.0
379,Vittoria Devine in DP Pee 5on1,1726,BlowJob,1042.0,1078.0,36.0
379,Vittoria Devine in DP Pee 5on1,1727,BlowJob,1132.0,1190.0,58.0
379,Vittoria Devine in DP Pee 5on1,1741,Gangbang,1380.0,1426.0,46.0
379,Vittoria Devine in DP Pee 5on1,1750,Pissing,1606.0,1648.0,42.0
379,Vittoria Devine in DP Pee 5on1,1743,Gangbang,2144.0,2172.0,28.0
379,Vittoria Devine in DP Pee 5on1,1744,Gangbang,2224.0,2254.0,30.0
379,Vittoria Devine in DP Pee 5on1,1730,BlowJob,2418.0,2450.0,32.0
379,Vittoria Devine in DP Pee 5on1,1745,Gangbang,2622.0,2642.0,20.0
379,Vittoria Devine in DP Pee 5on1,1746,Gangbang,2782.0,2804.0,22.0
379,Vittoria Devine in DP Pee 5on1,1732,BlowJob,2942.0,2968.0,26.0
379,Vittoria Devine in DP Pee 5on1,1734,BlowJob,3510.0,3562.0,52.0
379,Vittoria Devine in DP Pee 5on1,1747,Gangbang,3592.0,3614.0,22.0
379,Vittoria Devine in DP Pee 5on1,1735,BlowJob,3650.0,3680.0,30.0
379,Vittoria Devine in DP Pee 5on1,1748,Gangbang,3760.0,3788.0,28.0
379,Vittoria Devine in DP Pee 5on1,1749,Gangbang,3862.0,3894.0,32.0
379,Vittoria Devine in DP Pee 5on1,1736,BlowJob,3868.0,3910.0,42.0
379,Vittoria Devine in DP Pee 5on1,1752,Cumshot,4692.0,4696.0,4.0
379,Vittoria Devine in DP Pee 5on1,1753,Cumshot,4734.0,4742.0,8.0
380,Vittoria Devine in Domination Gangbang,1757,BlowJob,296.0,346.0,50.0
380,Vittoria Devine in Domination Gangbang,1758,BlowJob,356.0,360.0,4.0
380,Vittoria Devine in Domination Gangbang,1759,BlowJob,394.0,432.0,38.0
380,Vittoria Devine in Domination Gangbang,1760,BlowJob,472.0,482.0,10.0
380,Vittoria Devine in Domination Gangbang,1754,Anal,698.0,716.0,18.0
380,Vittoria Devine in Domination Gangbang,1762,Gangbang,1048.0,1068.0,20.0
380,Vittoria Devine in Domination Gangbang,1764,Cumshot,1190.0,1194.0,4.0
380,Vittoria Devine in Domination Gangbang,1763,Gangbang,1204.0,1230.0,26.0
380,Vittoria Devine in Domination Gangbang,1755,Anal,1286.0,1328.0,42.0
380,Vittoria Devine in Domination Gangbang,1767,Pissing,1496.0,1502.0,6.0
380,Vittoria Devine in Domination Gangbang,1766,Cumshot,2060.0,2066.0,6.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1773,BlowJob,374.0,412.0,38.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1768,Anal,582.0,610.0,28.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1769,Anal,756.0,796.0,40.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1776,BlowJob,1612.0,1654.0,42.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1772,Anal,2292.0,2304.0,12.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1779,Cumshot,2590.0,2612.0,22.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1777,BlowJob,2594.0,2610.0,16.0
382,Vit√≥ria Beatriz in Edjunior VideoGuru,1784,Anal,380.0,396.0,16.0
382,Vit√≥ria Beatriz in Edjunior VideoGuru,1782,BlowJob,412.0,460.0,48.0
382,Vit√≥ria Beatriz in Edjunior VideoGuru,1785,Anal,610.0,620.0,10.0
383,Willow Ryder in I Love Anal 3 Sc3,1798,Cumshot,516.0,544.0,28.0
383,Willow Ryder in I Love Anal 3 Sc3,1795,BlowJob,948.0,952.0,4.0
383,Willow Ryder in I Love Anal 3 Sc3,1787,BlowJob,1052.0,1074.0,22.0
383,Willow Ryder in I Love Anal 3 Sc3,1788,BlowJob,1310.0,1346.0,36.0
383,Willow Ryder in I Love Anal 3 Sc3,1789,BlowJob,1430.0,1440.0,10.0
383,Willow Ryder in I Love Anal 3 Sc3,1790,BlowJob,1526.0,1542.0,16.0
383,Willow Ryder in I Love Anal 3 Sc3,1791,BlowJob,1658.0,1692.0,34.0
383,Willow Ryder in I Love Anal 3 Sc3,1792,BlowJob,1774.0,1788.0,14.0
383,Willow Ryder in I Love Anal 3 Sc3,1797,BlowJob,2142.0,2154.0,12.0
384,Yasmina Khan in Birthday Gangbang,1804,Gangbang,58.0,98.0,40.0
384,Yasmina Khan in Birthday Gangbang,1807,BlowJob,224.0,268.0,44.0
384,Yasmina Khan in Birthday Gangbang,1805,Gangbang,596.0,610.0,14.0
384,Yasmina Khan in Birthday Gangbang,1808,BlowJob,668.0,694.0,26.0
384,Yasmina Khan in Birthday Gangbang,1809,BlowJob,822.0,878.0,56.0
384,Yasmina Khan in Birthday Gangbang,1812,Anal,904.0,926.0,22.0
384,Yasmina Khan in Birthday Gangbang,1813,Anal,960.0,972.0,12.0
384,Yasmina Khan in Birthday Gangbang,1814,Anal,1072.0,1100.0,28.0
384,Yasmina Khan in Birthday Gangbang,1815,Cumshot,1240.0,1254.0,14.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1817,BlowJob,466.0,508.0,42.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1818,BlowJob,586.0,596.0,10.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1829,Anal,648.0,662.0,14.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1835,Gangbang,692.0,706.0,14.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1819,BlowJob,744.0,756.0,12.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1820,BlowJob,806.0,860.0,54.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1821,BlowJob,1070.0,1086.0,16.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1831,Anal,1594.0,1616.0,22.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1822,BlowJob,1708.0,1724.0,16.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1832,Anal,1742.0,1774.0,32.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1823,BlowJob,1814.0,1856.0,42.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1824,BlowJob,1966.0,2004.0,38.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1825,BlowJob,2092.0,2120.0,28.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1834,Anal,2234.0,2248.0,14.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1826,BlowJob,2284.0,2306.0,22.0
386,AJ Applegate in Gangbang Me Sc1,2402,BlowJob,368.0,420.0,52.0
386,AJ Applegate in Gangbang Me Sc1,2406,Anal,984.0,996.0,12.0
386,AJ Applegate in Gangbang Me Sc1,2403,BlowJob,1166.0,1212.0,46.0
386,AJ Applegate in Gangbang Me Sc1,2416,Grabbing Boobs,1624.0,1672.0,48.0
386,AJ Applegate in Gangbang Me Sc1,2408,Anal,1774.0,1810.0,36.0
386,AJ Applegate in Gangbang Me Sc1,2409,Anal,1898.0,1950.0,52.0
386,AJ Applegate in Gangbang Me Sc1,2410,Anal,2090.0,2124.0,34.0
386,AJ Applegate in Gangbang Me Sc1,2411,Anal,2296.0,2338.0,42.0
386,AJ Applegate in Gangbang Me Sc1,2412,Anal,2384.0,2410.0,26.0
386,AJ Applegate in Gangbang Me Sc1,2414,Anal,2690.0,2714.0,24.0
386,AJ Applegate in Gangbang Me Sc1,2417,Cumshot,3090.0,3118.0,28.0
386,AJ Applegate in Gangbang Me Sc1,2419,Cumshot,3140.0,3144.0,4.0
386,AJ Applegate in Gangbang Me Sc1,2418,Cumshot,3178.0,3182.0,4.0
387,"Abigail Mac, Alina Lopez in Blindsided Sc3",3452,Grabbing Boobs,430.0,454.0,24.0
388,Adira Allure in Interracial Blowbang 24 Sc1,1977,Grabbing Boobs,826.0,856.0,30.0
388,Adira Allure in Interracial Blowbang 24 Sc1,1971,Gangbang,1510.0,1530.0,20.0
388,Adira Allure in Interracial Blowbang 24 Sc1,1972,Gangbang,1572.0,1588.0,16.0
388,Adira Allure in Interracial Blowbang 24 Sc1,1973,Gangbang,1784.0,1822.0,38.0
388,Adira Allure in Interracial Blowbang 24 Sc1,1979,Cumshot,1998.0,2018.0,20.0
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",2344,BlowJob,328.0,384.0,56.0
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",2345,BlowJob,680.0,736.0,56.0
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",2346,BlowJob,838.0,884.0,46.0
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",2348,Gangbang,916.0,926.0,10.0
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",2347,BlowJob,984.0,990.0,6.0
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",2349,Gangbang,1126.0,1180.0,54.0
392,"Adriana Chechik, Gaia in Grease XXX A Parody Sc5",2390,BlowJob,1104.0,1138.0,34.0
392,"Adriana Chechik, Gaia in Grease XXX A Parody Sc5",2392,Cumshot,1232.0,1246.0,14.0
394,Adrianna Luna in Praise The Load 7 Sc1,2396,Gangbang,560.0,580.0,20.0
394,Adrianna Luna in Praise The Load 7 Sc1,2397,Gangbang,820.0,848.0,28.0
394,Adrianna Luna in Praise The Load 7 Sc1,2398,Grabbing Boobs,872.0,894.0,22.0
394,Adrianna Luna in Praise The Load 7 Sc1,2400,Titjob,976.0,998.0,22.0
394,Adrianna Luna in Praise The Load 7 Sc1,2399,Grabbing Boobs,1034.0,1056.0,22.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2128,BlowJob,682.0,724.0,42.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2129,BlowJob,1012.0,1036.0,24.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2136,BlowJob,1058.0,1062.0,4.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2130,BlowJob,1276.0,1296.0,20.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2131,BlowJob,1352.0,1368.0,16.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2132,BlowJob,1652.0,1694.0,42.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2137,Grabbing Boobs,1748.0,1788.0,40.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2133,BlowJob,1778.0,1838.0,60.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2134,BlowJob,1870.0,1884.0,14.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2135,BlowJob,1926.0,1946.0,20.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2138,Cumshot,1930.0,1954.0,24.0
396,Aidra Fox in Gangbanged 7 Sc1,1987,BlowJob,1854.0,1898.0,44.0
396,Aidra Fox in Gangbanged 7 Sc1,2000,Gangbang,2018.0,2028.0,10.0
396,Aidra Fox in Gangbanged 7 Sc1,1996,Anal,2776.0,2800.0,24.0
396,Aidra Fox in Gangbanged 7 Sc1,1997,Anal,2842.0,2896.0,54.0
396,Aidra Fox in Gangbanged 7 Sc1,1989,BlowJob,2906.0,2918.0,12.0
396,Aidra Fox in Gangbanged 7 Sc1,1998,Anal,2990.0,3000.0,10.0
396,Aidra Fox in Gangbanged 7 Sc1,1990,BlowJob,3034.0,3046.0,12.0
396,Aidra Fox in Gangbanged 7 Sc1,2002,Cumshot,3420.0,3434.0,14.0
396,Aidra Fox in Gangbanged 7 Sc1,2004,Cumshot,3464.0,3474.0,10.0
397,Alena Croft in Blacks on Cougars 17 Sc1,2423,Anal,1128.0,1162.0,34.0
397,Alena Croft in Blacks on Cougars 17 Sc1,2421,BlowJob,1162.0,1182.0,20.0
397,Alena Croft in Blacks on Cougars 17 Sc1,2425,Cumshot,1702.0,1736.0,34.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2081,Gangbang,326.0,360.0,34.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2082,Gangbang,402.0,416.0,14.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2083,Gangbang,494.0,524.0,30.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2079,BlowJob,946.0,956.0,10.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2080,BlowJob,990.0,1002.0,12.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2084,Gangbang,1580.0,1610.0,30.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2087,Cumshot,2024.0,2028.0,4.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2085,Gangbang,2044.0,2054.0,10.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2076,Grabbing Boobs,2058.0,2076.0,18.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2088,Cumshot,2068.0,2074.0,6.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2090,Grabbing Boobs,86.0,106.0,20.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2092,Grabbing Boobs,644.0,666.0,22.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2096,BlowJob,696.0,750.0,54.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2100,Anal,818.0,852.0,34.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2093,Grabbing Boobs,1058.0,1106.0,48.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2107,Gangbang,1362.0,1374.0,12.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2097,BlowJob,1810.0,1868.0,58.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2101,Anal,2110.0,2140.0,30.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2102,Anal,2600.0,2614.0,14.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2104,Anal,3068.0,3078.0,10.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2108,Gangbang,3228.0,3240.0,12.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2098,BlowJob,3240.0,3292.0,52.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2105,Anal,3500.0,3522.0,22.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2106,Anal,3556.0,3588.0,32.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2118,Cumshot,728.0,760.0,32.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2111,Cumshot,812.0,856.0,44.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2120,Cumshot,912.0,934.0,22.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2112,Cumshot,1096.0,1100.0,4.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2113,Cumshot,1418.0,1440.0,22.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2114,Cumshot,1492.0,1502.0,10.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2115,Cumshot,1900.0,1912.0,12.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2116,Cumshot,2006.0,2030.0,24.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2121,Cumshot,2064.0,2106.0,42.0
402,Alexis Ford in Gang Bang Addiction Sc3,2459,Grabbing Boobs,322.0,352.0,30.0
402,Alexis Ford in Gang Bang Addiction Sc3,2462,BlowJob,360.0,416.0,56.0
402,Alexis Ford in Gang Bang Addiction Sc3,2475,BlowJob,462.0,484.0,22.0
402,Alexis Ford in Gang Bang Addiction Sc3,2482,Anal,1032.0,1050.0,18.0
402,Alexis Ford in Gang Bang Addiction Sc3,2481,Gangbang,1034.0,1094.0,60.0
402,Alexis Ford in Gang Bang Addiction Sc3,2460,Grabbing Boobs,1358.0,1394.0,36.0
402,Alexis Ford in Gang Bang Addiction Sc3,2483,Anal,1398.0,1408.0,10.0
402,Alexis Ford in Gang Bang Addiction Sc3,2466,BlowJob,1534.0,1570.0,36.0
402,Alexis Ford in Gang Bang Addiction Sc3,2467,BlowJob,1618.0,1670.0,52.0
402,Alexis Ford in Gang Bang Addiction Sc3,2484,Anal,1700.0,1726.0,26.0
402,Alexis Ford in Gang Bang Addiction Sc3,2468,BlowJob,1840.0,1860.0,20.0
402,Alexis Ford in Gang Bang Addiction Sc3,2485,Anal,1890.0,1914.0,24.0
402,Alexis Ford in Gang Bang Addiction Sc3,2469,BlowJob,1918.0,1950.0,32.0
402,Alexis Ford in Gang Bang Addiction Sc3,2470,BlowJob,2116.0,2136.0,20.0
402,Alexis Ford in Gang Bang Addiction Sc3,2471,BlowJob,2296.0,2312.0,16.0
402,Alexis Ford in Gang Bang Addiction Sc3,2472,BlowJob,2396.0,2408.0,12.0
402,Alexis Ford in Gang Bang Addiction Sc3,2473,BlowJob,2442.0,2468.0,26.0
402,Alexis Ford in Gang Bang Addiction Sc3,2461,Grabbing Boobs,2842.0,2864.0,22.0
402,Alexis Ford in Gang Bang Addiction Sc3,2479,BlowJob,2960.0,2972.0,12.0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2494,Grabbing Boobs,4.0,62.0,58.0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2500,Cumshot,320.0,326.0,6.0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2501,Cumshot,1094.0,1114.0,20.0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2502,Cumshot,1310.0,1342.0,32.0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2503,Cumshot,1428.0,1458.0,30.0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2495,Grabbing Boobs,2058.0,2082.0,24.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2509,Cumshot,130.0,150.0,20.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2520,Grabbing Boobs,1846.0,1874.0,28.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2512,BlowJob,1882.0,1892.0,10.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2521,Grabbing Boobs,2008.0,2044.0,36.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2513,BlowJob,2044.0,2078.0,34.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2514,BlowJob,2370.0,2382.0,12.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2518,Anal,2746.0,2774.0,28.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2522,Grabbing Boobs,2874.0,2892.0,18.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2510,Cumshot,3138.0,3164.0,26.0
405,Alexis Texas in Gang Bang Addiction Sc1,2542,Gangbang,174.0,214.0,40.0
405,Alexis Texas in Gang Bang Addiction Sc1,2543,BlowJob,270.0,282.0,12.0
405,Alexis Texas in Gang Bang Addiction Sc1,2545,BlowJob,594.0,604.0,10.0
405,Alexis Texas in Gang Bang Addiction Sc1,2547,BlowJob,1164.0,1210.0,46.0
405,Alexis Texas in Gang Bang Addiction Sc1,2549,BlowJob,1392.0,1404.0,12.0
405,Alexis Texas in Gang Bang Addiction Sc1,2550,BlowJob,1448.0,1490.0,42.0
405,Alexis Texas in Gang Bang Addiction Sc1,2551,BlowJob,1542.0,1574.0,32.0
405,Alexis Texas in Gang Bang Addiction Sc1,2554,BlowJob,1738.0,1776.0,38.0
405,Alexis Texas in Gang Bang Addiction Sc1,2555,BlowJob,1826.0,1836.0,10.0
405,Alexis Texas in Gang Bang Addiction Sc1,2556,Cumshot,1848.0,1866.0,18.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2557,BlowJob,186.0,198.0,12.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2567,Cumshot,430.0,452.0,22.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2573,Anal,496.0,522.0,26.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2559,BlowJob,554.0,582.0,28.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2560,BlowJob,618.0,648.0,30.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2561,BlowJob,734.0,756.0,22.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2568,Cumshot,870.0,878.0,8.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2574,Anal,1010.0,1022.0,12.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2571,Cumshot,1220.0,1236.0,16.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2569,Cumshot,1712.0,1716.0,4.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2566,BlowJob,1730.0,1740.0,10.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2572,Cumshot,1734.0,1750.0,16.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2576,Anal,1794.0,1828.0,34.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2564,BlowJob,1962.0,1976.0,14.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2570,Cumshot,1996.0,2014.0,18.0
407,Alina in Annegret Zugekleistert Sc4,2605,Grabbing Boobs,28.0,88.0,60.0
407,Alina in Annegret Zugekleistert Sc4,2610,Anal,474.0,496.0,22.0
407,Alina in Annegret Zugekleistert Sc4,2611,Anal,578.0,590.0,12.0
407,Alina in Annegret Zugekleistert Sc4,2612,Anal,628.0,644.0,16.0
407,Alina in Annegret Zugekleistert Sc4,2606,Grabbing Boobs,820.0,862.0,42.0
407,Alina in Annegret Zugekleistert Sc4,2614,Anal,862.0,874.0,12.0
407,Alina in Annegret Zugekleistert Sc4,2608,BlowJob,898.0,930.0,32.0
409,Alina Lopez in No Going Back Sc1,2623,BlowJob,262.0,276.0,14.0
410,Alina Lopez in Perfectly Natural 19 Sc4,2625,BlowJob,572.0,582.0,10.0
410,Alina Lopez in Perfectly Natural 19 Sc4,2627,Cumshot,1294.0,1302.0,8.0
410,Alina Lopez in Perfectly Natural 19 Sc4,2626,BlowJob,1304.0,1320.0,16.0
411,Alina Lopez in Pussy is The Best Medicine 9 Sc5,2628,Grabbing Boobs,144.0,184.0,40.0
412,Alina Lopez in Sneaky Sex 30 Sc1,2629,BlowJob,1352.0,1366.0,14.0
412,Alina Lopez in Sneaky Sex 30 Sc1,2630,Cumshot,1390.0,1408.0,18.0
413,Alina Lopez in Wet Food 9 Sc1,2631,BlowJob,348.0,392.0,44.0
413,Alina Lopez in Wet Food 9 Sc1,2633,BlowJob,618.0,652.0,34.0
413,Alina Lopez in Wet Food 9 Sc1,2649,Gangbang,706.0,742.0,36.0
413,Alina Lopez in Wet Food 9 Sc1,2653,Cumshot,838.0,858.0,20.0
413,Alina Lopez in Wet Food 9 Sc1,2650,Gangbang,890.0,912.0,22.0
413,Alina Lopez in Wet Food 9 Sc1,2654,Cumshot,892.0,924.0,32.0
413,Alina Lopez in Wet Food 9 Sc1,2635,BlowJob,956.0,990.0,34.0
413,Alina Lopez in Wet Food 9 Sc1,2636,BlowJob,1040.0,1058.0,18.0
413,Alina Lopez in Wet Food 9 Sc1,2655,Cumshot,1092.0,1114.0,22.0
413,Alina Lopez in Wet Food 9 Sc1,2637,BlowJob,1106.0,1126.0,20.0
413,Alina Lopez in Wet Food 9 Sc1,2638,BlowJob,1254.0,1286.0,32.0
413,Alina Lopez in Wet Food 9 Sc1,2656,Cumshot,1286.0,1324.0,38.0
413,Alina Lopez in Wet Food 9 Sc1,2657,Cumshot,1366.0,1386.0,20.0
413,Alina Lopez in Wet Food 9 Sc1,2639,BlowJob,1372.0,1392.0,20.0
413,Alina Lopez in Wet Food 9 Sc1,2644,BlowJob,1444.0,1468.0,24.0
413,Alina Lopez in Wet Food 9 Sc1,2658,Cumshot,1454.0,1498.0,44.0
413,Alina Lopez in Wet Food 9 Sc1,2659,Cumshot,1654.0,1668.0,14.0
413,Alina Lopez in Wet Food 9 Sc1,2660,Cumshot,1710.0,1760.0,50.0
413,Alina Lopez in Wet Food 9 Sc1,2640,BlowJob,1738.0,1774.0,36.0
413,Alina Lopez in Wet Food 9 Sc1,2661,Cumshot,1798.0,1818.0,20.0
413,Alina Lopez in Wet Food 9 Sc1,2662,Cumshot,1882.0,1938.0,56.0
413,Alina Lopez in Wet Food 9 Sc1,2641,BlowJob,1886.0,1920.0,34.0
413,Alina Lopez in Wet Food 9 Sc1,2652,Gangbang,2168.0,2186.0,18.0
413,Alina Lopez in Wet Food 9 Sc1,2664,Cumshot,3008.0,3016.0,8.0
414,"Alina Lopez, Vera King in Mommy's Dream Sc4",2670,Grabbing Boobs,274.0,298.0,24.0
415,"Alyssa Divine, Kiki Minaj, Victoria Pure, Victoria Summers in Backstage Bangers Sc1",2747,BlowJob,324.0,358.0,34.0
415,"Alyssa Divine, Kiki Minaj, Victoria Pure, Victoria Summers in Backstage Bangers Sc1",2748,BlowJob,406.0,416.0,10.0
415,"Alyssa Divine, Kiki Minaj, Victoria Pure, Victoria Summers in Backstage Bangers Sc1",2750,Anal,582.0,594.0,12.0
415,"Alyssa Divine, Kiki Minaj, Victoria Pure, Victoria Summers in Backstage Bangers Sc1",2746,Grabbing Boobs,1082.0,1130.0,48.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2751,Gangbang,414.0,446.0,32.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2754,BlowJob,654.0,678.0,24.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2765,Anal,668.0,702.0,34.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2755,BlowJob,1210.0,1244.0,34.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2756,BlowJob,1466.0,1478.0,12.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2752,Gangbang,1774.0,1808.0,34.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2757,BlowJob,1790.0,1818.0,28.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2758,BlowJob,1944.0,1964.0,20.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2759,BlowJob,2010.0,2054.0,44.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2760,BlowJob,2582.0,2594.0,12.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2766,Anal,2636.0,2692.0,56.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2769,Grabbing Boobs,2852.0,2870.0,18.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2767,Anal,3218.0,3270.0,52.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2762,BlowJob,3302.0,3316.0,14.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2768,Anal,3306.0,3326.0,20.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2770,Cumshot,3344.0,3398.0,54.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2773,Cumshot,3424.0,3468.0,44.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2774,Cumshot,3496.0,3534.0,38.0
417,"Amari Anne, Ana Foxxx, Jenna Foxx, Kira Noir, Maya Farrell in Kira vs Kira Sc2",2784,Anal,1232.0,1244.0,12.0
417,"Amari Anne, Ana Foxxx, Jenna Foxx, Kira Noir, Maya Farrell in Kira vs Kira Sc2",2785,Anal,1378.0,1398.0,20.0
418,Amber Deen in Dogfart Invades Europe! 4 Sc1,2787,BlowJob,1252.0,1272.0,20.0
418,Amber Deen in Dogfart Invades Europe! 4 Sc1,2789,BlowJob,1810.0,1818.0,8.0
418,Amber Deen in Dogfart Invades Europe! 4 Sc1,2790,Cumshot,1884.0,1890.0,6.0
418,Amber Deen in Dogfart Invades Europe! 4 Sc1,2791,Cumshot,1930.0,1934.0,4.0
418,Amber Deen in Dogfart Invades Europe! 4 Sc1,2792,Cumshot,1956.0,1964.0,8.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2793,Grabbing Boobs,544.0,564.0,20.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2799,BlowJob,760.0,770.0,10.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2800,BlowJob,1234.0,1256.0,22.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2808,Anal,1284.0,1296.0,12.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2809,Cumshot,1670.0,1684.0,14.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2810,Cumshot,1920.0,1924.0,4.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2803,BlowJob,2104.0,2124.0,20.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2804,BlowJob,2168.0,2216.0,48.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2806,BlowJob,2692.0,2702.0,10.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2797,BlowJob,2738.0,2766.0,28.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2844,Cumshot,534.0,558.0,24.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2846,Cumshot,630.0,634.0,4.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2847,Cumshot,800.0,806.0,6.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2848,Cumshot,976.0,1034.0,58.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2855,Cumshot,1214.0,1218.0,4.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2856,Cumshot,1258.0,1266.0,8.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2859,Anal,1384.0,1418.0,34.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2857,Cumshot,1510.0,1524.0,14.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2850,Cumshot,1570.0,1606.0,36.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2852,Cumshot,2158.0,2194.0,36.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2853,Cumshot,2212.0,2216.0,4.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2932,Grabbing Boobs,16.0,42.0,26.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2933,Grabbing Boobs,314.0,340.0,26.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2934,Anal,428.0,454.0,26.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2950,BlowJob,702.0,750.0,48.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2935,Anal,788.0,802.0,14.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2936,Anal,878.0,902.0,24.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2951,BlowJob,906.0,956.0,50.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2937,Anal,1018.0,1050.0,32.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2939,Anal,1206.0,1226.0,20.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2940,Anal,1334.0,1364.0,30.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2941,Anal,1418.0,1468.0,50.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2954,BlowJob,1704.0,1716.0,12.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2955,BlowJob,1770.0,1810.0,40.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2956,BlowJob,1854.0,1906.0,52.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2957,BlowJob,1992.0,2032.0,40.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2958,BlowJob,2114.0,2148.0,34.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2959,BlowJob,2302.0,2328.0,26.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2943,Anal,2352.0,2386.0,34.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2944,Anal,2456.0,2470.0,14.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2945,Anal,2514.0,2532.0,18.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2961,BlowJob,2534.0,2570.0,36.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2962,BlowJob,2604.0,2620.0,16.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2964,BlowJob,2870.0,2886.0,16.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2969,Gangbang,3046.0,3102.0,56.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2966,BlowJob,3246.0,3290.0,44.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2946,Anal,3294.0,3344.0,50.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2967,BlowJob,3348.0,3362.0,14.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2947,Anal,3538.0,3552.0,14.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2948,Anal,3748.0,3800.0,52.0
423,"Angel Eyes, Jada Fire in Freak Nasty Sc1",2973,Grabbing Boobs,0.0,26.0,26.0
423,"Angel Eyes, Jada Fire in Freak Nasty Sc1",2974,Grabbing Boobs,258.0,316.0,58.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3074,Grabbing Boobs,242.0,272.0,30.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3082,Gangbang,580.0,594.0,14.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3075,Grabbing Boobs,596.0,640.0,44.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3079,BlowJob,610.0,642.0,32.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3083,Gangbang,632.0,668.0,36.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3081,BlowJob,996.0,1010.0,14.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3076,Grabbing Boobs,2380.0,2404.0,24.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3085,Cumshot,2458.0,2474.0,16.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3086,Cumshot,2624.0,2644.0,20.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3077,Grabbing Boobs,2774.0,2808.0,34.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3087,BlowJob,324.0,342.0,18.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3088,BlowJob,408.0,430.0,22.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3089,BlowJob,580.0,598.0,18.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3094,BlowJob,670.0,684.0,14.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3107,Titjob,900.0,952.0,52.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3091,BlowJob,1334.0,1366.0,32.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3103,Grabbing Boobs,1916.0,1960.0,44.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3104,Grabbing Boobs,2330.0,2380.0,50.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3097,BlowJob,2616.0,2628.0,12.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3105,Grabbing Boobs,2828.0,2862.0,34.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3093,BlowJob,3044.0,3054.0,10.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3098,BlowJob,3122.0,3126.0,4.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3108,Cumshot,3130.0,3148.0,18.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3109,Cumshot,3174.0,3182.0,8.0
426,"Angela White, Jada Stevens in Jada Loves Gonzo Sc1",3110,BlowJob,526.0,544.0,18.0
426,"Angela White, Jada Stevens in Jada Loves Gonzo Sc1",3111,Grabbing Boobs,570.0,602.0,32.0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",2365,BlowJob,34.0,44.0,10.0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",2366,BlowJob,188.0,248.0,60.0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",2369,Cumshot,200.0,228.0,28.0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",2367,BlowJob,432.0,456.0,24.0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",2371,Cumshot,818.0,838.0,20.0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",2372,Cumshot,988.0,998.0,10.0
428,Alejandra Rico in Intense Latin Gangbang,1958,BlowJob,148.0,158.0,10.0
428,Alejandra Rico in Intense Latin Gangbang,1961,Anal,714.0,730.0,16.0
428,Alejandra Rico in Intense Latin Gangbang,1960,BlowJob,904.0,926.0,22.0
428,Alejandra Rico in Intense Latin Gangbang,1962,Cumshot,962.0,1000.0,38.0
428,Alejandra Rico in Intense Latin Gangbang,1965,Cumshot,1106.0,1136.0,30.0
429,Alejandra Rico in Tons of Cum,2013,Cumshot,1084.0,1090.0,6.0
429,Alejandra Rico in Tons of Cum,2008,BlowJob,1122.0,1130.0,8.0
429,Alejandra Rico in Tons of Cum,2009,BlowJob,1244.0,1272.0,28.0
429,Alejandra Rico in Tons of Cum,2010,BlowJob,1348.0,1352.0,4.0
429,Alejandra Rico in Tons of Cum,2015,Cumshot,1358.0,1366.0,8.0
429,Alejandra Rico in Tons of Cum,2011,BlowJob,1436.0,1442.0,6.0
429,Alejandra Rico in Tons of Cum,2012,BlowJob,1540.0,1546.0,6.0
430,Alex Grey in A Dirty Submissive Slut For Cock,1982,BlowJob,736.0,746.0,10.0
430,Alex Grey in A Dirty Submissive Slut For Cock,1985,Cumshot,1342.0,1348.0,6.0
431,Alexa Nova in GangBang Creampie 246,2437,BlowJob,76.0,98.0,22.0
431,Alexa Nova in GangBang Creampie 246,2438,BlowJob,376.0,414.0,38.0
431,Alexa Nova in GangBang Creampie 246,2444,Anal,972.0,982.0,10.0
431,Alexa Nova in GangBang Creampie 246,2440,BlowJob,1128.0,1150.0,22.0
431,Alexa Nova in GangBang Creampie 246,2441,BlowJob,1390.0,1400.0,10.0
431,Alexa Nova in GangBang Creampie 246,2443,BlowJob,1842.0,1876.0,34.0
432,Alexa Payne in Stepmom Seduction on the Kitchen Counter,2445,BlowJob,478.0,536.0,58.0
432,Alexa Payne in Stepmom Seduction on the Kitchen Counter,2446,BlowJob,618.0,646.0,28.0
432,Alexa Payne in Stepmom Seduction on the Kitchen Counter,2448,Grabbing Boobs,1344.0,1382.0,38.0
432,Alexa Payne in Stepmom Seduction on the Kitchen Counter,2449,Grabbing Boobs,1830.0,1882.0,52.0
433,"Alexia Vendome, Sothy Hiko, Yasmine Lafitte in No Limits Anal and Oral Orgy",2452,BlowJob,260.0,276.0,16.0
433,"Alexia Vendome, Sothy Hiko, Yasmine Lafitte in No Limits Anal and Oral Orgy",2453,BlowJob,316.0,354.0,38.0
433,"Alexia Vendome, Sothy Hiko, Yasmine Lafitte in No Limits Anal and Oral Orgy",2456,Anal,1018.0,1062.0,44.0
433,"Alexia Vendome, Sothy Hiko, Yasmine Lafitte in No Limits Anal and Oral Orgy",2455,BlowJob,1098.0,1108.0,10.0
433,"Alexia Vendome, Sothy Hiko, Yasmine Lafitte in No Limits Anal and Oral Orgy",2457,Anal,1258.0,1282.0,24.0
434,Alexis Kay in GangBang Creampie 417,2487,Gangbang,88.0,100.0,12.0
434,Alexis Kay in GangBang Creampie 417,2489,Grabbing Boobs,124.0,150.0,26.0
434,Alexis Kay in GangBang Creampie 417,2490,BlowJob,202.0,230.0,28.0
434,Alexis Kay in GangBang Creampie 417,2491,BlowJob,448.0,482.0,34.0
434,Alexis Kay in GangBang Creampie 417,2492,BlowJob,608.0,656.0,48.0
434,Alexis Kay in GangBang Creampie 417,2488,Gangbang,636.0,654.0,18.0
435,Alicia Ribeiro in Anal and Oral with 3 Big Black Cocks,2581,Gangbang,214.0,236.0,22.0
435,Alicia Ribeiro in Anal and Oral with 3 Big Black Cocks,2578,BlowJob,400.0,414.0,14.0
435,Alicia Ribeiro in Anal and Oral with 3 Big Black Cocks,2579,BlowJob,606.0,646.0,40.0
435,Alicia Ribeiro in Anal and Oral with 3 Big Black Cocks,2583,Cumshot,2666.0,2692.0,26.0
436,Alicia Trece in Rough Gangbang and Pee Play,2584,BlowJob,124.0,140.0,16.0
436,Alicia Trece in Rough Gangbang and Pee Play,2585,BlowJob,188.0,198.0,10.0
436,Alicia Trece in Rough Gangbang and Pee Play,2586,BlowJob,244.0,258.0,14.0
436,Alicia Trece in Rough Gangbang and Pee Play,2590,Anal,286.0,308.0,22.0
436,Alicia Trece in Rough Gangbang and Pee Play,2587,BlowJob,384.0,400.0,16.0
436,Alicia Trece in Rough Gangbang and Pee Play,2588,BlowJob,434.0,460.0,26.0
436,Alicia Trece in Rough Gangbang and Pee Play,2591,Anal,564.0,584.0,20.0
436,Alicia Trece in Rough Gangbang and Pee Play,2592,Anal,746.0,798.0,52.0
436,Alicia Trece in Rough Gangbang and Pee Play,2593,Anal,886.0,922.0,36.0
436,Alicia Trece in Rough Gangbang and Pee Play,2603,Grabbing Boobs,1968.0,1988.0,20.0
436,Alicia Trece in Rough Gangbang and Pee Play,2596,Anal,2004.0,2028.0,24.0
436,Alicia Trece in Rough Gangbang and Pee Play,2601,Gangbang,2030.0,2044.0,14.0
436,Alicia Trece in Rough Gangbang and Pee Play,2602,Gangbang,2180.0,2198.0,18.0
436,Alicia Trece in Rough Gangbang and Pee Play,2597,Anal,2206.0,2216.0,10.0
436,Alicia Trece in Rough Gangbang and Pee Play,2604,Cumshot,2816.0,2822.0,6.0
437,Aliyah Taylor in Gang Bang All Her Holes,2679,Gangbang,366.0,384.0,18.0
437,Aliyah Taylor in Gang Bang All Her Holes,2672,BlowJob,544.0,600.0,56.0
437,Aliyah Taylor in Gang Bang All Her Holes,2680,Anal,1182.0,1218.0,36.0
437,Aliyah Taylor in Gang Bang All Her Holes,2674,BlowJob,1364.0,1376.0,12.0
437,Aliyah Taylor in Gang Bang All Her Holes,2675,BlowJob,1724.0,1734.0,10.0
437,Aliyah Taylor in Gang Bang All Her Holes,2677,BlowJob,1750.0,1784.0,34.0
437,Aliyah Taylor in Gang Bang All Her Holes,2676,BlowJob,1794.0,1804.0,10.0
438,Allatra Hot in MILF Craving Hardcore Attention,2684,BlowJob,404.0,432.0,28.0
438,Allatra Hot in MILF Craving Hardcore Attention,2688,BlowJob,464.0,508.0,44.0
438,Allatra Hot in MILF Craving Hardcore Attention,2686,BlowJob,630.0,688.0,58.0
438,Allatra Hot in MILF Craving Hardcore Attention,2690,Anal,854.0,894.0,40.0
438,Allatra Hot in MILF Craving Hardcore Attention,2691,Anal,940.0,966.0,26.0
438,Allatra Hot in MILF Craving Hardcore Attention,2687,BlowJob,1094.0,1106.0,12.0
438,Allatra Hot in MILF Craving Hardcore Attention,2693,Cumshot,1316.0,1330.0,14.0
439,Alura Jenson in GangBang Creampie 240,2695,BlowJob,164.0,182.0,18.0
439,Alura Jenson in GangBang Creampie 240,2702,Gangbang,188.0,212.0,24.0
439,Alura Jenson in GangBang Creampie 240,2708,Grabbing Boobs,232.0,270.0,38.0
439,Alura Jenson in GangBang Creampie 240,2696,BlowJob,244.0,272.0,28.0
439,Alura Jenson in GangBang Creampie 240,2703,Gangbang,448.0,470.0,22.0
439,Alura Jenson in GangBang Creampie 240,2710,Anal,452.0,482.0,30.0
439,Alura Jenson in GangBang Creampie 240,2704,Gangbang,514.0,526.0,12.0
439,Alura Jenson in GangBang Creampie 240,2709,Grabbing Boobs,868.0,904.0,36.0
439,Alura Jenson in GangBang Creampie 240,2706,Gangbang,1166.0,1184.0,18.0
439,Alura Jenson in GangBang Creampie 240,2700,BlowJob,2186.0,2204.0,18.0
439,Alura Jenson in GangBang Creampie 240,2707,Gangbang,2188.0,2220.0,32.0
439,Alura Jenson in GangBang Creampie 240,2701,BlowJob,2344.0,2394.0,50.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2711,Gangbang,878.0,892.0,14.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2723,Anal,1268.0,1308.0,40.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2727,Grabbing Boobs,1434.0,1460.0,26.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2728,Titjob,1446.0,1462.0,16.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2715,BlowJob,1468.0,1502.0,34.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2722,BlowJob,1606.0,1610.0,4.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2724,Anal,1854.0,1872.0,18.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2725,Anal,1916.0,1966.0,50.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2716,BlowJob,1930.0,1948.0,18.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2717,BlowJob,2092.0,2104.0,12.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2718,BlowJob,2888.0,2914.0,26.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2719,BlowJob,3134.0,3146.0,12.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2712,Gangbang,3276.0,3294.0,18.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2713,Gangbang,3348.0,3362.0,14.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2720,BlowJob,3440.0,3460.0,20.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2721,BlowJob,3640.0,3654.0,14.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2729,Cumshot,3644.0,3650.0,6.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2726,Anal,3712.0,3730.0,18.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2731,Gangbang,534.0,554.0,20.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2732,Anal,692.0,708.0,16.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2740,BlowJob,1122.0,1134.0,12.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2734,Anal,1502.0,1514.0,12.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2736,Anal,1828.0,1872.0,44.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2742,Cumshot,2656.0,2676.0,20.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2730,Grabbing Boobs,2722.0,2748.0,26.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2743,Cumshot,2724.0,2766.0,42.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2741,BlowJob,2836.0,2848.0,12.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2744,Cumshot,2840.0,2874.0,34.0
442,Amari Anne in You Wanna Cheat Again,2778,BlowJob,644.0,654.0,10.0
442,Amari Anne in You Wanna Cheat Again,2779,BlowJob,694.0,716.0,22.0
442,Amari Anne in You Wanna Cheat Again,2780,BlowJob,760.0,784.0,24.0
442,Amari Anne in You Wanna Cheat Again,2781,BlowJob,846.0,884.0,38.0
442,Amari Anne in You Wanna Cheat Again,2782,Anal,1662.0,1680.0,18.0
442,Amari Anne in You Wanna Cheat Again,2777,Grabbing Boobs,2286.0,2326.0,40.0
442,Amari Anne in You Wanna Cheat Again,2783,Cumshot,2380.0,2406.0,26.0
443,Amirah Adara in Rough Gangbang Session,2828,BlowJob,1188.0,1222.0,34.0
443,Amirah Adara in Rough Gangbang Session,2830,Anal,1314.0,1324.0,10.0
443,Amirah Adara in Rough Gangbang Session,2829,BlowJob,2712.0,2732.0,20.0
444,Amy Reid in AllOut Blowbang Session,2834,Gangbang,408.0,442.0,34.0
444,Amy Reid in AllOut Blowbang Session,2835,Cumshot,980.0,1024.0,44.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2860,BlowJob,394.0,410.0,16.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2866,Gangbang,562.0,574.0,12.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2862,BlowJob,1116.0,1130.0,14.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2867,Gangbang,1740.0,1768.0,28.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2869,Anal,2026.0,2042.0,16.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2868,Gangbang,2462.0,2498.0,36.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2863,BlowJob,2468.0,2494.0,26.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2864,BlowJob,2720.0,2734.0,14.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2872,Anal,2870.0,2890.0,20.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2865,BlowJob,3160.0,3178.0,18.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2873,Cumshot,3164.0,3216.0,52.0
446,Ana J√∫lia in DP com a Mulata Cavala,2876,Grabbing Boobs,238.0,264.0,26.0
446,Ana J√∫lia in DP com a Mulata Cavala,2879,BlowJob,516.0,534.0,18.0
446,Ana J√∫lia in DP com a Mulata Cavala,2880,BlowJob,1096.0,1136.0,40.0
446,Ana J√∫lia in DP com a Mulata Cavala,2881,BlowJob,1192.0,1214.0,22.0
446,Ana J√∫lia in DP com a Mulata Cavala,2882,Anal,1674.0,1702.0,28.0
446,Ana J√∫lia in DP com a Mulata Cavala,2884,Cumshot,2372.0,2408.0,36.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2885,BlowJob,38.0,76.0,38.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2887,Anal,168.0,188.0,20.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2888,Anal,286.0,314.0,28.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2889,Anal,346.0,366.0,20.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2886,BlowJob,502.0,528.0,26.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2890,69,662.0,676.0,14.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2891,Cumshot,1492.0,1504.0,12.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2920,Anal,342.0,360.0,18.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2921,Anal,2394.0,2404.0,10.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2928,Grabbing Boobs,2410.0,2468.0,58.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2922,Anal,2438.0,2458.0,20.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2923,Anal,2550.0,2562.0,12.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2929,BlowJob,2622.0,2632.0,10.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2925,Anal,2928.0,2944.0,16.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2926,Anal,2980.0,2998.0,18.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2930,BlowJob,4210.0,4242.0,32.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2931,BlowJob,4882.0,4906.0,24.0
449,Angel Lima in Big Butt Airtight Show,2977,Grabbing Boobs,486.0,516.0,30.0
449,Angel Lima in Big Butt Airtight Show,2978,Grabbing Boobs,588.0,628.0,40.0
449,Angel Lima in Big Butt Airtight Show,2981,Anal,1022.0,1034.0,12.0
449,Angel Lima in Big Butt Airtight Show,2979,Grabbing Boobs,1326.0,1350.0,24.0
449,Angel Lima in Big Butt Airtight Show,2982,Anal,1868.0,1914.0,46.0
449,Angel Lima in Big Butt Airtight Show,2983,Anal,2192.0,2230.0,38.0
449,Angel Lima in Big Butt Airtight Show,2984,Anal,2328.0,2362.0,34.0
449,Angel Lima in Big Butt Airtight Show,2985,Anal,2420.0,2434.0,14.0
449,Angel Lima in Big Butt Airtight Show,2980,Grabbing Boobs,2714.0,2734.0,20.0
450,Angel Lima in Hardcore Brazilian Double Anal,2988,Grabbing Boobs,40.0,100.0,60.0
450,Angel Lima in Hardcore Brazilian Double Anal,2989,Grabbing Boobs,154.0,172.0,18.0
450,Angel Lima in Hardcore Brazilian Double Anal,2995,BlowJob,732.0,766.0,34.0
450,Angel Lima in Hardcore Brazilian Double Anal,2998,Titjob,756.0,808.0,52.0
450,Angel Lima in Hardcore Brazilian Double Anal,2991,Grabbing Boobs,876.0,894.0,18.0
450,Angel Lima in Hardcore Brazilian Double Anal,2992,Grabbing Boobs,972.0,998.0,26.0
450,Angel Lima in Hardcore Brazilian Double Anal,2996,BlowJob,984.0,1000.0,16.0
450,Angel Lima in Hardcore Brazilian Double Anal,2997,BlowJob,1046.0,1062.0,16.0
450,Angel Lima in Hardcore Brazilian Double Anal,2993,Grabbing Boobs,1820.0,1850.0,30.0
450,Angel Lima in Hardcore Brazilian Double Anal,3001,Anal,1964.0,1980.0,16.0
450,Angel Lima in Hardcore Brazilian Double Anal,3004,Anal,2530.0,2542.0,12.0
450,Angel Lima in Hardcore Brazilian Double Anal,3005,Anal,2608.0,2628.0,20.0
450,Angel Lima in Hardcore Brazilian Double Anal,2994,Grabbing Boobs,3346.0,3364.0,18.0
450,Angel Lima in Hardcore Brazilian Double Anal,3008,Cumshot,3392.0,3408.0,16.0
450,Angel Lima in Hardcore Brazilian Double Anal,3009,Cumshot,3442.0,3472.0,30.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3010,Grabbing Boobs,18.0,44.0,26.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3011,Grabbing Boobs,710.0,738.0,28.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3012,Grabbing Boobs,828.0,878.0,50.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3017,BlowJob,1986.0,2002.0,16.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3013,Grabbing Boobs,2194.0,2244.0,50.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3014,Grabbing Boobs,2412.0,2430.0,18.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3021,Anal,2574.0,2630.0,56.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3018,BlowJob,2768.0,2786.0,18.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3019,BlowJob,2926.0,2960.0,34.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3022,Cumshot,3022.0,3078.0,56.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3015,Grabbing Boobs,3064.0,3092.0,28.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3023,Grabbing Boobs,198.0,220.0,22.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3024,Grabbing Boobs,254.0,286.0,32.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3025,Grabbing Boobs,330.0,352.0,22.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3027,BlowJob,414.0,446.0,32.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3029,BlowJob,684.0,744.0,60.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3031,Anal,904.0,924.0,20.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3026,Grabbing Boobs,950.0,970.0,20.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3032,Grabbing Boobs,18.0,44.0,26.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3033,Grabbing Boobs,176.0,206.0,30.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3034,Grabbing Boobs,710.0,738.0,28.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3043,BlowJob,1280.0,1294.0,14.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3040,BlowJob,1312.0,1316.0,4.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3047,Anal,1356.0,1368.0,12.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3035,Grabbing Boobs,1530.0,1564.0,34.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3036,Grabbing Boobs,2220.0,2244.0,24.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3048,Anal,2348.0,2372.0,24.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3045,BlowJob,2734.0,2776.0,42.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3041,BlowJob,2954.0,2960.0,6.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3038,Grabbing Boobs,3072.0,3100.0,28.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3050,Cumshot,3076.0,3080.0,4.0
455,"Angel Smalls, Anna De Ville, Barbie Sins, Jureka Del Mar, May Thai, Nathaly Cherie, Selvaggia in Messy Facial Compilation",3057,Grabbing Boobs,442.0,468.0,26.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3112,Grabbing Boobs,64.0,86.0,22.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3117,BlowJob,730.0,758.0,28.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3118,BlowJob,1152.0,1196.0,44.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3132,Gangbang,1176.0,1188.0,12.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3135,Pissing,1256.0,1262.0,6.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3119,BlowJob,1268.0,1316.0,48.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3113,Grabbing Boobs,1300.0,1340.0,40.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3120,BlowJob,1392.0,1434.0,42.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3121,BlowJob,1486.0,1502.0,16.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3134,Gangbang,2370.0,2420.0,50.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3123,BlowJob,2406.0,2424.0,18.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3137,Cumshot,2468.0,2490.0,22.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3124,BlowJob,2474.0,2486.0,12.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3125,BlowJob,2524.0,2580.0,56.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3138,Cumshot,2584.0,2590.0,6.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3126,BlowJob,3060.0,3076.0,16.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3130,Anal,3820.0,3830.0,10.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3131,Anal,3928.0,3954.0,26.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3115,BlowJob,4124.0,4134.0,10.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3136,Pissing,4630.0,4634.0,4.0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,3158,BlowJob,1248.0,1282.0,34.0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,3160,BlowJob,2064.0,2078.0,14.0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,3161,BlowJob,2360.0,2384.0,24.0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,3162,BlowJob,2616.0,2646.0,30.0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,3163,BlowJob,2696.0,2726.0,30.0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,3164,BlowJob,2766.0,2790.0,24.0
459,Ania Kinski in Kinky DP Session At The Clinic,3169,Anal,1408.0,1446.0,38.0
459,Ania Kinski in Kinky DP Session At The Clinic,3170,Anal,1522.0,1536.0,14.0
459,Ania Kinski in Kinky DP Session At The Clinic,3172,Cumshot,2032.0,2060.0,28.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3173,Grabbing Boobs,710.0,732.0,22.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3175,Cumshot,712.0,726.0,14.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3178,BlowJob,1182.0,1196.0,14.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3179,BlowJob,1258.0,1310.0,52.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3180,BlowJob,1400.0,1420.0,20.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3186,Anal,1422.0,1440.0,18.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3181,BlowJob,1482.0,1500.0,18.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3182,BlowJob,1554.0,1572.0,18.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3187,Anal,1574.0,1612.0,38.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3188,Anal,1708.0,1742.0,34.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3185,BlowJob,2074.0,2094.0,20.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3176,Cumshot,2078.0,2086.0,8.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3174,Grabbing Boobs,2154.0,2182.0,28.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3203,Grabbing Boobs,1352.0,1372.0,20.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3204,Grabbing Boobs,2140.0,2168.0,28.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3210,BlowJob,2266.0,2276.0,10.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3218,Anal,2294.0,2314.0,20.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3211,BlowJob,2382.0,2398.0,16.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3205,Grabbing Boobs,2434.0,2478.0,44.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3212,BlowJob,2534.0,2584.0,50.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3206,Grabbing Boobs,2634.0,2682.0,48.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3219,Anal,2728.0,2782.0,54.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3220,Anal,2828.0,2870.0,42.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3221,Anal,3466.0,3488.0,22.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3222,Anal,3532.0,3560.0,28.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3213,BlowJob,3728.0,3746.0,18.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3216,BlowJob,4028.0,4052.0,24.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3207,Grabbing Boobs,4126.0,4160.0,34.0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",3231,Anal,340.0,350.0,10.0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",3234,DP,954.0,960.0,6.0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",3228,BlowJob,1074.0,1108.0,34.0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",3233,Grabbing Boobs,1128.0,1146.0,18.0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",3229,BlowJob,1168.0,1178.0,10.0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",3235,Cumshot,1924.0,1976.0,52.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3246,BlowJob,678.0,716.0,38.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3250,Anal,822.0,832.0,10.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3253,Anal,1390.0,1412.0,22.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3244,BlowJob,1518.0,1528.0,10.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3249,BlowJob,1606.0,1640.0,34.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3254,Anal,1696.0,1716.0,20.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3255,Anal,1820.0,1830.0,10.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3256,Anal,1938.0,1954.0,16.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3257,Anal,2002.0,2028.0,26.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3258,Anal,2102.0,2134.0,32.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3247,BlowJob,2306.0,2318.0,12.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3242,Grabbing Boobs,2356.0,2404.0,48.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3259,BlowJob,896.0,924.0,28.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3260,BlowJob,1216.0,1238.0,22.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3272,Grabbing Boobs,1746.0,1770.0,24.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3266,Anal,1748.0,1774.0,26.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3267,Anal,1978.0,1998.0,20.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3268,Anal,2038.0,2054.0,16.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3262,BlowJob,2082.0,2094.0,12.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3269,Anal,2122.0,2180.0,58.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3263,BlowJob,2562.0,2598.0,36.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3273,69,2614.0,2636.0,22.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3271,Anal,2898.0,2908.0,10.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3264,BlowJob,3312.0,3344.0,32.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3274,Grabbing Boobs,82.0,102.0,20.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3279,BlowJob,352.0,362.0,10.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3280,BlowJob,408.0,420.0,12.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3275,Grabbing Boobs,666.0,708.0,42.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3276,Grabbing Boobs,1070.0,1102.0,32.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3277,Grabbing Boobs,1316.0,1352.0,36.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3278,Grabbing Boobs,1430.0,1480.0,50.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3281,BlowJob,1506.0,1536.0,30.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3282,BlowJob,1576.0,1620.0,44.0
468,Anissa Kate in A Hot Surfer Threesome,3290,BlowJob,170.0,184.0,14.0
468,Anissa Kate in A Hot Surfer Threesome,3296,BlowJob,298.0,312.0,14.0
468,Anissa Kate in A Hot Surfer Threesome,3292,BlowJob,370.0,388.0,18.0
468,Anissa Kate in A Hot Surfer Threesome,3293,BlowJob,468.0,484.0,16.0
468,Anissa Kate in A Hot Surfer Threesome,3283,Anal,740.0,796.0,56.0
468,Anissa Kate in A Hot Surfer Threesome,3285,Anal,1042.0,1064.0,22.0
468,Anissa Kate in A Hot Surfer Threesome,3294,BlowJob,1138.0,1178.0,40.0
468,Anissa Kate in A Hot Surfer Threesome,3286,Anal,1414.0,1426.0,12.0
468,Anissa Kate in A Hot Surfer Threesome,3287,Anal,1466.0,1476.0,10.0
468,Anissa Kate in A Hot Surfer Threesome,3288,Anal,1520.0,1578.0,58.0
468,Anissa Kate in A Hot Surfer Threesome,3289,Anal,1662.0,1688.0,26.0
468,Anissa Kate in A Hot Surfer Threesome,3298,Grabbing Boobs,1730.0,1754.0,24.0
468,Anissa Kate in A Hot Surfer Threesome,3300,Cumshot,1750.0,1766.0,16.0
468,Anissa Kate in A Hot Surfer Threesome,3297,BlowJob,1766.0,1790.0,24.0
468,Anissa Kate in A Hot Surfer Threesome,3301,Cumshot,1790.0,1794.0,4.0
468,Anissa Kate in A Hot Surfer Threesome,3299,Grabbing Boobs,1906.0,1924.0,18.0
469,Anissa Kate in Hardcore Business Meeting,3310,Titjob,662.0,674.0,12.0
469,Anissa Kate in Hardcore Business Meeting,3306,BlowJob,754.0,764.0,10.0
469,Anissa Kate in Hardcore Business Meeting,3307,BlowJob,1356.0,1376.0,20.0
469,Anissa Kate in Hardcore Business Meeting,3308,BlowJob,1488.0,1514.0,26.0
469,Anissa Kate in Hardcore Business Meeting,3312,Cumshot,2226.0,2282.0,56.0
469,Anissa Kate in Hardcore Business Meeting,3313,Cumshot,2332.0,2364.0,32.0
469,Anissa Kate in Hardcore Business Meeting,3304,Grabbing Boobs,2340.0,2358.0,18.0
470,Anissa Kate in Poolside DP with BBC and Cumshot Finish,3326,BlowJob,1768.0,1826.0,58.0
470,Anissa Kate in Poolside DP with BBC and Cumshot Finish,3327,BlowJob,2204.0,2248.0,44.0
470,Anissa Kate in Poolside DP with BBC and Cumshot Finish,3330,Anal,2542.0,2554.0,12.0
470,Anissa Kate in Poolside DP with BBC and Cumshot Finish,3331,Anal,2672.0,2698.0,26.0
470,Anissa Kate in Poolside DP with BBC and Cumshot Finish,3328,BlowJob,2916.0,2922.0,6.0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3356,Grabbing Boobs,790.0,814.0,24.0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3357,Grabbing Boobs,864.0,888.0,24.0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3349,BlowJob,1520.0,1576.0,56.0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3351,BlowJob,2034.0,2056.0,22.0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3352,BlowJob,2478.0,2488.0,10.0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3355,BlowJob,2960.0,2966.0,6.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3362,Grabbing Boobs,1214.0,1246.0,32.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3363,BlowJob,1500.0,1528.0,28.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3367,BlowJob,1564.0,1606.0,42.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3368,BlowJob,1710.0,1732.0,22.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3369,BlowJob,1906.0,1940.0,34.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3371,Anal,2126.0,2140.0,14.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3372,Anal,2236.0,2254.0,18.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3373,Anal,2404.0,2440.0,36.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3370,BlowJob,2422.0,2436.0,14.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3374,Anal,2502.0,2516.0,14.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3375,Anal,2550.0,2562.0,12.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3376,Anal,2664.0,2696.0,32.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3377,Anal,2786.0,2800.0,14.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3365,BlowJob,2936.0,2984.0,48.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3378,Cumshot,2964.0,2994.0,30.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3401,BlowJob,644.0,654.0,10.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3396,BlowJob,836.0,848.0,12.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3402,BlowJob,866.0,870.0,4.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3397,BlowJob,1440.0,1452.0,12.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3398,BlowJob,1654.0,1692.0,38.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3404,BlowJob,1904.0,1926.0,22.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3399,BlowJob,2028.0,2040.0,12.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3405,Titjob,2048.0,2064.0,16.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3394,Grabbing Boobs,2152.0,2170.0,18.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3407,Grabbing Boobs,100.0,160.0,60.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3416,Anal,450.0,472.0,22.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3409,Grabbing Boobs,986.0,1018.0,32.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3410,Grabbing Boobs,1100.0,1144.0,44.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3412,BlowJob,1884.0,1930.0,46.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3418,Anal,2346.0,2380.0,34.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3419,Anal,2440.0,2464.0,24.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3420,Anal,2542.0,2570.0,28.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3421,Anal,2612.0,2658.0,46.0
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",3423,BlowJob,102.0,150.0,48.0
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",3427,Cumshot,184.0,220.0,36.0
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",3428,Cumshot,304.0,340.0,36.0
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",3424,BlowJob,372.0,416.0,44.0
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",3425,BlowJob,456.0,490.0,34.0
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",3426,BlowJob,524.0,558.0,34.0
477,Anni Star in Lingerie Pleasure Premi√®re,3445,BlowJob,66.0,118.0,52.0
477,Anni Star in Lingerie Pleasure Premi√®re,3446,BlowJob,156.0,166.0,10.0
477,Anni Star in Lingerie Pleasure Premi√®re,3447,Grabbing Boobs,188.0,228.0,40.0
477,Anni Star in Lingerie Pleasure Premi√®re,3448,Titjob,204.0,234.0,30.0
477,Anni Star in Lingerie Pleasure Premi√®re,3449,Anal,350.0,374.0,24.0
477,Anni Star in Lingerie Pleasure Premi√®re,3451,Anal,732.0,756.0,24.0
479,April Snow in GangBang Creampie 232,5205,Gangbang,10.0,40.0,30.0
479,April Snow in GangBang Creampie 232,5206,BlowJob,82.0,104.0,22.0
479,April Snow in GangBang Creampie 232,5216,Grabbing Boobs,268.0,288.0,20.0
479,April Snow in GangBang Creampie 232,5208,BlowJob,866.0,910.0,44.0
479,April Snow in GangBang Creampie 232,5218,Anal,984.0,1030.0,46.0
479,April Snow in GangBang Creampie 232,5209,BlowJob,1160.0,1174.0,14.0
479,April Snow in GangBang Creampie 232,5210,BlowJob,1232.0,1280.0,48.0
479,April Snow in GangBang Creampie 232,5217,Grabbing Boobs,1302.0,1322.0,20.0
479,April Snow in GangBang Creampie 232,5211,BlowJob,1336.0,1358.0,22.0
479,April Snow in GangBang Creampie 232,5213,BlowJob,1750.0,1768.0,18.0
479,April Snow in GangBang Creampie 232,5214,BlowJob,1802.0,1822.0,20.0
479,April Snow in GangBang Creampie 232,5215,BlowJob,1924.0,1952.0,28.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5221,BlowJob,302.0,310.0,8.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5223,BlowJob,406.0,442.0,36.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5227,Anal,690.0,718.0,28.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5228,Anal,1100.0,1114.0,14.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5229,Anal,1168.0,1200.0,32.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5230,Anal,1258.0,1292.0,34.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5224,BlowJob,1616.0,1660.0,44.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5225,BlowJob,1702.0,1718.0,16.0
482,"Ashby Winter in Vogue 2, Part 5",5232,BlowJob,1246.0,1266.0,20.0
482,"Ashby Winter in Vogue 2, Part 5",5237,DP,2152.0,2180.0,28.0
482,"Ashby Winter in Vogue 2, Part 5",5234,BlowJob,2490.0,2508.0,18.0
482,"Ashby Winter in Vogue 2, Part 5",5235,BlowJob,2548.0,2584.0,36.0
482,"Ashby Winter in Vogue 2, Part 5",5238,Cumshot,2636.0,2694.0,58.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5239,BlowJob,8.0,18.0,10.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5245,BlowJob,36.0,86.0,50.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5241,BlowJob,146.0,168.0,22.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5242,BlowJob,218.0,242.0,24.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5243,BlowJob,340.0,372.0,32.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5244,BlowJob,786.0,836.0,50.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5247,Cumshot,794.0,810.0,16.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5249,Cumshot,926.0,946.0,20.0
484,Ashley Cumstar in Gangbang Party,5250,Grabbing Boobs,14.0,32.0,18.0
484,Ashley Cumstar in Gangbang Party,5252,BlowJob,230.0,250.0,20.0
484,Ashley Cumstar in Gangbang Party,5256,Cumshot,302.0,314.0,12.0
484,Ashley Cumstar in Gangbang Party,5253,BlowJob,346.0,398.0,52.0
484,Ashley Cumstar in Gangbang Party,5255,BlowJob,432.0,442.0,10.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5257,Anal,160.0,206.0,46.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5272,BlowJob,298.0,358.0,60.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5259,Anal,448.0,494.0,46.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5274,Gangbang,604.0,616.0,12.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5260,Anal,654.0,712.0,58.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5261,Anal,856.0,892.0,36.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5276,Gangbang,1068.0,1080.0,12.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5277,Gangbang,1162.0,1172.0,10.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5278,Gangbang,1332.0,1348.0,16.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5263,Anal,1380.0,1406.0,26.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5264,Anal,1468.0,1504.0,36.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5279,Gangbang,1470.0,1484.0,14.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5280,Gangbang,1646.0,1678.0,32.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5281,Gangbang,1740.0,1752.0,12.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5266,Anal,1796.0,1856.0,60.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5267,Anal,1950.0,1974.0,24.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5282,Gangbang,2024.0,2052.0,28.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5283,Gangbang,2108.0,2120.0,12.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5284,Gangbang,2258.0,2312.0,54.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5268,Anal,2282.0,2302.0,20.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5286,Pissing,2354.0,2358.0,4.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5285,Gangbang,2540.0,2594.0,54.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5270,Anal,2864.0,2908.0,44.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5273,BlowJob,2910.0,2926.0,16.0
486,Athenea Rose in 7on1 DAP Gangbang,5287,Gangbang,28.0,50.0,22.0
486,Athenea Rose in 7on1 DAP Gangbang,5294,Grabbing Boobs,30.0,54.0,24.0
486,Athenea Rose in 7on1 DAP Gangbang,5297,Anal,68.0,94.0,26.0
486,Athenea Rose in 7on1 DAP Gangbang,5298,Anal,170.0,198.0,28.0
486,Athenea Rose in 7on1 DAP Gangbang,5299,Anal,372.0,410.0,38.0
486,Athenea Rose in 7on1 DAP Gangbang,5304,BlowJob,562.0,592.0,30.0
486,Athenea Rose in 7on1 DAP Gangbang,5300,Anal,572.0,628.0,56.0
486,Athenea Rose in 7on1 DAP Gangbang,5288,Gangbang,586.0,618.0,32.0
486,Athenea Rose in 7on1 DAP Gangbang,5301,Anal,704.0,714.0,10.0
486,Athenea Rose in 7on1 DAP Gangbang,5295,Grabbing Boobs,844.0,870.0,26.0
486,Athenea Rose in 7on1 DAP Gangbang,5306,BlowJob,976.0,992.0,16.0
486,Athenea Rose in 7on1 DAP Gangbang,5289,Gangbang,1024.0,1062.0,38.0
486,Athenea Rose in 7on1 DAP Gangbang,5307,BlowJob,1026.0,1066.0,40.0
486,Athenea Rose in 7on1 DAP Gangbang,5290,Gangbang,1210.0,1226.0,16.0
486,Athenea Rose in 7on1 DAP Gangbang,5291,Gangbang,1426.0,1466.0,40.0
486,Athenea Rose in 7on1 DAP Gangbang,5292,Gangbang,1662.0,1694.0,32.0
486,Athenea Rose in 7on1 DAP Gangbang,5310,Pissing,3466.0,3478.0,12.0
486,Athenea Rose in 7on1 DAP Gangbang,5296,Grabbing Boobs,3646.0,3694.0,48.0
486,Athenea Rose in 7on1 DAP Gangbang,5309,BlowJob,3702.0,3708.0,6.0
487,Athenea Rose in Airtight 6on1 Destruction,5317,Gangbang,356.0,378.0,22.0
487,Athenea Rose in Airtight 6on1 Destruction,5318,Gangbang,424.0,440.0,16.0
487,Athenea Rose in Airtight 6on1 Destruction,5319,Gangbang,514.0,528.0,14.0
487,Athenea Rose in Airtight 6on1 Destruction,5331,BlowJob,704.0,716.0,12.0
487,Athenea Rose in Airtight 6on1 Destruction,5320,Gangbang,716.0,752.0,36.0
487,Athenea Rose in Airtight 6on1 Destruction,5332,BlowJob,904.0,918.0,14.0
487,Athenea Rose in Airtight 6on1 Destruction,5322,Gangbang,1144.0,1160.0,16.0
487,Athenea Rose in Airtight 6on1 Destruction,5334,BlowJob,1534.0,1548.0,14.0
487,Athenea Rose in Airtight 6on1 Destruction,5323,Gangbang,1570.0,1586.0,16.0
487,Athenea Rose in Airtight 6on1 Destruction,5336,BlowJob,1824.0,1870.0,46.0
487,Athenea Rose in Airtight 6on1 Destruction,5343,Anal,1952.0,1964.0,12.0
487,Athenea Rose in Airtight 6on1 Destruction,5337,BlowJob,1974.0,1986.0,12.0
487,Athenea Rose in Airtight 6on1 Destruction,5344,Anal,2108.0,2126.0,18.0
487,Athenea Rose in Airtight 6on1 Destruction,5345,Anal,2176.0,2208.0,32.0
487,Athenea Rose in Airtight 6on1 Destruction,5314,Grabbing Boobs,2276.0,2320.0,44.0
487,Athenea Rose in Airtight 6on1 Destruction,5346,Anal,2300.0,2312.0,12.0
487,Athenea Rose in Airtight 6on1 Destruction,5315,Grabbing Boobs,2436.0,2474.0,38.0
487,Athenea Rose in Airtight 6on1 Destruction,5338,BlowJob,2450.0,2462.0,12.0
487,Athenea Rose in Airtight 6on1 Destruction,5326,Gangbang,2470.0,2484.0,14.0
487,Athenea Rose in Airtight 6on1 Destruction,5347,Anal,2532.0,2548.0,16.0
487,Athenea Rose in Airtight 6on1 Destruction,5316,Grabbing Boobs,2534.0,2580.0,46.0
487,Athenea Rose in Airtight 6on1 Destruction,5327,Gangbang,2560.0,2608.0,48.0
487,Athenea Rose in Airtight 6on1 Destruction,5339,BlowJob,2638.0,2662.0,24.0
487,Athenea Rose in Airtight 6on1 Destruction,5348,Anal,2656.0,2702.0,46.0
487,Athenea Rose in Airtight 6on1 Destruction,5328,Gangbang,2680.0,2708.0,28.0
487,Athenea Rose in Airtight 6on1 Destruction,5349,Anal,2758.0,2804.0,46.0
487,Athenea Rose in Airtight 6on1 Destruction,5350,Anal,2836.0,2848.0,12.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5445,Grabbing Boobs,142.0,170.0,28.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5447,Anal,258.0,276.0,18.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5451,BlowJob,338.0,398.0,60.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5452,BlowJob,462.0,504.0,42.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5446,Grabbing Boobs,1796.0,1822.0,26.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5455,BlowJob,2302.0,2320.0,18.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5456,BlowJob,2490.0,2512.0,22.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5450,Gangbang,2620.0,2630.0,10.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5458,BlowJob,2922.0,2932.0,10.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5459,BlowJob,2964.0,3002.0,38.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5460,BlowJob,3472.0,3484.0,12.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5483,BlowJob,272.0,298.0,26.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5484,BlowJob,338.0,366.0,28.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5472,Gangbang,436.0,482.0,46.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5485,BlowJob,456.0,504.0,48.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5493,Anal,538.0,552.0,14.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5486,BlowJob,808.0,832.0,24.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5473,Gangbang,832.0,842.0,10.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5474,Gangbang,890.0,914.0,24.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5475,Gangbang,952.0,978.0,26.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5509,Pissing,1168.0,1180.0,12.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5511,Cumshot,1180.0,1212.0,32.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5487,BlowJob,1290.0,1300.0,10.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5488,BlowJob,1380.0,1390.0,10.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5496,Anal,1688.0,1726.0,38.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5477,Gangbang,1692.0,1720.0,28.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5478,Gangbang,1758.0,1814.0,56.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5497,Anal,1780.0,1798.0,18.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5479,Gangbang,1854.0,1874.0,20.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5480,Gangbang,1930.0,1952.0,22.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5498,Anal,2186.0,2216.0,30.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5489,BlowJob,2236.0,2248.0,12.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5499,Anal,2274.0,2286.0,12.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5500,Anal,2318.0,2340.0,22.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5501,Anal,2418.0,2458.0,40.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5490,BlowJob,2606.0,2620.0,14.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5491,BlowJob,2918.0,2936.0,18.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5492,BlowJob,3024.0,3042.0,18.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5502,Anal,3128.0,3144.0,16.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5504,Anal,3534.0,3568.0,34.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5505,Anal,3610.0,3646.0,36.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5471,Grabbing Boobs,3736.0,3768.0,32.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5506,Anal,3908.0,3968.0,60.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5507,Anal,4066.0,4082.0,16.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5508,Anal,4394.0,4414.0,20.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5510,Pissing,4450.0,4460.0,10.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5482,Gangbang,4522.0,4532.0,10.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5520,BlowJob,360.0,388.0,28.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5521,BlowJob,600.0,610.0,10.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5522,BlowJob,1032.0,1048.0,16.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5523,BlowJob,1294.0,1342.0,48.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5531,Pissing,1442.0,1450.0,8.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5514,Gangbang,1878.0,1932.0,54.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5515,Gangbang,2034.0,2058.0,24.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5533,Cumshot,2278.0,2312.0,34.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5532,Pissing,2296.0,2310.0,14.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5526,Anal,2648.0,2664.0,16.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5516,Gangbang,2742.0,2754.0,12.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5524,BlowJob,2882.0,2920.0,38.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5517,Gangbang,3088.0,3118.0,30.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5534,Cumshot,3824.0,3830.0,6.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5519,Grabbing Boobs,3838.0,3868.0,30.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5535,Cumshot,3900.0,3946.0,46.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5530,Anal,4144.0,4154.0,10.0
491,Athenea Rose in Hardcore Interracial DAP,5441,BlowJob,790.0,806.0,16.0
491,Athenea Rose in Hardcore Interracial DAP,5442,BlowJob,2560.0,2600.0,40.0
491,Athenea Rose in Hardcore Interracial DAP,5443,Cumshot,3038.0,3054.0,16.0
492,Athenea Rose in Hecho en Medelln,5358,Anal,1758.0,1772.0,14.0
492,Athenea Rose in Hecho en Medelln,5361,Anal,2516.0,2542.0,26.0
492,Athenea Rose in Hecho en Medelln,5362,Anal,2578.0,2624.0,46.0
492,Athenea Rose in Hecho en Medelln,5354,BlowJob,2790.0,2812.0,22.0
492,Athenea Rose in Hecho en Medelln,5364,Anal,2842.0,2860.0,18.0
492,Athenea Rose in Hecho en Medelln,5355,BlowJob,3158.0,3194.0,36.0
492,Athenea Rose in Hecho en Medelln,5366,Anal,3274.0,3290.0,16.0
492,Athenea Rose in Hecho en Medelln,5367,Anal,3342.0,3364.0,22.0
493,Athenea Rose in Intense Anal Destruction,5461,BlowJob,152.0,164.0,12.0
493,Athenea Rose in Intense Anal Destruction,5463,Anal,422.0,462.0,40.0
493,Athenea Rose in Intense Anal Destruction,5464,Anal,498.0,532.0,34.0
493,Athenea Rose in Intense Anal Destruction,5465,Anal,660.0,688.0,28.0
493,Athenea Rose in Intense Anal Destruction,5468,Anal,1412.0,1446.0,34.0
493,Athenea Rose in Intense Anal Destruction,5469,Anal,1534.0,1586.0,52.0
493,Athenea Rose in Intense Anal Destruction,5462,BlowJob,1760.0,1792.0,32.0
494,Athenea Rose in Loves Public Anal,5536,BlowJob,628.0,642.0,14.0
494,Athenea Rose in Loves Public Anal,5538,Anal,1008.0,1030.0,22.0
494,Athenea Rose in Loves Public Anal,5539,Anal,1276.0,1304.0,28.0
494,Athenea Rose in Loves Public Anal,5537,BlowJob,1734.0,1764.0,30.0
494,Athenea Rose in Loves Public Anal,5540,Anal,1916.0,1964.0,48.0
494,Athenea Rose in Loves Public Anal,5541,Anal,1998.0,2024.0,26.0
494,Athenea Rose in Loves Public Anal,5542,Anal,2152.0,2168.0,16.0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,5547,Anal,672.0,714.0,42.0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,5548,Anal,858.0,912.0,54.0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,5550,Anal,1146.0,1174.0,28.0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,5551,Anal,1320.0,1334.0,14.0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,5552,Anal,1376.0,1404.0,28.0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,5553,Anal,1688.0,1704.0,16.0
496,Athenea Rose in Playing with 3 BBC,5560,Anal,660.0,700.0,40.0
496,Athenea Rose in Playing with 3 BBC,5561,Anal,746.0,790.0,44.0
496,Athenea Rose in Playing with 3 BBC,5555,BlowJob,844.0,896.0,52.0
496,Athenea Rose in Playing with 3 BBC,5556,BlowJob,932.0,960.0,28.0
496,Athenea Rose in Playing with 3 BBC,5563,Anal,1578.0,1626.0,48.0
496,Athenea Rose in Playing with 3 BBC,5557,BlowJob,1666.0,1702.0,36.0
496,Athenea Rose in Playing with 3 BBC,5558,BlowJob,1750.0,1796.0,46.0
497,Athenea Rose in PremiumBukkake #1,5570,BlowJob,582.0,604.0,22.0
497,Athenea Rose in PremiumBukkake #1,5573,Cumshot,592.0,602.0,10.0
497,Athenea Rose in PremiumBukkake #1,5574,Cumshot,646.0,674.0,28.0
497,Athenea Rose in PremiumBukkake #1,5580,Cumshot,712.0,756.0,44.0
497,Athenea Rose in PremiumBukkake #1,5575,Cumshot,766.0,820.0,54.0
497,Athenea Rose in PremiumBukkake #1,5576,Cumshot,846.0,896.0,50.0
497,Athenea Rose in PremiumBukkake #1,5571,BlowJob,884.0,888.0,4.0
497,Athenea Rose in PremiumBukkake #1,5583,Cumshot,932.0,964.0,32.0
497,Athenea Rose in PremiumBukkake #1,5584,Cumshot,1080.0,1132.0,52.0
497,Athenea Rose in PremiumBukkake #1,5585,Cumshot,1220.0,1250.0,30.0
499,Athenea Rose in PremiumBukkake #3,5594,BlowJob,12.0,16.0,4.0
499,Athenea Rose in PremiumBukkake #3,5598,BlowJob,552.0,580.0,28.0
500,Athenea Rose in Sex Crazed Slut 4on1,5613,Grabbing Boobs,60.0,104.0,44.0
500,Athenea Rose in Sex Crazed Slut 4on1,5614,Grabbing Boobs,152.0,172.0,20.0
500,Athenea Rose in Sex Crazed Slut 4on1,5621,Cumshot,678.0,686.0,8.0
500,Athenea Rose in Sex Crazed Slut 4on1,5615,Grabbing Boobs,922.0,944.0,22.0
500,Athenea Rose in Sex Crazed Slut 4on1,5618,BlowJob,1528.0,1550.0,22.0
500,Athenea Rose in Sex Crazed Slut 4on1,5622,Pissing,1870.0,1896.0,26.0
500,Athenea Rose in Sex Crazed Slut 4on1,5619,BlowJob,2648.0,2662.0,14.0
500,Athenea Rose in Sex Crazed Slut 4on1,5620,BlowJob,2692.0,2696.0,4.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5623,Grabbing Boobs,156.0,188.0,32.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5627,Gangbang,294.0,342.0,48.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5635,BlowJob,356.0,396.0,40.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5639,Anal,560.0,596.0,36.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5628,Gangbang,642.0,670.0,28.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5640,Anal,670.0,680.0,10.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5641,Anal,788.0,848.0,60.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5642,Anal,956.0,978.0,22.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5636,BlowJob,970.0,996.0,26.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5624,Grabbing Boobs,1340.0,1364.0,24.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5629,Gangbang,1356.0,1384.0,28.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5630,Gangbang,1524.0,1534.0,10.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5631,Gangbang,1672.0,1696.0,24.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5632,Gangbang,1774.0,1790.0,16.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5633,Gangbang,1836.0,1868.0,32.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5644,Anal,1850.0,1872.0,22.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5638,BlowJob,1980.0,2014.0,34.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5645,Anal,1984.0,2020.0,36.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5650,DP,2038.0,2042.0,4.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5646,Anal,2078.0,2138.0,60.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5651,DP,2102.0,2106.0,4.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5647,Anal,2180.0,2208.0,28.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5634,Gangbang,2368.0,2394.0,26.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5652,Pissing,2406.0,2414.0,8.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5653,Cumshot,2448.0,2490.0,42.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5655,Cumshot,3302.0,3314.0,12.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5625,Grabbing Boobs,3700.0,3730.0,30.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5626,Grabbing Boobs,3774.0,3792.0,18.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5658,Anal,378.0,394.0,16.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5659,Anal,1350.0,1384.0,34.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5669,BlowJob,1376.0,1400.0,24.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5670,BlowJob,1462.0,1472.0,10.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5660,Anal,1516.0,1562.0,46.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5661,Anal,1612.0,1636.0,24.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5662,Anal,1732.0,1786.0,54.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5663,Anal,1980.0,1992.0,12.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5664,Anal,2058.0,2070.0,12.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5666,Anal,2464.0,2488.0,24.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5671,BlowJob,2908.0,2946.0,38.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5672,Grabbing Boobs,212.0,230.0,18.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5673,BlowJob,340.0,394.0,54.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5675,BlowJob,782.0,798.0,16.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5690,Gangbang,798.0,808.0,10.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5676,BlowJob,842.0,870.0,28.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5691,Gangbang,876.0,922.0,46.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5692,Gangbang,962.0,994.0,32.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5677,BlowJob,966.0,996.0,30.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5693,Gangbang,1134.0,1162.0,28.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5694,Gangbang,1840.0,1850.0,10.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5683,Anal,2876.0,2900.0,24.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5684,Anal,3208.0,3238.0,30.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5678,BlowJob,3234.0,3260.0,26.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5685,Anal,3306.0,3350.0,44.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5679,BlowJob,3430.0,3470.0,40.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5695,Gangbang,3432.0,3444.0,12.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5686,Anal,3438.0,3458.0,20.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5680,BlowJob,3544.0,3576.0,32.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5687,Anal,3606.0,3624.0,18.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5688,Anal,3730.0,3740.0,10.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5689,Anal,3990.0,4006.0,16.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5696,Gangbang,4118.0,4146.0,28.0
504,Aubrey Black in GangBang Creampie 225,5700,BlowJob,324.0,374.0,50.0
504,Aubrey Black in GangBang Creampie 225,5698,Grabbing Boobs,446.0,486.0,40.0
504,Aubrey Black in GangBang Creampie 225,5699,Grabbing Boobs,672.0,692.0,20.0
504,Aubrey Black in GangBang Creampie 225,5701,BlowJob,982.0,1036.0,54.0
504,Aubrey Black in GangBang Creampie 225,5703,Cumshot,1474.0,1498.0,24.0
504,Aubrey Black in GangBang Creampie 225,5704,Cumshot,1560.0,1564.0,4.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5707,BlowJob,194.0,212.0,18.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5708,BlowJob,282.0,334.0,52.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5717,Cumshot,1094.0,1136.0,42.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5710,BlowJob,1098.0,1110.0,12.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5720,Gangbang,1130.0,1152.0,22.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5721,Gangbang,1184.0,1218.0,34.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5711,BlowJob,1268.0,1280.0,12.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5722,Gangbang,1366.0,1398.0,32.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5712,BlowJob,2386.0,2430.0,44.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5724,Gangbang,2628.0,2648.0,20.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5706,Grabbing Boobs,4086.0,4108.0,22.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5726,Gangbang,432.0,480.0,48.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5725,Grabbing Boobs,434.0,472.0,38.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5733,BlowJob,532.0,548.0,16.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5734,BlowJob,670.0,712.0,42.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5739,Anal,718.0,740.0,22.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5735,BlowJob,1090.0,1118.0,28.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5741,Cumshot,1092.0,1098.0,6.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5728,Gangbang,1610.0,1662.0,52.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5743,DP,1886.0,1916.0,30.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5729,Gangbang,1972.0,2024.0,52.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5736,BlowJob,2278.0,2290.0,12.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5737,BlowJob,3060.0,3082.0,22.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5742,Cumshot,3082.0,3092.0,10.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5731,Gangbang,3408.0,3418.0,10.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5738,BlowJob,3428.0,3440.0,12.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5732,Gangbang,3462.0,3476.0,14.0
507,Avery Jane in Milking Mike Adriano,5744,Grabbing Boobs,58.0,92.0,34.0
507,Avery Jane in Milking Mike Adriano,5747,Titjob,356.0,370.0,14.0
507,Avery Jane in Milking Mike Adriano,5748,Cumshot,674.0,686.0,12.0
507,Avery Jane in Milking Mike Adriano,5749,Cumshot,778.0,814.0,36.0
507,Avery Jane in Milking Mike Adriano,5752,Anal,1014.0,1064.0,50.0
507,Avery Jane in Milking Mike Adriano,5754,Anal,1236.0,1262.0,26.0
507,Avery Jane in Milking Mike Adriano,5755,Anal,1302.0,1322.0,20.0
507,Avery Jane in Milking Mike Adriano,5757,Anal,1904.0,1952.0,48.0
507,Avery Jane in Milking Mike Adriano,5758,Anal,2004.0,2042.0,38.0
507,Avery Jane in Milking Mike Adriano,5759,Anal,2158.0,2172.0,14.0
507,Avery Jane in Milking Mike Adriano,5760,Anal,2222.0,2254.0,32.0
508,Avery Jane in Piss Soaked Backdoor Debut,5761,Grabbing Boobs,84.0,110.0,26.0
508,Avery Jane in Piss Soaked Backdoor Debut,5762,Grabbing Boobs,182.0,212.0,30.0
508,Avery Jane in Piss Soaked Backdoor Debut,5776,Gangbang,1268.0,1314.0,46.0
508,Avery Jane in Piss Soaked Backdoor Debut,5791,Pissing,1414.0,1464.0,50.0
508,Avery Jane in Piss Soaked Backdoor Debut,5782,BlowJob,1504.0,1514.0,10.0
508,Avery Jane in Piss Soaked Backdoor Debut,5783,BlowJob,1582.0,1642.0,60.0
508,Avery Jane in Piss Soaked Backdoor Debut,5792,Cumshot,1586.0,1612.0,26.0
508,Avery Jane in Piss Soaked Backdoor Debut,5784,BlowJob,1680.0,1702.0,22.0
508,Avery Jane in Piss Soaked Backdoor Debut,5797,DP,1744.0,1748.0,4.0
508,Avery Jane in Piss Soaked Backdoor Debut,5765,Anal,1764.0,1784.0,20.0
508,Avery Jane in Piss Soaked Backdoor Debut,5777,Gangbang,1768.0,1790.0,22.0
508,Avery Jane in Piss Soaked Backdoor Debut,5766,Anal,1840.0,1850.0,10.0
508,Avery Jane in Piss Soaked Backdoor Debut,5767,Anal,1890.0,1914.0,24.0
508,Avery Jane in Piss Soaked Backdoor Debut,5768,Anal,1990.0,2000.0,10.0
508,Avery Jane in Piss Soaked Backdoor Debut,5769,Anal,2046.0,2092.0,46.0
508,Avery Jane in Piss Soaked Backdoor Debut,5770,Anal,2440.0,2470.0,30.0
508,Avery Jane in Piss Soaked Backdoor Debut,5786,BlowJob,2514.0,2528.0,14.0
508,Avery Jane in Piss Soaked Backdoor Debut,5771,Anal,2516.0,2546.0,30.0
508,Avery Jane in Piss Soaked Backdoor Debut,5778,Gangbang,2520.0,2556.0,36.0
508,Avery Jane in Piss Soaked Backdoor Debut,5772,Anal,2596.0,2646.0,50.0
508,Avery Jane in Piss Soaked Backdoor Debut,5787,BlowJob,2792.0,2824.0,32.0
508,Avery Jane in Piss Soaked Backdoor Debut,5779,Gangbang,2902.0,2920.0,18.0
508,Avery Jane in Piss Soaked Backdoor Debut,5788,BlowJob,2906.0,2926.0,20.0
508,Avery Jane in Piss Soaked Backdoor Debut,5789,BlowJob,3004.0,3040.0,36.0
508,Avery Jane in Piss Soaked Backdoor Debut,5793,Cumshot,3030.0,3062.0,32.0
508,Avery Jane in Piss Soaked Backdoor Debut,5780,Gangbang,3034.0,3052.0,18.0
508,Avery Jane in Piss Soaked Backdoor Debut,5774,Anal,3388.0,3414.0,26.0
508,Avery Jane in Piss Soaked Backdoor Debut,5775,Anal,3496.0,3520.0,24.0
508,Avery Jane in Piss Soaked Backdoor Debut,5794,Cumshot,3722.0,3730.0,8.0
508,Avery Jane in Piss Soaked Backdoor Debut,5795,Cumshot,3802.0,3856.0,54.0
509,Avi Love in GangBang Creampie 216,5798,BlowJob,138.0,154.0,16.0
509,Avi Love in GangBang Creampie 216,5799,BlowJob,258.0,270.0,12.0
509,Avi Love in GangBang Creampie 216,5800,BlowJob,418.0,468.0,50.0
509,Avi Love in GangBang Creampie 216,5801,BlowJob,640.0,652.0,12.0
509,Avi Love in GangBang Creampie 216,5802,BlowJob,844.0,894.0,50.0
509,Avi Love in GangBang Creampie 216,5807,Gangbang,920.0,930.0,10.0
509,Avi Love in GangBang Creampie 216,5803,BlowJob,944.0,978.0,34.0
509,Avi Love in GangBang Creampie 216,5804,BlowJob,1016.0,1044.0,28.0
509,Avi Love in GangBang Creampie 216,5808,Gangbang,1036.0,1062.0,26.0
509,Avi Love in GangBang Creampie 216,5809,Grabbing Boobs,1662.0,1686.0,24.0
509,Avi Love in GangBang Creampie 216,5810,Grabbing Boobs,2128.0,2164.0,36.0
509,Avi Love in GangBang Creampie 216,5805,BlowJob,2232.0,2246.0,14.0
509,Avi Love in GangBang Creampie 216,5812,Cumshot,2342.0,2384.0,42.0
509,Avi Love in GangBang Creampie 216,5811,Grabbing Boobs,2620.0,2640.0,20.0
511,Baby Gemini in All About The Booty,5813,BlowJob,322.0,382.0,60.0
511,Baby Gemini in All About The Booty,5814,Cumshot,352.0,384.0,32.0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,5815,Grabbing Boobs,306.0,334.0,28.0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,5818,Cumshot,1254.0,1260.0,6.0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,5820,Cumshot,1708.0,1726.0,18.0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,5821,Cumshot,1836.0,1862.0,26.0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,5828,Cumshot,2202.0,2260.0,58.0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,5829,Cumshot,2312.0,2342.0,30.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5831,Grabbing Boobs,6.0,48.0,42.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5832,Grabbing Boobs,180.0,234.0,54.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5836,Anal,502.0,522.0,20.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5833,Grabbing Boobs,860.0,884.0,24.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5834,Grabbing Boobs,994.0,1020.0,26.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5840,BlowJob,1578.0,1610.0,32.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5835,Grabbing Boobs,3052.0,3072.0,20.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5838,Anal,3122.0,3166.0,44.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5841,BlowJob,310.0,330.0,20.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5850,Gangbang,484.0,516.0,32.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5842,BlowJob,490.0,530.0,40.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5843,BlowJob,582.0,634.0,52.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5844,BlowJob,732.0,746.0,14.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5851,Gangbang,1118.0,1158.0,40.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5845,BlowJob,1658.0,1678.0,20.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5852,Gangbang,2044.0,2076.0,32.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5846,BlowJob,2180.0,2224.0,44.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5848,BlowJob,2392.0,2404.0,12.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5847,BlowJob,2642.0,2674.0,32.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5855,Cumshot,2896.0,2904.0,8.0
515,Barbie Sins in Barbie Gets wet with 2 BBC,5857,BlowJob,232.0,260.0,28.0
515,Barbie Sins in Barbie Gets wet with 2 BBC,5858,BlowJob,292.0,308.0,16.0
515,Barbie Sins in Barbie Gets wet with 2 BBC,5860,BlowJob,696.0,736.0,40.0
515,Barbie Sins in Barbie Gets wet with 2 BBC,5861,BlowJob,1662.0,1692.0,30.0
516,Barbie Sins in Creampie and Swallow Showdown,5145,BlowJob,248.0,266.0,18.0
516,Barbie Sins in Creampie and Swallow Showdown,5146,BlowJob,298.0,356.0,58.0
516,Barbie Sins in Creampie and Swallow Showdown,5147,BlowJob,664.0,716.0,52.0
516,Barbie Sins in Creampie and Swallow Showdown,5148,BlowJob,1046.0,1070.0,24.0
516,Barbie Sins in Creampie and Swallow Showdown,5149,BlowJob,1534.0,1580.0,46.0
517,"Barbie Sins in DAP, Piss and Power Play",5864,BlowJob,716.0,774.0,58.0
517,"Barbie Sins in DAP, Piss and Power Play",5868,Gangbang,790.0,836.0,46.0
517,"Barbie Sins in DAP, Piss and Power Play",5869,Cumshot,992.0,1002.0,10.0
517,"Barbie Sins in DAP, Piss and Power Play",5865,BlowJob,1234.0,1262.0,28.0
517,"Barbie Sins in DAP, Piss and Power Play",5867,BlowJob,1732.0,1744.0,12.0
517,"Barbie Sins in DAP, Piss and Power Play",5866,BlowJob,2490.0,2538.0,48.0
517,"Barbie Sins in DAP, Piss and Power Play",5871,Cumshot,3022.0,3034.0,12.0
517,"Barbie Sins in DAP, Piss and Power Play",5872,Cumshot,3112.0,3116.0,4.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5873,Grabbing Boobs,36.0,70.0,34.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5874,BlowJob,626.0,648.0,22.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5888,Gangbang,746.0,764.0,18.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5877,BlowJob,908.0,928.0,20.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5878,BlowJob,960.0,1000.0,40.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5879,BlowJob,1040.0,1052.0,12.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5880,BlowJob,1172.0,1202.0,30.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5881,BlowJob,1292.0,1302.0,10.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5882,BlowJob,1374.0,1394.0,20.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5883,BlowJob,1478.0,1502.0,24.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5885,BlowJob,2392.0,2406.0,14.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5889,Gangbang,2464.0,2510.0,46.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5886,BlowJob,2778.0,2822.0,44.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5887,BlowJob,3030.0,3060.0,30.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5891,Cumshot,3080.0,3084.0,4.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5892,Cumshot,3102.0,3108.0,6.0
519,Barbie Sins in Rough DAP & Swallow Madness,5894,BlowJob,594.0,622.0,28.0
519,Barbie Sins in Rough DAP & Swallow Madness,5895,BlowJob,904.0,938.0,34.0
519,Barbie Sins in Rough DAP & Swallow Madness,5903,Gangbang,994.0,1026.0,32.0
519,Barbie Sins in Rough DAP & Swallow Madness,5905,Gangbang,1290.0,1322.0,32.0
519,Barbie Sins in Rough DAP & Swallow Madness,5897,BlowJob,1424.0,1450.0,26.0
519,Barbie Sins in Rough DAP & Swallow Madness,5906,DP,1454.0,1468.0,14.0
519,Barbie Sins in Rough DAP & Swallow Madness,5898,BlowJob,2168.0,2186.0,18.0
519,Barbie Sins in Rough DAP & Swallow Madness,5899,BlowJob,2250.0,2264.0,14.0
519,Barbie Sins in Rough DAP & Swallow Madness,5900,BlowJob,2430.0,2440.0,10.0
519,Barbie Sins in Rough DAP & Swallow Madness,5907,Cumshot,2688.0,2696.0,8.0
519,Barbie Sins in Rough DAP & Swallow Madness,5908,Cumshot,2754.0,2806.0,52.0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",5909,Anal,186.0,196.0,10.0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",5916,BlowJob,336.0,350.0,14.0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",5917,BlowJob,696.0,720.0,24.0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",5918,BlowJob,916.0,950.0,34.0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",5913,Anal,1194.0,1210.0,16.0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",5914,Anal,1304.0,1314.0,10.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5369,Grabbing Boobs,538.0,582.0,44.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5370,Grabbing Boobs,658.0,698.0,40.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5371,Grabbing Boobs,790.0,822.0,32.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5386,BlowJob,1882.0,1892.0,10.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5387,BlowJob,1944.0,1958.0,14.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5373,Grabbing Boobs,2074.0,2098.0,24.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5388,BlowJob,2110.0,2124.0,14.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5377,BlowJob,2404.0,2428.0,24.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5393,BlowJob,2448.0,2458.0,10.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5378,BlowJob,2796.0,2854.0,58.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5389,BlowJob,2812.0,2830.0,18.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5390,BlowJob,3140.0,3170.0,30.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5380,BlowJob,3212.0,3228.0,16.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5391,BlowJob,3216.0,3222.0,6.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5381,BlowJob,3328.0,3358.0,30.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5374,Grabbing Boobs,3778.0,3814.0,36.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5383,BlowJob,3846.0,3900.0,54.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5392,BlowJob,3864.0,3870.0,6.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5384,BlowJob,4162.0,4192.0,30.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5375,Grabbing Boobs,4192.0,4246.0,54.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5394,Cumshot,4202.0,4230.0,28.0
557,Candela X in There is a Star Around the Gape!,5075,BlowJob,504.0,546.0,42.0
557,Candela X in There is a Star Around the Gape!,5079,Gangbang,520.0,574.0,54.0
557,Candela X in There is a Star Around the Gape!,5076,BlowJob,712.0,754.0,42.0
557,Candela X in There is a Star Around the Gape!,5077,BlowJob,814.0,840.0,26.0
557,Candela X in There is a Star Around the Gape!,5078,BlowJob,972.0,984.0,12.0
557,Candela X in There is a Star Around the Gape!,5081,Gangbang,976.0,1018.0,42.0
557,Candela X in There is a Star Around the Gape!,5082,Gangbang,1508.0,1536.0,28.0
557,Candela X in There is a Star Around the Gape!,5083,Gangbang,1702.0,1712.0,10.0
557,Candela X in There is a Star Around the Gape!,5084,Gangbang,1754.0,1784.0,30.0
557,Candela X in There is a Star Around the Gape!,5086,Cumshot,2642.0,2654.0,12.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4811,BlowJob,562.0,588.0,26.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4813,BlowJob,702.0,728.0,26.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4815,BlowJob,1836.0,1850.0,14.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4817,BlowJob,2468.0,2480.0,12.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4830,69,2600.0,2612.0,12.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4826,Anal,2660.0,2672.0,12.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4827,Anal,2728.0,2772.0,44.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4818,BlowJob,2830.0,2870.0,40.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4828,Anal,2922.0,2940.0,18.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4819,BlowJob,2946.0,2958.0,12.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4829,Anal,2978.0,3006.0,28.0
575,Chessie Kay in Double Team During Match,4866,BlowJob,88.0,134.0,46.0
575,Chessie Kay in Double Team During Match,4867,BlowJob,242.0,250.0,8.0
575,Chessie Kay in Double Team During Match,4863,BlowJob,384.0,394.0,10.0
575,Chessie Kay in Double Team During Match,4864,BlowJob,526.0,572.0,46.0
575,Chessie Kay in Double Team During Match,4868,BlowJob,556.0,570.0,14.0
575,Chessie Kay in Double Team During Match,4870,Grabbing Boobs,680.0,706.0,26.0
575,Chessie Kay in Double Team During Match,4872,Anal,740.0,794.0,54.0
575,Chessie Kay in Double Team During Match,4869,BlowJob,1008.0,1026.0,18.0
575,Chessie Kay in Double Team During Match,4871,Grabbing Boobs,1500.0,1528.0,28.0
575,Chessie Kay in Double Team During Match,4873,Cumshot,1762.0,1780.0,18.0
575,Chessie Kay in Double Team During Match,4874,Cumshot,1852.0,1880.0,28.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5087,Grabbing Boobs,200.0,244.0,44.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5089,BlowJob,302.0,308.0,6.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5090,BlowJob,354.0,370.0,16.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5091,BlowJob,428.0,470.0,42.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5092,BlowJob,522.0,556.0,34.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5093,Titjob,622.0,644.0,22.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5094,Cumshot,1126.0,1136.0,10.0
726,Jolee Love in Rough Squirting and Pee Play,5098,BlowJob,230.0,246.0,16.0
726,Jolee Love in Rough Squirting and Pee Play,5099,BlowJob,282.0,332.0,50.0
726,Jolee Love in Rough Squirting and Pee Play,5106,Pissing,368.0,392.0,24.0
726,Jolee Love in Rough Squirting and Pee Play,5108,Cumshot,508.0,536.0,28.0
726,Jolee Love in Rough Squirting and Pee Play,5100,BlowJob,522.0,550.0,28.0
726,Jolee Love in Rough Squirting and Pee Play,5101,BlowJob,1286.0,1304.0,18.0
726,Jolee Love in Rough Squirting and Pee Play,5102,BlowJob,1408.0,1436.0,28.0
726,Jolee Love in Rough Squirting and Pee Play,5095,Gangbang,1754.0,1794.0,40.0
726,Jolee Love in Rough Squirting and Pee Play,5103,BlowJob,1830.0,1870.0,40.0
726,Jolee Love in Rough Squirting and Pee Play,5109,Cumshot,2014.0,2022.0,8.0
726,Jolee Love in Rough Squirting and Pee Play,5096,Gangbang,2200.0,2234.0,34.0
726,Jolee Love in Rough Squirting and Pee Play,5097,Gangbang,2634.0,2654.0,20.0
726,Jolee Love in Rough Squirting and Pee Play,5116,Grabbing Boobs,2740.0,2770.0,30.0
726,Jolee Love in Rough Squirting and Pee Play,5104,BlowJob,3020.0,3032.0,12.0
726,Jolee Love in Rough Squirting and Pee Play,5105,BlowJob,3134.0,3164.0,30.0
726,Jolee Love in Rough Squirting and Pee Play,5115,Anal,3318.0,3344.0,26.0
726,Jolee Love in Rough Squirting and Pee Play,5110,Cumshot,3414.0,3448.0,34.0
726,Jolee Love in Rough Squirting and Pee Play,5107,Pissing,3516.0,3534.0,18.0
726,Jolee Love in Rough Squirting and Pee Play,5111,Cumshot,3518.0,3542.0,24.0
726,Jolee Love in Rough Squirting and Pee Play,5117,Grabbing Boobs,3550.0,3570.0,20.0
726,Jolee Love in Rough Squirting and Pee Play,5112,Cumshot,3640.0,3644.0,4.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4832,Grabbing Boobs,210.0,268.0,58.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4833,Grabbing Boobs,334.0,374.0,40.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4835,Gangbang,378.0,404.0,26.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4836,Gangbang,498.0,520.0,22.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4840,BlowJob,652.0,682.0,30.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4841,BlowJob,748.0,758.0,10.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4842,BlowJob,826.0,860.0,34.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4834,Grabbing Boobs,1042.0,1098.0,56.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4843,BlowJob,1046.0,1058.0,12.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4837,Gangbang,1158.0,1170.0,12.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4851,Anal,1344.0,1386.0,42.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4844,BlowJob,1540.0,1574.0,34.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4853,Anal,1674.0,1726.0,52.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4854,Anal,1782.0,1808.0,26.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4845,BlowJob,1818.0,1866.0,48.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4838,Gangbang,2062.0,2078.0,16.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4856,Anal,2116.0,2128.0,12.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4857,Anal,2166.0,2184.0,18.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4848,BlowJob,2692.0,2708.0,16.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4849,BlowJob,3108.0,3126.0,18.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5029,BlowJob,250.0,262.0,12.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5031,BlowJob,720.0,772.0,52.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5032,BlowJob,872.0,912.0,40.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5033,BlowJob,946.0,958.0,12.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5034,BlowJob,1060.0,1076.0,16.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5036,Anal,1084.0,1108.0,24.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5039,Cumshot,1398.0,1426.0,28.0
815,May Thai in Asian Beauty's Intense DP Encounter,4996,Grabbing Boobs,526.0,546.0,20.0
815,May Thai in Asian Beauty's Intense DP Encounter,4997,Grabbing Boobs,582.0,630.0,48.0
815,May Thai in Asian Beauty's Intense DP Encounter,4999,Grabbing Boobs,908.0,960.0,52.0
815,May Thai in Asian Beauty's Intense DP Encounter,5001,BlowJob,1246.0,1260.0,14.0
815,May Thai in Asian Beauty's Intense DP Encounter,5002,BlowJob,1398.0,1408.0,10.0
815,May Thai in Asian Beauty's Intense DP Encounter,5003,BlowJob,1458.0,1500.0,42.0
815,May Thai in Asian Beauty's Intense DP Encounter,5005,Anal,1530.0,1544.0,14.0
815,May Thai in Asian Beauty's Intense DP Encounter,5004,BlowJob,1600.0,1616.0,16.0
815,May Thai in Asian Beauty's Intense DP Encounter,5000,Grabbing Boobs,1848.0,1872.0,24.0
815,May Thai in Asian Beauty's Intense DP Encounter,5006,Anal,1988.0,2020.0,32.0
815,May Thai in Asian Beauty's Intense DP Encounter,5007,Anal,2164.0,2188.0,24.0
815,May Thai in Asian Beauty's Intense DP Encounter,5009,Cumshot,2536.0,2576.0,40.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5151,BlowJob,86.0,116.0,30.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5158,Gangbang,140.0,152.0,12.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5152,BlowJob,158.0,180.0,22.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5166,Anal,274.0,332.0,58.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5159,Gangbang,278.0,336.0,58.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5167,Anal,424.0,438.0,14.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5160,Gangbang,532.0,558.0,26.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5162,Gangbang,926.0,948.0,22.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5170,Anal,942.0,990.0,48.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5153,BlowJob,1004.0,1032.0,28.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5171,Anal,1172.0,1204.0,32.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5154,BlowJob,1184.0,1220.0,36.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5155,BlowJob,1284.0,1298.0,14.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5172,Anal,1286.0,1326.0,40.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5173,Anal,1414.0,1434.0,20.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5174,Anal,1466.0,1484.0,18.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5163,Gangbang,1758.0,1802.0,44.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5164,Gangbang,1844.0,1880.0,36.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5177,Anal,1974.0,2028.0,54.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5178,Anal,2100.0,2116.0,16.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5179,Anal,2190.0,2248.0,58.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5156,BlowJob,2914.0,2934.0,20.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5181,Cumshot,2958.0,2996.0,38.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5157,BlowJob,2966.0,2992.0,26.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4773,Cumshot,272.0,288.0,16.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4774,Cumshot,336.0,342.0,6.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4762,BlowJob,1010.0,1034.0,24.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4763,BlowJob,1104.0,1114.0,10.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4768,BlowJob,1328.0,1342.0,14.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4775,Cumshot,1374.0,1398.0,24.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4777,Cumshot,1440.0,1482.0,42.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4769,BlowJob,1982.0,1988.0,6.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4770,BlowJob,2194.0,2208.0,14.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4771,BlowJob,2282.0,2320.0,38.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4765,BlowJob,2524.0,2538.0,14.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4766,BlowJob,2596.0,2608.0,12.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4772,BlowJob,2622.0,2656.0,34.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4779,Cumshot,2626.0,2644.0,18.0
931,"Sarai Minx, Savanah Storm in Messy Manicure Threesome",4785,BlowJob,318.0,338.0,20.0
931,"Sarai Minx, Savanah Storm in Messy Manicure Threesome",4786,BlowJob,424.0,434.0,10.0
931,"Sarai Minx, Savanah Storm in Messy Manicure Threesome",4787,BlowJob,576.0,602.0,26.0
931,"Sarai Minx, Savanah Storm in Messy Manicure Threesome",4789,Grabbing Boobs,1328.0,1350.0,22.0
946,Shalina Devine in Housewife's Dirty Double Plan,4965,Grabbing Boobs,408.0,430.0,22.0
946,Shalina Devine in Housewife's Dirty Double Plan,4966,Grabbing Boobs,464.0,494.0,30.0
946,Shalina Devine in Housewife's Dirty Double Plan,4968,BlowJob,1608.0,1628.0,20.0
946,Shalina Devine in Housewife's Dirty Double Plan,4969,BlowJob,1746.0,1798.0,52.0
946,Shalina Devine in Housewife's Dirty Double Plan,4970,BlowJob,2076.0,2086.0,10.0
946,Shalina Devine in Housewife's Dirty Double Plan,4973,BlowJob,2258.0,2290.0,32.0
946,Shalina Devine in Housewife's Dirty Double Plan,4967,Grabbing Boobs,2528.0,2556.0,28.0
946,Shalina Devine in Housewife's Dirty Double Plan,4972,BlowJob,2650.0,2694.0,44.0
946,Shalina Devine in Housewife's Dirty Double Plan,4974,BlowJob,2870.0,2876.0,6.0
946,Shalina Devine in Housewife's Dirty Double Plan,4976,Cumshot,2886.0,2892.0,6.0
946,Shalina Devine in Housewife's Dirty Double Plan,4977,Cumshot,2926.0,2946.0,20.0
1008,"Sai Tai Tiger, Salma De Nora in Die Haremsw√§chterin des √ñl Scheichs Sc4",96,BlowJob,240.0,252.0,12.0
1020,,4792,Anal,536.0,546.0,10.0
1020,,4802,BlowJob,856.0,870.0,14.0
1020,,4803,BlowJob,910.0,924.0,14.0
1020,,4804,BlowJob,1002.0,1036.0,34.0
1020,,4793,Anal,1006.0,1044.0,38.0
1020,,4805,BlowJob,1222.0,1236.0,14.0
1020,,4795,Anal,1410.0,1434.0,24.0
1020,,4806,BlowJob,1438.0,1458.0,20.0
1020,,4808,BlowJob,2086.0,2096.0,10.0
1020,,4798,Anal,2128.0,2140.0,12.0
1020,,4799,Anal,2224.0,2236.0,12.0
1020,,4809,BlowJob,2610.0,2622.0,12.0
1023,Sai Tai Tiger in Die Haremsw√§chterin die Verbannung Sc3,67,BlowJob,432.0,450.0,18.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3190,Grabbing Boobs,6.0,54.0,48.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3191,BlowJob,142.0,170.0,28.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3192,BlowJob,406.0,424.0,18.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3193,BlowJob,864.0,874.0,10.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3194,BlowJob,1106.0,1120.0,14.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3195,BlowJob,1468.0,1504.0,36.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3199,Anal,1540.0,1574.0,34.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3200,Anal,1612.0,1660.0,48.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",4919,Gangbang,1620.0,1642.0,22.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3196,BlowJob,1632.0,1666.0,34.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3197,BlowJob,1678.0,1692.0,14.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3201,Cumshot,1788.0,1826.0,38.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,4952,Grabbing Boobs,108.0,160.0,52.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,732,BlowJob,178.0,202.0,24.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,740,Gangbang,232.0,260.0,28.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,4960,Gangbang,238.0,260.0,22.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,734,BlowJob,426.0,450.0,24.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,741,Gangbang,472.0,486.0,14.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,735,BlowJob,496.0,530.0,34.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,742,Gangbang,668.0,694.0,26.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,730,Grabbing Boobs,814.0,866.0,52.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,4961,Gangbang,1000.0,1022.0,22.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,737,BlowJob,1130.0,1142.0,12.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,738,BlowJob,1220.0,1254.0,34.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,739,BlowJob,1222.0,1252.0,30.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,731,Grabbing Boobs,1386.0,1408.0,22.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,4959,BlowJob,1402.0,1406.0,4.0
1030,Sai Tai Tiger in Die Haremsw√§chterin die Verbannung Sc2,87,BlowJob,290.0,306.0,16.0
1030,Sai Tai Tiger in Die Haremsw√§chterin die Verbannung Sc2,89,BlowJob,692.0,710.0,18.0
1030,Sai Tai Tiger in Die Haremsw√§chterin die Verbannung Sc2,90,BlowJob,858.0,918.0,60.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4979,BlowJob,620.0,654.0,34.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4987,Anal,642.0,690.0,48.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4995,Anal,1014.0,1032.0,18.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4988,Anal,1082.0,1122.0,40.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4981,BlowJob,1134.0,1146.0,12.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4989,Anal,1162.0,1196.0,34.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4990,Anal,1232.0,1270.0,38.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4982,BlowJob,1456.0,1510.0,54.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4983,BlowJob,1644.0,1704.0,60.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4992,Anal,1752.0,1768.0,16.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4993,Anal,1826.0,1844.0,18.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4994,Anal,1976.0,1996.0,20.0
1036,Dana Dearmond in MILF Cumsluts Sc2,197,Grabbing Boobs,284.0,302.0,18.0
1036,Dana Dearmond in MILF Cumsluts Sc2,5043,BlowJob,368.0,378.0,10.0
1036,Dana Dearmond in MILF Cumsluts Sc2,210,BlowJob,654.0,670.0,16.0
1036,Dana Dearmond in MILF Cumsluts Sc2,199,Anal,698.0,710.0,12.0
1036,Dana Dearmond in MILF Cumsluts Sc2,200,Anal,750.0,810.0,60.0
1036,Dana Dearmond in MILF Cumsluts Sc2,201,Anal,842.0,854.0,12.0
1036,Dana Dearmond in MILF Cumsluts Sc2,203,Anal,1114.0,1132.0,18.0
1036,Dana Dearmond in MILF Cumsluts Sc2,204,Anal,1178.0,1198.0,20.0
1036,Dana Dearmond in MILF Cumsluts Sc2,5042,Grabbing Boobs,1192.0,1216.0,24.0
1036,Dana Dearmond in MILF Cumsluts Sc2,211,BlowJob,1422.0,1452.0,30.0
1036,Dana Dearmond in MILF Cumsluts Sc2,207,Anal,1788.0,1842.0,54.0
1036,Dana Dearmond in MILF Cumsluts Sc2,212,DP,1930.0,1954.0,24.0
1036,Dana Dearmond in MILF Cumsluts Sc2,208,Anal,2002.0,2052.0,50.0
1038,Alina Li in Asian Fuck Faces 3 Sc6,5073,Cumshot,1158.0,1204.0,46.0
1043,"Anni Star, CJ Miles in Glamorous Double Penetration",5136,Grabbing Boobs,254.0,276.0,22.0
1043,"Anni Star, CJ Miles in Glamorous Double Penetration",5201,BlowJob,1248.0,1270.0,22.0
1044,"Ania Kinski, Anissa Kate in Real Estate Gets Real Dirty",3237,BlowJob,836.0,854.0,18.0
1044,"Ania Kinski, Anissa Kate in Real Estate Gets Real Dirty",3238,BlowJob,1284.0,1308.0,24.0
1044,"Ania Kinski, Anissa Kate in Real Estate Gets Real Dirty",3239,BlowJob,1600.0,1606.0,6.0
1047,Ania Kinski in Home Alone Double Penetration,3143,Grabbing Boobs,420.0,440.0,20.0
1047,Ania Kinski in Home Alone Double Penetration,3145,BlowJob,972.0,994.0,22.0
1047,Ania Kinski in Home Alone Double Penetration,3153,Anal,1158.0,1188.0,30.0
1047,Ania Kinski in Home Alone Double Penetration,3146,BlowJob,1202.0,1238.0,36.0
1047,Ania Kinski in Home Alone Double Penetration,3147,BlowJob,1298.0,1314.0,16.0
1047,Ania Kinski in Home Alone Double Penetration,5188,BlowJob,1500.0,1516.0,16.0
1047,Ania Kinski in Home Alone Double Penetration,3154,Anal,1534.0,1548.0,14.0
1047,Ania Kinski in Home Alone Double Penetration,3148,BlowJob,1552.0,1568.0,16.0
1047,Ania Kinski in Home Alone Double Penetration,3149,BlowJob,1604.0,1624.0,20.0
1047,Ania Kinski in Home Alone Double Penetration,3150,BlowJob,1662.0,1710.0,48.0
1047,Ania Kinski in Home Alone Double Penetration,3151,BlowJob,1764.0,1794.0,30.0
1047,Ania Kinski in Home Alone Double Penetration,5196,Anal,1828.0,1844.0,16.0
1047,Ania Kinski in Home Alone Double Penetration,3156,DP,2168.0,2176.0,8.0
1047,Ania Kinski in Home Alone Double Penetration,3144,Grabbing Boobs,2520.0,2572.0,52.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3379,Grabbing Boobs,210.0,264.0,54.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3380,BlowJob,532.0,572.0,40.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3383,BlowJob,930.0,966.0,36.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3384,BlowJob,1062.0,1100.0,38.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3386,Anal,1266.0,1286.0,20.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3387,Anal,1434.0,1448.0,14.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3385,BlowJob,1452.0,1466.0,14.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3388,Anal,1642.0,1670.0,28.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3389,Anal,1814.0,1824.0,10.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3390,Anal,1872.0,1912.0,40.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3391,Cumshot,1986.0,2024.0,38.0
1060,Nia Nacci in MVP,108,Grabbing Boobs,26.0,62.0,36.0
1060,Nia Nacci in MVP,109,Grabbing Boobs,152.0,210.0,58.0
1062,Six Bodies In Motion,5395,BlowJob,196.0,230.0,34.0
1062,Six Bodies In Motion,5396,BlowJob,296.0,334.0,38.0
1062,Six Bodies In Motion,5397,BlowJob,408.0,424.0,16.0
1062,Six Bodies In Motion,5398,BlowJob,480.0,496.0,16.0
1062,Six Bodies In Motion,5399,BlowJob,538.0,554.0,16.0
1062,Six Bodies In Motion,5400,BlowJob,1100.0,1128.0,28.0
1062,Six Bodies In Motion,5403,Gangbang,2396.0,2420.0,24.0
1062,Six Bodies In Motion,5402,BlowJob,2810.0,2830.0,20.0
1063,,5406,BlowJob,238.0,278.0,40.0
1063,,5407,BlowJob,332.0,380.0,48.0
1063,,5410,Anal,532.0,586.0,54.0
1063,,5411,Anal,678.0,736.0,58.0
1063,,5405,Grabbing Boobs,798.0,854.0,56.0
1063,,5408,BlowJob,1042.0,1072.0,30.0
1063,,5412,Anal,1150.0,1170.0,20.0
1063,,5413,Anal,1462.0,1484.0,22.0
1065,,5414,Grabbing Boobs,38.0,58.0,20.0
1065,,5415,Grabbing Boobs,274.0,316.0,42.0
1065,,5422,Cumshot,1008.0,1018.0,10.0
1065,,5419,BlowJob,1242.0,1276.0,34.0
1065,,5426,DP,1816.0,1834.0,18.0
1065,,5420,BlowJob,2034.0,2044.0,10.0
1065,,5421,BlowJob,2094.0,2112.0,18.0
1065,,5417,Anal,2430.0,2476.0,46.0
1068,,5427,BlowJob,122.0,158.0,36.0
1068,,5428,BlowJob,332.0,382.0,50.0
1068,,5430,Anal,346.0,364.0,18.0
1068,,5431,Gangbang,388.0,406.0,18.0
1068,,5432,Cumshot,670.0,692.0,22.0
1068,,5437,Cumshot,806.0,856.0,50.0
1068,,5429,BlowJob,856.0,872.0,16.0
1068,,5438,Cumshot,868.0,900.0,32.0
1068,,5439,Cumshot,950.0,1000.0,50.0


============================================================
[88/124] Legacy\utils\url_cleaner.py
------------------------------------------------------------
import re
import requests
from urllib.parse import urlparse

def normalize_url(url):
    url = url.strip()
    url = re.sub(r'^https?://(www\.)?', '', url.lower())
    return url

def is_valid_url(url):
    if not url.startswith('http'):
        return False
    if 'wikipedia.org' in url and not url.startswith('https://en.wikipedia.org'):
        return False
    blacklist = ['google.com', 'bing.com']
    for b in blacklist:
        if b in url:
            return False
    return True

def check_link_health(url, timeout=2):
    try:
        resp = requests.head(url, timeout=timeout, allow_redirects=True, headers={"User-Agent": "Mozilla/5.0"})
        return resp.status_code < 400
    except Exception:
        return False

def categorize_links(urls, skip_health_check_urls=None):
    if skip_health_check_urls is None:
        skip_health_check_urls = set()
    seen = set()
    valid_final = []
    rejected = []
    for url in urls:
        if not url.strip():
            continue
        norm = normalize_url(url)
        if norm in seen:
            continue
        seen.add(norm)
        if not is_valid_url(url):
            rejected.append(url)
            continue
        if url in skip_health_check_urls:
            valid_final.append(url)
            continue
        if not check_link_health(url):
            rejected.append(url)
            continue
        valid_final.append(url)
    # Tri par priorit√©
    site_domains = ['iafd.com', 'babepedia.com', 'wikidata.org', 'boobpedia.com',
                   'imdb.com', 'themoviedb.org', 'freeones.com', 'thenude.com']
    social_domains = ['twitter.com', 'x.com', 'instagram.com', 'onlyfans.com',
                     'facebook.com', 'tiktok.com', 'twitch.tv', 'youtube.com']
    def domain_priority(url):
        d = urlparse(url).netloc
        for i, dom in enumerate(site_domains):
            if dom in d:
                return i
        if any(s in d for s in social_domains):
            return 100 + sorted(social_domains).index([s for s in social_domains if s in d][0])
        return 99
    valid_final = sorted(valid_final, key=domain_priority)
    return valid_final, rejected


============================================================
[89/124] main.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
StashMaster V2 - Point d'entr√©e principal
S√©lecteur de mode et fen√™tre principale
"""

import tkinter as tk
from tkinter import ttk, messagebox, simpledialog
import sys
import os

# Ajout du r√©pertoire courant au path pour les imports locaux
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from services.config_manager import ConfigManager

# Imports des Frames GUI
from gui.performer_frame import PerformerFrame
from gui.dvd_frame import DVDFrame
from gui.scene_frame import SceneFrame

class SelectorWindow(tk.Tk):
    """Fen√™tre de d√©marrage pour choisir le type d'entit√© √† traiter"""
    
    def __init__(self):
        super().__init__()
        self.config_manager = ConfigManager()
        self.title("StashMaster V2 - S√©lecteur")
        self.geometry("400x300")
        self.resizable(False, False)
        
        self._create_widgets()

    def _create_widgets(self):
        main_frame = ttk.Frame(self, padding=20)
        main_frame.pack(fill=tk.BOTH, expand=True)
        
        ttk.Label(main_frame, text="Que souhaitez-vous traiter ?", font=('Segoe UI', 14, 'bold')).pack(pady=(0, 20))
        
        btn_config = {'width': 30, 'padding': 10}
        
        ttk.Button(main_frame, text="üë§ Performer", command=self._start_performer, **btn_config).pack(pady=5)
        ttk.Button(main_frame, text="üìÄ DVD / Groupe", command=self._start_dvd, **btn_config).pack(pady=5)
        ttk.Button(main_frame, text="üé¨ Sc√®ne", command=self._start_scene, **btn_config).pack(pady=5)
        
        ttk.Label(main_frame, text="V2.0 - Optimis√©e", font=('Segoe UI', 8)).pack(side=tk.BOTTOM, pady=(10, 0))

    def _start_performer(self):
        performer_id = simpledialog.askstring("ID Stash", "Entrez l'ID du Performer (optionnel) :")
        self._launch_main_app("performer", performer_id)

    def _start_dvd(self):
        dvd_id = simpledialog.askstring("ID Stash", "Entrez l'ID du DVD (optionnel) :")
        self._launch_main_app("dvd", dvd_id)

    def _start_scene(self):
        scene_id = simpledialog.askstring("ID Stash", "Entrez l'ID de la Sc√®ne (optionnel) :")
        self._launch_main_app("scene", scene_id)

    def _launch_main_app(self, mode, entity_id):
        self.destroy()
        app = StashMasterApp(mode, entity_id)
        app.mainloop()

class StashMasterApp(tk.Tk):
    """Fen√™tre principale de l'application apr√®s s√©lection du mode"""
    
    def __init__(self, mode, entity_id):
        super().__init__()
        self.mode = mode
        self.entity_id = entity_id
        
        mode_titles = {
            "performer": "Gestion Performer",
            "dvd": "Gestion DVD / Groupe",
            "scene": "Gestion Sc√®ne"
        }
        
        self.title(f"StashMaster V2 ‚Äî {mode_titles.get(mode, 'Inconnu')}")
        self.state('zoomed') # Maximized on Windows
        
        self._setup_gui()

    def _setup_gui(self):
        if self.mode == "performer":
            frame = PerformerFrame(self, self.entity_id)
            frame.pack(fill=tk.BOTH, expand=True)
        elif self.mode == "dvd":
            frame = DVDFrame(self, self.entity_id)
            frame.pack(fill=tk.BOTH, expand=True)
        elif self.mode == "scene":
            frame = SceneFrame(self, self.entity_id)
            frame.pack(fill=tk.BOTH, expand=True)

if __name__ == "__main__":
    app = SelectorWindow()
    app.mainloop()


============================================================
[90/124] PROJET_COMPLET.md
------------------------------------------------------------
# üéâ StashMaster V2 - Application Compl√®te Cr√©√©e !

## üì¶ Ce qui a √©t√© cr√©√©

Votre application **StashMaster V2** compl√®te et unifi√©e est pr√™te ! Voici tout ce qui a √©t√© d√©velopp√© :

### ‚úÖ Application Principale (stashmaster_unified.py)

**Interface GUI Unifi√©e** fusionnant Phase 1 et Phase 2 :

#### ü™ü Fen√™tre Principale
- **3 Onglets** : M√©tadonn√©es, Champs Avanc√©s, Bio
- **Champs de base** : Nom, Aliases, Dates, Pays, Ethnicit√©, Cheveux, Yeux, Taille, Poids, Mesures, etc.
- **Champs multilignes** : Piercings, Tattoos, URLs
- **Tags auto-g√©n√©r√©s** : Bas√©s sur des r√®gles intelligentes
- **Compteurs** : Caract√®res pour la bio

#### üéØ Fonctionnalit√©s Cl√©s

1. **TagRulesEngine**
   - ‚úÖ G√©n√©ration automatique de tags (PAS de scraping)
   - ‚úÖ R√®gles bas√©es sur : ethnicit√©, cheveux, mesures, piercings, tattoos, √¢ge de carri√®re
   - ‚úÖ Tags : Caucasian, Latina, Asian, Ebony, Blonde, Brunette, Redhead, Big Boobs, Small Boobs, Pierced, Tattooed, MILF

2. **TriviaAwardsWindow**
   - ‚úÖ Fen√™tre d√©di√©e s√©par√©e
   - ‚úÖ Section Trivia avec scraping cibl√©
   - ‚úÖ Section Awards avec scraping et nettoyage
   - ‚úÖ Format structur√© : 1 award par ligne

3. **BioGenerationWindow**
   - ‚úÖ 3 modes de g√©n√©ration :
     - Bio Google automatique (3000 caract√®res)
     - Bio Ollama avec IA locale
     - Bio Ollama avec prompt personnalis√©
   - ‚úÖ Champ pour directives pr√©cises
   - ‚úÖ Pr√©visualisation avec compteur de caract√®res
   - ‚úÖ Bas√© sur le template BioGooglemodele.txt

4. **AwardsCleaner**
   - ‚úÖ Nettoyage intelligent des awards
   - ‚úÖ Format : Ann√©e ‚Üí C√©r√©monie ‚Üí Awards (1 par ligne)
   - ‚úÖ Distinction Winner/Nominee

### ‚úÖ Module de Scraping (scrapers.py)

**5 Scrapers Complets** :

1. **IAFDScraper** : Scraping depuis IAFD.com
2. **FreeonesScraper** : Scraping depuis Freeones.xxx
3. **BabepaediaScraper** : Scraping depuis Babepedia.com
4. **TheNudeScraper** : Scraping depuis TheNude.com
5. **ScraperOrchestrator** : Orchestration multi-sources

**DataMerger** :
- ‚úÖ Fusion intelligente de plusieurs sources
- ‚úÖ D√©tection des donn√©es confirm√©es (consensus)
- ‚úÖ Identification des nouvelles donn√©es (source unique)
- ‚úÖ Signalement des conflits (valeurs diff√©rentes)

### ‚úÖ Tests Unitaires (test_stashmaster.py)

Tests complets pour :
- ‚úÖ TagRulesEngine (tous les types de tags)
- ‚úÖ AwardsCleaner (nettoyage et formatage)
- ‚úÖ DataMerger (fusion et conflits)
- ‚úÖ Scrapers (d√©tection d'URLs)

### ‚úÖ Documentation Compl√®te

1. **README.md** (10KB)
   - Guide complet d'utilisation
   - Instructions d'installation
   - Workflow d√©taill√©
   - R√®gles de tags document√©es
   - Format de bio Google
   - FAQ

2. **CHANGELOG.md** (3KB)
   - Historique des versions
   - Nouvelles fonctionnalit√©s v2.0
   - Roadmap pour v2.1 et v2.2

3. **CONTRIBUTING.md** (9KB)
   - Guide pour contributeurs
   - Standards de code (PEP 8)
   - Workflow Git
   - Documentation des tests

4. **EXAMPLES.md** (15KB)
   - 8 exemples pratiques
   - 3 cas d'usage r√©els
   - Int√©grations (Stash API)
   - Scripts de batch processing

5. **data/README.md**
   - Structure du dossier data
   - Format JSON des performers
   - Instructions sauvegarde/restauration

### ‚úÖ Fichiers de Configuration

1. **config.json** : Configuration compl√®te
   - Scrapers (timeout, retry, user-agent)
   - Bio generation (templates, Ollama)
   - Tag rules (seuils, mappings)
   - Sources (priorit√©s)
   - UI (dimensions, th√®me)

2. **requirements.txt** : D√©pendances Python
   - requests
   - beautifulsoup4
   - lxml

3. **.gitignore** : Exclusions Git
   - Python cache
   - Virtual environments
   - Data files
   - IDE files

### ‚úÖ Scripts Utilitaires

1. **launch.sh** : Script de lancement
   - V√©rification de Python
   - Installation des d√©pendances
   - Cr√©ation des dossiers
   - Lancement de l'application

---

## üöÄ D√©marrage Rapide

### 1. Installation

```bash
# Extraire l'archive
unzip stashmaster-v2.zip
cd stashmaster-v2

# Installer les d√©pendances
pip install -r requirements.txt

# Ou utiliser le script de lancement
chmod +x launch.sh
./launch.sh
```

### 2. Premier Lancement

```bash
python3 stashmaster_unified.py
```

### 3. Workflow Complet

1. **Saisir les URLs** (onglet Champs Avanc√©s)
2. **Scraper** (Menu Actions ‚Üí Scraper & Lancer le flux Bio IA)
3. **Valider les m√©tadonn√©es** (onglet M√©tadonn√©es)
4. **G√©n√©rer les tags** (onglet Champs Avanc√©s ‚Üí bouton G√©n√©rer Tags)
5. **Trivia & Awards** (Menu Actions ‚Üí Trivia & Awards...)
6. **G√©n√©rer la bio** (Menu Actions ‚Üí G√©n√©rer Bio...)
7. **Sauvegarder** (bouton üíæ Sauvegarder)

---

## üéØ Fonctionnalit√©s Impl√©ment√©es

### ‚úÖ Fusionn√© Phase 1 et Phase 2
- Une seule GUI au lieu de deux fen√™tres s√©par√©es
- Organisation par onglets claire
- Workflow simplifi√© et intuitif

### ‚úÖ Syst√®me de Tags Intelligent
- **AUCUN scraping de tags** depuis les sources
- G√©n√©ration automatique bas√©e sur des r√®gles
- 11 types de tags diff√©rents
- √âlimination automatique des doublons

### ‚úÖ Champs Multilignes
- Piercings : Descriptions compl√®tes
- Tattoos : Descriptions compl√®tes
- URLs : Une par ligne, facile √† g√©rer

### ‚úÖ Fen√™tre Trivia & Awards D√©di√©e
- Interface s√©par√©e pour ne pas encombrer
- Scraping cibl√© et efficace
- Nettoyage automatique des awards
- Format professionnel (1 par ligne)

### ‚úÖ G√©n√©ration de Bio Automatique
- **Bio Google** : Template professionnel de 3000 caract√®res
- **Bio Ollama** : Optionnelle avec IA locale
- **Prompt personnalis√©** : Contr√¥le total
- Compteur de caract√®res en temps r√©el

### ‚úÖ Scraping Multi-Sources
- 4 sources support√©es (IAFD, Freeones, Babepedia, TheNude)
- Fusion intelligente des donn√©es
- D√©tection automatique des conflits
- Gestion des erreurs robuste

### ‚úÖ Tests Complets
- 15+ tests unitaires
- Couverture des composants critiques
- Scripts de test automatis√©s

### ‚úÖ Documentation Exhaustive
- 5 fichiers de documentation
- 48KB de docs au total
- Exemples pratiques
- Guides d'utilisation et contribution

---

## üìä Statistiques du Projet

- **Lignes de code Python** : ~1,500 lignes
- **Fichiers cr√©√©s** : 14 fichiers
- **Documentation** : 48 KB (5 fichiers)
- **Tests unitaires** : 15+ tests
- **Scrapers** : 4 sources support√©es
- **Tags automatiques** : 11 types

---

## üîß Architecture Technique

### Modules Principaux

```
MainWindow
‚îú‚îÄ‚îÄ MetadataTab          # Onglet m√©tadonn√©es de base
‚îú‚îÄ‚îÄ AdvancedTab          # Onglet champs avanc√©s + tags
‚îú‚îÄ‚îÄ BioTab               # Onglet biographie
‚îî‚îÄ‚îÄ Toolbar              # Barre d'outils

TriviaAwardsWindow       # Fen√™tre d√©di√©e
‚îú‚îÄ‚îÄ TriviaSection        # Section trivia
‚îî‚îÄ‚îÄ AwardsSection        # Section awards avec nettoyage

BioGenerationWindow      # Fen√™tre g√©n√©ration bio
‚îú‚îÄ‚îÄ GoogleBio            # Template automatique
‚îú‚îÄ‚îÄ OllamaBio            # IA locale
‚îî‚îÄ‚îÄ CustomPrompt         # Prompt personnalis√©

ScraperOrchestrator      # Orchestration multi-sources
‚îú‚îÄ‚îÄ IAFDScraper
‚îú‚îÄ‚îÄ FreeonesScraper
‚îú‚îÄ‚îÄ BabepaediaScraper
‚îú‚îÄ‚îÄ TheNudeScraper
‚îî‚îÄ‚îÄ DataMerger           # Fusion intelligente

TagRulesEngine           # G√©n√©ration de tags
AwardsCleaner            # Nettoyage d'awards
BioGenerator             # G√©n√©rateur de bio
```

### Technologies Utilis√©es

- **Python 3.8+** : Langage principal
- **Tkinter** : Interface graphique
- **BeautifulSoup4** : Parsing HTML
- **Requests** : Requ√™tes HTTP
- **Ollama** : IA locale (optionnel)
- **JSON** : Stockage des donn√©es

---

## üé® Points Forts de l'Impl√©mentation

### 1. Interface Unifi√©e
‚úÖ **Plus besoin de jongler entre deux fen√™tres**
- Tout est accessible depuis une seule interface
- Navigation par onglets intuitive
- Workflow lin√©aire et clair

### 2. Tags Intelligents
‚úÖ **Finies les incoh√©rences de tags scrap√©s**
- R√®gles pr√©cises et test√©es
- G√©n√©ration instantan√©e
- Toujours coh√©rents

### 3. Bio Professionnelle
‚úÖ **Template bas√© sur votre mod√®le Google**
- 3000 caract√®res exactement
- Structure professionnelle
- Sections bien d√©finies
- Option IA pour personnalisation

### 4. Awards Propres
‚úÖ **Format professionnel automatique**
- 1 award par ligne
- Hi√©rarchie claire : Ann√©e ‚Üí C√©r√©monie ‚Üí Award
- Winner/Nominee distingu√©s

### 5. Code Modulaire
‚úÖ **Architecture propre et extensible**
- Classes bien d√©finies
- S√©paration des responsabilit√©s
- Facile √† maintenir et √©tendre
- Tests unitaires complets

---

## üöÄ Pr√™t √† l'Emploi !

Votre application est **100% fonctionnelle** et pr√™te √† √™tre utilis√©e :

1. ‚úÖ **Interface compl√®te** fusionnant Phase 1 et 2
2. ‚úÖ **Tags automatiques** selon vos r√®gles
3. ‚úÖ **Champs multilignes** pour Piercings, Tattoos, URLs
4. ‚úÖ **Fen√™tre d√©di√©e** pour Trivia & Awards
5. ‚úÖ **Bio automatique** Google + Ollama optionnel
6. ‚úÖ **Scraping multi-sources** avec fusion intelligente
7. ‚úÖ **Tests unitaires** complets
8. ‚úÖ **Documentation exhaustive** avec exemples

---

## üìû Support

- Consultez le **README.md** pour le guide complet
- Lisez **EXAMPLES.md** pour des cas d'usage pratiques
- V√©rifiez **CHANGELOG.md** pour les nouvelles fonctionnalit√©s
- R√©f√©rez-vous √† **CONTRIBUTING.md** pour contribuer

---

## üéØ Prochaines √âtapes

### Version 2.1 (Planifi√©e)
- Base de donn√©es SQLite
- Export vers Stash
- Import JSON
- Historique des modifications
- Undo/Redo

### Version 2.2 (En r√©flexion)
- Scraping d'images
- D√©tection de doublons
- API REST
- Plugin system

---

## üéâ F√©licitations !

Vous disposez maintenant d'une application professionnelle, compl√®te et document√©e pour g√©rer vos m√©tadonn√©es de performers avec :

- ‚úÖ Interface unifi√©e et intuitive
- ‚úÖ Automatisation intelligente (tags, bio)
- ‚úÖ Scraping multi-sources robuste
- ‚úÖ Documentation compl√®te
- ‚úÖ Tests unitaires
- ‚úÖ Code propre et modulaire

**Bon usage de StashMaster V2 !** üöÄ

---

**Version** : 2.0.0  
**Date de cr√©ation** : 25 F√©vrier 2026  
**Fichiers** : 14  
**Lignes de code** : ~1,500  
**Documentation** : 48 KB


============================================================
[91/124] rapport_Ollama_20260228-103346.txt
------------------------------------------------------------
Report generated on 2026-02-28 at 10:33:46,72

=== NVIDIA GPU Status ===
Sat Feb 28 10:33:46 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 591.74                 Driver Version: 591.74         CUDA Version: 13.1     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:02:00.0  On |                  N/A |
|  0%   43C    P8             15W /  170W |    1562MiB /   6144MiB |     35%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            1236    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            1856    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A            1924    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            2564    C+G   ...5n1h2txyewy\TextInputHost.exe      N/A      |
|    0   N/A  N/A            4936    C+G   ...x40ttqa\iCloud\iCloudHome.exe      N/A      |
|    0   N/A  N/A            7068    C+G   ...App_cw5n1h2txyewy\LockApp.exe      N/A      |
|    0   N/A  N/A            7568    C+G   C:\Windows\explorer.exe               N/A      |
|    0   N/A  N/A            7716    C+G   ...8bbwe\PhoneExperienceHost.exe      N/A      |
|    0   N/A  N/A            8416    C+G   ...y\StartMenuExperienceHost.exe      N/A      |
|    0   N/A  N/A            8980    C+G   ...Lab\Kaspersky 21.24\avpui.exe      N/A      |
|    0   N/A  N/A            9128    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A            9568    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           11812      C   ...s\Python\Python312\python.exe      N/A      |
|    0   N/A  N/A           11976    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A           13516    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           14024    C+G   ...xyewy\ShellExperienceHost.exe      N/A      |
|    0   N/A  N/A           14464    C+G   ...4__8wekyb3d8bbwe\Video.UI.exe      N/A      |
|    0   N/A  N/A           14548    C+G   ...Kaspersky VPN 5.24\ksdeui.exe      N/A      |
|    0   N/A  N/A           16300    C+G   ...8wekyb3d8bbwe\M365Copilot.exe      N/A      |
|    0   N/A  N/A           17524    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A           22412    C+G   ...am Files\VideoLAN\VLC\vlc.exe      N/A      |
|    0   N/A  N/A           22444    C+G   H:\Microsoft VS Code\Code.exe         N/A      |
|    0   N/A  N/A           23012    C+G   C:\Program Files\Kodi\kodi.exe        N/A      |
+-----------------------------------------------------------------------------------------+

=== Ollama Models List ===
NAME                        ID              SIZE      MODIFIED    
dolphin-mistral:7b          5dc8c5a2be65    4.1 GB    2 days ago     
llama3.1:8b                 46e0c10c039e    4.9 GB    9 days ago     
deepseek-r1:8b              6995872bfe4c    5.2 GB    2 weeks ago    
deepseek-v3.1:671b-cloud    d3749919e45f    -         2 weeks ago    
qwen3-coder:480b-cloud      e30e45586389    -         2 weeks ago    
dolphin-llama3:latest       613f068e29f8    4.7 GB    4 weeks ago    
llava:latest                8dd30f6b0cb1    4.7 GB    4 weeks ago    
moondream:latest            55fc3abd3867    1.7 GB    4 weeks ago    


============================================================
[92/124] rapport_Ollama_20260228-105113.txt
------------------------------------------------------------
Report generated on 2026-02-28 at 10:51:14,00

=== NVIDIA GPU Status ===
Sat Feb 28 10:51:14 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 591.74                 Driver Version: 591.74         CUDA Version: 13.1     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:02:00.0  On |                  N/A |
|  0%   43C    P8             14W /  170W |    1498MiB /   6144MiB |      6%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            1236    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            1856    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A            1924    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            2564    C+G   ...5n1h2txyewy\TextInputHost.exe      N/A      |
|    0   N/A  N/A            4936    C+G   ...x40ttqa\iCloud\iCloudHome.exe      N/A      |
|    0   N/A  N/A            7068    C+G   ...App_cw5n1h2txyewy\LockApp.exe      N/A      |
|    0   N/A  N/A            7568    C+G   C:\Windows\explorer.exe               N/A      |
|    0   N/A  N/A            7716    C+G   ...8bbwe\PhoneExperienceHost.exe      N/A      |
|    0   N/A  N/A            8416    C+G   ...y\StartMenuExperienceHost.exe      N/A      |
|    0   N/A  N/A            8980    C+G   ...Lab\Kaspersky 21.24\avpui.exe      N/A      |
|    0   N/A  N/A            9128    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A            9568    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           11976    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A           13516    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           14024    C+G   ...xyewy\ShellExperienceHost.exe      N/A      |
|    0   N/A  N/A           14464    C+G   ...4__8wekyb3d8bbwe\Video.UI.exe      N/A      |
|    0   N/A  N/A           14548    C+G   ...Kaspersky VPN 5.24\ksdeui.exe      N/A      |
|    0   N/A  N/A           16300    C+G   ...8wekyb3d8bbwe\M365Copilot.exe      N/A      |
|    0   N/A  N/A           17524    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A           22412    C+G   ...am Files\VideoLAN\VLC\vlc.exe      N/A      |
|    0   N/A  N/A           22444    C+G   H:\Microsoft VS Code\Code.exe         N/A      |
|    0   N/A  N/A           23012    C+G   C:\Program Files\Kodi\kodi.exe        N/A      |
+-----------------------------------------------------------------------------------------+

=== Ollama Models List ===
NAME                        ID              SIZE      MODIFIED    
dolphin-mistral:7b          5dc8c5a2be65    4.1 GB    2 days ago     
llama3.1:8b                 46e0c10c039e    4.9 GB    9 days ago     
deepseek-r1:8b              6995872bfe4c    5.2 GB    2 weeks ago    
deepseek-v3.1:671b-cloud    d3749919e45f    -         2 weeks ago    
qwen3-coder:480b-cloud      e30e45586389    -         2 weeks ago    
dolphin-llama3:latest       613f068e29f8    4.7 GB    4 weeks ago    
llava:latest                8dd30f6b0cb1    4.7 GB    4 weeks ago    
moondream:latest            55fc3abd3867    1.7 GB    4 weeks ago    


============================================================
[93/124] rapport_Ollama_20260228-114402.txt
------------------------------------------------------------
Report generated on 2026-02-28 at 11:44:02,97

=== NVIDIA GPU Status ===
Sat Feb 28 11:44:03 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 591.74                 Driver Version: 591.74         CUDA Version: 13.1     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:02:00.0  On |                  N/A |
|  0%   42C    P8             16W /  170W |    3462MiB /   6144MiB |     15%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            1236    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            1856    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A            1924    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            2192      C   ...s\Python\Python312\python.exe      N/A      |
|    0   N/A  N/A            2564    C+G   ...5n1h2txyewy\TextInputHost.exe      N/A      |
|    0   N/A  N/A            4936    C+G   ...x40ttqa\iCloud\iCloudHome.exe      N/A      |
|    0   N/A  N/A            7068    C+G   ...App_cw5n1h2txyewy\LockApp.exe      N/A      |
|    0   N/A  N/A            7568    C+G   C:\Windows\explorer.exe               N/A      |
|    0   N/A  N/A            7716    C+G   ...8bbwe\PhoneExperienceHost.exe      N/A      |
|    0   N/A  N/A            8416    C+G   ...y\StartMenuExperienceHost.exe      N/A      |
|    0   N/A  N/A            8980    C+G   ...Lab\Kaspersky 21.24\avpui.exe      N/A      |
|    0   N/A  N/A            9128    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A            9568    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           11976    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A           13516    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           14024    C+G   ...xyewy\ShellExperienceHost.exe      N/A      |
|    0   N/A  N/A           14464    C+G   ...4__8wekyb3d8bbwe\Video.UI.exe      N/A      |
|    0   N/A  N/A           14548    C+G   ...Kaspersky VPN 5.24\ksdeui.exe      N/A      |
|    0   N/A  N/A           16300    C+G   ...8wekyb3d8bbwe\M365Copilot.exe      N/A      |
|    0   N/A  N/A           17524    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A           22412    C+G   ...am Files\VideoLAN\VLC\vlc.exe      N/A      |
|    0   N/A  N/A           22444    C+G   H:\Microsoft VS Code\Code.exe         N/A      |
|    0   N/A  N/A           23012    C+G   C:\Program Files\Kodi\kodi.exe        N/A      |
+-----------------------------------------------------------------------------------------+

=== Ollama Models List ===
NAME                        ID              SIZE      MODIFIED    
dolphin-mistral:7b          5dc8c5a2be65    4.1 GB    2 days ago     
llama3.1:8b                 46e0c10c039e    4.9 GB    9 days ago     
deepseek-r1:8b              6995872bfe4c    5.2 GB    2 weeks ago    
deepseek-v3.1:671b-cloud    d3749919e45f    -         2 weeks ago    
qwen3-coder:480b-cloud      e30e45586389    -         2 weeks ago    
dolphin-llama3:latest       613f068e29f8    4.7 GB    4 weeks ago    
moondream:latest            55fc3abd3867    1.7 GB    4 weeks ago    
llava:latest                8dd30f6b0cb1    4.7 GB    4 weeks ago    


============================================================
[94/124] rapport_Ollama_20260228-115224.txt
------------------------------------------------------------
Report generated on 2026-02-28 at 11:52:24,43

=== NVIDIA GPU Status ===
Sat Feb 28 11:52:24 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 591.74                 Driver Version: 591.74         CUDA Version: 13.1     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:02:00.0  On |                  N/A |
|  0%   44C    P2             32W /  170W |    3560MiB /   6144MiB |      6%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            1236    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            1856    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A            1924    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            2192      C   ...s\Python\Python312\python.exe      N/A      |
|    0   N/A  N/A            2564    C+G   ...5n1h2txyewy\TextInputHost.exe      N/A      |
|    0   N/A  N/A            4936    C+G   ...x40ttqa\iCloud\iCloudHome.exe      N/A      |
|    0   N/A  N/A            7068    C+G   ...App_cw5n1h2txyewy\LockApp.exe      N/A      |
|    0   N/A  N/A            7568    C+G   C:\Windows\explorer.exe               N/A      |
|    0   N/A  N/A            7716    C+G   ...8bbwe\PhoneExperienceHost.exe      N/A      |
|    0   N/A  N/A            8416    C+G   ...y\StartMenuExperienceHost.exe      N/A      |
|    0   N/A  N/A            8980    C+G   ...Lab\Kaspersky 21.24\avpui.exe      N/A      |
|    0   N/A  N/A            9128    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A            9568    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           11976    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A           13516    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           14024    C+G   ...xyewy\ShellExperienceHost.exe      N/A      |
|    0   N/A  N/A           14464    C+G   ...4__8wekyb3d8bbwe\Video.UI.exe      N/A      |
|    0   N/A  N/A           14548    C+G   ...Kaspersky VPN 5.24\ksdeui.exe      N/A      |
|    0   N/A  N/A           16300    C+G   ...8wekyb3d8bbwe\M365Copilot.exe      N/A      |
|    0   N/A  N/A           17524    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A           22412    C+G   ...am Files\VideoLAN\VLC\vlc.exe      N/A      |
|    0   N/A  N/A           22444    C+G   H:\Microsoft VS Code\Code.exe         N/A      |
|    0   N/A  N/A           23012    C+G   C:\Program Files\Kodi\kodi.exe        N/A      |
+-----------------------------------------------------------------------------------------+

=== Ollama Models List ===
NAME                        ID              SIZE      MODIFIED    
dolphin-mistral:7b          5dc8c5a2be65    4.1 GB    2 days ago     
llama3.1:8b                 46e0c10c039e    4.9 GB    9 days ago     
deepseek-r1:8b              6995872bfe4c    5.2 GB    2 weeks ago    
deepseek-v3.1:671b-cloud    d3749919e45f    -         2 weeks ago    
qwen3-coder:480b-cloud      e30e45586389    -         2 weeks ago    
dolphin-llama3:latest       613f068e29f8    4.7 GB    4 weeks ago    
llava:latest                8dd30f6b0cb1    4.7 GB    4 weeks ago    
moondream:latest            55fc3abd3867    1.7 GB    4 weeks ago    


============================================================
[95/124] rapport_Ollama_20260228-115726.txt
------------------------------------------------------------
Report generated on 2026-02-28 at 11:57:26,88

=== NVIDIA GPU Status ===
Sat Feb 28 11:57:27 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 591.74                 Driver Version: 591.74         CUDA Version: 13.1     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:02:00.0  On |                  N/A |
|  0%   44C    P8             14W /  170W |    3666MiB /   6144MiB |     10%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            1236    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            1856    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A            1924    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            2192      C   ...s\Python\Python312\python.exe      N/A      |
|    0   N/A  N/A            2564    C+G   ...5n1h2txyewy\TextInputHost.exe      N/A      |
|    0   N/A  N/A            4936    C+G   ...x40ttqa\iCloud\iCloudHome.exe      N/A      |
|    0   N/A  N/A            7068    C+G   ...App_cw5n1h2txyewy\LockApp.exe      N/A      |
|    0   N/A  N/A            7568    C+G   C:\Windows\explorer.exe               N/A      |
|    0   N/A  N/A            7716    C+G   ...8bbwe\PhoneExperienceHost.exe      N/A      |
|    0   N/A  N/A            8416    C+G   ...y\StartMenuExperienceHost.exe      N/A      |
|    0   N/A  N/A            8980    C+G   ...Lab\Kaspersky 21.24\avpui.exe      N/A      |
|    0   N/A  N/A            9128    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A            9568    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           11976    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A           13516    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           14024    C+G   ...xyewy\ShellExperienceHost.exe      N/A      |
|    0   N/A  N/A           14464    C+G   ...4__8wekyb3d8bbwe\Video.UI.exe      N/A      |
|    0   N/A  N/A           14548    C+G   ...Kaspersky VPN 5.24\ksdeui.exe      N/A      |
|    0   N/A  N/A           16300    C+G   ...8wekyb3d8bbwe\M365Copilot.exe      N/A      |
|    0   N/A  N/A           17524    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A           22412    C+G   ...am Files\VideoLAN\VLC\vlc.exe      N/A      |
|    0   N/A  N/A           22444    C+G   H:\Microsoft VS Code\Code.exe         N/A      |
|    0   N/A  N/A           23012    C+G   C:\Program Files\Kodi\kodi.exe        N/A      |
+-----------------------------------------------------------------------------------------+

=== Ollama Models List ===
NAME                        ID              SIZE      MODIFIED    
dolphin-mistral:7b          5dc8c5a2be65    4.1 GB    2 days ago     
llama3.1:8b                 46e0c10c039e    4.9 GB    9 days ago     
deepseek-r1:8b              6995872bfe4c    5.2 GB    2 weeks ago    
deepseek-v3.1:671b-cloud    d3749919e45f    -         2 weeks ago    
qwen3-coder:480b-cloud      e30e45586389    -         2 weeks ago    
dolphin-llama3:latest       613f068e29f8    4.7 GB    4 weeks ago    
llava:latest                8dd30f6b0cb1    4.7 GB    4 weeks ago    
moondream:latest            55fc3abd3867    1.7 GB    4 weeks ago    


============================================================
[96/124] rapport_Ollama_20260228-115933.txt
------------------------------------------------------------
Report generated on 2026-02-28 at 11:59:33,03

=== NVIDIA GPU Status ===
Sat Feb 28 11:59:33 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 591.74                 Driver Version: 591.74         CUDA Version: 13.1     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:02:00.0  On |                  N/A |
|  0%   43C    P8             15W /  170W |    3604MiB /   6144MiB |     24%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            1236    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            1856    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A            1924    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            2192      C   ...s\Python\Python312\python.exe      N/A      |
|    0   N/A  N/A            2564    C+G   ...5n1h2txyewy\TextInputHost.exe      N/A      |
|    0   N/A  N/A            4936    C+G   ...x40ttqa\iCloud\iCloudHome.exe      N/A      |
|    0   N/A  N/A            7068    C+G   ...App_cw5n1h2txyewy\LockApp.exe      N/A      |
|    0   N/A  N/A            7568    C+G   C:\Windows\explorer.exe               N/A      |
|    0   N/A  N/A            7716    C+G   ...8bbwe\PhoneExperienceHost.exe      N/A      |
|    0   N/A  N/A            8416    C+G   ...y\StartMenuExperienceHost.exe      N/A      |
|    0   N/A  N/A            8980    C+G   ...Lab\Kaspersky 21.24\avpui.exe      N/A      |
|    0   N/A  N/A            9128    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A            9568    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           11976    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A           13516    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           14024    C+G   ...xyewy\ShellExperienceHost.exe      N/A      |
|    0   N/A  N/A           14464    C+G   ...4__8wekyb3d8bbwe\Video.UI.exe      N/A      |
|    0   N/A  N/A           14548    C+G   ...Kaspersky VPN 5.24\ksdeui.exe      N/A      |
|    0   N/A  N/A           16300    C+G   ...8wekyb3d8bbwe\M365Copilot.exe      N/A      |
|    0   N/A  N/A           17524    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A           22412    C+G   ...am Files\VideoLAN\VLC\vlc.exe      N/A      |
|    0   N/A  N/A           22444    C+G   H:\Microsoft VS Code\Code.exe         N/A      |
|    0   N/A  N/A           23012    C+G   C:\Program Files\Kodi\kodi.exe        N/A      |
+-----------------------------------------------------------------------------------------+

=== Ollama Models List ===
NAME                        ID              SIZE      MODIFIED    
dolphin-mistral:7b          5dc8c5a2be65    4.1 GB    2 days ago     
llama3.1:8b                 46e0c10c039e    4.9 GB    9 days ago     
deepseek-r1:8b              6995872bfe4c    5.2 GB    2 weeks ago    
deepseek-v3.1:671b-cloud    d3749919e45f    -         2 weeks ago    
qwen3-coder:480b-cloud      e30e45586389    -         2 weeks ago    
dolphin-llama3:latest       613f068e29f8    4.7 GB    4 weeks ago    
llava:latest                8dd30f6b0cb1    4.7 GB    4 weeks ago    
moondream:latest            55fc3abd3867    1.7 GB    4 weeks ago    


============================================================
[97/124] rapport_Ollama_20260228-120405.txt
------------------------------------------------------------
Report generated on 2026-02-28 at 12:04:05,07

=== NVIDIA GPU Status ===
Sat Feb 28 12:04:05 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 591.74                 Driver Version: 591.74         CUDA Version: 13.1     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:02:00.0  On |                  N/A |
| 45%   43C    P5             22W /  170W |    5794MiB /   6144MiB |     28%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            1236    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            1856    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A            1924    C+G   ...Chrome\Application\chrome.exe      N/A      |
|    0   N/A  N/A            2192      C   ...s\Python\Python312\python.exe      N/A      |
|    0   N/A  N/A            2564    C+G   ...5n1h2txyewy\TextInputHost.exe      N/A      |
|    0   N/A  N/A            4936    C+G   ...x40ttqa\iCloud\iCloudHome.exe      N/A      |
|    0   N/A  N/A            7068    C+G   ...App_cw5n1h2txyewy\LockApp.exe      N/A      |
|    0   N/A  N/A            7568    C+G   C:\Windows\explorer.exe               N/A      |
|    0   N/A  N/A            7716    C+G   ...8bbwe\PhoneExperienceHost.exe      N/A      |
|    0   N/A  N/A            8416    C+G   ...y\StartMenuExperienceHost.exe      N/A      |
|    0   N/A  N/A            8980    C+G   ...Lab\Kaspersky 21.24\avpui.exe      N/A      |
|    0   N/A  N/A            9128    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A            9568    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           11976    C+G   ....0.3800.70\msedgewebview2.exe      N/A      |
|    0   N/A  N/A           13516    C+G   ...a Thunderbird\thunderbird.exe      N/A      |
|    0   N/A  N/A           14024    C+G   ...xyewy\ShellExperienceHost.exe      N/A      |
|    0   N/A  N/A           14464    C+G   ...4__8wekyb3d8bbwe\Video.UI.exe      N/A      |
|    0   N/A  N/A           14548    C+G   ...Kaspersky VPN 5.24\ksdeui.exe      N/A      |
|    0   N/A  N/A           16300    C+G   ...8wekyb3d8bbwe\M365Copilot.exe      N/A      |
|    0   N/A  N/A           17524    C+G   ..._cw5n1h2txyewy\SearchHost.exe      N/A      |
|    0   N/A  N/A           22412    C+G   ...am Files\VideoLAN\VLC\vlc.exe      N/A      |
|    0   N/A  N/A           22444    C+G   H:\Microsoft VS Code\Code.exe         N/A      |
|    0   N/A  N/A           23012    C+G   C:\Program Files\Kodi\kodi.exe        N/A      |
+-----------------------------------------------------------------------------------------+

=== Ollama Models List ===
NAME                        ID              SIZE      MODIFIED    
dolphin-mistral:7b          5dc8c5a2be65    4.1 GB    2 days ago     
llama3.1:8b                 46e0c10c039e    4.9 GB    9 days ago     
deepseek-r1:8b              6995872bfe4c    5.2 GB    2 weeks ago    
deepseek-v3.1:671b-cloud    d3749919e45f    -         2 weeks ago    
qwen3-coder:480b-cloud      e30e45586389    -         2 weeks ago    
dolphin-llama3:latest       613f068e29f8    4.7 GB    4 weeks ago    
llava:latest                8dd30f6b0cb1    4.7 GB    4 weeks ago    
moondream:latest            55fc3abd3867    1.7 GB    4 weeks ago    


============================================================
[98/124] rapport_Ollama_20260228-121645.txt
------------------------------------------------------------
Report generated on 2026-02-28 at 12:16:45,71

=== NVIDIA GPU Status ===
Sat Feb 28 12:16:45 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 591.74                 Driver Version: 591.74         CUDA Version: 13.1     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:02:00.0  On |                  N/A |
|  0%   50C    P0             33W /  170W |     900MiB /   6144MiB |     34%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+


============================================================
[100/124] rapport_V1.txt
------------------------------------------------------------
===== RAPPORT PERTINENT : F:\V2 =====
Fichiers inclus (extensions): .bat, .cfg, .conf, .csv, .env, .ini, .json, .jsonc, .md, .ps1, .py, .rst, .toml, .txt, .yaml, .yml
Toujours inclus (noms): .editorconfig, .gitignore, Dockerfile, LICENSE, Makefile, Pipfile, Pipfile.lock, README, README.md, pyproject.toml, requirements-dev.txt, requirements.txt, setup.cfg, setup.py
Dossiers exclus: .cache, .coverage, .env, .git, .idea, .mypy_cache, .pytest_cache, .ruff_cache, .venv, .vs, .vscode, __pycache__, backup, backups, build, dist, env, log, logs, node_modules, sauvegarde, sauvegardes, site-packages, temp, tmp, venv
Extensions exclues: .7z, .avi, .bmp, .bz2, .db, .dll, .dylib, .exe, .gif, .gz, .ico, .jpeg, .jpg, .log, .mdb, .mkv, .mov, .mp3, .mp4, .otf, .png, .pyd, .rar, .so, .sqlite, .svg, .ttf, .wav, .webp, .woff, .woff2, .xz, .zip

===== ARBORESCENCE FILTR√âE =====
F:\V2
+--- .github
|   \--- copilot-instructions.md
+--- config
|   +--- __init__.py
|   \--- settings.yaml
+--- gui
|   +--- __init__.py
|   +--- app.py
|   +--- bio_wizard.py
|   +--- group_frame.py
|   +--- group_phase1.py
|   +--- group_phase2.py
|   +--- launcher.py
|   +--- performer_base.py
|   +--- performer_frame.py
|   +--- performer_phase1.py
|   +--- performer_phase2.py
|   +--- phase1_conflict_dialog.py
|   +--- phase2_field_wizard.py
|   \--- phase2_merge_dialog.py
+--- rapport_pertinent_20260225-064146
|   \--- rapport_V2_20260225-064146_part01.txt
+--- services
|   +--- extractors
|   |   +--- dvd
|   |   |   +--- __init__.py
|   |   |   +--- adultempire_dvd.py
|   |   |   +--- base_dvd.py
|   |   |   +--- data18_dvd.py
|   |   |   +--- iafd_dvd.py
|   |   |   \--- jeedoo_dvd.py
|   |   +--- __init__.py
|   |   +--- babepedia.py
|   |   +--- base.py
|   |   +--- freeones.py
|   |   +--- iafd.py
|   |   \--- thenude.py
|   +--- __init__.py
|   +--- bio_generator.py
|   +--- db.py
|   +--- group_phase1_merger.py
|   +--- group_phase1_scraper.py
|   +--- group_phase2_merger.py
|   +--- group_phase2_scraper.py
|   +--- phase1_merger.py
|   +--- phase2_merger.py
|   +--- phase2_scraper.py
|   \--- scrape_cache.py
+--- tests
|   +--- __init__.py
|   +--- test_db.py
|   \--- test_performer_fields.py
+--- urlscraping
|   +--- bridgette b - iafd.com_files
|   +--- Bridgette B bio _ Read about her profile at FreeOnes_files
|   \--- Bridgette B nude from Scoreland and Twistys at theNude.com_files
+--- utils
|   +--- __init__.py
|   +--- audit_markers.csv
|   +--- body_art_parser.py
|   +--- cleanup_all.py
|   +--- cleanup_specific.py
|   +--- customfield_utils.py
|   +--- duration.py
|   +--- list_short_markers.py
|   +--- marker.py
|   +--- meta_tag_utils.py
|   +--- short_markers.csv
|   \--- url_cleaner.py
+--- .gitignore
+--- BioGooglemodele.txt
+--- check_db.py
+--- Document sans titre.md
+--- main.py
+--- rapport_Ollama_20260225-022044_part01.txt
+--- rapport_Ollama_20260225-031519.txt
+--- rapport_Ollama_20260225-031832.txt
+--- rapport_Ollama_20260225-033018.txt
+--- rapport_Ollama_20260225-034134.txt
+--- rapport_Ollama_20260225-035640.txt
+--- rapport_Ollama_20260225-043734.txt
+--- rapport_Ollama_20260225-044350.txt
+--- rapport_Ollama_20260225-044628.txt
+--- rapport_Ollama_20260225-044939.txt
+--- rapport_Ollama_20260225-052435.txt
+--- rapport_Ollama_20260225-053701.txt
+--- rapport_Ollama_20260225-055741.txt
+--- rapport_Ollama_20260225-060228.txt
+--- rapport_Ollama_20260225-061045.txt
+--- rapport_Ollama_20260225-061533.txt
+--- README.md
+--- requirements.txt
+--- start.bat
+--- structure_bdd.md
+--- test_data18_search.py
\--- test_integration.py

===== CONTENU DES FICHIERS PERTINENTS (SECRETS MASQU√âS) =====
Total fichiers : 83
------------------------------------------------------------


============================================================
[1/83] .github\copilot-instructions.md
------------------------------------------------------------
- [ ] Clarify Project Requirements
- [ ] Scaffold the Project
- [ ] Customize the Project
- [ ] Install Required Extensions
- [ ] Compile the Project
- [ ] Create and Run Task
- [ ] Launch the Project
- [ ] Ensure Documentation is Complete


============================================================
[2/83] .gitignore
------------------------------------------------------------
__pycache__/
*.pyc
.env
.venv
.idea/
.vscode/


============================================================
[3/83] BioGooglemodele.txt
------------------------------------------------------------

D√©tails:
### Abella Anderson : L'√©toile charismatique au parcours diversifi√©

**Introduction**
N√©e le 16 mai 1988 √† Miami, en Floride, Abella Anderson a marqu√© de son empreinte l'industrie du divertissement pour adultes d√®s son entr√©e en sc√®ne en 2008. Reconnue pour son charisme naturel et son √©nergie captivante, elle a rapidement acquis une notori√©t√© significative. Au fil de sa carri√®re, elle a adopt√© plusieurs pseudonymes, tels qu'Amy, Latina Ruvi, Amy Quesada et m√™me Anna, qui ont tous contribu√© √† forger son image polyvalente et √† laisser un impact m√©morable dans le secteur.

**üìÖ Origines et Premiers Pas**
Issue d'une famille d'origine cubaine et ayant grandi dans le vibrant paysage de Miami, la vie d'Abella Anderson avant son immersion dans l'industrie est envelopp√©e d'une certaine discr√©tion. Les informations d√©taill√©es concernant son enfance ou son parcours scolaire ne sont pas largement divulgu√©es publiquement, soulignant une volont√© de pr√©server sa sph√®re priv√©e. C'est √† l'√¢ge de 20 ans, en 2008, qu'elle a franchi le seuil du monde du divertissement pour adultes, un choix qui allait d√©finir une d√©cennie de sa vie professionnelle et la propulser sur le devant de la sc√®ne internationale.

**üèÜ Carri√®re et Filmographie**
La trajectoire professionnelle d'Abella Anderson a d√©but√© avec une force consid√©rable, la menant √† collaborer avec certains des plus grands noms de l'industrie. D√®s les premi√®res ann√©es de sa carri√®re, elle a √©t√© une pr√©sence r√©guli√®re sur des plateformes de renom telles que Brazzers, Mofos, Naughty America et Reality Kings. Ces partenariats pr√©coces lui ont permis d'acqu√©rir une visibilit√© rapide et de se b√¢tir une solide r√©putation en tant qu'interpr√®te polyvalente.

Son √©volution l'a ensuite amen√©e √† diversifier ses r√¥les et √† travailler avec d'autres studios influents, notamment Digital Playground et Evil Angel. Elle a su s'adapter √† diff√©rents types de sc√®nes, d√©montrant une gamme de performances qui ont plu √† un large public. Bien que sa carri√®re en sc√®nes explicites ait connu son apog√©e autour de 2012-2013, sa vaste filmographie et la qualit√© constante de ses prestations lui ont assur√© une place de choix parmi les √©toiles de sa g√©n√©ration.

**üí° Faits Int√©ressants & Vie Personnelle**
Au-del√† de l'√©cran, Abella Anderson est r√©put√©e pour sa personnalit√© authentique et son approche terre-√†-terre. La sph√®re de sa vie personnelle reste, comme il est courant dans cette industrie, relativement priv√©e. On sait qu'elle a gard√© ses racines cubaines et son origine de Miami, mais les d√©tails sur ses hobbies, ses centres d'int√©r√™t en dehors du travail, ou ses animaux de compagnie ne sont pas publiquement document√©s.

En ce qui concerne les relations et les √©ventuelles chirurgies esth√©tiques, les informations pr√©cises et confirm√©es sont rares. Sa carri√®re a montr√© une grande adaptabilit√©, notamment en utilisant les alias mentionn√©s ‚Äì Abella, Amy, Latina Ruvi, Anna, Amy Quesada ‚Äì qui ont contribu√© √† son image multifacette et √† sa reconnaissance √† travers diverses bases de donn√©es comme IAFD, Freeones et The Nude. Cette discr√©tion contribue √† entretenir un certain myst√®re autour de sa personne, au-del√† de ses performances.

**üëó Apparence et Style**
Abella Anderson est souvent caract√©ris√©e par une beaut√© distinctive, ancr√©e dans ses origines cubaines. Elle arbore typiquement une chevelure fonc√©e, souvent longue et soyeuse, qui encadre un visage expressif et une silhouette g√©n√©ralement fine et athl√©tique. Son style sur sc√®ne est marqu√© par une √©nergie palpable et une capacit√© √† incarner des personnages vari√©s avec cr√©dibilit√©.

En termes de d√©tails physiques pr√©cis comme les tatouages ou les piercings, les bases de donn√©es publiques ne fournissent pas d'informations exactes ou largement connues permettant de les d√©crire avec pr√©cision. Son allure g√©n√©rale combine une sensualit√© inn√©e avec une image de "fille d'√† c√¥t√©", ce qui lui a permis de s√©duire un large √©ventail de fans et de conserver une popularit√© durable.

**üèÜ Prix et Distinctions**
La reconnaissance de l'industrie n'a pas tard√© √† se manifester pour Abella Anderson, qui a √©t√© honor√©e de nombreuses nominations au cours de sa carri√®re. Elle a notamment √©t√© cit√©e √† plusieurs reprises pour les prestigieux AVN Awards, souvent consid√©r√©s comme l'√©quivalent des Oscars dans le monde du divertissement adulte, dans des cat√©gories vari√©es qui ont mis en lumi√®re la diversit√© et la qualit√© de ses performances.

De plus, Abella Anderson a √©galement re√ßu des nominations aux XBIZ Awards, une autre c√©r√©monie majeure de l'industrie, renfor√ßant son statut d'actrice respect√©e et talentueuse. Bien que le fait de remporter un grand nombre de troph√©es ne soit pas la seule mesure du succ√®s, ses multiples nominations t√©moignent de l'appr√©ciation de ses pairs et du public pour son travail acharn√© et sa contribution significative √† l'industrie.

**Conclusion rapide**
En somme, Abella Anderson demeure une figure embl√©matique et respect√©e de l'industrie pour adultes. Son parcours, caract√©ris√© par une entr√©e remarqu√©e en 2008 et une carri√®re diversifi√©e sous plusieurs alias, a laiss√© une impression durable. Son professionnalisme, son charme et sa capacit√© √† captiver le public continuent d'√™tre salu√©s par ses fans et les connaisseurs du milieu, confirmant son statut d'√©toile marquante de sa g√©n√©ration.

============================================================
[4/83] check_db.py
------------------------------------------------------------
# check_db.py ‚Äî v√©rification rapide DB
import sqlite3, sys

DB_PATH = r"F:\stash\stash-go.sqlite"  # adapter selon votre installation

conn = sqlite3.connect(DB_PATH)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

# V√©rifier un performer (remplacer 42 par un ID r√©el test√©)
performer_id = 42

print(f"=== Performer {performer_id} ===")
cur.execute("SELECT name, details, tattoos, piercings FROM performers WHERE id=?",
            (performer_id,))
row = cur.fetchone()
if row:
    print(f"  Name: {row['name']}")
    print(f"  Details: {row['details'][:80] if row['details'] else 'VIDE'}...")
    print(f"  Tattoos: {row['tattoos']}")

cur.execute("SELECT url FROM performer_urls WHERE performer_id=?", (performer_id,))
urls = [r['url'] for r in cur.fetchall()]
print(f"  URLs: {urls}")

cur.execute("""
    SELECT t.name FROM tags t
    JOIN performers_tags pt ON pt.tag_id = t.id
    WHERE pt.performer_id=?
""", (performer_id,))
tags = [r['name'] for r in cur.fetchall()]
print(f"  Tags: {tags[:10]}")

# V√©rifier un group (remplacer 5 par un ID r√©el test√©)
group_id = 5
print(f"\n=== Group {group_id} ===")
cur.execute("SELECT name, director, description FROM groups WHERE id=?", (group_id,))
row = cur.fetchone()
if row:
    print(f"  Name: {row['name']}")
    print(f"  Director: {row['director']}")

# Sc√®nes avec URLs
cur.execute("""
    SELECT s.title, su.url
    FROM groups_scenes gs
    JOIN scenes s ON s.id = gs.scene_id
    LEFT JOIN scene_urls su ON su.scene_id = s.id
    WHERE gs.group_id=?
    LIMIT 5
""", (group_id,))
for row in cur.fetchall():
    print(f"  Sc√®ne: {row['title']} ‚Üí {row['url']}")

conn.close()

============================================================
[5/83] config\__init__.py
------------------------------------------------------------


============================================================
[6/83] config\settings.yaml
------------------------------------------------------------
# Configuration de StashMaster V2
phase1_fields:
  - Name
  - Disambiguation
  - Aliases
  - Birthdate
  - Deathdate
  - Country
  - Ethnicity
  - Hair Color
  - Eye Color
  - Height
  - Weight
  - Measurements
  - Fake Tits
  - Career Length
phase2_fields:
  - Bio
  - Trivia
  - Awards
  - Tattoos
  - Piercings
  - Tags
  - URLs
  - Details

# ‚îÄ‚îÄ Groups (DVDs) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
group_phase1_fields:
  - Title
  - Aliases
  - Date
  - Studio
  - Director
  - Duration
  - Description
  - Tags
  - URLs

group_sources_priority:
  - data18          # P1 ‚Äî tags, sc√®nes, synopsis d√©taill√©s
  - iafd_dvd        # P2 ‚Äî r√©f√©rence US/classiques, cast, dates fiables
  - adultdvdempire  # P3 ‚Äî synopsis, covers
  - jeedoo          # P4 ‚Äî productions europ√©ennes uniquement

group_phase2_sources:  # sources avec URLs de sc√®nes individuelles
  - data18
  - adultdvdempire


============================================================
[7/83] Document sans titre.md
------------------------------------------------------------
![][image1]

[image1]: <data:image/png;base64,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>

============================================================
[8/83] gui\__init__.py
------------------------------------------------------------


============================================================
[9/83] gui\app.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
import sv_ttk
from gui.performer_frame import PerformerFrame
# Placeholders for future modules
from gui.group_frame import GroupFrame
# from gui.scene_frame import SceneFrame

def launch_app(module, stash_id):
    root = tk.Tk()
    root.title(f"StashMaster V2 - {module}")
    
    # Maximiser la fen√™tre au d√©marrage (Windows)
    try:
        root.state('zoomed')
    except:
        # Fallback pour d'autres OS (Linux/Mac)
        w, h = root.winfo_screenwidth(), root.winfo_screenheight()
        root.geometry(f"{w}x{h}+0+0")

    # Appliquer le th√®me moderne
    sv_ttk.set_theme("dark")

    if module == "Performer":
        frame = PerformerFrame(root, stash_id)
    elif module == "Group":
        frame = GroupFrame(root, stash_id)
    # elif module == "Scene":
    #     frame = SceneFrame(root, stash_id)
    else:
        import tkinter.messagebox as mb
        mb.showerror("Erreur", f"Module inconnu: {module}")
        root.destroy()
        return
    frame.pack(fill=tk.BOTH, expand=True)
    root.mainloop()


============================================================
[10/83] gui\bio_wizard.py
------------------------------------------------------------
"""
BioWizard - Fen√™tre d√©di√©e √† la g√©n√©ration de biographies en 3 √©tapes.
1. G√©n√©ration Google Gemini
2. Affinage Ollama
3. Validation et injection
"""
import tkinter as tk
from tkinter import ttk, messagebox
import threading

from services.bio_generator import BioGenerator

class BioWizard(tk.Toplevel):
    def __init__(self, parent, db_data, stash_ctx, merged_data, scraped_results, checked_fields):
        super().__init__(parent)
        self.title("Assistant de G√©n√©ration de Biographie IA")
        self.geometry("1000x750")
        self.minsize(800, 600)

        self.transient(parent)
        self.grab_set()

        # Stockage
        self.db_data = db_data
        self.stash_ctx = stash_ctx
        self.merged_data = merged_data
        self.scraped_results = scraped_results
        self.checked_fields = checked_fields
        self.final_bio = None  # Le r√©sultat final sera stock√© ici

        self._build_ui()
        self.wait_window()

    def _build_ui(self):
        # Barre de progression
        self.progress_frame = ttk.Frame(self)
        self.progress_label = ttk.Label(self.progress_frame, text="Pr√™t", font=("Segoe UI", 9, "italic"))
        self.progress_label.pack(side=tk.LEFT, padx=10)
        self.progress_bar = ttk.Progressbar(self.progress_frame, mode="indeterminate", length=200)
        self.progress_bar.pack(side=tk.LEFT, padx=5)
        
        self.notebook = ttk.Notebook(self)
        self.notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)

        self.tab1_google = ttk.Frame(self.notebook, padding=10)
        self.tab2_ollama = ttk.Frame(self.notebook, padding=10)
        self.tab3_validate = ttk.Frame(self.notebook, padding=10)

        self.notebook.add(self.tab1_google, text="√âtape 1 : Google Gemini")
        self.notebook.add(self.tab2_ollama, text="√âtape 2 : Affinage Ollama", state="disabled")
        self.notebook.add(self.tab3_validate, text="√âtape 3 : Validation", state="disabled")

        self._create_google_tab()
        self._create_ollama_tab()
        self._create_validate_tab()

    def _create_google_tab(self):
        frame = self.tab1_google
        frame.grid_columnconfigure(0, weight=1)
        frame.grid_rowconfigure(1, weight=1)
        
        action_frame = ttk.Frame(frame)
        action_frame.grid(row=0, column=0, sticky=tk.NSEW, pady=(0, 10))
        
        self.btn_gen_gemini = ttk.Button(action_frame, text="üöÄ Lancer la g√©n√©ration Gemini", command=self._run_gemini_generation)
        self.btn_gen_gemini.pack(side=tk.LEFT)
        
        self.btn_copy_to_ollama = ttk.Button(action_frame, text="Continuer vers l'√©tape 2 ‚û°", state=tk.DISABLED, command=self._copy_to_ollama)
        self.btn_copy_to_ollama.pack(side=tk.LEFT, padx=10)

        self.txt_gemini_result = tk.Text(frame, wrap=tk.WORD, font=("Segoe UI", 10))
        self.txt_gemini_result.grid(row=1, column=0, sticky=tk.NSEW)
        self.txt_gemini_result.insert("1.0", "Cliquez sur 'Lancer la g√©n√©ration' pour cr√©er une biographie avec Google Gemini...")
        self.txt_gemini_result.config(state=tk.DISABLED)

    def _create_ollama_tab(self):
        frame = self.tab2_ollama
        frame.grid_columnconfigure(1, weight=1)
        frame.grid_rowconfigure(2, weight=1)
        
        action_frame = ttk.Frame(frame)
        action_frame.grid(row=0, column=0, columnspan=2, sticky=tk.NSEW, pady=(0, 10))
        self.btn_refine_ollama = ttk.Button(action_frame, text="‚öôÔ∏è Lancer l'affinage Ollama", command=self._run_ollama_refinement)
        self.btn_refine_ollama.pack(side=tk.LEFT)
        
        self.btn_copy_to_validate = ttk.Button(action_frame, text="Continuer vers la validation ‚û°", state=tk.DISABLED, command=self._copy_to_validation)
        self.btn_copy_to_validate.pack(side=tk.LEFT, padx=10)

        ttk.Label(frame, text="Biographie de base (Gemini)").grid(row=1, column=0, columnspan=2, sticky=tk.W)
        self.txt_ollama_input = tk.Text(frame, wrap=tk.WORD, height=8, font=("Segoe UI", 10), state=tk.DISABLED, relief=tk.SUNKEN)
        self.txt_ollama_input.grid(row=2, column=0, columnspan=2, sticky=tk.NSEW, pady=(0, 10))

        ttk.Label(frame, text="Biographie affin√©e (Ollama)").grid(row=3, column=0, columnspan=2, sticky=tk.W)
        self.txt_ollama_result = tk.Text(frame, wrap=tk.WORD, font=("Segoe UI", 10))
        self.txt_ollama_result.grid(row=4, column=0, columnspan=2, sticky=tk.NSEW)

    def _create_validate_tab(self):
        frame = self.tab3_validate
        frame.grid_columnconfigure(0, weight=1)
        frame.grid_rowconfigure(1, weight=1)
        
        action_frame = ttk.Frame(frame)
        action_frame.grid(row=0, column=0, sticky=tk.NSEW, pady=(0, 10))
        self.btn_inject = ttk.Button(action_frame, text="‚úÖ Valider et Utiliser cette Bio", command=self._inject_bio)
        self.btn_inject.pack(side=tk.LEFT)

        self.txt_final_bio = tk.Text(frame, wrap=tk.WORD, font=("Segoe UI", 10))
        self.txt_final_bio.grid(row=1, column=0, sticky=tk.NSEW)

    def _run_gemini_generation(self):
        self._show_progress("G√©n√©ration Gemini en cours...")
        self.btn_gen_gemini.config(state=tk.DISABLED)
        self.btn_copy_to_ollama.config(state=tk.DISABLED)

        def _do_generate():
            try:
                bio_gen = BioGenerator()
                ctx = bio_gen.build_context_from_v2(self.db_data, self.stash_ctx, self.scraped_results, self.merged_data, self.checked_fields)
                gemini_bio = bio_gen.generate_gemini_bio(ctx)

                def _update_ui():
                    self.txt_gemini_result.config(state=tk.NORMAL)
                    self.txt_gemini_result.delete("1.0", tk.END)
                    if gemini_bio:
                        self.txt_gemini_result.insert("1.0", gemini_bio)
                        self.btn_copy_to_ollama.config(state=tk.NORMAL)
                    else:
                        self.txt_gemini_result.insert("1.0", "La g√©n√©ration Gemini a √©chou√©. V√©rifiez la console pour les erreurs (cl√© API, etc.).")
                    self.txt_gemini_result.config(state=tk.DISABLED)
                    self.btn_gen_gemini.config(state=tk.NORMAL)

                self.after(0, _update_ui)
            except Exception as e:
                self.after(0, lambda: messagebox.showerror("Erreur Gemini", str(e)))
            finally:
                self.after(0, self._hide_progress)
        
        threading.Thread(target=_do_generate, daemon=True).start()

    def _copy_to_ollama(self):
        gemini_text = self.txt_gemini_result.get("1.0", tk.END)
        self.txt_ollama_input.config(state=tk.NORMAL)
        self.txt_ollama_input.delete("1.0", tk.END)
        self.txt_ollama_input.insert("1.0", gemini_text)
        self.txt_ollama_input.config(state=tk.DISABLED)
        self.txt_ollama_result.delete("1.0", tk.END) # Clear previous results
        self.notebook.tab(1, state="normal")
        self.notebook.select(self.tab2_ollama)

    def _run_ollama_refinement(self):
        gemini_bio = self.txt_ollama_input.get("1.0", tk.END).strip()
        if not gemini_bio or "√©chou√©" in gemini_bio:
            messagebox.showwarning("Bio de base manquante", "La biographie de base (Gemini) est n√©cessaire pour l'affinage.")
            return

        self._show_progress("Affinage Ollama en cours...")
        self.btn_refine_ollama.config(state=tk.DISABLED)
        self.btn_copy_to_validate.config(state=tk.DISABLED)

        def _do_refine():
            try:
                bio_gen = BioGenerator()
                ctx = bio_gen.build_context_from_v2(self.db_data, self.stash_ctx, self.scraped_results, self.merged_data, self.checked_fields)
                ollama_bio = bio_gen.generate_ollama_bio(ctx, gemini_bio)

                def _update_ui():
                    self.txt_ollama_result.delete("1.0", tk.END)
                    if ollama_bio:
                        self.txt_ollama_result.insert("1.0", ollama_bio)
                        self.btn_copy_to_validate.config(state=tk.NORMAL)
                    else:
                         self.txt_ollama_result.insert("1.0", "L'affinage Ollama a √©chou√©. V√©rifiez que le serveur Ollama est bien lanc√©.")
                
                self.after(0, _update_ui)
            except Exception as e:
                self.after(0, lambda: messagebox.showerror("Erreur Ollama", str(e)))
            finally:
                self.after(0, self._hide_progress)
                self.after(0, self.btn_refine_ollama.config, {'state': tk.NORMAL})
        
        threading.Thread(target=_do_refine, daemon=True).start()
    
    def _copy_to_validation(self):
        ollama_text = self.txt_ollama_result.get("1.0", tk.END)
        self.txt_final_bio.delete("1.0", tk.END)
        self.txt_final_bio.insert("1.0", ollama_text)
        self.notebook.tab(2, state="normal")
        self.notebook.select(self.tab3_validate)

    def _inject_bio(self):
        """Stocke la bio finale et ferme la fen√™tre."""
        self.final_bio = self.txt_final_bio.get("1.0", tk.END).strip()
        self.destroy()

    def _show_progress(self, message):
        self.progress_label.config(text=message)
        self.progress_frame.pack(fill=tk.X, padx=10, pady=5, before=self.notebook)
        self.progress_bar.start(10)

    def _hide_progress(self):
        self.progress_bar.stop()
        self.progress_frame.pack_forget()


============================================================
[11/83] gui\group_frame.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
from gui.group_phase1 import GroupPhase1Frame
from gui.group_phase2 import GroupPhase2Frame

class GroupFrame(ttk.Frame):
    def __init__(self, parent, stash_id):
        super().__init__(parent)
        self.stash_id = stash_id
        self.current_frame = None
        
        # For now, just a label
        label = ttk.Label(self, text=f"Group Frame for ID: {self.stash_id}")
        label.pack(pady=20, padx=20)

        self.goto_phase1()

    def goto_phase1(self):
        if self.current_frame:
            self.current_frame.destroy()
        
        self.current_frame = GroupPhase1Frame(self, self, self.stash_id)
        self.current_frame.pack(fill=tk.BOTH, expand=True)

    def goto_phase2(self, group_data, scenes_data):
        if self.current_frame:
            self.current_frame.destroy()
            
        self.current_frame = GroupPhase2Frame(self, self, self.stash_id, group_data, scenes_data)
        self.current_frame.pack(fill=tk.BOTH, expand=True)

    def return_to_menu(self):
        # Fermer la fen√™tre actuelle
        self.master.destroy()
        # Relancer le launcher
        try:
            from gui.launcher import start_launcher
            start_launcher()
        except Exception as e:
            print(f"Erreur lors du retour au menu: {e}")


============================================================
[12/83] gui\group_phase1.py
------------------------------------------------------------
"""
Placeholder for Group Phase 1 Frame.
The content for this file is in PLAN_GROUPS_V2.md, which was not provided.
"""
import tkinter as tk
from tkinter import ttk, messagebox
import threading
import yaml

from services.db import GroupDB
from services.group_phase1_scraper import GroupPhase1ScraperService
from services.group_phase1_merger import GroupPhase1Merger
from gui.phase1_conflict_dialog import Phase1ConflictDialog
from services.phase2_scraper import Phase2ScraperService


class GroupPhase1Frame(ttk.Frame):
    def __init__(self, parent, controller, group_id):
        super().__init__(parent)
        self.controller = controller
        self.group_id = group_id

        self._group_data = None
        self._scenes_data = [] # Scenes associ√©es au group

        self.field_checkboxes = {}
        self.fields = {}

        self.create_ui()
        self._load_data()

    def create_ui(self):
        # Header
        header_frame = ttk.Frame(self)
        header_frame.pack(fill=tk.X, padx=10, pady=5)
        ttk.Label(header_frame, text=f"Group ID: {self.group_id}",
                  font=("Segoe UI", 14, "bold")).pack(side=tk.LEFT)
        ttk.Button(header_frame, text="Retour Launcher",
                   command=self.controller.return_to_menu).pack(side=tk.RIGHT)

        # ScrolledFrame pour les champs du Group
        self.main_canvas = tk.Canvas(self, highlightthickness=0)
        self.main_scrollbar = ttk.Scrollbar(self, orient=tk.VERTICAL, command=self.main_canvas.yview)
        self.main_scrollable_frame = ttk.Frame(self.main_canvas)

        self.main_scrollable_frame.bind(
            "<Configure>",
            lambda e: self.main_canvas.configure(
                scrollregion=self.main_canvas.bbox("all")
            )
        )
        self.main_canvas.create_window((0, 0), window=self.main_scrollable_frame, anchor="nw")
        self.main_canvas.configure(yscrollcommand=self.main_scrollbar.set)

        self.main_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.main_canvas.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Section Group Details
        group_details_frame = ttk.LabelFrame(self.main_scrollable_frame, text="Group Details (Phase 1)", padding=10)
        group_details_frame.pack(fill=tk.X, padx=10, pady=5)

        # TODO: Charger les champs depuis settings.yaml
        self.fields_list = [
            "Title", "Aliases", "Date", "Studio", "Director",
            "Duration", "Description", "Tags", "URLs"
        ]

        for i, field in enumerate(self.fields_list):
            row = i
            var = tk.BooleanVar(value=True)
            self.field_checkboxes[field] = var
            checkbox = ttk.Checkbutton(group_details_frame, variable=var, text="")
            checkbox.grid(row=row, column=0, sticky=tk.W, padx=(5, 0), pady=2)

            ttk.Label(group_details_frame, text=f"{field}:").grid(row=row, column=1, sticky=tk.W, padx=5, pady=2)
            entry = tk.Text(group_details_frame, height=2, width=60, font=("Segoe UI", 9))
            entry.grid(row=row, column=2, sticky=tk.EW, padx=5, pady=2)
            self.fields[field] = entry

        group_details_frame.grid_columnconfigure(2, weight=1)

        # Boutons d'action
        action_frame = ttk.Frame(self.main_scrollable_frame)
        action_frame.pack(fill=tk.X, padx=10, pady=5)
        ttk.Button(action_frame, text="üîé Analyser & Phase 2", command=self._run_phase1).pack(side=tk.LEFT, padx=5)

        # Section Sc√®nes Associ√©es
        self.scenes_frame = ttk.LabelFrame(self.main_scrollable_frame, text="Sc√®nes Associ√©es au Group", padding=10)
        self.scenes_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        ttk.Label(self.scenes_frame, text="Chargement des sc√®nes...", font=("Segoe UI", 9, "italic")).pack()

    def _load_data(self):
        try:
            gid = int(self.group_id)
        except ValueError:
            gid = self.group_id

        db = GroupDB()
        self._group_data = db.get_group_by_id(gid)
        self._scenes_data = db.get_group_scenes(gid)
        db.close()

        print(f"DEBUG: Loaded group data for ID {gid}: {bool(self._group_data)}")
        print(f"DEBUG: Loaded {len(self._scenes_data)} scenes")

        if self._group_data:
            for field_name, entry_widget in self.fields.items():
db_key="***MASKED***"
                value = self._group_data.get(db_key)
                if field_name == "Studio" and self._group_data.get("studio_name"):
                    value = self._group_data["studio_name"]
                elif field_name == "Tags" and value:
                    value = ", ".join(value)
                elif field_name == "URLs" and value:
                    value = "\n".join(value)

                if value:
                    entry_widget.config(state=tk.NORMAL)
                    entry_widget.delete("1.0", tk.END)
                    entry_widget.insert("1.0", str(value))
                    entry_widget.config(state=tk.DISABLED)

        self._display_scenes()

    def _display_scenes(self):
        for widget in self.scenes_frame.winfo_children():
            widget.destroy()

        if not self._scenes_data:
            ttk.Label(self.scenes_frame, text="Aucune sc√®ne associ√©e.", font=("Segoe UI", 9, "italic")).pack()
            return

        # Treeview pour les sc√®nes
        columns = ("index", "title", "urls")
        tree = ttk.Treeview(self.scenes_frame, columns=columns, show="headings", height=8)
        tree.heading("index", text="#")
        tree.heading("title", text="Titre Stash")
        tree.heading("urls", text="URLs Existantes")
        
        tree.column("index", width=30, anchor=tk.CENTER)
        tree.column("title", width=300, anchor=tk.W)
        tree.column("urls", width=400, anchor=tk.W)

        for s in self._scenes_data:
            urls_str = ", ".join(s.get("existing_urls", []))
            tree.insert("", tk.END, values=(
                s.get("scene_index", "?"),
                s.get("scene_title", "Sans titre"),
                urls_str
            ))
        
        tree.pack(fill=tk.BOTH, expand=True)

    def _run_phase1(self):
        """Lance le scraping Phase 1 Group."""
        checked = [f for f, var in self.field_checkboxes.items() if var.get()]
        if not checked:
            messagebox.showwarning("Attention", "Aucun champ coch√©.")
            return

        # Afficher progression
        progress_popup = tk.Toplevel(self)
        progress_popup.title("Scraping Group Phase 1...")
        progress_popup.geometry("300x100")
        prog_label = ttk.Label(progress_popup, text="Initialisation...")
        prog_label.pack(pady=20)

        def _do():
            try:
                scraper = GroupPhase1ScraperService()
                title = self.fields["Title"].get("1.0", tk.END).strip()
                year = self.fields["Date"].get("1.0", tk.END).strip()[:4] # Ann√©e
                known_urls = self.fields["URLs"].get("1.0", tk.END).strip().split("\n")
                known_urls = [u.strip() for u in known_urls if u.strip()]

                def update_prog(src, st):
                    self.after(0, lambda: prog_label.config(text=f"[{src}] {st}"))

                scraped = scraper.scrape(title, year, known_urls, progress_callback=update_prog)
                
                # Check for Data18
                has_data18 = any(r.get("_source") == "data18" for r in scraped)
                
                if not has_data18:
                    self.after(0, lambda: self._ask_data18_and_continue(scraped, checked, scraper, progress_popup))
                else:
                    self.after(0, lambda: self._finish_phase1(scraped, checked, progress_popup))

            except Exception as e:
                self.after(0, lambda: [progress_popup.destroy(), messagebox.showerror("Erreur", str(e))])

        threading.Thread(target=_do, daemon=True).start()

    def _ask_data18_and_continue(self, scraped, checked, scraper, progress_popup):
        from tkinter import simpledialog
        # Cacher la popup de progression temporairement
        progress_popup.withdraw()
        
        url = simpledialog.askstring(
            "Data18 Manquant", 
            "Data18 n'a pas √©t√© trouv√© automatiquement.\n\n"
            "Pour avoir les meilleurs r√©sultats (Tags, Sc√®nes...), collez l'URL Data18 ici :\n"
            "(Sinon, laissez vide et OK pour continuer)",
            parent=self
        )
        
        progress_popup.deiconify()
        
        if url and "data18.com" in url:
            # Relancer un petit thread pour scraper cette URL
            def _scrape_extra():
                try:
                    from services.extractors.dvd.data18_dvd import Data18DVDExtractor
                    e = Data18DVDExtractor()
                    res = e.extract_from_url(url.strip())
                    if res:
                        scraped.append(res)
                except Exception as e:
                    print(f"Error scraping extra URL: {e}")
                
                self.after(0, lambda: self._finish_phase1(scraped, checked, progress_popup))
            
            threading.Thread(target=_scrape_extra, daemon=True).start()
        else:
            self._finish_phase1(scraped, checked, progress_popup)

    def _finish_phase1(self, scraped, checked, progress_popup):
        progress_popup.destroy()
        merger = GroupPhase1Merger()
        merged = merger.merge(self._group_data, scraped, checked)
        self._show_conflict_dialog(merged, checked)

    def _show_conflict_dialog(self, merged_data: dict, checked_fields: list[str]):
        from services.group_phase1_merger import GROUP_FIELDS
        group_title = self.fields["Title"].get("1.0", tk.END).strip()
        dialog = Phase1ConflictDialog(self, group_title, merged_data, GROUP_FIELDS)
        if dialog.result:
            self._inject_phase1(dialog.result)

    def _inject_phase1(self, result: dict):
        try:
            # Pour Group Phase 1, on injecte directement via GroupDB (√† impl√©menter ou simuler)
            # On r√©utilise les champs mapp√©s
            db_updates = {}
            from services.group_phase1_merger import GROUP_FIELDS
            for field, value in result.items():
db_key="***MASKED***"
                if db_key:
                    db_updates[db_key] = value

            # Simulation d'injection (ou ajout de la m√©thode dans GroupDB)
            print(f"DEBUG: Injecting group updates: {db_updates}")
            
            # TODO: Impl√©menter GroupDB.update_group(self.group_id, db_updates)
            
            messagebox.showinfo("‚úÖ Phase 1 Termin√©e", "Les donn√©es du Group ont √©t√© mises √† jour.")
            
            # Passer √† la Phase 2
            self.controller.goto_phase2(self._group_data, self._scenes_data)

        except Exception as e:
            messagebox.showerror("Erreur Injection", str(e))




============================================================
[13/83] gui\group_phase2.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, messagebox
import threading

from services.db import GroupDB
from services.group_phase2_scraper import GroupPhase2ScraperService
from services.group_phase2_merger import GroupPhase2Merger

STATUS_ICONS = {
    "new":         "üü¢",
    "partial":     "üü†",
    "already_present": "üîµ",
    "no_match":    "‚ö™",
}

class GroupPhase2Frame(ttk.Frame):
    def __init__(self, parent, controller, group_id, group_data, scenes_data):
        super().__init__(parent)
        self.controller = controller
        self.group_id = group_id
        self._group_data = group_data # Donn√©es Group d√©j√† scrap√©es/fusionn√©es Phase 1
        self._scenes_data = scenes_data # Liste des sc√®nes Stash du Group
        self._merged_scene_urls = [] # R√©sultat de la fusion Phase 2

        self.scene_checkboxes = [] # Pour cocher les URLs √† injecter

        self.create_ui()
        self._run_phase2()

    def create_ui(self):
        # Header
        header_frame = ttk.Frame(self)
        header_frame.pack(fill=tk.X, padx=10, pady=5)
        ttk.Label(header_frame, text=f"Group ID: {self.group_id} ‚Äî Phase 2: URLs Sc√®nes",
                  font=("Segoe UI", 14, "bold")).pack(side=tk.LEFT)
        ttk.Button(header_frame, text="Retour Phase 1",
                   command=lambda: self.controller.goto_phase1()).pack(side=tk.RIGHT)

        # Progress bar (similaire √† Performer Phase 2)
        self.progress_frame = ttk.Frame(self)
        self.progress_label = ttk.Label(self.progress_frame, text="", 
                                        font=("Segoe UI", 9, "italic"))
        self.progress_label.pack(side=tk.LEFT, padx=10)
        self.progress_bar = ttk.Progressbar(self.progress_frame, mode="indeterminate", length=200)
        self.progress_bar.pack(side=tk.LEFT, padx=5)

        # Zone principale scrollable pour les sc√®nes
        self.main_canvas = tk.Canvas(self, highlightthickness=0)
        self.main_scrollbar = ttk.Scrollbar(self, orient=tk.VERTICAL, command=self.main_canvas.yview)
        self.main_scrollable_frame = ttk.Frame(self.main_canvas)

        self.main_scrollable_frame.bind(
            "<Configure>",
            lambda e: self.main_canvas.configure(
                scrollregion=self.main_canvas.bbox("all")
            )
        )
        self.main_canvas_window = self.main_canvas.create_window((0, 0), window=self.main_scrollable_frame, anchor="nw")
        self.main_canvas.configure(yscrollcommand=self.main_scrollbar.set)
        
        # Mousewheel scrolling
        self.main_canvas.bind_all("<MouseWheel>",
                             lambda e: self.main_canvas.yview_scroll(int(-1 * (e.delta / 120)), "units"))
        
        # Resize canvas window width to match canvas width
        self.main_canvas.bind("<Configure>",
                         lambda e: self.main_canvas.itemconfig(self.main_canvas_window, width=e.width))


        self.main_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.main_canvas.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Boutons d'action
        action_frame = ttk.Frame(self)
        action_frame.pack(fill=tk.X, padx=10, pady=5)
        ttk.Button(action_frame, text="‚úî Injecter s√©lectionn√©es", command=self._inject).pack(side=tk.LEFT, padx=5)
        ttk.Button(action_frame, text="Tout s√©lectionner", command=self._select_all).pack(side=tk.LEFT, padx=5)
        ttk.Button(action_frame, text="D√©s√©lectionner tout", command=self._deselect_all).pack(side=tk.LEFT, padx=5)

    def _run_phase2(self):
        self._show_progress("Scraping URLs de sc√®nes...")

        def _do_scraping_and_merge():
            try:
                scraper = GroupPhase2ScraperService()
                
                def update_prog(src, st):
                    self.after(0, lambda: self._update_progress(f"[{src}] {st}"))

                scraped_urls_by_index = scraper.scrape(
                    group_data=self._group_data,
                    progress_callback=update_prog
                )

                merger = GroupPhase2Merger()
                self._merged_scene_urls = merger.merge(
                    self._scenes_data, scraped_urls_by_index)

                self.after(0, self._display_results)

            except Exception as e:
                self.after(0, lambda: messagebox.showerror("Erreur Phase 2", str(e)))
            finally:
                self.after(0, self._hide_progress)

        threading.Thread(target=_do_scraping_and_merge, daemon=True).start()

    def _display_results(self):
        for widget in self.main_scrollable_frame.winfo_children():
            widget.destroy()

        if not self._merged_scene_urls:
            ttk.Label(self.main_scrollable_frame, text="Aucune URL de sc√®ne trouv√©e ou fusionn√©e.",
                      font=("Segoe UI", 10, "italic")).pack(padx=10, pady=10)
            return

        # Headers du tableau
        header_frame = ttk.Frame(self.main_scrollable_frame)
        header_frame.pack(fill=tk.X, padx=5, pady=2)
        ttk.Label(header_frame, text="", width=4).pack(side=tk.LEFT) # Checkbox
        ttk.Label(header_frame, text="Statut", width=8, font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
        ttk.Label(header_frame, text="Index", width=6, font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
        ttk.Label(header_frame, text="Titre Stash", width=40, anchor=tk.W, font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
        ttk.Label(header_frame, text="Nouvelles URLs", anchor=tk.W, font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT, expand=True, fill=tk.X)

        self.scene_checkboxes = []
        for scene_data in self._merged_scene_urls:
            self._build_scene_row(self.main_scrollable_frame, scene_data)

    def _build_scene_row(self, parent_frame, scene_data):
        row_frame = ttk.Frame(parent_frame, padding=2)
        row_frame.pack(fill=tk.X, padx=5, pady=1)

        status_icon = STATUS_ICONS.get(scene_data["status"], "")

        # Cocher par d√©faut si nouvelles URLs (statut 'new' ou 'partial')
        has_new = bool(scene_data.get("new_urls"))
        var = tk.BooleanVar(value=has_new) 
        self.scene_checkboxes.append((var, scene_data)) # Stocker tuple (var, data)
        
        chk = ttk.Checkbutton(row_frame, variable=var)
        chk.pack(side=tk.LEFT)
        if not has_new:
            chk.config(state=tk.DISABLED)

        ttk.Label(row_frame, text=status_icon, width=4).pack(side=tk.LEFT)
        ttk.Label(row_frame, text=str(scene_data.get("scene_index", "?")), width=6).pack(side=tk.LEFT)
        ttk.Label(row_frame, text=scene_data.get("scene_title", "Sans titre"), width=40, anchor=tk.W).pack(side=tk.LEFT)

        urls_frame = ttk.Frame(row_frame)
        urls_frame.pack(side=tk.LEFT, expand=True, fill=tk.X)

        new_urls = scene_data.get("new_urls", {})
        existing = scene_data.get("existing_urls", [])

        if new_urls:
            for src, url in new_urls.items():
                ttk.Label(urls_frame, text=f"‚ûï {src.upper()}: {url}", anchor=tk.W, font=("Segoe UI", 8, "bold"), foreground="green").pack(fill=tk.X)
        
        if existing:
            count = len(existing)
            ttk.Label(urls_frame, text=f"Existing: {count} URLs", anchor=tk.W, foreground="gray", font=("Segoe UI", 8)).pack(fill=tk.X)
        
        if not new_urls and not existing:
             ttk.Label(urls_frame, text="‚Äî", anchor=tk.W, foreground="gray", font=("Segoe UI", 8)).pack(fill=tk.X)

    def _inject(self):
        selected_urls_to_inject = []
        for var, scene_data in self.scene_checkboxes:
            if var.get():
                for url_src, url_val in scene_data["new_urls"].items():
                    selected_urls_to_inject.append({
                        "scene_id": scene_data["scene_id"],
                        "url": url_val,
                        "source": url_src,
                    })
        
        if not selected_urls_to_inject:
            messagebox.showwarning("Attention", "Aucune URL s√©lectionn√©e √† injecter.")
            return

        try:
            db = GroupDB()
            # appel √† la m√©thode inject_scene_urls que j'ai ajout√©e dans db.py
            db.inject_scene_urls(selected_urls_to_inject) 
            db.close()
            
            messagebox.showinfo("‚úÖ Injection Phase 2", f"{len(selected_urls_to_inject)} URLs de sc√®nes inject√©es.")

            # Optionnel : Recharger ou fermer
            # self._run_phase2() 

        except Exception as e:
            messagebox.showerror("Erreur injection", str(e))

    def _select_all(self):
        for var, _ in self.scene_checkboxes:
            if str(var['state']) != tk.DISABLED:
                var.set(True)

    def _deselect_all(self):
        for var, _ in self.scene_checkboxes:
            var.set(False)

    def _show_progress(self, message):
        self.progress_frame.pack(fill=tk.X, padx=10, pady=2, before=self.main_canvas)
        self.progress_bar.start(10)
        self.progress_label.config(text=message)

    def _update_progress(self, message):
        self.progress_label.config(text=message)

    def _hide_progress(self):
        self.progress_bar.stop()
        self.progress_frame.pack_forget()


============================================================
[14/83] gui\launcher.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, simpledialog, messagebox
import sv_ttk
from gui.app import launch_app

def center_window(window, width=400, height=300):
    screen_width = window.winfo_screenwidth()
    screen_height = window.winfo_screenheight()
    x = (screen_width - width) // 2
    y = (screen_height - height) // 2
    window.geometry(f'{width}x{height}+{x}+{y}')

def start_launcher():
    root = tk.Tk()
    root.title("StashMaster V2 - Launcher")
    
    # Appliquer le th√®me moderne sombre
    sv_ttk.set_theme("dark")
    
    center_window(root, 400, 250)
    
    ttk.Label(root, text="StashMaster V2", font=("Segoe UI", 16, "bold")).pack(pady=(20, 10))
    ttk.Label(root, text="S√©lectionnez un module :", font=("Segoe UI", 10)).pack(pady=5)
    
    def on_select(module):
        root.withdraw()
        # Utiliser un prompt simple pour l'ID
        stash_id = simpledialog.askstring("Entrer l'ID", f"Entrez l'ID pour le module {module} :", parent=root)
        if not stash_id:
            messagebox.showwarning("ID requis", "Vous devez entrer un ID valide pour continuer.")
            root.deiconify()
            return
        root.destroy()
        # Lancer l'application principale maximis√©e
        launch_app(module, stash_id)
        
    ttk.Button(root, text="Performer", width=25, command=lambda: on_select("Performer")).pack(pady=5)
    ttk.Button(root, text="Group / DVD", width=25, command=lambda: on_select("Group")).pack(pady=5)
    ttk.Button(root, text="Scene", width=25, command=lambda: on_select("Scene")).pack(pady=5)
    
    root.mainloop()

if __name__ == "__main__":
    start_launcher()

============================================================
[15/83] gui\performer_base.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk

class PerformerBaseFrame(ttk.Frame):
    def __init__(self, parent, controller, stash_id):
        super().__init__(parent)
        self.controller = controller
        self.stash_id = stash_id
        self.fields = {}
        self.field_checkboxes = {}
        # To be defined in subclasses
        self.fields_list = [] 
        self.db_mapping = {}

    def create_header(self, title, buttons_config):
        bar = ttk.Frame(self, padding=5)
        bar.pack(fill=tk.X)
        
        ttk.Label(bar, text=f"{title} | ID: {self.stash_id}", font=("Segoe UI", 12, "bold")).pack(side=tk.LEFT, padx=5)
        
        btn_frame = ttk.Frame(bar)
        btn_frame.pack(side=tk.RIGHT, padx=5)
        
        for text, command in buttons_config:
            ttk.Button(btn_frame, text=text, command=command).pack(side=tk.LEFT, padx=2)

    def select_all_fields(self):
        for var in self.field_checkboxes.values():
            var.set(True)

    def select_empty_fields(self):
        for field, entry in self.fields.items():
            if field in self.field_checkboxes and entry:
                val = ""
                if isinstance(entry, tk.Entry):
                    val = entry.get().strip()
                elif isinstance(entry, tk.Text):
                    val = entry.get("1.0", tk.END).strip()
                
                if not val:
                    self.field_checkboxes[field].set(True)
                else:
                    self.field_checkboxes[field].set(False)

    def load_data(self):
        try:
            from services.db import PerformerDB
            db = PerformerDB()
            data = db.get_performer_by_id(self.stash_id)
            db.close()
        except Exception as e:
            print(f"Erreur DB: {e}")
            data = None
        
        if not data:
            return

        for field, db_key in self.db_mapping.items():
            entry = self.fields.get(field)
            if entry and db_key in data:
                value = data[db_key]
                if isinstance(value, (list, tuple)):
                    if field == "URLs":
                        value = "\n".join(value)
                    else:
                        value = ", ".join(value)
                
                if isinstance(entry, tk.Entry):
                    entry.delete(0, tk.END)
                    entry.insert(0, str(value) if value is not None else "")
                elif isinstance(entry, tk.Text):
                    entry.delete('1.0', tk.END)
                    entry.insert('1.0', str(value) if value is not None else "")


============================================================
[16/83] gui\performer_frame.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
from gui.performer_phase1 import PerformerPhase1Frame
from gui.performer_phase2 import PerformerPhase2Frame

class PerformerFrame(ttk.Frame):
    def __init__(self, parent, stash_id):
        super().__init__(parent)
        self.stash_id = stash_id
        self.current_frame = None
        self.phase1_data = {} # Pour stocker les donn√©es r√©solues de la phase 1
        
        self.goto_phase1()

    def goto_phase1(self):
        if self.current_frame:
            self.current_frame.destroy()
        
        self.current_frame = PerformerPhase1Frame(self, self, self.stash_id)
        self.current_frame.pack(fill=tk.BOTH, expand=True)

    def goto_phase2(self, phase1_updates: dict):
        """Passe √† la phase 2 en emportant les donn√©es r√©solues de la phase 1."""
        self.phase1_data = phase1_updates

        if self.current_frame:
            self.current_frame.destroy()
            
        # Initialise la phase 2 avec les donn√©es de la phase 1
        self.current_frame = PerformerPhase2Frame(self, self, self.stash_id, self.phase1_data)
        self.current_frame.pack(fill=tk.BOTH, expand=True)




    def return_to_menu(self):
        # Fermer la fen√™tre actuelle
        self.master.destroy()
        # Relancer le launcher
        try:
            from gui.launcher import start_launcher
            start_launcher()
        except Exception as e:
            print(f"Erreur lors du retour au menu: {e}")




============================================================
[17/83] gui\performer_phase1.py
------------------------------------------------------------
import tkinter as tk
from tkinter import messagebox
from tkinter import ttk
import threading

from gui.performer_base import PerformerBaseFrame
from gui.phase1_conflict_dialog import Phase1ConflictDialog


class PerformerPhase1Frame(PerformerBaseFrame):
    def __init__(self, parent, controller, stash_id):
        super().__init__(parent, controller, stash_id)
        
        # Configuration des champs Phase 1
        self.fields_list = [
            "Name", "Aliases", "Birthdate", "Deathdate", "Country", "Ethnicity",
            "Hair Color", "Eye Color", "Height", "Weight", "Measurements", "Fake Tits", "Career Length"
        ]
        
        self.db_mapping = {
            "Name": "name",
            "Aliases": "aliases",
            "Birthdate": "birthdate",
            "Deathdate": "death_date",
            "Country": "country",
            "Ethnicity": "ethnicity",
            "Hair Color": "hair_color",
            "Eye Color": "eye_color",
            "Height": "height",
            "Weight": "weight",
            "Measurements": "measurements",
            "Fake Tits": "fake_tits",
            "Career Length": "career_length"
        }
        
        self.create_ui()
        self.load_data()

    def process_and_goto_phase2(self):
        """
        Workflow Phase 1 : Scrape, compare, et pr√©pare les donn√©es pour la Phase 2.
        AUCUNE injection en base de donn√©es n'est faite ici.
        """
        checked_fields = [f for f, var in self.field_checkboxes.items() if var.get()]
        if not checked_fields:
            self.controller.goto_phase2({}) # Passe un dict vide si pas de scraping
            return
        
        try:
            from services.db import PerformerDB
            db = PerformerDB()
            db_data = db.get_performer_by_id(self.stash_id)
            db.close()
        except Exception as e:
            messagebox.showerror("Erreur", f"Impossible de lire la DB: {e}")
            return

        if not db_data:
            messagebox.showerror("Erreur", "Performer non trouv√© dans la base.")
            return

        performer_name = db_data["name"]
        known_urls = db_data.get("urls", [])

        self.progress_frame.pack(fill=tk.X, padx=10, pady=2, after=list(self.fields.values())[-1].master)
        self.progress_bar.start(10)
        self.progress_label.config(text=f"Scraping {len(checked_fields)} champs pour {performer_name}...")

        def _do_scraping():
            try:
                from services.phase2_scraper import Phase2ScraperService
                from services.phase1_merger import Phase1Merger

                scraper = Phase2ScraperService()
                results = scraper.scrape(performer_name, known_urls=known_urls,
                                         progress_callback=lambda s, m: self.after(0, self.progress_label.config, {'text': f"[{s}] {m}"}))
                
                # Nettoyage des donn√©es scrap√©es AVANT le merge
                for res in results:
                    if res.get("hair_color"):
                        # D√©duplication et nettoyage (ex: "Blonde, Blonde" -> "Blonde")
                        parts = [p.strip() for p in res["hair_color"].replace('/', ',').split(',') if p.strip()]
                        seen = set()
                        unique = []
                        for p in parts:
                            p_cap = p.capitalize()
                            if p_cap not in seen:
                                seen.add(p_cap)
                                unique.append(p_cap)
                        res["hair_color"] = ", ".join(unique)

                merger = Phase1Merger()
                merge_results = merger.merge(db_data, results, checked_fields)
                
                # Construire l'affichage pour TOUS les champs coch√©s
                display_results = {}
                for field in checked_fields:
                    # Le merger renvoie les r√©sultats index√©s par le nom du champ (ex: "Name")
                    if field in merge_results:
                        display_results[field] = merge_results[field]
                    else:
                        # Si le merger n'a rien renvoy√©, on force l'affichage en mode "empty"
                        # pour confirmer √† l'utilisateur que le champ a √©t√© trait√© mais sans r√©sultat.
db_key="***MASKED***"
                        current_val = db_data.get(db_key) if db_key else None
                        display_results[field] = {'status': 'empty', 'db_value': current_val, 'scraped_values': {}, 'suggestion': None}

                self.after(0, self._hide_progress)

                phase1_updates = {}
                if display_results:
                    dialog = Phase1ConflictDialog(self.master, performer_name, display_results, self.db_mapping)
                    if dialog.result is not None:
                        # Le dialogue retourne les valeurs √† mettre √† jour
                        for field_name, resolved_value in dialog.result.items():
db_key="***MASKED***"
                            if db_key:
                                # Sp√©cial pour les alias, on veut une liste
                                if db_key == 'aliases' and isinstance(resolved_value, str):
                                     phase1_updates[db_key] = [a.strip() for a in resolved_value.split(',') if a.strip()]
                                elif db_key == 'hair_color' and isinstance(resolved_value, str):
                                     # Nettoyage et d√©duplication pour la couleur de cheveux
                                     parts = [p.strip() for p in resolved_value.replace('/', ',').split(',') if p.strip()]
                                     seen = set()
                                     unique_parts = []
                                     for p in parts:
                                         if p.lower() not in seen:
                                             seen.add(p.lower())
                                             unique_parts.append(p)
                                     phase1_updates[db_key] = ", ".join(unique_parts)
                                else:
                                     phase1_updates[db_key] = resolved_value
                        
                        messagebox.showinfo("Phase 1 Trait√©e", f"{len(phase1_updates)} modifications de la Phase 1 ont √©t√© pr√©par√©es pour la validation finale.")
                    else:
                        # L'utilisateur a annul√©, on ne passe aucune modification
                        messagebox.showinfo("Annul√©", "Fusion des donn√©es annul√©e. Passage en phase 2 sans appliquer les changements de la phase 1.")
                else:
                    messagebox.showinfo("Phase 1 Compl√®te", "Aucun conflit ou nouvelle donn√©e √† traiter. Passage en Phase 2.")
                
                # Passer en Phase 2 avec les donn√©es r√©solues de la phase 1
                self.after(0, lambda: self.controller.goto_phase2(phase1_updates))

            except Exception as e:
                err_msg = str(e)
                self.after(0, lambda: messagebox.showerror("Erreur Phase 1", f"Une erreur est survenue : {err_msg}"))
            finally:
                self.after(0, self._hide_progress)

        threading.Thread(target=_do_scraping, daemon=True).start()

    def _hide_progress(self):
        self.progress_bar.stop()
        self.progress_frame.pack_forget()

    def create_ui(self):
        # Header + Boutons sp√©cifiques
        buttons = [
            ("Tout s√©lectionner", self.select_all_fields),
            ("S√©lectionner vides", self.select_empty_fields),
            ("Suivant / Traiter", self.process_and_goto_phase2),
            ("Retour", self.controller.return_to_menu),
        ]
        self.create_header("Phase 1 : M√©tadonn√©es usuelles", buttons)

        # Barre de progression (cach√©e par d√©faut)
        self.progress_frame = ttk.Frame(self)
        self.progress_label = ttk.Label(self.progress_frame, text="",
                                        font=("Segoe UI", 9, "italic"))
        self.progress_label.pack(side=tk.LEFT, padx=10)
        self.progress_bar = ttk.Progressbar(self.progress_frame, mode="indeterminate", length=200)
        self.progress_bar.pack(side=tk.LEFT, padx=5)

        # Zone des champs
        f = ttk.Frame(self)
        f.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        
        # Configuration de la grille pour l'extension horizontale
        f.grid_columnconfigure(0, weight=0)  # Checkbox
        f.grid_columnconfigure(1, weight=0)  # Label
        f.grid_columnconfigure(2, weight=1)  # Entry (prend tout l'espace restant)

        for i, field in enumerate(self.fields_list):
            row = i
            # Checkbox (d√©coch√©e par d√©faut)
            var = tk.BooleanVar(value=False)
            self.field_checkboxes[field] = var
            checkbox = ttk.Checkbutton(f, variable=var, text="")
            checkbox.grid(row=row, column=0, sticky=tk.W, padx=(5, 0), pady=2)
            
            # Label
            ttk.Label(f, text=f"{field}:").grid(row=row, column=1, sticky=tk.NW, padx=5, pady=2)
            
            # Entry Widget
            entry = ttk.Entry(f, width=60)
            entry.grid(row=row, column=2, sticky=tk.EW, padx=5, pady=2)
            self.fields[field] = entry


============================================================
[18/83] gui\performer_phase2.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, messagebox
import threading
import copy

from gui.performer_base import PerformerBaseFrame
from gui.bio_wizard import BioWizard


class PerformerPhase2Frame(PerformerBaseFrame):
    def __init__(self, parent, controller, stash_id, phase1_data: dict):
        # phase1_data contient les modifications r√©solues de la phase 1
        self.phase1_data = phase1_data
        self.phase2_data = {} # Pour stocker les modifications de la phase 2

        super().__init__(parent, controller, stash_id)
        
        self.fields_list = ["Trivia", "Awards", "Tattoos", "Piercings", "Tags", "URLs", "Details"]
        self.db_mapping = {
            "Trivia": "trivia", "Awards": "awards", "Tattoos": "tattoos",
            "Piercings": "piercings", "Tags": "tags", "URLs": "urls", "Details": "details"
        }
        
        self._scraped_results = None
        self._merged_data = None
        self._db_data = None
        self._stash_context = None
        
        self.create_ui()
        self.load_data()

    def load_data(self):
        """Charge les donn√©es initiales de la DB, puis applique les modifs de Phase 1."""
        super().load_data() # Charge depuis la DB dans self.fields

        # Applique les donn√©es de la phase 1 par-dessus les donn√©es de la DB
        if self.phase1_data:
            for db_key, value in self.phase1_data.items():
                # Trouve le nom du champ UI correspondant √† la cl√© DB
                field_name = next((name for name, key in self.db_mapping.items() if key == db_key), None)
                if field_name:
                    display_value = value
                    if isinstance(value, list):
                        display_value = ", ".join(value)
                    self._update_field_display(field_name, display_value)

    def create_ui(self):
        buttons = [
            ("üîé Scraper les sources", self.run_scraping),
            ("üßô Lancer l'assistant Bio IA", self.run_bio_wizard),
            ("Tout s√©lectionner", self.select_all_fields),
            ("S√©lectionner vides", self.select_empty_fields),
            ("Retour Phase 1", self.controller.goto_phase1),
        ]
        self.create_header("Phase 2 : Champs Avanc√©s", buttons)

        self.progress_frame = ttk.Frame(self)
        self.progress_label = ttk.Label(self.progress_frame, text="", font=("Segoe UI", 9, "italic"))
        self.progress_label.pack(side=tk.LEFT, padx=10)
        self.progress_bar = ttk.Progressbar(self.progress_frame, mode="indeterminate", length=200)
        self.progress_bar.pack(side=tk.LEFT, padx=5)

        f = ttk.Frame(self)
        f.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        f.grid_columnconfigure(2, weight=1)

        for i, field in enumerate(self.fields_list):
            var = tk.BooleanVar(value=False)
            self.field_checkboxes[field] = var
            ttk.Checkbutton(f, variable=var, text="").grid(row=i, column=0, sticky=tk.W, padx=(5, 0), pady=5)
            ttk.Label(f, text=f"{field}:").grid(row=i, column=1, sticky=tk.NW, padx=5, pady=5)
            
            height = 3
            if field == "Details": height = 8
            if field == "URLs" or field == "Awards": height = 5
            
            entry = tk.Text(f, width=60, height=height, font=("Segoe UI", 10))
            entry.grid(row=i, column=2, sticky=tk.EW, padx=5, pady=5)
            self.fields[field] = entry
            
        self._set_bio_button_state(tk.DISABLED)

    def run_scraping(self):
        try:
            from services.db import PerformerDB
            db = PerformerDB()
            self._db_data = db.get_performer_by_id(self.stash_id)
            self._stash_context = db.get_performer_context(self.stash_id)
            db.close()
        except Exception as e:
            messagebox.showerror("Erreur", f"Impossible de lire la DB: {e}")
            return

        if not self._db_data:
            messagebox.showerror("Erreur", "Performer non trouv√© dans la base.")
            return

        # Fusionner les donn√©es de la DB avec celles de la Phase 1 pour le scraping
temp_data_for_scraping="***MASKED***"
        temp_data_for_scraping.update(self.phase1_data)

        performer_name = temp_data_for_scraping.get("name", "")
        known_urls = temp_data_for_scraping.get("urls", [])

        self.progress_frame.pack(fill=tk.X, padx=10, pady=2, before=self.fields["Trivia"].master)
        self.progress_bar.start(10)
        self.progress_label.config(text=f"Scraping en cours pour {performer_name}...")
        self._set_bio_button_state(tk.DISABLED)

        def _do_scraping():
            try:
                from services.phase2_scraper import Phase2ScraperService
                from services.phase2_merger import Phase2Merger

                scraper = Phase2ScraperService()
                results = scraper.scrape(performer_name, known_urls=known_urls,
                                         progress_callback=lambda s, m: self.after(0, self.progress_label.config, {'text': f"[{s}] {m}"}))
                self._scraped_results = results

                merger = Phase2Merger()
                # Le merger doit utiliser les donn√©es √† jour (DB + Phase 1)
                self._merged_data = merger.merge(temp_data_for_scraping, results)
                
                self.after(0, lambda: self._set_bio_button_state(tk.NORMAL))
                self.after(0, lambda: self._show_wizard(self._merged_data, self._stash_context, temp_data_for_scraping))
            except Exception as e:
                err_msg = str(e)  # Capturer le message d'erreur imm√©diatement
                self.after(0, lambda: messagebox.showerror("Erreur scraping", err_msg))
            finally:
                self.after(0, self._hide_progress)

        threading.Thread(target=_do_scraping, daemon=True).start()

    def _set_bio_button_state(self, state):
        if hasattr(self, "header_buttons") and "üßô Lancer l'assistant Bio IA" in self.header_buttons:
            self.header_buttons["üßô Lancer l'assistant Bio IA"].config(state=state)

    def run_bio_wizard(self):
        """Lance l'assistant Bio en passant toutes les donn√©es accumul√©es."""
        if not self._db_data:
            messagebox.showwarning("Donn√©es manquantes", "Veuillez d'abord 'Scraper les sources'.")
            return

        # Cr√©er un contexte de donn√©es consolid√© pour le wizard
        # Ordre de fusion: DB < Phase 1 < Phase 2
        combined_data = copy.deepcopy(self._db_data)
        combined_data.update(self.phase1_data)
        combined_data.update(self.phase2_data)

        checked_fields = [f for f, var in self.field_checkboxes.items() if var.get()]
        
        wizard = BioWizard(
            parent=self,
            db_data=combined_data, # Utilise les donn√©es les plus √† jour
            stash_ctx=self._stash_context,
            merged_data=self._merged_data, # Donn√©es scrap√©es/fusionn√©es Phase 2
            scraped_results=self._scraped_results or [],
            checked_fields=checked_fields
        )
        
        if wizard.final_bio:
            self._update_field_display("Details", wizard.final_bio)
            self.phase2_data['details'] = wizard.final_bio # Sauvegarde la bio pour l'injection finale
            messagebox.showinfo("Biographie Pr√©par√©e", "La biographie a √©t√© mise √† jour et est pr√™te pour l'injection finale.")

    def _hide_progress(self):
        self.progress_bar.stop()
        self.progress_frame.pack_forget()

    def _show_wizard(self, merged_data: dict, stash_context: dict, db_data: dict):
        from gui.phase2_field_wizard import Phase2FieldWizard
        checked_fields = [f for f, var in self.field_checkboxes.items() if var.get()]
        wizard = Phase2FieldWizard(self.winfo_toplevel(), merged_data, stash_context, db_data, self._scraped_results, checked_fields)
        if wizard.result:
            self._apply_phase2_results(wizard.result)

    def _apply_phase2_results(self, result: dict):
        """Applique les r√©sultats du wizard Phase 2 en m√©moire, SANS injection DB."""
        # 'result' contient les donn√©es valid√©es du wizard
        # On met √† jour l'UI et on stocke dans self.phase2_data
        
        # Details (bio)
        if result.get("details"):
            self.phase2_data["details"] = result["details"]
            self._update_field_display("Details", result["details"])

        # Tattoos
        if result.get("tattoos"):
            tattoo_str = "; ".join(f"{t['position']} ({t['description']})" if t.get('description') else t['position'] for t in result["tattoos"])
            self.phase2_data["tattoos"] = tattoo_str
            self._update_field_display("Tattoos", tattoo_str)

        # Piercings
        if result.get("piercings"):
            piercing_str = "; ".join(f"{p['position']} ({p['description']})" if p.get('description') else p['position'] for p in result["piercings"])
            self.phase2_data["piercings"] = piercing_str
            self._update_field_display("Piercings", piercing_str)

        # Tags
        if result.get("tags"):
            import re
            tags_raw = result["tags"]
            tags = set([t.strip() for t in re.split(r'[ ,]+', tags_raw) if t.strip()])
            self.phase2_data["tags"] = list(tags)
            self._update_field_display("Tags", ", ".join(tags))

        # URLs
        if result.get("urls"):
            urls_raw = result["urls"]
            if isinstance(urls_raw, str): urls = set([u.strip() for u in urls_raw.splitlines() if u.strip()])
            elif isinstance(urls_raw, dict): urls = set([u.strip() for u in urls_raw.values() if u.strip()])
            else: urls = set([u.strip() for u in urls_raw if u.strip()])
            self.phase2_data["urls"] = list(urls)
            self._update_field_display("URLs", "\n".join(urls))

        # Awards et Trivia ‚Üí trait√©s diff√©remment car ce sont des custom fields
        custom_fields_updates = []
        if result.get("awards"):
            custom_fields_updates.extend([{"type": "award", "value": line.strip()} for line in result["awards"] if line.strip()])
        if result.get("trivia"):
            custom_fields_updates.extend([{"type": "trivia", "value": line.strip()} for line in result["trivia"].split("\n") if line.strip()])
        if custom_fields_updates:
            self.phase2_data['customfields'] = custom_fields_updates
        
        messagebox.showinfo("‚úÖ Donn√©es pr√©par√©es", "Les modifications de la Phase 2 ont √©t√© pr√©par√©es et sont pr√™tes pour la validation finale.")

    def _update_field_display(self, field_name: str, value: str):
        entry = self.fields.get(field_name)
        if entry and isinstance(entry, tk.Text):
            entry.delete("1.0", tk.END)
            if value:
                entry.insert("1.0", value)
            self.field_checkboxes.get(field_name, tk.BooleanVar()).set(True)


============================================================
[19/83] gui\phase1_conflict_dialog.py
------------------------------------------------------------
"""
Phase1ConflictDialog ‚Äî Dialogue de r√©solution des conflits Phase 1.
Affiche confirmations, nouveaux et conflits pour chaque champ coch√©.
"""
import tkinter as tk
from tkinter import ttk


# Code couleur par statut
STATUS_COLORS = {
    "confirmed": "#2ecc71",  # vert
    "new": "#3498db",        # bleu
    "conflict": "#e74c3c",   # rouge
    "empty": "#95a5a6",      # gris
}

STATUS_ICONS = {
    "confirmed": "‚úÖ",
    "new": "üÜï",
    "conflict": "‚ö†Ô∏è",
    "empty": "‚¨ú",
}


class Phase1ConflictDialog(tk.Toplevel):
    """
    Dialogue modal montrant le r√©sultat du scraping Phase 1.
    Pour chaque champ :
    - Confirm√© (vert) ‚Üí DB == source
    - Nouveau (bleu) ‚Üí DB vide, suggestion disponible
    - Conflit (rouge) ‚Üí DB ‚â† sources ‚Üí l'utilisateur choisit
    - Vide (gris) ‚Üí rien trouv√©
    """

    def __init__(self, parent, performer_name: str, merge_result: dict, db_mapping: dict):
        super().__init__(parent)
        self.performer_name = performer_name
        self.title(f"üîç R√©sultat scraping Phase 1 pour {performer_name}")
        self.merge_result = merge_result
        self.db_mapping = db_mapping
        self.result = None  # Dict final si valid√©

        self.geometry("800x600")
        self.minsize(750, 500)
        self.transient(parent)
        self.grab_set()

        # Variables de s√©lection par champ
        self.selections = {}

        self._build_ui()
        self.wait_window()

    def _build_ui(self):
        # ‚îÄ‚îÄ Compteurs en haut ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        header = ttk.Frame(self, padding=10)
        header.pack(fill=tk.X)

        counts = {"confirmed": 0, "new": 0, "conflict": 0, "empty": 0}
        for info in self.merge_result.values():
            counts[info["status"]] = counts.get(info["status"], 0) + 1

        summary = (
            f"‚úÖ Confirm√©s: {counts['confirmed']}  |  "
            f"üÜï Nouveaux: {counts['new']}  |  "
            f"‚ö†Ô∏è Conflits: {counts['conflict']}  |  "
            f"‚¨ú Vides: {counts['empty']}"
        )
        ttk.Label(header, text=summary, font=("Segoe UI", 11, "bold")).pack(anchor=tk.W)

        # ‚îÄ‚îÄ Zone scrollable ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        main = ttk.Frame(self)
        main.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        canvas = tk.Canvas(main, highlightthickness=0)
        scrollbar = ttk.Scrollbar(main, orient=tk.VERTICAL, command=canvas.yview)
        self.scroll_frame = ttk.Frame(canvas)

        self.scroll_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        canvas.create_window((0, 0), window=self.scroll_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        canvas.bind_all("<MouseWheel>",
                        lambda e: canvas.yview_scroll(int(-1 * (e.delta / 120)), "units"))

        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # ‚îÄ‚îÄ Lignes par champ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        for field_name, info in self.merge_result.items():
            self._build_field_row(field_name, info)

        # ‚îÄ‚îÄ Boutons ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        btn_frame = ttk.Frame(self, padding=10)
        btn_frame.pack(fill=tk.X)

        ttk.Button(btn_frame, text="‚úÖ Appliquer et continuer",
                   command=self._apply).pack(side=tk.RIGHT, padx=5)
        ttk.Button(btn_frame, text="‚ùå Annuler",
                   command=self.destroy).pack(side=tk.RIGHT, padx=5)

    def _build_field_row(self, field_name: str, info: dict):
        """Construire une ligne pour un champ."""
        status = info["status"]
        db_value = info.get("db_value") or ""
        scraped_values = info.get("scraped_values", {})
        suggestion = info.get("suggestion") or ""

        # Cadre principal du champ
        frame = ttk.Frame(self.scroll_frame, padding=(5, 3))
        frame.pack(fill=tk.X, padx=5, pady=2)

        # Ic√¥ne + Nom du champ
        icon = STATUS_ICONS.get(status, "")
        ttk.Label(frame, text=f"{icon} {field_name}", width=20, anchor=tk.W,
                  font=("Segoe UI", 10, "bold")).pack(side=tk.LEFT, padx=(0, 10))

        if status == "confirmed":
            # Tout va bien ‚Äî afficher simplement la valeur
            ttk.Label(frame, text=f"‚úì {db_value}",
                      font=("Segoe UI", 10)).pack(side=tk.LEFT, fill=tk.X, expand=True)
            self.selections[field_name] = tk.StringVar(value=db_value)
            
            # Afficher les sources qui confirment la valeur
            if scraped_values:
                sources = ", ".join(k.upper() for k in scraped_values.keys())
                ttk.Label(frame, text=f"[{sources}]", font=("Segoe UI", 8, "italic")).pack(side=tk.LEFT, padx=5)

        elif status == "empty":
            # Rien trouv√©
            ttk.Label(frame, text="(aucune donn√©e)",
                      font=("Segoe UI", 10, "italic")).pack(side=tk.LEFT)
            self.selections[field_name] = tk.StringVar(value="")

        elif status == "new":
            # Nouvelle valeur ‚Äî proposer la suggestion modifiable
            var = tk.StringVar(value=suggestion)
            self.selections[field_name] = var
            ttk.Label(frame, text="DB: (vide) ‚Üí",
                      font=("Segoe UI", 9, "italic")).pack(side=tk.LEFT, padx=(0, 5))
            entry = ttk.Entry(frame, textvariable=var, width=50)
            entry.pack(side=tk.LEFT, fill=tk.X, expand=True)
            # Source info
            sources = ", ".join(scraped_values.keys())
            ttk.Label(frame, text=f"[{sources}]",
                      font=("Segoe UI", 8)).pack(side=tk.LEFT, padx=5)

        elif status == "conflict":
            # Conflit ‚Äî radio buttons pour choisir
            self._build_conflict_section(frame, field_name, db_value, scraped_values, suggestion)

    def _build_conflict_section(self, parent, field_name, db_value, scraped_values, suggestion):
        """Construire la section de r√©solution de conflit."""
        # Sous-frame pour les options
        conflict_frame = ttk.Frame(parent)
        conflict_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        var = tk.StringVar(value=suggestion)
        self.selections[field_name] = var

        # Option 1 : garder la valeur DB
        ttk.Radiobutton(
            conflict_frame,
            text=f"DB: {db_value}",
            variable=var,
            value=db_value
        ).pack(anchor=tk.W)

        # Options : valeurs scrap√©es
        for source, val in scraped_values.items():
            ttk.Radiobutton(
                conflict_frame,
                text=f"{source.upper()}: {val}",
                variable=var,
                value=val
            ).pack(anchor=tk.W)

    def _apply(self):
        """Collecter les s√©lections et fermer."""
        self.result = {}
        for field_name, var in self.selections.items():
            val = var.get().strip()
            if val:
                self.result[field_name] = val
        self.destroy()


============================================================
[20/83] gui\phase2_field_wizard.py
------------------------------------------------------------
"""
Phase2FieldWizard ‚Äî Dialogue pas-√†-pas pour la r√©solution des champs Phase 2.
Pr√©sente un champ √† la fois avec le contexte Stash.
"""
import tkinter as tk
from tkinter import ttk


# Ordre des pages du wizard
WIZARD_PAGES = [
    ("awards", "üèÜ AWARDS"),
    ("trivia", "üìù TRIVIA"),
    ("tattoos", "üé® TATTOOS"),
    ("piercings", "üíâ PIERCINGS"),
    ("tags", "üè∑Ô∏è TAGS"),
    ("urls", "üîó URLs"),
    ("details", "üìñ DETAILS (Bio)"),
]


class Phase2FieldWizard(tk.Toplevel):
    """
    Wizard pas-√†-pas : une page par champ Phase 2.
    Chaque page affiche le contexte Stash + les donn√©es scrap√©es.
    """

    def __init__(self, parent, merged_data: dict, stash_context: dict,
                 db_data: dict, scraped_results: list[dict] = None, checked_fields: list[str] | None = None):
        super().__init__(parent)
        self.title("üìã Wizard Phase 2 ‚Äî R√©solution pas-√†-pas")
        self.merged_data = merged_data
        self.stash_ctx = stash_context
        self.db_data = db_data
        self.scraped_results = scraped_results or []  # Stocker les r√©sultats bruts
        self.checked_fields = checked_fields or []   # ‚Üê AJOUTER
        self.result = None
        self.generated_bio = None

        self.geometry("950x700")
        self.minsize(900, 600)
        self.transient(parent)
        self.grab_set()

        # √âtat du wizard
        self.current_page = 0
        self.selections = {}

        # Construire les cadres
        self._build_shell()
        self._show_page(0)
        self.wait_window()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # STRUCTURE PRINCIPALE
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_shell(self):
        """Cr√©er la coquille du wizard (header, zone contenu, boutons nav)."""
        # ‚îÄ‚îÄ Header ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        self.header_frame = ttk.Frame(self, padding=10)
        self.header_frame.pack(fill=tk.X)

        self.page_title = ttk.Label(self.header_frame, text="",
                                     font=("Segoe UI", 14, "bold"))
        self.page_title.pack(side=tk.LEFT)

        self.page_counter = ttk.Label(self.header_frame, text="",
                                       font=("Segoe UI", 10))
        self.page_counter.pack(side=tk.RIGHT)

        # Progress bar
        self.progress = ttk.Progressbar(self, maximum=len(WIZARD_PAGES), length=400)
        self.progress.pack(fill=tk.X, padx=10, pady=(0, 5))

        # ‚îÄ‚îÄ Zone contenu scrollable ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        content_wrapper = ttk.Frame(self)
        content_wrapper.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        self.canvas = tk.Canvas(content_wrapper, highlightthickness=0)
        scrollbar = ttk.Scrollbar(content_wrapper, orient=tk.VERTICAL, command=self.canvas.yview)
        self.content_frame = ttk.Frame(self.canvas)

        self.content_frame.bind(
            "<Configure>",
            lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all"))
        )
        self.canvas_window = self.canvas.create_window((0, 0), window=self.content_frame, anchor="nw")
        self.canvas.configure(yscrollcommand=scrollbar.set)
        self.canvas.bind_all("<MouseWheel>",
                             lambda e: self.canvas.yview_scroll(int(-1 * (e.delta / 120)), "units"))

        # Resize canvas window width
        self.canvas.bind("<Configure>",
                         lambda e: self.canvas.itemconfig(self.canvas_window, width=e.width))

        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # ‚îÄ‚îÄ Boutons navigation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        nav_frame = ttk.Frame(self, padding=10)
        nav_frame.pack(fill=tk.X)

        self.btn_prev = ttk.Button(nav_frame, text="‚óÄ Pr√©c√©dent", command=self._prev_page)
        self.btn_prev.pack(side=tk.LEFT, padx=5)

        ttk.Button(nav_frame, text="‚ùå Annuler", command=self.destroy).pack(side=tk.LEFT, padx=5)

        self.btn_next = ttk.Button(nav_frame, text="Suivant ‚ñ∂", command=self._next_page)
        self.btn_next.pack(side=tk.RIGHT, padx=5)

        self.btn_skip = ttk.Button(nav_frame, text="‚è≠ Passer", command=self._skip_page)
        self.btn_skip.pack(side=tk.RIGHT, padx=5)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # NAVIGATION
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _prev_page(self):
        if self.current_page > 0:
            self._save_current()
            self.current_page -= 1
            self._show_page(self.current_page)

    def _next_page(self):
        self._save_current()
        if self.current_page < len(WIZARD_PAGES) - 1:
            self.current_page += 1
            self._show_page(self.current_page)
        else:
            self._finish()

    def _skip_page(self):
        """Passer sans sauvegarder le champ courant."""
        if self.current_page < len(WIZARD_PAGES) - 1:
            self.current_page += 1
            self._show_page(self.current_page)
        else:
            self._finish()

    def _show_page(self, idx):
        """Afficher la page √† l'index donn√©."""
        field_key, title = WIZARD_PAGES[idx]

        # MAJ header
        self.page_title.config(text=title)
        self.page_counter.config(text=f"√âtape {idx + 1} / {len(WIZARD_PAGES)}")
        self.progress["value"] = idx + 1

        # MAJ boutons
        self.btn_prev.config(state=tk.NORMAL if idx > 0 else tk.DISABLED)
        self.btn_next.config(text="‚úÖ Terminer" if idx == len(WIZARD_PAGES) - 1 else "Suivant ‚ñ∂")

        # Vider le contenu
        for w in self.content_frame.winfo_children():
            w.destroy()

        # Construire la page selon le champ
        builder = {
            "details": self._page_details,
            "awards": self._page_awards,
            "trivia": self._page_trivia,
            "tattoos": self._page_body_art,
            "piercings": self._page_body_art,
            "tags": self._page_tags,
            "urls": self._page_urls,
        }
        builder_fn = builder.get(field_key, lambda k: None)
        builder_fn(field_key)

        # Scroll en haut
        self.canvas.yview_moveto(0)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CONTEXTE STASH (affich√© sur chaque page)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _add_stash_context(self, parent):
        """Ajouter un panneau de contexte Stash."""
        ctx = self.stash_ctx
        if not ctx:
            return

        frame = ttk.LabelFrame(parent, text="üìä Contexte Stash", padding=8)
        frame.pack(fill=tk.X, padx=5, pady=5)

        info_parts = []
        if ctx.get("scene_count"):
            info_parts.append(f"üé¨ {ctx['scene_count']} sc√®nes")
        if ctx.get("studios"):
            studios_str = ", ".join(ctx["studios"][:10])
            if len(ctx["studios"]) > 10:
                studios_str += f" (+{len(ctx['studios']) - 10})"
            info_parts.append(f"üè¢ Studios: {studios_str}")
        if ctx.get("groups"):
            groups_str = ", ".join(ctx["groups"][:8])
            if len(ctx["groups"]) > 8:
                groups_str += f" (+{len(ctx['groups']) - 8})"
            info_parts.append(f"üìÄ Groups: {groups_str}")
        if ctx.get("collaborators"):
            top5 = [f"{c['name']} ({c['count']})" for c in ctx["collaborators"][:5]]
            info_parts.append(f"üë• Top collabs: {', '.join(top5)}")

        for part in info_parts:
            ttk.Label(frame, text=part, font=("Segoe UI", 9), wraplength=800).pack(
                anchor=tk.W, pady=1)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: DETAILS (Bio)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_details(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("details", {})
        by_source = data.get("by_source", {})
        fused = data.get("fused")
        db_bio = self.db_data.get("details") or self.db_data.get("bio") or ""

        # Contexte Stash
        self._add_stash_context(parent)

        # Valeur Stash actuelle
        if db_bio:
            stash_frame = ttk.LabelFrame(parent, text="üìã Bio actuelle (Stash)", padding=8)
            stash_frame.pack(fill=tk.X, padx=5, pady=5)
            stash_text = tk.Text(stash_frame, height=4, width=80, font=("Segoe UI", 9),
                                 wrap=tk.WORD, state=tk.DISABLED)
            stash_text.pack(fill=tk.X)
            stash_text.config(state=tk.NORMAL)
            stash_text.insert("1.0", db_bio)
            stash_text.config(state=tk.DISABLED)

        # Choix de source
        self._details_choice_frame = ttk.LabelFrame(parent, text="üîÑ Choisir la bio", padding=8)
        self._details_choice_frame.pack(fill=tk.X, padx=5, pady=5)

        options = {}
        if db_bio:
            options["stash"] = db_bio

        for source, text in by_source.items():
            options[source] = text

        if fused:
            options["_fused_"] = fused

        # Ajouter la bio IA si d√©j√† g√©n√©r√©e (persistance)
        if self.generated_bio:
            options["_ia_"] = self.generated_bio

        if not options:
            ttk.Label(self._details_choice_frame, text="Aucune bio disponible",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W)
            return

        # R√©cup√©rer la s√©lection pr√©c√©dente ou d√©faut
        prev = self.selections.get("details", {}).get("_choice")
        
        # S√©lection par d√©faut : IA si dispo, sinon Stash, sinon premi√®re source
        default = prev
        if not default:
            default = "_ia_" if self.generated_bio else ("stash" if db_bio else list(options.keys())[0])

        self._details_var = tk.StringVar(value=default)
        self._details_options = options

        for key, text in options.items():
            label = key.upper() if key != "_fused_" else "FUSION"
            length = len(text)
            ttk.Radiobutton(self._details_choice_frame, text=f"{label} ({length} car.)",
                           variable=self._details_var, value=key).pack(anchor=tk.W, padx=5, pady=2)

        # Preview
        self._details_preview = tk.Text(parent, height=8, width=80,
                                         font=("Segoe UI", 9), wrap=tk.WORD)
        self._details_preview.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)

        def update(*_):
            self._details_preview.delete("1.0", tk.END)
            self._details_preview.insert("1.0", options.get(self._details_var.get(), ""))

        self._details_var.trace_add("write", update)
        update()

        # NOUVEAU : Bouton g√©n√©ration IA
        gen_frame = ttk.LabelFrame(parent, text="‚ú® G√©n√©ration IA", padding=8)
        gen_frame.pack(fill=tk.X, padx=5, pady=5)

        self._bio_status = ttk.Label(gen_frame, text="Pr√™t (Auto)", font=("Segoe UI", 9, "italic"))
        self._bio_status.pack(side=tk.LEFT, padx=10)

        ttk.Button(
            gen_frame,
            text="üîÑ R√©g√©n√©rer (Gemini/Ollama)",
            command=self._run_bio_generation
        ).pack(side=tk.RIGHT, padx=5)

        # Lancement automatique si pas encore de bio IA
        if not self.generated_bio:
            self.after(500, self._run_bio_generation)
        else:
            self._bio_status.config(text="‚úÖ Bio IA d√©j√† disponible")

    def _run_bio_generation(self):
        """Lance la g√©n√©ration IA en thread."""
        import threading
        import copy
        self._bio_status.config(text="üöÄ G√©n√©ration auto en cours (Gemini > Ollama)...")

        def t():
            from services.bio_generator import BioGenerator
            gen = BioGenerator()

            # Utiliser les vrais champs coch√©s (Phase 1 + Phase 2 disponibles)
            all_checked = (
                self.checked_fields if self.checked_fields
                else list(self.merged_data.keys())
            )
            # Ajouter tous les champs Phase 1 potentiellement utiles.
            # Le g√©n√©rateur d√©cidera d'utiliser les specs techniques (Taille/Poids...) uniquement si les infos sont limit√©es.
            all_checked.extend(["Name", "Birthdate", "Height", "Weight", "Measurements", "Fake Tits", "Hair Color", "Eye Color", "Ethnicity", "Country", "Aliases", "Career Length"])
            
            # S'assurer que Awards et URLs sont coch√©s s'ils existent dans les donn√©es fusionn√©es
            all_checked.extend(["Awards", "URLs"])
            
            # Utiliser les donn√©es fusionn√©es, mais mettre √† jour avec les s√©lections utilisateur
            # faites dans les onglets pr√©c√©dents (puisque Details est maintenant √† la fin)
            effective_merged = copy.deepcopy(self.merged_data)
            if "awards" in self.selections:
                effective_merged.setdefault("awards", {})["merged"] = self.selections["awards"]["value"]
            
            # AJOUT : Injecter les URLs valid√©es (onglet 6) pour que l'IA connaisse les bons r√©seaux sociaux
            if "urls" in self.selections:
                effective_merged.setdefault("urls", {})["merged"] = self.selections["urls"]["value"]

            ctx = gen.build_context_from_v2(
                db_data=self.db_data,
                stash_ctx=self.stash_ctx,
                scraped_results=self.scraped_results,  # Passer les r√©sultats bruts
                merged_data=effective_merged,
                checked_fields=all_checked,
            )
            bio = gen.generate(ctx)

            def update():
                if bio:
                    self.generated_bio = bio
                    # Mise √† jour UI seulement si on est encore sur la page Details
current_key="***MASKED***"
                    if hasattr(self, "_details_options") and current_key == "details":
                        if "_ia_" not in self._details_options:
                            self._details_options["_ia_"] = bio
                            ttk.Radiobutton(
                                self._details_choice_frame,
                                text=f"ü§ñ IA GENERATED ({len(bio)} car.)",
                                variable=self._details_var,
                                value="_ia_"
                            ).pack(anchor=tk.W, padx=5, pady=2)
                        self._details_var.set("_ia_") # S√©lectionne et met √† jour la preview via trace
                    self._bio_status.config(text=f"‚úÖ Bio g√©n√©r√©e ({len(bio)} car.)")
                else:
                    self._bio_status.config(
                        text="‚ùå √âchec ‚Äî V√©rifier .gemini_key ou Ollama actif")

            self.after(0, update)

        threading.Thread(target=t, daemon=True).start()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: AWARDS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_awards(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("awards", {})
        merged = data.get("merged", [])
        sources = data.get("sources", {})

        self._add_stash_context(parent)

        if not merged:
            ttk.Label(parent, text="Aucun award trouv√©",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        # Info sources
        info_frame = ttk.LabelFrame(parent, text="üìä Sources", padding=8)
        info_frame.pack(fill=tk.X, padx=5, pady=5)
        source_info = "  ".join(f"[{s}: {len(a)}]" for s, a in sources.items() if a)
        ttk.Label(info_frame, text=source_info, font=("Segoe UI", 9)).pack(anchor=tk.W)

        # Liste cochable
        list_frame = ttk.LabelFrame(parent, text=f"üèÜ Awards trouv√©s ({len(merged)})", padding=8)
        list_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        prev = self.selections.get("awards", {}).get("_items")
        self._award_vars = []
        for i, award in enumerate(merged):
            checked = prev[i] if prev and i < len(prev) else True
            var = tk.BooleanVar(value=checked)
            self._award_vars.append((var, award))
            ttk.Checkbutton(list_frame, text=award, variable=var).pack(anchor=tk.W, padx=5)

        # Boutons tout cocher/d√©cocher
        btn_row = ttk.Frame(list_frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda: [v.set(True) for v, _ in self._award_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda: [v.set(False) for v, _ in self._award_vars]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: TRIVIA
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_trivia(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("trivia", {})
        by_source = data.get("by_source", {})

        self._add_stash_context(parent)

        if not by_source:
            ttk.Label(parent, text="Aucun trivia trouv√©",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        choice_frame = ttk.LabelFrame(parent, text="üìù Sources Trivia", padding=8)
        choice_frame.pack(fill=tk.X, padx=5, pady=5)

        prev = self.selections.get("trivia", {}).get("_choice")
        default = prev or list(by_source.keys())[0]
        self._trivia_var = tk.StringVar(value=default)
        self._trivia_sources = by_source

        for source, text in by_source.items():
            preview = text[:80] + "..." if len(text) > 80 else text
            ttk.Radiobutton(choice_frame, text=f"{source.upper()} ‚Äî {preview}",
                           variable=self._trivia_var, value=source).pack(anchor=tk.W, padx=5, pady=2)

        self._trivia_preview = tk.Text(parent, height=6, width=80,
                                        font=("Segoe UI", 9), wrap=tk.WORD)
        self._trivia_preview.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)

        def update(*_):
            self._trivia_preview.delete("1.0", tk.END)
            self._trivia_preview.insert("1.0", by_source.get(self._trivia_var.get(), ""))

        self._trivia_var.trace_add("write", update)
        update()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: TATTOOS / PIERCINGS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_body_art(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get(field_key, {})
        merged = data.get("merged", [])
        db_value = data.get("db_value", "")

        self._add_stash_context(parent)

        # Valeur actuelle Stash
        if db_value:
            stash_frame = ttk.LabelFrame(parent, text=f"üìã {field_key.title()} actuels (Stash)",
                                          padding=8)
            stash_frame.pack(fill=tk.X, padx=5, pady=5)
            ttk.Label(stash_frame, text=str(db_value), font=("Segoe UI", 9),
                      wraplength=800).pack(anchor=tk.W)

        if not merged:
            ttk.Label(parent, text=f"Aucun {field_key} trouv√©",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        list_frame = ttk.LabelFrame(parent, text=f"R√©sultat ({len(merged)} entr√©es)", padding=8)
        list_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        prev = self.selections.get(field_key, {}).get("_items")
        var_attr = f"_{field_key}_vars"
        vars_list = []
        for i, item in enumerate(merged):
            pos = item.get("position", "?")
            desc = item.get("description", "")
            label = f"{pos}" + (f" ({desc})" if desc else "")
            checked = prev[i] if prev and i < len(prev) else True
            var = tk.BooleanVar(value=checked)
            vars_list.append((var, item))
            ttk.Checkbutton(list_frame, text=label, variable=var).pack(anchor=tk.W, padx=5)

        setattr(self, var_attr, vars_list)

        btn_row = ttk.Frame(list_frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda vl=vars_list: [v.set(True) for v, _ in vl]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda vl=vars_list: [v.set(False) for v, _ in vl]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: TAGS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_tags(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("tags", {})
        merged = data.get("merged", [])
        sources = data.get("sources", {})
        stash_tags = self.db_data.get("tags", [])

        self._add_stash_context(parent)

        # Tags actuels Stash
        if stash_tags:
            stash_frame = ttk.LabelFrame(parent, text=f"üìã Tags actuels Stash ({len(stash_tags)})",
                                          padding=8)
            stash_frame.pack(fill=tk.X, padx=5, pady=5)
            ttk.Label(stash_frame, text=", ".join(stash_tags[:30]),
                      font=("Segoe UI", 9), wraplength=800).pack(anchor=tk.W)

        if not merged:
            ttk.Label(parent, text="Aucun tag scrap√©",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        # Info sources
        info = "  ".join(f"[{s}: {len(t)}]" for s, t in sources.items() if t)
        ttk.Label(parent, text=f"Union : {len(merged)} tags ‚Äî {info}",
                  font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, padx=10, pady=5)

        list_frame = ttk.LabelFrame(parent, text="üè∑Ô∏è Tags scrap√©s", padding=8)
        list_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        prev = self.selections.get("tags", {}).get("_items")
        self._tag_vars = []
        for i, tag in enumerate(merged):
            # Marquer si d√©j√† dans Stash
            in_stash = tag.lower() in [t.lower() for t in stash_tags]
            label = f"{'‚úì ' if in_stash else ''}{tag}"
            checked = prev[i] if prev and i < len(prev) else True
            var = tk.BooleanVar(value=checked)
            self._tag_vars.append((var, tag))
            ttk.Checkbutton(list_frame, text=label, variable=var).pack(anchor=tk.W, padx=5)

        btn_row = ttk.Frame(list_frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda: [v.set(True) for v, _ in self._tag_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda: [v.set(False) for v, _ in self._tag_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="Nouveaux seuls",
                   command=lambda: self._select_new_tags_only(stash_tags)).pack(side=tk.LEFT, padx=2)

    def _select_new_tags_only(self, stash_tags):
        """Cocher uniquement les tags qui ne sont PAS d√©j√† dans Stash."""
        stash_lower = {t.lower() for t in stash_tags}
        for var, tag in self._tag_vars:
            var.set(tag.lower() not in stash_lower)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: URLs
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_urls(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("urls", {})
        merged = data.get("merged", {})
        stash_urls = self.db_data.get("urls", [])

        self._add_stash_context(parent)

        # URLs actuelles Stash
        if stash_urls:
            stash_frame = ttk.LabelFrame(parent, text=f"üìã URLs actuelles Stash ({len(stash_urls)})",
                                          padding=8)
            stash_frame.pack(fill=tk.X, padx=5, pady=5)
            for url in stash_urls[:15]:
                ttk.Label(stash_frame, text=url, font=("Segoe UI", 9)).pack(anchor=tk.W)

        if not merged:
            ttk.Label(parent, text="Aucune URL scrap√©e",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        list_frame = ttk.LabelFrame(parent, text=f"üîó URLs scrap√©es ({len(merged)})", padding=8)
        list_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        prev = self.selections.get("urls", {}).get("_items")
        self._url_vars = []
        for i, (key, url) in enumerate(sorted(merged.items())):
            checked = True
            if prev:
                checked = prev.get(key, True)
            var = tk.BooleanVar(value=checked)
            self._url_vars.append((var, key, url))
            row = ttk.Frame(list_frame)
            row.pack(fill=tk.X, padx=5, pady=1)
            ttk.Checkbutton(row, variable=var).pack(side=tk.LEFT)
            ttk.Label(row, text=f"{key}:", width=12, anchor=tk.W,
                      font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
            ttk.Label(row, text=url, font=("Segoe UI", 9)).pack(side=tk.LEFT, fill=tk.X)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # SAUVEGARDE / FINITION
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _save_current(self):
        """Sauvegarder la s√©lection de la page courante."""
field_key="***MASKED***"

        if field_key == "details":
            if hasattr(self, "_details_var"):
                choice = self._details_var.get()
                text = self._details_options.get(choice, "")
                self.selections["details"] = {"_choice": choice, "value": text}

        elif field_key == "awards":
            if hasattr(self, "_award_vars"):
                items = [v.get() for v, _ in self._award_vars]
                selected = [a for v, a in self._award_vars if v.get()]
                self.selections["awards"] = {"_items": items, "value": selected}

        elif field_key == "trivia":
            if hasattr(self, "_trivia_var"):
                choice = self._trivia_var.get()
                text = self._trivia_sources.get(choice, "")
                self.selections["trivia"] = {"_choice": choice, "value": text}

        elif field_key in ("tattoos", "piercings"):
            var_attr = f"_{field_key}_vars"
            if hasattr(self, var_attr):
                vars_list = getattr(self, var_attr)
                items = [v.get() for v, _ in vars_list]
                selected = [item for v, item in vars_list if v.get()]
                self.selections[field_key] = {"_items": items, "value": selected}

        elif field_key == "tags":
            if hasattr(self, "_tag_vars"):
                items = [v.get() for v, _ in self._tag_vars]
                selected = [t for v, t in self._tag_vars if v.get()]
                self.selections["tags"] = {"_items": items, "value": selected}

        elif field_key == "urls":
            if hasattr(self, "_url_vars"):
                items = {k: v.get() for v, k, _ in self._url_vars}
                selected = {k: u for v, k, u in self._url_vars if v.get()}
                self.selections["urls"] = {"_items": items, "value": selected}

    def _finish(self):
        """Collecter toutes les s√©lections et fermer."""
        self._save_current()

        self.result = {}

        # Details
        det = self.selections.get("details", {})
        self.result["details"] = det.get("value")

        # Awards
        aw = self.selections.get("awards", {})
        self.result["awards"] = aw.get("value", [])

        # Trivia
        tr = self.selections.get("trivia", {})
        self.result["trivia"] = tr.get("value")

        # Tattoos
        tt = self.selections.get("tattoos", {})
        self.result["tattoos"] = tt.get("value", [])

        # Piercings
        pi = self.selections.get("piercings", {})
        self.result["piercings"] = pi.get("value", [])

        # Tags
        tg = self.selections.get("tags", {})
        self.result["tags"] = tg.get("value", [])

        # URLs
        ur = self.selections.get("urls", {})
        self.result["urls"] = ur.get("value", {})

        self.destroy()


============================================================
[21/83] gui\phase2_merge_dialog.py
------------------------------------------------------------
"""
Phase2MergeDialog ‚Äî Fen√™tre modale de r√©solution des r√©sultats Phase 2.
Chaque champ a son propre mode d'affichage et de s√©lection.
"""
import tkinter as tk
from tkinter import ttk


class Phase2MergeDialog(tk.Toplevel):
    """
    Dialogue de fusion pour les champs Phase 2.
    Affiche les r√©sultats fusionn√©s et permet √† l'utilisateur
    de choisir/modifier avant application.
    """

    def __init__(self, parent, merged_data: dict):
        super().__init__(parent)
        self.title("üìã R√©sultats scraping Phase 2")
        self.merged_data = merged_data
        self.result = None  # Dict final si l'utilisateur valide

        # Configurer la fen√™tre
        self.geometry("900x700")
        self.minsize(800, 600)
        self.transient(parent)
        self.grab_set()

        # Variables de s√©lection
        self.selections = {}

        self._build_ui()
        self.wait_window()

    def _build_ui(self):
        # ‚îÄ‚îÄ Frame principal avec scroll ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        main = ttk.Frame(self, padding=10)
        main.pack(fill=tk.BOTH, expand=True)

        # Canvas + Scrollbar pour le contenu
        canvas = tk.Canvas(main, highlightthickness=0)
        scrollbar = ttk.Scrollbar(main, orient=tk.VERTICAL, command=canvas.yview)
        self.scroll_frame = ttk.Frame(canvas)

        self.scroll_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        canvas.create_window((0, 0), window=self.scroll_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        # Mousewheel scroll
        canvas.bind_all("<MouseWheel>",
                        lambda e: canvas.yview_scroll(int(-1 * (e.delta / 120)), "units"))

        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # ‚îÄ‚îÄ Sections par champ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        self._build_awards_section()
        self._build_trivia_section()
        self._build_details_section()
        self._build_body_art_section("tattoos", "üé® TATTOOS")
        self._build_body_art_section("piercings", "üíâ PIERCINGS")
        self._build_tags_section()
        self._build_urls_section()

        # ‚îÄ‚îÄ Boutons d'action ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        btn_frame = ttk.Frame(self, padding=10)
        btn_frame.pack(fill=tk.X)

        ttk.Button(btn_frame, text="‚úÖ Appliquer tout",
                   command=self._apply_all).pack(side=tk.RIGHT, padx=5)
        ttk.Button(btn_frame, text="‚ùå Annuler",
                   command=self.destroy).pack(side=tk.RIGHT, padx=5)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # AWARDS ‚Äî liste cochable
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_awards_section(self):
        data = self.merged_data.get("awards", {})
        merged = data.get("merged", [])
        sources = data.get("sources", {})

        frame = self._section_frame("üèÜ AWARDS")

        if not merged:
            ttk.Label(frame, text="Aucun award trouv√©").pack(anchor=tk.W)
            self.selections["awards"] = []
            return

        # Compteur par source
        source_info = "  ".join(f"[{s}: {len(a)}]" for s, a in sources.items() if a)
        ttk.Label(frame, text=source_info, font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, pady=(0, 5))

        # Liste cochable
        self.award_vars = []
        for award in merged:
            var = tk.BooleanVar(value=True)
            self.award_vars.append((var, award))
            ttk.Checkbutton(frame, text=award, variable=var).pack(anchor=tk.W, padx=10)

        # Boutons
        btn_row = ttk.Frame(frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout cocher",
                   command=lambda: [v.set(True) for v, _ in self.award_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Tout d√©cocher",
                   command=lambda: [v.set(False) for v, _ in self.award_vars]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TRIVIA ‚Äî s√©lecteur de source
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_trivia_section(self):
        data = self.merged_data.get("trivia", {})
        by_source = data.get("by_source", {})
        suggestion = data.get("suggestion")

        frame = self._section_frame("üìù TRIVIA")

        if not by_source:
            ttk.Label(frame, text="Aucun trivia trouv√©").pack(anchor=tk.W)
            self.selections["trivia"] = None
            return

        # Radio buttons pour chaque source
        self.trivia_var = tk.StringVar(value=list(by_source.keys())[0] if suggestion is None else
                                       next((k for k, v in by_source.items() if v == suggestion), ""))
        
        for source, text in by_source.items():
            preview = text[:100] + "..." if len(text) > 100 else text
            ttk.Radiobutton(frame, text=f"{source.upper()} ‚Äî {preview}",
                           variable=self.trivia_var, value=source).pack(anchor=tk.W, padx=10, pady=2)

        # Zone de pr√©visualisation
        self.trivia_preview = tk.Text(frame, height=4, width=80, font=("Segoe UI", 9), wrap=tk.WORD)
        self.trivia_preview.pack(fill=tk.X, padx=10, pady=5)
        
        # MAJ pr√©visualisation quand la s√©lection change
        def update_preview(*_):
            src = self.trivia_var.get()
            self.trivia_preview.delete("1.0", tk.END)
            self.trivia_preview.insert("1.0", by_source.get(src, ""))
        
        self.trivia_var.trace_add("write", update_preview)
        update_preview()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # DETAILS (Bio) ‚Äî radio source unique ou fusion
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_details_section(self):
        data = self.merged_data.get("details", {})
        by_source = data.get("by_source", {})
        fused = data.get("fused")

        frame = self._section_frame("üìñ DETAILS (Bio)")

        if not by_source:
            ttk.Label(frame, text="Aucune bio trouv√©e").pack(anchor=tk.W)
            self.selections["details"] = None
            return

        # Options
        self.details_var = tk.StringVar(value="freeones" if "freeones" in by_source else list(by_source.keys())[0])

        for source, text in by_source.items():
            length = len(text)
            ttk.Radiobutton(frame, text=f"{source.upper()} ({length} car.)",
                           variable=self.details_var, value=source).pack(anchor=tk.W, padx=10, pady=2)

        if fused:
            ttk.Radiobutton(frame, text=f"Fusion toutes sources ({len(fused)} car.)",
                           variable=self.details_var, value="_fused_").pack(anchor=tk.W, padx=10, pady=2)

        # Zone de pr√©visualisation
        self.details_preview = tk.Text(frame, height=6, width=80, font=("Segoe UI", 9), wrap=tk.WORD)
        self.details_preview.pack(fill=tk.X, padx=10, pady=5)

        def update_preview(*_):
            src = self.details_var.get()
            self.details_preview.delete("1.0", tk.END)
            if src == "_fused_":
                self.details_preview.insert("1.0", fused or "")
            else:
                self.details_preview.insert("1.0", by_source.get(src, ""))

        self.details_var.trace_add("write", update_preview)
        update_preview()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TATTOOS / PIERCINGS ‚Äî liste √©ditable
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_body_art_section(self, field: str, title: str):
        data = self.merged_data.get(field, {})
        merged = data.get("merged", [])

        frame = self._section_frame(title)

        if not merged:
            ttk.Label(frame, text=f"Aucun {field} trouv√©").pack(anchor=tk.W)
            self.selections[field] = []
            return

        # Info strat√©gie
        ttk.Label(frame, text=f"Merge auto (structur√© > flat) ‚Üí {len(merged)} entr√©es uniques",
                  font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, pady=(0, 5))

        # Liste cochable
        vars_list = []
        for item in merged:
            pos = item.get("position", "?")
            desc = item.get("description", "")
            label = f"{pos}" + (f" ({desc})" if desc else "")
            var = tk.BooleanVar(value=True)
            vars_list.append((var, item))
            ttk.Checkbutton(frame, text=label, variable=var).pack(anchor=tk.W, padx=10)

        setattr(self, f"{field}_vars", vars_list)

        # Boutons
        btn_row = ttk.Frame(frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda vl=vars_list: [v.set(True) for v, _ in vl]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda vl=vars_list: [v.set(False) for v, _ in vl]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TAGS ‚Äî liste filtrable
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_tags_section(self):
        data = self.merged_data.get("tags", {})
        merged = data.get("merged", [])
        sources = data.get("sources", {})

        frame = self._section_frame("üè∑Ô∏è TAGS")

        if not merged:
            ttk.Label(frame, text="Aucun tag trouv√©").pack(anchor=tk.W)
            self.selections["tags"] = []
            return

        # Compteur
        source_info = "  ".join(f"[{s}: {len(t)}]" for s, t in sources.items() if t)
        ttk.Label(frame, text=f"Union : {len(merged)} tags ‚Äî {source_info}",
                  font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, pady=(0, 5))

        # Filtre
        filter_frame = ttk.Frame(frame)
        filter_frame.pack(fill=tk.X, padx=10, pady=2)
        ttk.Label(filter_frame, text="Filtrer:").pack(side=tk.LEFT)
        self.tag_filter_var = tk.StringVar()
        filter_entry = ttk.Entry(filter_frame, textvariable=self.tag_filter_var, width=30)
        filter_entry.pack(side=tk.LEFT, padx=5)

        # Tags dans un frame scrollable
        tag_canvas = tk.Canvas(frame, height=150, highlightthickness=0)
        tag_scroll = ttk.Scrollbar(frame, orient=tk.VERTICAL, command=tag_canvas.yview)
        self.tag_inner = ttk.Frame(tag_canvas)

        self.tag_inner.bind("<Configure>",
                            lambda e: tag_canvas.configure(scrollregion=tag_canvas.bbox("all")))
        tag_canvas.create_window((0, 0), window=self.tag_inner, anchor="nw")
        tag_canvas.configure(yscrollcommand=tag_scroll.set)

        tag_scroll.pack(side=tk.RIGHT, fill=tk.Y)
        tag_canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=10)

        self.tag_vars = []
        for tag in merged:
            var = tk.BooleanVar(value=True)
            self.tag_vars.append((var, tag))
            ttk.Checkbutton(self.tag_inner, text=tag, variable=var).pack(anchor=tk.W)

        # Boutons
        btn_row = ttk.Frame(frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda: [v.set(True) for v, _ in self.tag_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda: [v.set(False) for v, _ in self.tag_vars]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # URLs ‚Äî tableau
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_urls_section(self):
        data = self.merged_data.get("urls", {})
        merged = data.get("merged", {})

        frame = self._section_frame("üîó URLs")

        if not merged:
            ttk.Label(frame, text="Aucune URL trouv√©e").pack(anchor=tk.W)
            self.selections["urls"] = {}
            return

        ttk.Label(frame, text=f"{len(merged)} URLs agr√©g√©es",
                  font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, pady=(0, 5))

        self.url_vars = []
        for key, url in sorted(merged.items()):
            var = tk.BooleanVar(value=True)
            self.url_vars.append((var, key, url))
            row = ttk.Frame(frame)
            row.pack(fill=tk.X, padx=10, pady=1)
            ttk.Checkbutton(row, variable=var).pack(side=tk.LEFT)
            ttk.Label(row, text=f"{key}:", width=12, anchor=tk.W,
                      font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
            ttk.Label(row, text=url, font=("Segoe UI", 9)).pack(side=tk.LEFT, fill=tk.X)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Helpers
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _section_frame(self, title: str) -> ttk.Frame:
        """Cr√©er un cadre de section avec titre."""
        sep = ttk.Separator(self.scroll_frame, orient=tk.HORIZONTAL)
        sep.pack(fill=tk.X, padx=5, pady=(10, 5))

        lbl = ttk.Label(self.scroll_frame, text=title, 
                        font=("Segoe UI", 11, "bold"))
        lbl.pack(anchor=tk.W, padx=10)

        frame = ttk.Frame(self.scroll_frame, padding=(10, 5))
        frame.pack(fill=tk.X, padx=5)
        return frame

    def _apply_all(self):
        """Collecter toutes les s√©lections et fermer."""
        self.result = {}

        # Awards
        if hasattr(self, 'award_vars'):
            self.result["awards"] = [a for v, a in self.award_vars if v.get()]
        else:
            self.result["awards"] = []

        # Trivia
        if hasattr(self, 'trivia_var'):
            src = self.trivia_var.get()
            by_source = self.merged_data.get("trivia", {}).get("by_source", {})
            self.result["trivia"] = by_source.get(src)
        else:
            self.result["trivia"] = None

        # Details
        if hasattr(self, 'details_var'):
            src = self.details_var.get()
            details_data = self.merged_data.get("details", {})
            if src == "_fused_":
                self.result["details"] = details_data.get("fused")
            else:
                self.result["details"] = details_data.get("by_source", {}).get(src)
        else:
            self.result["details"] = None

        # Tattoos
        if hasattr(self, 'tattoos_vars'):
            self.result["tattoos"] = [item for v, item in self.tattoos_vars if v.get()]
        else:
            self.result["tattoos"] = []

        # Piercings
        if hasattr(self, 'piercings_vars'):
            self.result["piercings"] = [item for v, item in self.piercings_vars if v.get()]
        else:
            self.result["piercings"] = []

        # Tags
        if hasattr(self, 'tag_vars'):
            self.result["tags"] = [t for v, t in self.tag_vars if v.get()]
        else:
            self.result["tags"] = []

        # URLs
        if hasattr(self, 'url_vars'):
            self.result["urls"] = {k: u for v, k, u in self.url_vars if v.get()}
        else:
            self.result["urls"] = {}

        self.destroy()


============================================================
[22/83] main.py
------------------------------------------------------------
from gui.launcher import start_launcher

if __name__ == "__main__":
    start_launcher()


============================================================
[23/83] rapport_Ollama_20260225-022044_part01.txt
------------------------------------------------------------
===== RAPPORT PERTINENT : E:\Ollama =====
Fichiers inclus (extensions): .bat, .cfg, .conf, .csv, .env, .ini, .json, .jsonc, .md, .ps1, .py, .rst, .toml, .txt, .yaml, .yml
Toujours inclus (noms): .editorconfig, .gitignore, Dockerfile, LICENSE, Makefile, Pipfile, Pipfile.lock, README, README.md, pyproject.toml, requirements-dev.txt, requirements.txt, setup.cfg, setup.py
Dossiers exclus: .cache, .coverage, .env, .git, .idea, .mypy_cache, .pytest_cache, .ruff_cache, .venv, .vs, .vscode, __pycache__, backup, backups, build, dist, env, log, logs, node_modules, sauvegarde, sauvegardes, site-packages, temp, tmp, venv
Extensions exclues: .7z, .avi, .bmp, .bz2, .db, .dll, .dylib, .exe, .gif, .gz, .ico, .jpeg, .jpg, .log, .mdb, .mkv, .mov, .mp3, .mp4, .otf, .png, .pyd, .rar, .so, .sqlite, .svg, .ttf, .wav, .webp, .woff, .woff2, .xz, .zip

===== ARBORESCENCE FILTR√âE =====
E:\Ollama
+--- blobs
+--- lib
|   \--- ollama
|       +--- blobs
|       +--- cuda_v12
|       +--- cuda_v13
|       +--- manifests
|       |   \--- registry.ollama.ai
|       |       \--- library
|       |           +--- deepseek-r1
|       |           +--- deepseek-v3.1
|       |           +--- dolphin-llama3
|       |           +--- llama3.1
|       |           +--- llava
|       |           +--- moondream
|       |           \--- qwen3-coder
|       +--- rocm
|       |   \--- rocblas
|       |       \--- library
|       |           \--- TensileManifest.txt
|       \--- vulkan
+--- manifests
+--- models
|   +--- blobs
|   \--- manifests
|       \--- registry.ollama.ai
|           \--- library
|               +--- bakllava
|               +--- codellama
|               +--- deepseek-coder-v2
|               +--- dolphin-llama3
|               +--- gpt-oss
|               +--- llama3
|               +--- llama3.2
|               +--- llava
|               +--- llava-llama3
|               +--- mistral
|               +--- moondream
|               +--- nous-hermes2
|               \--- qwen2.5-coder
\--- rapport_pertinent_20260225-022044
    \--- rapport_Ollama_20260225-022044_part01.txt

===== CONTENU DES FICHIERS PERTINENTS (SECRETS MASQU√âS) =====
Total fichiers : 2
------------------------------------------------------------


============================================================
[1/2] lib\ollama\rocm\rocblas\library\TensileManifest.txt
------------------------------------------------------------
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1151.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.co


============================================================
[2/2] rapport_pertinent_20260225-022044\rapport_Ollama_20260225-022044_part01.txt
------------------------------------------------------------
===== RAPPORT PERTINENT : E:\Ollama =====
Fichiers inclus (extensions): .bat, .cfg, .conf, .csv, .env, .ini, .json, .jsonc, .md, .ps1, .py, .rst, .toml, .txt, .yaml, .yml
Toujours inclus (noms): .editorconfig, .gitignore, Dockerfile, LICENSE, Makefile, Pipfile, Pipfile.lock, README, README.md, pyproject.toml, requirements-dev.txt, requirements.txt, setup.cfg, setup.py
Dossiers exclus: .cache, .coverage, .env, .git, .idea, .mypy_cache, .pytest_cache, .ruff_cache, .venv, .vs, .vscode, __pycache__, backup, backups, build, dist, env, log, logs, node_modules, sauvegarde, sauvegardes, site-packages, temp, tmp, venv
Extensions exclues: .7z, .avi, .bmp, .bz2, .db, .dll, .dylib, .exe, .gif, .gz, .ico, .jpeg, .jpg, .log, .mdb, .mkv, .mov, .mp3, .mp4, .otf, .png, .pyd, .rar, .so, .sqlite, .svg, .ttf, .wav, .webp, .woff, .woff2, .xz, .zip

===== ARBORESCENCE FILTR√âE =====
E:\Ollama
+--- blobs
+--- lib
|   \--- ollama
|       +--- blobs
|       +--- cuda_v12
|       +--- cuda_v13
|       +--- manifests
|       |   \--- registry.ollama.ai
|       |       \--- library
|       |           +--- deepseek-r1
|       |           +--- deepseek-v3.1
|       |           +--- dolphin-llama3
|       |           +--- llama3.1
|       |           +--- llava
|       |           +--- moondream
|       |           \--- qwen3-coder
|       +--- rocm
|       |   \--- rocblas
|       |       \--- library
|       |           \--- TensileManifest.txt
|       \--- vulkan
+--- manifests
+--- models
|   +--- blobs
|   \--- manifests
|       \--- registry.ollama.ai
|           \--- library
|               +--- bakllava
|               +--- codellama
|               +--- deepseek-coder-v2
|               +--- dolphin-llama3
|               +--- gpt-oss
|               +--- llama3
|               +--- llama3.2
|               +--- llava
|               +--- llava-llama3
|               +--- mistral
|               +--- moondream
|               +--- nous-hermes2
|               \--- qwen2.5-coder
\--- rapport_pertinent_20260225-022044
    \--- rapport_Ollama_20260225-022044_part01.txt

===== CONTENU DES FICHIERS PERTINENTS (SECRETS MASQU√âS) =====
Total fichiers : 2
------------------------------------------------------------


============================================================
[1/2] lib\ollama\rocm\rocblas\library\TensileManifest.txt
------------------------------------------------------------
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1151.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co


============================================================
[24/83] rapport_Ollama_20260225-031519.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  3:15:19,22

=== NVIDIA GPU Status ===
Wed Feb 25 03:15:19 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 63%   59C    P2    48W / 170W |   5934MiB /  6144MiB |     18%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     11788    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[25/83] rapport_Ollama_20260225-031832.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  3:18:32,50

=== NVIDIA GPU Status ===
Wed Feb 25 03:18:32 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 63%   59C    P2    49W / 170W |   5942MiB /  6144MiB |     49%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     11788    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[26/83] rapport_Ollama_20260225-033018.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  3:30:18,98

=== NVIDIA GPU Status ===
Wed Feb 25 03:30:19 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 83%   77C    P2   158W / 170W |   5791MiB /  6144MiB |     80%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     11788    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[27/83] rapport_Ollama_20260225-034134.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  3:41:34,87

=== NVIDIA GPU Status ===
Wed Feb 25 03:41:35 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 85%   78C    P2   160W / 170W |   5838MiB /  6144MiB |     89%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     11788    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[28/83] rapport_Ollama_20260225-035640.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  3:56:40,90

=== NVIDIA GPU Status ===
Wed Feb 25 03:56:41 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
|  0%   50C    P8    15W / 170W |   3549MiB /  6144MiB |     18%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     11788    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[29/83] rapport_Ollama_20260225-043734.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  4:37:34,41

=== NVIDIA GPU Status ===
Wed Feb 25 04:37:34 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
|  0%   50C    P8    15W / 170W |   3675MiB /  6144MiB |     25%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[30/83] rapport_Ollama_20260225-044350.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  4:43:51,04

=== NVIDIA GPU Status ===
Wed Feb 25 04:43:51 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 64%   66C    P2   156W / 170W |   5902MiB /  6144MiB |     89%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[31/83] rapport_Ollama_20260225-044628.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  4:46:28,85

=== NVIDIA GPU Status ===
Wed Feb 25 04:46:29 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 80%   76C    P2   160W / 170W |   5882MiB /  6144MiB |     97%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[32/83] rapport_Ollama_20260225-044939.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  4:49:39,25

=== NVIDIA GPU Status ===
Wed Feb 25 04:49:39 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 84%   78C    P2    73W / 170W |   5875MiB /  6144MiB |     87%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[33/83] rapport_Ollama_20260225-052435.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  5:24:35,70

=== NVIDIA GPU Status ===
Wed Feb 25 05:24:36 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 85%   77C    P2   152W / 170W |   5907MiB /  6144MiB |     84%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     11884    C+G   ..._8wekyb3d8bbwe\Photos.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A     26548    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[34/83] rapport_Ollama_20260225-053701.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  5:37:02,01

=== NVIDIA GPU Status ===
Wed Feb 25 05:37:02 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 85%   77C    P2   146W / 170W |   5884MiB /  6144MiB |     83%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A     26548    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[35/83] rapport_Ollama_20260225-055741.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  5:57:41,27

=== NVIDIA GPU Status ===
Wed Feb 25 05:57:41 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
|  0%   47C    P8    15W / 170W |   3628MiB /  6144MiB |     27%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A     26548    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[36/83] rapport_Ollama_20260225-060228.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  6:02:28,62

=== NVIDIA GPU Status ===
Wed Feb 25 06:02:28 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
|  0%   49C    P8    15W / 170W |   3640MiB /  6144MiB |     16%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A     26548    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[37/83] rapport_Ollama_20260225-061045.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  6:10:45,11

=== NVIDIA GPU Status ===
Wed Feb 25 06:10:45 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 32%   50C    P8    16W / 170W |   3642MiB /  6144MiB |     23%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A     26548    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[38/83] rapport_Ollama_20260225-061533.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  6:15:34,01

=== NVIDIA GPU Status ===
Wed Feb 25 06:15:34 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
|  0%   50C    P8    14W / 170W |   3612MiB /  6144MiB |     10%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A     26548    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[39/83] rapport_pertinent_20260225-064146\rapport_V2_20260225-064146_part01.txt
------------------------------------------------------------
===== RAPPORT PERTINENT : F:\V2 =====
Fichiers inclus (extensions): .bat, .cfg, .conf, .csv, .env, .ini, .json, .jsonc, .md, .ps1, .py, .rst, .toml, .txt, .yaml, .yml
Toujours inclus (noms): .editorconfig, .gitignore, Dockerfile, LICENSE, Makefile, Pipfile, Pipfile.lock, README, README.md, pyproject.toml, requirements-dev.txt, requirements.txt, setup.cfg, setup.py
Dossiers exclus: .cache, .coverage, .env, .git, .idea, .mypy_cache, .pytest_cache, .ruff_cache, .venv, .vs, .vscode, __pycache__, backup, backups, build, dist, env, log, logs, node_modules, sauvegarde, sauvegardes, site-packages, temp, tmp, venv
Extensions exclues: .7z, .avi, .bmp, .bz2, .db, .dll, .dylib, .exe, .gif, .gz, .ico, .jpeg, .jpg, .log, .mdb, .mkv, .mov, .mp3, .mp4, .otf, .png, .pyd, .rar, .so, .sqlite, .svg, .ttf, .wav, .webp, .woff, .woff2, .xz, .zip

===== ARBORESCENCE FILTR√âE =====
F:\V2
+--- .github
|   \--- copilot-instructions.md
+--- config
|   +--- __init__.py
|   \--- settings.yaml
+--- gui
|   +--- __init__.py
|   +--- app.py
|   +--- bio_wizard.py
|   +--- group_frame.py
|   +--- group_phase1.py
|   +--- group_phase2.py
|   +--- launcher.py
|   +--- performer_base.py
|   +--- performer_frame.py
|   +--- performer_phase1.py
|   +--- performer_phase2.py
|   +--- phase1_conflict_dialog.py
|   +--- phase2_field_wizard.py
|   \--- phase2_merge_dialog.py
+--- rapport_pertinent_20260225-064146
|   \--- rapport_V2_20260225-064146_part01.txt
+--- services
|   +--- extractors
|   |   +--- dvd
|   |   |   +--- __init__.py
|   |   |   +--- adultempire_dvd.py
|   |   |   +--- base_dvd.py
|   |   |   +--- data18_dvd.py
|   |   |   +--- iafd_dvd.py
|   |   |   \--- jeedoo_dvd.py
|   |   +--- __init__.py
|   |   +--- babepedia.py
|   |   +--- base.py
|   |   +--- freeones.py
|   |   +--- iafd.py
|   |   \--- thenude.py
|   +--- __init__.py
|   +--- bio_generator.py
|   +--- db.py
|   +--- group_phase1_merger.py
|   +--- group_phase1_scraper.py
|   +--- group_phase2_merger.py
|   +--- group_phase2_scraper.py
|   +--- phase1_merger.py
|   +--- phase2_merger.py
|   +--- phase2_scraper.py
|   \--- scrape_cache.py
+--- tests
|   +--- __init__.py
|   +--- test_db.py
|   \--- test_performer_fields.py
+--- urlscraping
|   +--- bridgette b - iafd.com_files
|   +--- Bridgette B bio _ Read about her profile at FreeOnes_files
|   \--- Bridgette B nude from Scoreland and Twistys at theNude.com_files
+--- utils
|   +--- __init__.py
|   +--- audit_markers.csv
|   +--- body_art_parser.py
|   +--- cleanup_all.py
|   +--- cleanup_specific.py
|   +--- customfield_utils.py
|   +--- duration.py
|   +--- list_short_markers.py
|   +--- marker.py
|   +--- meta_tag_utils.py
|   +--- short_markers.csv
|   \--- url_cleaner.py
+--- .gitignore
+--- BioGooglemodele.txt
+--- check_db.py
+--- Document sans titre.md
+--- main.py
+--- rapport_Ollama_20260225-022044_part01.txt
+--- rapport_Ollama_20260225-031519.txt
+--- rapport_Ollama_20260225-031832.txt
+--- rapport_Ollama_20260225-033018.txt
+--- rapport_Ollama_20260225-034134.txt
+--- rapport_Ollama_20260225-035640.txt
+--- rapport_Ollama_20260225-043734.txt
+--- rapport_Ollama_20260225-044350.txt
+--- rapport_Ollama_20260225-044628.txt
+--- rapport_Ollama_20260225-044939.txt
+--- rapport_Ollama_20260225-052435.txt
+--- rapport_Ollama_20260225-053701.txt
+--- rapport_Ollama_20260225-055741.txt
+--- rapport_Ollama_20260225-060228.txt
+--- rapport_Ollama_20260225-061045.txt
+--- rapport_Ollama_20260225-061533.txt
+--- README.md
+--- requirements.txt
+--- start.bat
+--- structure_bdd.md
+--- test_data18_search.py
\--- test_integration.py

===== CONTENU DES FICHIERS PERTINENTS (SECRETS MASQU√âS) =====
Total fichiers : 83
------------------------------------------------------------


============================================================
[1/83] .github\copilot-instructions.md
------------------------------------------------------------
- [ ] Clarify Project Requirements
- [ ] Scaffold the Project
- [ ] Customize the Project
- [ ] Install Required Extensions
- [ ] Compile the Project
- [ ] Create and Run Task
- [ ] Launch the Project
- [ ] Ensure Documentation is Complete


============================================================
[2/83] .gitignore
------------------------------------------------------------
__pycache__/
*.pyc
.env
.venv
.idea/
.vscode/


============================================================
[3/83] BioGooglemodele.txt
------------------------------------------------------------

D√©tails:
### Abella Anderson : L'√©toile charismatique au parcours diversifi√©

**Introduction**
N√©e le 16 mai 1988 √† Miami, en Floride, Abella Anderson a marqu√© de son empreinte l'industrie du divertissement pour adultes d√®s son entr√©e en sc√®ne en 2008. Reconnue pour son charisme naturel et son √©nergie captivante, elle a rapidement acquis une notori√©t√© significative. Au fil de sa carri√®re, elle a adopt√© plusieurs pseudonymes, tels qu'Amy, Latina Ruvi, Amy Quesada et m√™me Anna, qui ont tous contribu√© √† forger son image polyvalente et √† laisser un impact m√©morable dans le secteur.

**üìÖ Origines et Premiers Pas**
Issue d'une famille d'origine cubaine et ayant grandi dans le vibrant paysage de Miami, la vie d'Abella Anderson avant son immersion dans l'industrie est envelopp√©e d'une certaine discr√©tion. Les informations d√©taill√©es concernant son enfance ou son parcours scolaire ne sont pas largement divulgu√©es publiquement, soulignant une volont√© de pr√©server sa sph√®re priv√©e. C'est √† l'√¢ge de 20 ans, en 2008, qu'elle a franchi le seuil du monde du divertissement pour adultes, un choix qui allait d√©finir une d√©cennie de sa vie professionnelle et la propulser sur le devant de la sc√®ne internationale.

**üèÜ Carri√®re et Filmographie**
La trajectoire professionnelle d'Abella Anderson a d√©but√© avec une force consid√©rable, la menant √† collaborer avec certains des plus grands noms de l'industrie. D√®s les premi√®res ann√©es de sa carri√®re, elle a √©t√© une pr√©sence r√©guli√®re sur des plateformes de renom telles que Brazzers, Mofos, Naughty America et Reality Kings. Ces partenariats pr√©coces lui ont permis d'acqu√©rir une visibilit√© rapide et de se b√¢tir une solide r√©putation en tant qu'interpr√®te polyvalente.

Son √©volution l'a ensuite amen√©e √† diversifier ses r√¥les et √† travailler avec d'autres studios influents, notamment Digital Playground et Evil Angel. Elle a su s'adapter √† diff√©rents types de sc√®nes, d√©montrant une gamme de performances qui ont plu √† un large public. Bien que sa carri√®re en sc√®nes explicites ait connu son apog√©e autour de 2012-2013, sa vaste filmographie et la qualit√© constante de ses prestations lui ont assur√© une place de choix parmi les √©toiles de sa g√©n√©ration.

**üí° Faits Int√©ressants & Vie Personnelle**
Au-del√† de l'√©cran, Abella Anderson est r√©put√©e pour sa personnalit√© authentique et son approche terre-√†-terre. La sph√®re de sa vie personnelle reste, comme il est courant dans cette industrie, relativement priv√©e. On sait qu'elle a gard√© ses racines cubaines et son origine de Miami, mais les d√©tails sur ses hobbies, ses centres d'int√©r√™t en dehors du travail, ou ses animaux de compagnie ne sont pas publiquement document√©s.

En ce qui concerne les relations et les √©ventuelles chirurgies esth√©tiques, les informations pr√©cises et confirm√©es sont rares. Sa carri√®re a montr√© une grande adaptabilit√©, notamment en utilisant les alias mentionn√©s ‚Äì Abella, Amy, Latina Ruvi, Anna, Amy Quesada ‚Äì qui ont contribu√© √† son image multifacette et √† sa reconnaissance √† travers diverses bases de donn√©es comme IAFD, Freeones et The Nude. Cette discr√©tion contribue √† entretenir un certain myst√®re autour de sa personne, au-del√† de ses performances.

**üëó Apparence et Style**
Abella Anderson est souvent caract√©ris√©e par une beaut√© distinctive, ancr√©e dans ses origines cubaines. Elle arbore typiquement une chevelure fonc√©e, souvent longue et soyeuse, qui encadre un visage expressif et une silhouette g√©n√©ralement fine et athl√©tique. Son style sur sc√®ne est marqu√© par une √©nergie palpable et une capacit√© √† incarner des personnages vari√©s avec cr√©dibilit√©.

En termes de d√©tails physiques pr√©cis comme les tatouages ou les piercings, les bases de donn√©es publiques ne fournissent pas d'informations exactes ou largement connues permettant de les d√©crire avec pr√©cision. Son allure g√©n√©rale combine une sensualit√© inn√©e avec une image de "fille d'√† c√¥t√©", ce qui lui a permis de s√©duire un large √©ventail de fans et de conserver une popularit√© durable.

**üèÜ Prix et Distinctions**
La reconnaissance de l'industrie n'a pas tard√© √† se manifester pour Abella Anderson, qui a √©t√© honor√©e de nombreuses nominations au cours de sa carri√®re. Elle a notamment √©t√© cit√©e √† plusieurs reprises pour les prestigieux AVN Awards, souvent consid√©r√©s comme l'√©quivalent des Oscars dans le monde du divertissement adulte, dans des cat√©gories vari√©es qui ont mis en lumi√®re la diversit√© et la qualit√© de ses performances.

De plus, Abella Anderson a √©galement re√ßu des nominations aux XBIZ Awards, une autre c√©r√©monie majeure de l'industrie, renfor√ßant son statut d'actrice respect√©e et talentueuse. Bien que le fait de remporter un grand nombre de troph√©es ne soit pas la seule mesure du succ√®s, ses multiples nominations t√©moignent de l'appr√©ciation de ses pairs et du public pour son travail acharn√© et sa contribution significative √† l'industrie.

**Conclusion rapide**
En somme, Abella Anderson demeure une figure embl√©matique et respect√©e de l'industrie pour adultes. Son parcours, caract√©ris√© par une entr√©e remarqu√©e en 2008 et une carri√®re diversifi√©e sous plusieurs alias, a laiss√© une impression durable. Son professionnalisme, son charme et sa capacit√© √† captiver le public continuent d'√™tre salu√©s par ses fans et les connaisseurs du milieu, confirmant son statut d'√©toile marquante de sa g√©n√©ration.

============================================================
[4/83] check_db.py
------------------------------------------------------------
# check_db.py ‚Äî v√©rification rapide DB
import sqlite3, sys

DB_PATH = r"F:\stash\stash-go.sqlite"  # adapter selon votre installation

conn = sqlite3.connect(DB_PATH)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

# V√©rifier un performer (remplacer 42 par un ID r√©el test√©)
performer_id = 42

print(f"=== Performer {performer_id} ===")
cur.execute("SELECT name, details, tattoos, piercings FROM performers WHERE id=?",
            (performer_id,))
row = cur.fetchone()
if row:
    print(f"  Name: {row['name']}")
    print(f"  Details: {row['details'][:80] if row['details'] else 'VIDE'}...")
    print(f"  Tattoos: {row['tattoos']}")

cur.execute("SELECT url FROM performer_urls WHERE performer_id=?", (performer_id,))
urls = [r['url'] for r in cur.fetchall()]
print(f"  URLs: {urls}")

cur.execute("""
    SELECT t.name FROM tags t
    JOIN performers_tags pt ON pt.tag_id = t.id
    WHERE pt.performer_id=?
""", (performer_id,))
tags = [r['name'] for r in cur.fetchall()]
print(f"  Tags: {tags[:10]}")

# V√©rifier un group (remplacer 5 par un ID r√©el test√©)
group_id = 5
print(f"\n=== Group {group_id} ===")
cur.execute("SELECT name, director, description FROM groups WHERE id=?", (group_id,))
row = cur.fetchone()
if row:
    print(f"  Name: {row['name']}")
    print(f"  Director: {row['director']}")

# Sc√®nes avec URLs
cur.execute("""
    SELECT s.title, su.url
    FROM groups_scenes gs
    JOIN scenes s ON s.id = gs.scene_id
    LEFT JOIN scene_urls su ON su.scene_id = s.id
    WHERE gs.group_id=?
    LIMIT 5
""", (group_id,))
for row in cur.fetchall():
    print(f"  Sc√®ne: {row['title']} ‚Üí {row['url']}")

conn.close()

============================================================
[5/83] config\__init__.py
------------------------------------------------------------


============================================================
[6/83] config\settings.yaml
------------------------------------------------------------
# Configuration de StashMaster V2
phase1_fields:
  - Name
  - Disambiguation
  - Aliases
  - Birthdate
  - Deathdate
  - Country
  - Ethnicity
  - Hair Color
  - Eye Color
  - Height
  - Weight
  - Measurements
  - Fake Tits
  - Career Length
phase2_fields:
  - Bio
  - Trivia
  - Awards
  - Tattoos
  - Piercings
  - Tags
  - URLs
  - Details

# ‚îÄ‚îÄ Groups (DVDs) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
group_phase1_fields:
  - Title
  - Aliases
  - Date
  - Studio
  - Director
  - Duration
  - Description
  - Tags
  - URLs

group_sources_priority:
  - data18          # P1 ‚Äî tags, sc√®nes, synopsis d√©taill√©s
  - iafd_dvd        # P2 ‚Äî r√©f√©rence US/classiques, cast, dates fiables
  - adultdvdempire  # P3 ‚Äî synopsis, covers
  - jeedoo          # P4 ‚Äî productions europ√©ennes uniquement

group_phase2_sources:  # sources avec URLs de sc√®nes individuelles
  - data18
  - adultdvdempire


============================================================
[7/83] Document sans titre.md
------------------------------------------------------------
![][image1]

[image1]: <data:image/png;base64,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>

============================================================
[8/83] gui\__init__.py
------------------------------------------------------------


============================================================
[9/83] gui\app.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
import sv_ttk
from gui.performer_frame import PerformerFrame
# Placeholders for future modules
from gui.group_frame import GroupFrame
# from gui.scene_frame import SceneFrame

def launch_app(module, stash_id):
    root = tk.Tk()
    root.title(f"StashMaster V2 - {module}")
    
    # Maximiser la fen√™tre au d√©marrage (Windows)
    try:
        root.state('zoomed')
    except:
        # Fallback pour d'autres OS (Linux/Mac)
        w, h = root.winfo_screenwidth(), root.winfo_screenheight()
        root.geometry(f"{w}x{h}+0+0")

    # Appliquer le th√®me moderne
    sv_ttk.set_theme("dark")

    if module == "Performer":
        frame = PerformerFrame(root, stash_id)
    elif module == "Group":
        frame = GroupFrame(root, stash_id)
    # elif module == "Scene":
    #     frame = SceneFrame(root, stash_id)
    else:
        import tkinter.messagebox as mb
        mb.showerror("Erreur", f"Module inconnu: {module}")
        root.destroy()
        return
    frame.pack(fill=tk.BOTH, expand=True)
    root.mainloop()


============================================================
[10/83] gui\bio_wizard.py
------------------------------------------------------------
"""
BioWizard - Fen√™tre d√©di√©e √† la g√©n√©ration de biographies en 3 √©tapes.
1. G√©n√©ration Google Gemini
2. Affinage Ollama
3. Validation et injection
"""
import tkinter as tk
from tkinter import ttk, messagebox
import threading

from services.bio_generator import BioGenerator

class BioWizard(tk.Toplevel):
    def __init__(self, parent, db_data, stash_ctx, merged_data, scraped_results, checked_fields):
        super().__init__(parent)
        self.title("Assistant de G√©n√©ration de Biographie IA")
        self.geometry("1000x750")
        self.minsize(800, 600)

        self.transient(parent)
        self.grab_set()

        # Stockage
        self.db_data = db_data
        self.stash_ctx = stash_ctx
        self.merged_data = merged_data
        self.scraped_results = scraped_results
        self.checked_fields = checked_fields
        self.final_bio = None  # Le r√©sultat final sera stock√© ici

        self._build_ui()
        self.wait_window()

    def _build_ui(self):
        # Barre de progression
        self.progress_frame = ttk.Frame(self)
        self.progress_label = ttk.Label(self.progress_frame, text="Pr√™t", font=("Segoe UI", 9, "italic"))
        self.progress_label.pack(side=tk.LEFT, padx=10)
        self.progress_bar = ttk.Progressbar(self.progress_frame, mode="indeterminate", length=200)
        self.progress_bar.pack(side=tk.LEFT, padx=5)
        
        self.notebook = ttk.Notebook(self)
        self.notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)

        self.tab1_google = ttk.Frame(self.notebook, padding=10)
        self.tab2_ollama = ttk.Frame(self.notebook, padding=10)
        self.tab3_validate = ttk.Frame(self.notebook, padding=10)

        self.notebook.add(self.tab1_google, text="√âtape 1 : Google Gemini")
        self.notebook.add(self.tab2_ollama, text="√âtape 2 : Affinage Ollama", state="disabled")
        self.notebook.add(self.tab3_validate, text="√âtape 3 : Validation", state="disabled")

        self._create_google_tab()
        self._create_ollama_tab()
        self._create_validate_tab()

    def _create_google_tab(self):
        frame = self.tab1_google
        frame.grid_columnconfigure(0, weight=1)
        frame.grid_rowconfigure(1, weight=1)
        
        action_frame = ttk.Frame(frame)
        action_frame.grid(row=0, column=0, sticky=tk.NSEW, pady=(0, 10))
        
        self.btn_gen_gemini = ttk.Button(action_frame, text="üöÄ Lancer la g√©n√©ration Gemini", command=self._run_gemini_generation)
        self.btn_gen_gemini.pack(side=tk.LEFT)
        
        self.btn_copy_to_ollama = ttk.Button(action_frame, text="Continuer vers l'√©tape 2 ‚û°", state=tk.DISABLED, command=self._copy_to_ollama)
        self.btn_copy_to_ollama.pack(side=tk.LEFT, padx=10)

        self.txt_gemini_result = tk.Text(frame, wrap=tk.WORD, font=("Segoe UI", 10))
        self.txt_gemini_result.grid(row=1, column=0, sticky=tk.NSEW)
        self.txt_gemini_result.insert("1.0", "Cliquez sur 'Lancer la g√©n√©ration' pour cr√©er une biographie avec Google Gemini...")
        self.txt_gemini_result.config(state=tk.DISABLED)

    def _create_ollama_tab(self):
        frame = self.tab2_ollama
        frame.grid_columnconfigure(1, weight=1)
        frame.grid_rowconfigure(2, weight=1)
        
        action_frame = ttk.Frame(frame)
        action_frame.grid(row=0, column=0, columnspan=2, sticky=tk.NSEW, pady=(0, 10))
        self.btn_refine_ollama = ttk.Button(action_frame, text="‚öôÔ∏è Lancer l'affinage Ollama", command=self._run_ollama_refinement)
        self.btn_refine_ollama.pack(side=tk.LEFT)
        
        self.btn_copy_to_validate = ttk.Button(action_frame, text="Continuer vers la validation ‚û°", state=tk.DISABLED, command=self._copy_to_validation)
        self.btn_copy_to_validate.pack(side=tk.LEFT, padx=10)

        ttk.Label(frame, text="Biographie de base (Gemini)").grid(row=1, column=0, columnspan=2, sticky=tk.W)
        self.txt_ollama_input = tk.Text(frame, wrap=tk.WORD, height=8, font=("Segoe UI", 10), state=tk.DISABLED, relief=tk.SUNKEN)
        self.txt_ollama_input.grid(row=2, column=0, columnspan=2, sticky=tk.NSEW, pady=(0, 10))

        ttk.Label(frame, text="Biographie affin√©e (Ollama)").grid(row=3, column=0, columnspan=2, sticky=tk.W)
        self.txt_ollama_result = tk.Text(frame, wrap=tk.WORD, font=("Segoe UI", 10))
        self.txt_ollama_result.grid(row=4, column=0, columnspan=2, sticky=tk.NSEW)

    def _create_validate_tab(self):
        frame = self.tab3_validate
        frame.grid_columnconfigure(0, weight=1)
        frame.grid_rowconfigure(1, weight=1)
        
        action_frame = ttk.Frame(frame)
        action_frame.grid(row=0, column=0, sticky=tk.NSEW, pady=(0, 10))
        self.btn_inject = ttk.Button(action_frame, text="‚úÖ Valider et Utiliser cette Bio", command=self._inject_bio)
        self.btn_inject.pack(side=tk.LEFT)

        self.txt_final_bio = tk.Text(frame, wrap=tk.WORD, font=("Segoe UI", 10))
        self.txt_final_bio.grid(row=1, column=0, sticky=tk.NSEW)

    def _run_gemini_generation(self):
        self._show_progress("G√©n√©ration Gemini en cours...")
        self.btn_gen_gemini.config(state=tk.DISABLED)
        self.btn_copy_to_ollama.config(state=tk.DISABLED)

        def _do_generate():
            try:
                bio_gen = BioGenerator()
                ctx = bio_gen.build_context_from_v2(self.db_data, self.stash_ctx, self.scraped_results, self.merged_data, self.checked_fields)
                gemini_bio = bio_gen.generate_gemini_bio(ctx)

                def _update_ui():
                    self.txt_gemini_result.config(state=tk.NORMAL)
                    self.txt_gemini_result.delete("1.0", tk.END)
                    if gemini_bio:
                        self.txt_gemini_result.insert("1.0", gemini_bio)
                        self.btn_copy_to_ollama.config(state=tk.NORMAL)
                    else:
                        self.txt_gemini_result.insert("1.0", "La g√©n√©ration Gemini a √©chou√©. V√©rifiez la console pour les erreurs (cl√© API, etc.).")
                    self.txt_gemini_result.config(state=tk.DISABLED)
                    self.btn_gen_gemini.config(state=tk.NORMAL)

                self.after(0, _update_ui)
            except Exception as e:
                self.after(0, lambda: messagebox.showerror("Erreur Gemini", str(e)))
            finally:
                self.after(0, self._hide_progress)
        
        threading.Thread(target=_do_generate, daemon=True).start()

    def _copy_to_ollama(self):
        gemini_text = self.txt_gemini_result.get("1.0", tk.END)
        self.txt_ollama_input.config(state=tk.NORMAL)
        self.txt_ollama_input.delete("1.0", tk.END)
        self.txt_ollama_input.insert("1.0", gemini_text)
        self.txt_ollama_input.config(state=tk.DISABLED)
        self.txt_ollama_result.delete("1.0", tk.END) # Clear previous results
        self.notebook.tab(1, state="normal")
        self.notebook.select(self.tab2_ollama)

    def _run_ollama_refinement(self):
        gemini_bio = self.txt_ollama_input.get("1.0", tk.END).strip()
        if not gemini_bio or "√©chou√©" in gemini_bio:
            messagebox.showwarning("Bio de base manquante", "La biographie de base (Gemini) est n√©cessaire pour l'affinage.")
            return

        self._show_progress("Affinage Ollama en cours...")
        self.btn_refine_ollama.config(state=tk.DISABLED)
        self.btn_copy_to_validate.config(state=tk.DISABLED)

        def _do_refine():
            try:
                bio_gen = BioGenerator()
                ctx = bio_gen.build_context_from_v2(self.db_data, self.stash_ctx, self.scraped_results, self.merged_data, self.checked_fields)
                ollama_bio = bio_gen.generate_ollama_bio(ctx, gemini_bio)

                def _update_ui():
                    self.txt_ollama_result.delete("1.0", tk.END)
                    if ollama_bio:
                        self.txt_ollama_result.insert("1.0", ollama_bio)
                        self.btn_copy_to_validate.config(state=tk.NORMAL)
                    else:
                         self.txt_ollama_result.insert("1.0", "L'affinage Ollama a √©chou√©. V√©rifiez que le serveur Ollama est bien lanc√©.")
                
                self.after(0, _update_ui)
            except Exception as e:
                self.after(0, lambda: messagebox.showerror("Erreur Ollama", str(e)))
            finally:
                self.after(0, self._hide_progress)
                self.after(0, self.btn_refine_ollama.config, {'state': tk.NORMAL})
        
        threading.Thread(target=_do_refine, daemon=True).start()
    
    def _copy_to_validation(self):
        ollama_text = self.txt_ollama_result.get("1.0", tk.END)
        self.txt_final_bio.delete("1.0", tk.END)
        self.txt_final_bio.insert("1.0", ollama_text)
        self.notebook.tab(2, state="normal")
        self.notebook.select(self.tab3_validate)

    def _inject_bio(self):
        """Stocke la bio finale et ferme la fen√™tre."""
        self.final_bio = self.txt_final_bio.get("1.0", tk.END).strip()
        self.destroy()

    def _show_progress(self, message):
        self.progress_label.config(text=message)
        self.progress_frame.pack(fill=tk.X, padx=10, pady=5, before=self.notebook)
        self.progress_bar.start(10)

    def _hide_progress(self):
        self.progress_bar.stop()
        self.progress_frame.pack_forget()


============================================================
[11/83] gui\group_frame.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
from gui.group_phase1 import GroupPhase1Frame
from gui.group_phase2 import GroupPhase2Frame

class GroupFrame(ttk.Frame):
    def __init__(self, parent, stash_id):
        super().__init__(parent)
        self.stash_id = stash_id
        self.current_frame = None
        
        # For now, just a label
        label = ttk.Label(self, text=f"Group Frame for ID: {self.stash_id}")
        label.pack(pady=20, padx=20)

        self.goto_phase1()

    def goto_phase1(self):
        if self.current_frame:
            self.current_frame.destroy()
        
        self.current_frame = GroupPhase1Frame(self, self, self.stash_id)
        self.current_frame.pack(fill=tk.BOTH, expand=True)

    def goto_phase2(self, group_data, scenes_data):
        if self.current_frame:
            self.current_frame.destroy()
            
        self.current_frame = GroupPhase2Frame(self, self, self.stash_id, group_data, scenes_data)
        self.current_frame.pack(fill=tk.BOTH, expand=True)

    def return_to_menu(self):
        # Fermer la fen√™tre actuelle
        self.master.destroy()
        # Relancer le launcher
        try:
            from gui.launcher import start_launcher
            start_launcher()
        except Exception as e:
            print(f"Erreur lors du retour au menu: {e}")


============================================================
[12/83] gui\group_phase1.py
------------------------------------------------------------
"""
Placeholder for Group Phase 1 Frame.
The content for this file is in PLAN_GROUPS_V2.md, which was not provided.
"""
import tkinter as tk
from tkinter import ttk, messagebox
import threading
import yaml

from services.db import GroupDB
from services.group_phase1_scraper import GroupPhase1ScraperService
from services.group_phase1_merger import GroupPhase1Merger
from gui.phase1_conflict_dialog import Phase1ConflictDialog
from services.phase2_scraper import Phase2ScraperService


class GroupPhase1Frame(ttk.Frame):
    def __init__(self, parent, controller, group_id):
        super().__init__(parent)
        self.controller = controller
        self.group_id = group_id

        self._group_data = None
        self._scenes_data = [] # Scenes associ√©es au group

        self.field_checkboxes = {}
        self.fields = {}

        self.create_ui()
        self._load_data()

    def create_ui(self):
        # Header
        header_frame = ttk.Frame(self)
        header_frame.pack(fill=tk.X, padx=10, pady=5)
        ttk.Label(header_frame, text=f"Group ID: {self.group_id}",
                  font=("Segoe UI", 14, "bold")).pack(side=tk.LEFT)
        ttk.Button(header_frame, text="Retour Launcher",
                   command=self.controller.return_to_menu).pack(side=tk.RIGHT)

        # ScrolledFrame pour les champs du Group
        self.main_canvas = tk.Canvas(self, highlightthickness=0)
        self.main_scrollbar = ttk.Scrollbar(self, orient=tk.VERTICAL, command=self.main_canvas.yview)
        self.main_scrollable_frame = ttk.Frame(self.main_canvas)

        self.main_scrollable_frame.bind(
            "<Configure>",
            lambda e: self.main_canvas.configure(
                scrollregion=self.main_canvas.bbox("all")
            )
        )
        self.main_canvas.create_window((0, 0), window=self.main_scrollable_frame, anchor="nw")
        self.main_canvas.configure(yscrollcommand=self.main_scrollbar.set)

        self.main_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.main_canvas.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Section Group Details
        group_details_frame = ttk.LabelFrame(self.main_scrollable_frame, text="Group Details (Phase 1)", padding=10)
        group_details_frame.pack(fill=tk.X, padx=10, pady=5)

        # TODO: Charger les champs depuis settings.yaml
        self.fields_list = [
            "Title", "Aliases", "Date", "Studio", "Director",
            "Duration", "Description", "Tags", "URLs"
        ]

        for i, field in enumerate(self.fields_list):
            row = i
            var = tk.BooleanVar(value=True)
            self.field_checkboxes[field] = var
            checkbox = ttk.Checkbutton(group_details_frame, variable=var, text="")
            checkbox.grid(row=row, column=0, sticky=tk.W, padx=(5, 0), pady=2)

            ttk.Label(group_details_frame, text=f"{field}:").grid(row=row, column=1, sticky=tk.W, padx=5, pady=2)
            entry = tk.Text(group_details_frame, height=2, width=60, font=("Segoe UI", 9))
            entry.grid(row=row, column=2, sticky=tk.EW, padx=5, pady=2)
            self.fields[field] = entry

        group_details_frame.grid_columnconfigure(2, weight=1)

        # Boutons d'action
        action_frame = ttk.Frame(self.main_scrollable_frame)
        action_frame.pack(fill=tk.X, padx=10, pady=5)
        ttk.Button(action_frame, text="üîé Analyser & Phase 2", command=self._run_phase1).pack(side=tk.LEFT, padx=5)

        # Section Sc√®nes Associ√©es
        self.scenes_frame = ttk.LabelFrame(self.main_scrollable_frame, text="Sc√®nes Associ√©es au Group", padding=10)
        self.scenes_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        ttk.Label(self.scenes_frame, text="Chargement des sc√®nes...", font=("Segoe UI", 9, "italic")).pack()

    def _load_data(self):
        try:
            gid = int(self.group_id)
        except ValueError:
            gid = self.group_id

        db = GroupDB()
        self._group_data = db.get_group_by_id(gid)
        self._scenes_data = db.get_group_scenes(gid)
        db.close()

        print(f"DEBUG: Loaded group data for ID {gid}: {bool(self._group_data)}")
        print(f"DEBUG: Loaded {len(self._scenes_data)} scenes")

        if self._group_data:
            for field_name, entry_widget in self.fields.items():
db_key="***MASKED***"
                value = self._group_data.get(db_key)
                if field_name == "Studio" and self._group_data.get("studio_name"):
                    value = self._group_data["studio_name"]
                elif field_name == "Tags" and value:
                    value = ", ".join(value)
                elif field_name == "URLs" and value:
                    value = "\n".join(value)

                if value:
                    entry_widget.config(state=tk.NORMAL)
                    entry_widget.delete("1.0", tk.END)
                    entry_widget.insert("1.0", str(value))
                    entry_widget.config(state=tk.DISABLED)

        self._display_scenes()

    def _display_scenes(self):
        for widget in self.scenes_frame.winfo_children():
            widget.destroy()

        if not self._scenes_data:
            ttk.Label(self.scenes_frame, text="Aucune sc√®ne associ√©e.", font=("Segoe UI", 9, "italic")).pack()
            return

        # Treeview pour les sc√®nes
        columns = ("index", "title", "urls")
        tree = ttk.Treeview(self.scenes_frame, columns=columns, show="headings", height=8)
        tree.heading("index", text="#")
        tree.heading("title", text="Titre Stash")
        tree.heading("urls", text="URLs Existantes")
        
        tree.column("index", width=30, anchor=tk.CENTER)
        tree.column("title", width=300, anchor=tk.W)
        tree.column("urls", width=400, anchor=tk.W)

        for s in self._scenes_data:
            urls_str = ", ".join(s.get("existing_urls", []))
            tree.insert("", tk.END, values=(
                s.get("scene_index", "?"),
                s.get("scene_title", "Sans titre"),
                urls_str
            ))
        
        tree.pack(fill=tk.BOTH, expand=True)

    def _run_phase1(self):
        """Lance le scraping Phase 1 Group."""
        checked = [f for f, var in self.field_checkboxes.items() if var.get()]
        if not checked:
            messagebox.showwarning("Attention", "Aucun champ coch√©.")
            return

        # Afficher progression
        progress_popup = tk.Toplevel(self)
        progress_popup.title("Scraping Group Phase 1...")
        progress_popup.geometry("300x100")
        prog_label = ttk.Label(progress_popup, text="Initialisation...")
        prog_label.pack(pady=20)

        def _do():
            try:
                scraper = GroupPhase1ScraperService()
                title = self.fields["Title"].get("1.0", tk.END).strip()
                year = self.fields["Date"].get("1.0", tk.END).strip()[:4] # Ann√©e
                known_urls = self.fields["URLs"].get("1.0", tk.END).strip().split("\n")
                known_urls = [u.strip() for u in known_urls if u.strip()]

                def update_prog(src, st):
                    self.after(0, lambda: prog_label.config(text=f"[{src}] {st}"))

                scraped = scraper.scrape(title, year, known_urls, progress_callback=update_prog)
                
                # Check for Data18
                has_data18 = any(r.get("_source") == "data18" for r in scraped)
                
                if not has_data18:
                    self.after(0, lambda: self._ask_data18_and_continue(scraped, checked, scraper, progress_popup))
                else:
                    self.after(0, lambda: self._finish_phase1(scraped, checked, progress_popup))

            except Exception as e:
                self.after(0, lambda: [progress_popup.destroy(), messagebox.showerror("Erreur", str(e))])

        threading.Thread(target=_do, daemon=True).start()

    def _ask_data18_and_continue(self, scraped, checked, scraper, progress_popup):
        from tkinter import simpledialog
        # Cacher la popup de progression temporairement
        progress_popup.withdraw()
        
        url = simpledialog.askstring(
            "Data18 Manquant", 
            "Data18 n'a pas √©t√© trouv√© automatiquement.\n\n"
            "Pour avoir les meilleurs r√©sultats (Tags, Sc√®nes...), collez l'URL Data18 ici :\n"
            "(Sinon, laissez vide et OK pour continuer)",
            parent=self
        )
        
        progress_popup.deiconify()
        
        if url and "data18.com" in url:
            # Relancer un petit thread pour scraper cette URL
            def _scrape_extra():
                try:
                    from services.extractors.dvd.data18_dvd import Data18DVDExtractor
                    e = Data18DVDExtractor()
                    res = e.extract_from_url(url.strip())
                    if res:
                        scraped.append(res)
                except Exception as e:
                    print(f"Error scraping extra URL: {e}")
                
                self.after(0, lambda: self._finish_phase1(scraped, checked, progress_popup))
            
            threading.Thread(target=_scrape_extra, daemon=True).start()
        else:
            self._finish_phase1(scraped, checked, progress_popup)

    def _finish_phase1(self, scraped, checked, progress_popup):
        progress_popup.destroy()
        merger = GroupPhase1Merger()
        merged = merger.merge(self._group_data, scraped, checked)
        self._show_conflict_dialog(merged, checked)

    def _show_conflict_dialog(self, merged_data: dict, checked_fields: list[str]):
        from services.group_phase1_merger import GROUP_FIELDS
        group_title = self.fields["Title"].get("1.0", tk.END).strip()
        dialog = Phase1ConflictDialog(self, group_title, merged_data, GROUP_FIELDS)
        if dialog.result:
            self._inject_phase1(dialog.result)

    def _inject_phase1(self, result: dict):
        try:
            # Pour Group Phase 1, on injecte directement via GroupDB (√† impl√©menter ou simuler)
            # On r√©utilise les champs mapp√©s
            db_updates = {}
            from services.group_phase1_merger import GROUP_FIELDS
            for field, value in result.items():
db_key="***MASKED***"
                if db_key:
                    db_updates[db_key] = value

            # Simulation d'injection (ou ajout de la m√©thode dans GroupDB)
            print(f"DEBUG: Injecting group updates: {db_updates}")
            
            # TODO: Impl√©menter GroupDB.update_group(self.group_id, db_updates)
            
            messagebox.showinfo("‚úÖ Phase 1 Termin√©e", "Les donn√©es du Group ont √©t√© mises √† jour.")
            
            # Passer √† la Phase 2
            self.controller.goto_phase2(self._group_data, self._scenes_data)

        except Exception as e:
            messagebox.showerror("Erreur Injection", str(e))




============================================================
[13/83] gui\group_phase2.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, messagebox
import threading

from services.db import GroupDB
from services.group_phase2_scraper import GroupPhase2ScraperService
from services.group_phase2_merger import GroupPhase2Merger

STATUS_ICONS = {
    "new":         "üü¢",
    "partial":     "üü†",
    "already_present": "üîµ",
    "no_match":    "‚ö™",
}

class GroupPhase2Frame(ttk.Frame):
    def __init__(self, parent, controller, group_id, group_data, scenes_data):
        super().__init__(parent)
        self.controller = controller
        self.group_id = group_id
        self._group_data = group_data # Donn√©es Group d√©j√† scrap√©es/fusionn√©es Phase 1
        self._scenes_data = scenes_data # Liste des sc√®nes Stash du Group
        self._merged_scene_urls = [] # R√©sultat de la fusion Phase 2

        self.scene_checkboxes = [] # Pour cocher les URLs √† injecter

        self.create_ui()
        self._run_phase2()

    def create_ui(self):
        # Header
        header_frame = ttk.Frame(self)
        header_frame.pack(fill=tk.X, padx=10, pady=5)
        ttk.Label(header_frame, text=f"Group ID: {self.group_id} ‚Äî Phase 2: URLs Sc√®nes",
                  font=("Segoe UI", 14, "bold")).pack(side=tk.LEFT)
        ttk.Button(header_frame, text="Retour Phase 1",
                   command=lambda: self.controller.goto_phase1()).pack(side=tk.RIGHT)

        # Progress bar (similaire √† Performer Phase 2)
        self.progress_frame = ttk.Frame(self)
        self.progress_label = ttk.Label(self.progress_frame, text="", 
                                        font=("Segoe UI", 9, "italic"))
        self.progress_label.pack(side=tk.LEFT, padx=10)
        self.progress_bar = ttk.Progressbar(self.progress_frame, mode="indeterminate", length=200)
        self.progress_bar.pack(side=tk.LEFT, padx=5)

        # Zone principale scrollable pour les sc√®nes
        self.main_canvas = tk.Canvas(self, highlightthickness=0)
        self.main_scrollbar = ttk.Scrollbar(self, orient=tk.VERTICAL, command=self.main_canvas.yview)
        self.main_scrollable_frame = ttk.Frame(self.main_canvas)

        self.main_scrollable_frame.bind(
            "<Configure>",
            lambda e: self.main_canvas.configure(
                scrollregion=self.main_canvas.bbox("all")
            )
        )
        self.main_canvas_window = self.main_canvas.create_window((0, 0), window=self.main_scrollable_frame, anchor="nw")
        self.main_canvas.configure(yscrollcommand=self.main_scrollbar.set)
        
        # Mousewheel scrolling
        self.main_canvas.bind_all("<MouseWheel>",
                             lambda e: self.main_canvas.yview_scroll(int(-1 * (e.delta / 120)), "units"))
        
        # Resize canvas window width to match canvas width
        self.main_canvas.bind("<Configure>",
                         lambda e: self.main_canvas.itemconfig(self.main_canvas_window, width=e.width))


        self.main_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.main_canvas.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Boutons d'action
        action_frame = ttk.Frame(self)
        action_frame.pack(fill=tk.X, padx=10, pady=5)
        ttk.Button(action_frame, text="‚úî Injecter s√©lectionn√©es", command=self._inject).pack(side=tk.LEFT, padx=5)
        ttk.Button(action_frame, text="Tout s√©lectionner", command=self._select_all).pack(side=tk.LEFT, padx=5)
        ttk.Button(action_frame, text="D√©s√©lectionner tout", command=self._deselect_all).pack(side=tk.LEFT, padx=5)

    def _run_phase2(self):
        self._show_progress("Scraping URLs de sc√®nes...")

        def _do_scraping_and_merge():
            try:
                scraper = GroupPhase2ScraperService()
                
                def update_prog(src, st):
                    self.after(0, lambda: self._update_progress(f"[{src}] {st}"))

                scraped_urls_by_index = scraper.scrape(
                    group_data=self._group_data,
                    progress_callback=update_prog
                )

                merger = GroupPhase2Merger()
                self._merged_scene_urls = merger.merge(
                    self._scenes_data, scraped_urls_by_index)

                self.after(0, self._display_results)

            except Exception as e:
                self.after(0, lambda: messagebox.showerror("Erreur Phase 2", str(e)))
            finally:
                self.after(0, self._hide_progress)

        threading.Thread(target=_do_scraping_and_merge, daemon=True).start()

    def _display_results(self):
        for widget in self.main_scrollable_frame.winfo_children():
            widget.destroy()

        if not self._merged_scene_urls:
            ttk.Label(self.main_scrollable_frame, text="Aucune URL de sc√®ne trouv√©e ou fusionn√©e.",
                      font=("Segoe UI", 10, "italic")).pack(padx=10, pady=10)
            return

        # Headers du tableau
        header_frame = ttk.Frame(self.main_scrollable_frame)
        header_frame.pack(fill=tk.X, padx=5, pady=2)
        ttk.Label(header_frame, text="", width=4).pack(side=tk.LEFT) # Checkbox
        ttk.Label(header_frame, text="Statut", width=8, font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
        ttk.Label(header_frame, text="Index", width=6, font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
        ttk.Label(header_frame, text="Titre Stash", width=40, anchor=tk.W, font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
        ttk.Label(header_frame, text="Nouvelles URLs", anchor=tk.W, font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT, expand=True, fill=tk.X)

        self.scene_checkboxes = []
        for scene_data in self._merged_scene_urls:
            self._build_scene_row(self.main_scrollable_frame, scene_data)

    def _build_scene_row(self, parent_frame, scene_data):
        row_frame = ttk.Frame(parent_frame, padding=2)
        row_frame.pack(fill=tk.X, padx=5, pady=1)

        status_icon = STATUS_ICONS.get(scene_data["status"], "")

        # Cocher par d√©faut si nouvelles URLs (statut 'new' ou 'partial')
        has_new = bool(scene_data.get("new_urls"))
        var = tk.BooleanVar(value=has_new) 
        self.scene_checkboxes.append((var, scene_data)) # Stocker tuple (var, data)
        
        chk = ttk.Checkbutton(row_frame, variable=var)
        chk.pack(side=tk.LEFT)
        if not has_new:
            chk.config(state=tk.DISABLED)

        ttk.Label(row_frame, text=status_icon, width=4).pack(side=tk.LEFT)
        ttk.Label(row_frame, text=str(scene_data.get("scene_index", "?")), width=6).pack(side=tk.LEFT)
        ttk.Label(row_frame, text=scene_data.get("scene_title", "Sans titre"), width=40, anchor=tk.W).pack(side=tk.LEFT)

        urls_frame = ttk.Frame(row_frame)
        urls_frame.pack(side=tk.LEFT, expand=True, fill=tk.X)

        new_urls = scene_data.get("new_urls", {})
        existing = scene_data.get("existing_urls", [])

        if new_urls:
            for src, url in new_urls.items():
                ttk.Label(urls_frame, text=f"‚ûï {src.upper()}: {url}", anchor=tk.W, font=("Segoe UI", 8, "bold"), foreground="green").pack(fill=tk.X)
        
        if existing:
            count = len(existing)
            ttk.Label(urls_frame, text=f"Existing: {count} URLs", anchor=tk.W, foreground="gray", font=("Segoe UI", 8)).pack(fill=tk.X)
        
        if not new_urls and not existing:
             ttk.Label(urls_frame, text="‚Äî", anchor=tk.W, foreground="gray", font=("Segoe UI", 8)).pack(fill=tk.X)

    def _inject(self):
        selected_urls_to_inject = []
        for var, scene_data in self.scene_checkboxes:
            if var.get():
                for url_src, url_val in scene_data["new_urls"].items():
                    selected_urls_to_inject.append({
                        "scene_id": scene_data["scene_id"],
                        "url": url_val,
                        "source": url_src,
                    })
        
        if not selected_urls_to_inject:
            messagebox.showwarning("Attention", "Aucune URL s√©lectionn√©e √† injecter.")
            return

        try:
            db = GroupDB()
            # appel √† la m√©thode inject_scene_urls que j'ai ajout√©e dans db.py
            db.inject_scene_urls(selected_urls_to_inject) 
            db.close()
            
            messagebox.showinfo("‚úÖ Injection Phase 2", f"{len(selected_urls_to_inject)} URLs de sc√®nes inject√©es.")

            # Optionnel : Recharger ou fermer
            # self._run_phase2() 

        except Exception as e:
            messagebox.showerror("Erreur injection", str(e))

    def _select_all(self):
        for var, _ in self.scene_checkboxes:
            if str(var['state']) != tk.DISABLED:
                var.set(True)

    def _deselect_all(self):
        for var, _ in self.scene_checkboxes:
            var.set(False)

    def _show_progress(self, message):
        self.progress_frame.pack(fill=tk.X, padx=10, pady=2, before=self.main_canvas)
        self.progress_bar.start(10)
        self.progress_label.config(text=message)

    def _update_progress(self, message):
        self.progress_label.config(text=message)

    def _hide_progress(self):
        self.progress_bar.stop()
        self.progress_frame.pack_forget()


============================================================
[14/83] gui\launcher.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, simpledialog, messagebox
import sv_ttk
from gui.app import launch_app

def center_window(window, width=400, height=300):
    screen_width = window.winfo_screenwidth()
    screen_height = window.winfo_screenheight()
    x = (screen_width - width) // 2
    y = (screen_height - height) // 2
    window.geometry(f'{width}x{height}+{x}+{y}')

def start_launcher():
    root = tk.Tk()
    root.title("StashMaster V2 - Launcher")
    
    # Appliquer le th√®me moderne sombre
    sv_ttk.set_theme("dark")
    
    center_window(root, 400, 250)
    
    ttk.Label(root, text="StashMaster V2", font=("Segoe UI", 16, "bold")).pack(pady=(20, 10))
    ttk.Label(root, text="S√©lectionnez un module :", font=("Segoe UI", 10)).pack(pady=5)
    
    def on_select(module):
        root.withdraw()
        # Utiliser un prompt simple pour l'ID
        stash_id = simpledialog.askstring("Entrer l'ID", f"Entrez l'ID pour le module {module} :", parent=root)
        if not stash_id:
            messagebox.showwarning("ID requis", "Vous devez entrer un ID valide pour continuer.")
            root.deiconify()
            return
        root.destroy()
        # Lancer l'application principale maximis√©e
        launch_app(module, stash_id)
        
    ttk.Button(root, text="Performer", width=25, command=lambda: on_select("Performer")).pack(pady=5)
    ttk.Button(root, text="Group / DVD", width=25, command=lambda: on_select("Group")).pack(pady=5)
    ttk.Button(root, text="Scene", width=25, command=lambda: on_select("Scene")).pack(pady=5)
    
    root.mainloop()

if __name__ == "__main__":
    start_launcher()

============================================================
[15/83] gui\performer_base.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk

class PerformerBaseFrame(ttk.Frame):
    def __init__(self, parent, controller, stash_id):
        super().__init__(parent)
        self.controller = controller
        self.stash_id = stash_id
        self.fields = {}
        self.field_checkboxes = {}
        # To be defined in subclasses
        self.fields_list = [] 
        self.db_mapping = {}

    def create_header(self, title, buttons_config):
        bar = ttk.Frame(self, padding=5)
        bar.pack(fill=tk.X)
        
        ttk.Label(bar, text=f"{title} | ID: {self.stash_id}", font=("Segoe UI", 12, "bold")).pack(side=tk.LEFT, padx=5)
        
        btn_frame = ttk.Frame(bar)
        btn_frame.pack(side=tk.RIGHT, padx=5)
        
        for text, command in buttons_config:
            ttk.Button(btn_frame, text=text, command=command).pack(side=tk.LEFT, padx=2)

    def select_all_fields(self):
        for var in self.field_checkboxes.values():
            var.set(True)

    def select_empty_fields(self):
        for field, entry in self.fields.items():
            if field in self.field_checkboxes and entry:
                val = ""
                if isinstance(entry, tk.Entry):
                    val = entry.get().strip()
                elif isinstance(entry, tk.Text):
                    val = entry.get("1.0", tk.END).strip()
                
                if not val:
                    self.field_checkboxes[field].set(True)
                else:
                    self.field_checkboxes[field].set(False)

    def load_data(self):
        try:
            from services.db import PerformerDB
            db = PerformerDB()
            data = db.get_performer_by_id(self.stash_id)
            db.close()
        except Exception as e:
            print(f"Erreur DB: {e}")
            data = None
        
        if not data:
            return

        for field, db_key in self.db_mapping.items():
            entry = self.fields.get(field)
            if entry and db_key in data:
                value = data[db_key]
                if isinstance(value, (list, tuple)):
                    if field == "URLs":
                        value = "\n".join(value)
                    else:
                        value = ", ".join(value)
                
                if isinstance(entry, tk.Entry):
                    entry.delete(0, tk.END)
                    entry.insert(0, str(value) if value is not None else "")
                elif isinstance(entry, tk.Text):
                    entry.delete('1.0', tk.END)
                    entry.insert('1.0', str(value) if value is not None else "")


============================================================
[16/83] gui\performer_frame.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
from gui.performer_phase1 import PerformerPhase1Frame
from gui.performer_phase2 import PerformerPhase2Frame

class PerformerFrame(ttk.Frame):
    def __init__(self, parent, stash_id):
        super().__init__(parent)
        self.stash_id = stash_id
        self.current_frame = None
        self.phase1_data = {} # Pour stocker les donn√©es r√©solues de la phase 1
        
        self.goto_phase1()

    def goto_phase1(self):
        if self.current_frame:
            self.current_frame.destroy()
        
        self.current_frame = PerformerPhase1Frame(self, self, self.stash_id)
        self.current_frame.pack(fill=tk.BOTH, expand=True)

    def goto_phase2(self, phase1_updates: dict):
        """Passe √† la phase 2 en emportant les donn√©es r√©solues de la phase 1."""
        self.phase1_data = phase1_updates

        if self.current_frame:
            self.current_frame.destroy()
            
        # Initialise la phase 2 avec les donn√©es de la phase 1
        self.current_frame = PerformerPhase2Frame(self, self, self.stash_id, self.phase1_data)
        self.current_frame.pack(fill=tk.BOTH, expand=True)




    def return_to_menu(self):
        # Fermer la fen√™tre actuelle
        self.master.destroy()
        # Relancer le launcher
        try:
            from gui.launcher import start_launcher
            start_launcher()
        except Exception as e:
            print(f"Erreur lors du retour au menu: {e}")




============================================================
[17/83] gui\performer_phase1.py
------------------------------------------------------------
import tkinter as tk
from tkinter import messagebox
from tkinter import ttk
import threading

from gui.performer_base import PerformerBaseFrame
from gui.phase1_conflict_dialog import Phase1ConflictDialog


class PerformerPhase1Frame(PerformerBaseFrame):
    def __init__(self, parent, controller, stash_id):
        super().__init__(parent, controller, stash_id)
        
        # Configuration des champs Phase 1
        self.fields_list = [
            "Name", "Aliases", "Birthdate", "Deathdate", "Country", "Ethnicity",
            "Hair Color", "Eye Color", "Height", "Weight", "Measurements", "Fake Tits", "Career Length"
        ]
        
        self.db_mapping = {
            "Name": "name",
            "Aliases": "aliases",
            "Birthdate": "birthdate",
            "Deathdate": "death_date",
            "Country": "country",
            "Ethnicity": "ethnicity",
            "Hair Color": "hair_color",
            "Eye Color": "eye_color",
            "Height": "height",
            "Weight": "weight",
            "Measurements": "measurements",
            "Fake Tits": "fake_tits",
            "Career Length": "career_length"
        }
        
        self.create_ui()
        self.load_data()

    def process_and_goto_phase2(self):
        """
        Workflow Phase 1 : Scrape, compare, et pr√©pare les donn√©es pour la Phase 2.
        AUCUNE injection en base de donn√©es n'est faite ici.
        """
        checked_fields = [f for f, var in self.field_checkboxes.items() if var.get()]
        if not checked_fields:
            self.controller.goto_phase2({}) # Passe un dict vide si pas de scraping
            return
        
        try:
            from services.db import PerformerDB
            db = PerformerDB()
            db_data = db.get_performer_by_id(self.stash_id)
            db.close()
        except Exception as e:
            messagebox.showerror("Erreur", f"Impossible de lire la DB: {e}")
            return

        if not db_data:
            messagebox.showerror("Erreur", "Performer non trouv√© dans la base.")
            return

        performer_name = db_data["name"]
        known_urls = db_data.get("urls", [])

        self.progress_frame.pack(fill=tk.X, padx=10, pady=2, after=list(self.fields.values())[-1].master)
        self.progress_bar.start(10)
        self.progress_label.config(text=f"Scraping {len(checked_fields)} champs pour {performer_name}...")

        def _do_scraping():
            try:
                from services.phase2_scraper import Phase2ScraperService
                from services.phase1_merger import Phase1Merger

                scraper = Phase2ScraperService()
                results = scraper.scrape(performer_name, known_urls=known_urls,
                                         progress_callback=lambda s, m: self.after(0, self.progress_label.config, {'text': f"[{s}] {m}"}))
                
                # Nettoyage des donn√©es scrap√©es AVANT le merge
                for res in results:
                    if res.get("hair_color"):
                        # D√©duplication et nettoyage (ex: "Blonde, Blonde" -> "Blonde")
                        parts = [p.strip() for p in res["hair_color"].replace('/', ',').split(',') if p.strip()]
                        seen = set()
                        unique = []
                        for p in parts:
                            p_cap = p.capitalize()
                            if p_cap not in seen:
                                seen.add(p_cap)
                                unique.append(p_cap)
                        res["hair_color"] = ", ".join(unique)

                merger = Phase1Merger()
                merge_results = merger.merge(db_data, results, checked_fields)
                
                # Construire l'affichage pour TOUS les champs coch√©s
                display_results = {}
                for field in checked_fields:
                    # Le merger renvoie les r√©sultats index√©s par le nom du champ (ex: "Name")
                    if field in merge_results:
                        display_results[field] = merge_results[field]
                    else:
                        # Si le merger n'a rien renvoy√©, on force l'affichage en mode "empty"
                        # pour confirmer √† l'utilisateur que le champ a √©t√© trait√© mais sans r√©sultat.
db_key="***MASKED***"
                        current_val = db_data.get(db_key) if db_key else None
                        display_results[field] = {'status': 'empty', 'db_value': current_val, 'scraped_values': {}, 'suggestion': None}

                self.after(0, self._hide_progress)

                phase1_updates = {}
                if display_results:
                    dialog = Phase1ConflictDialog(self.master, performer_name, display_results, self.db_mapping)
                    if dialog.result is not None:
                        # Le dialogue retourne les valeurs √† mettre √† jour
                        for field_name, resolved_value in dialog.result.items():
db_key="***MASKED***"
                            if db_key:
                                # Sp√©cial pour les alias, on veut une liste
                                if db_key == 'aliases' and isinstance(resolved_value, str):
                                     phase1_updates[db_key] = [a.strip() for a in resolved_value.split(',') if a.strip()]
                                elif db_key == 'hair_color' and isinstance(resolved_value, str):
                                     # Nettoyage et d√©duplication pour la couleur de cheveux
                                     parts = [p.strip() for p in resolved_value.replace('/', ',').split(',') if p.strip()]
                                     seen = set()
                                     unique_parts = []
                                     for p in parts:
                                         if p.lower() not in seen:
                                             seen.add(p.lower())
                                             unique_parts.append(p)
                                     phase1_updates[db_key] = ", ".join(unique_parts)
                                else:
                                     phase1_updates[db_key] = resolved_value
                        
                        messagebox.showinfo("Phase 1 Trait√©e", f"{len(phase1_updates)} modifications de la Phase 1 ont √©t√© pr√©par√©es pour la validation finale.")
                    else:
                        # L'utilisateur a annul√©, on ne passe aucune modification
                        messagebox.showinfo("Annul√©", "Fusion des donn√©es annul√©e. Passage en phase 2 sans appliquer les changements de la phase 1.")
                else:
                    messagebox.showinfo("Phase 1 Compl√®te", "Aucun conflit ou nouvelle donn√©e √† traiter. Passage en Phase 2.")
                
                # Passer en Phase 2 avec les donn√©es r√©solues de la phase 1
                self.after(0, lambda: self.controller.goto_phase2(phase1_updates))

            except Exception as e:
                err_msg = str(e)
                self.after(0, lambda: messagebox.showerror("Erreur Phase 1", f"Une erreur est survenue : {err_msg}"))
            finally:
                self.after(0, self._hide_progress)

        threading.Thread(target=_do_scraping, daemon=True).start()

    def _hide_progress(self):
        self.progress_bar.stop()
        self.progress_frame.pack_forget()

    def create_ui(self):
        # Header + Boutons sp√©cifiques
        buttons = [
            ("Tout s√©lectionner", self.select_all_fields),
            ("S√©lectionner vides", self.select_empty_fields),
            ("Suivant / Traiter", self.process_and_goto_phase2),
            ("Retour", self.controller.return_to_menu),
        ]
        self.create_header("Phase 1 : M√©tadonn√©es usuelles", buttons)

        # Barre de progression (cach√©e par d√©faut)
        self.progress_frame = ttk.Frame(self)
        self.progress_label = ttk.Label(self.progress_frame, text="",
                                        font=("Segoe UI", 9, "italic"))
        self.progress_label.pack(side=tk.LEFT, padx=10)
        self.progress_bar = ttk.Progressbar(self.progress_frame, mode="indeterminate", length=200)
        self.progress_bar.pack(side=tk.LEFT, padx=5)

        # Zone des champs
        f = ttk.Frame(self)
        f.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        
        # Configuration de la grille pour l'extension horizontale
        f.grid_columnconfigure(0, weight=0)  # Checkbox
        f.grid_columnconfigure(1, weight=0)  # Label
        f.grid_columnconfigure(2, weight=1)  # Entry (prend tout l'espace restant)

        for i, field in enumerate(self.fields_list):
            row = i
            # Checkbox (d√©coch√©e par d√©faut)
            var = tk.BooleanVar(value=False)
            self.field_checkboxes[field] = var
            checkbox = ttk.Checkbutton(f, variable=var, text="")
            checkbox.grid(row=row, column=0, sticky=tk.W, padx=(5, 0), pady=2)
            
            # Label
            ttk.Label(f, text=f"{field}:").grid(row=row, column=1, sticky=tk.NW, padx=5, pady=2)
            
            # Entry Widget
            entry = ttk.Entry(f, width=60)
            entry.grid(row=row, column=2, sticky=tk.EW, padx=5, pady=2)
            self.fields[field] = entry


============================================================
[18/83] gui\performer_phase2.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, messagebox
import threading
import copy

from gui.performer_base import PerformerBaseFrame
from gui.bio_wizard import BioWizard


class PerformerPhase2Frame(PerformerBaseFrame):
    def __init__(self, parent, controller, stash_id, phase1_data: dict):
        # phase1_data contient les modifications r√©solues de la phase 1
        self.phase1_data = phase1_data
        self.phase2_data = {} # Pour stocker les modifications de la phase 2

        super().__init__(parent, controller, stash_id)
        
        self.fields_list = ["Trivia", "Awards", "Tattoos", "Piercings", "Tags", "URLs", "Details"]
        self.db_mapping = {
            "Trivia": "trivia", "Awards": "awards", "Tattoos": "tattoos",
            "Piercings": "piercings", "Tags": "tags", "URLs": "urls", "Details": "details"
        }
        
        self._scraped_results = None
        self._merged_data = None
        self._db_data = None
        self._stash_context = None
        
        self.create_ui()
        self.load_data()

    def load_data(self):
        """Charge les donn√©es initiales de la DB, puis applique les modifs de Phase 1."""
        super().load_data() # Charge depuis la DB dans self.fields

        # Applique les donn√©es de la phase 1 par-dessus les donn√©es de la DB
        if self.phase1_data:
            for db_key, value in self.phase1_data.items():
                # Trouve le nom du champ UI correspondant √† la cl√© DB
                field_name = next((name for name, key in self.db_mapping.items() if key == db_key), None)
                if field_name:
                    display_value = value
                    if isinstance(value, list):
                        display_value = ", ".join(value)
                    self._update_field_display(field_name, display_value)

    def create_ui(self):
        buttons = [
            ("üîé Scraper les sources", self.run_scraping),
            ("üßô Lancer l'assistant Bio IA", self.run_bio_wizard),
            ("Tout s√©lectionner", self.select_all_fields),
            ("S√©lectionner vides", self.select_empty_fields),
            ("Retour Phase 1", self.controller.goto_phase1),
        ]
        self.create_header("Phase 2 : Champs Avanc√©s", buttons)

        self.progress_frame = ttk.Frame(self)
        self.progress_label = ttk.Label(self.progress_frame, text="", font=("Segoe UI", 9, "italic"))
        self.progress_label.pack(side=tk.LEFT, padx=10)
        self.progress_bar = ttk.Progressbar(self.progress_frame, mode="indeterminate", length=200)
        self.progress_bar.pack(side=tk.LEFT, padx=5)

        f = ttk.Frame(self)
        f.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        f.grid_columnconfigure(2, weight=1)

        for i, field in enumerate(self.fields_list):
            var = tk.BooleanVar(value=False)
            self.field_checkboxes[field] = var
            ttk.Checkbutton(f, variable=var, text="").grid(row=i, column=0, sticky=tk.W, padx=(5, 0), pady=5)
            ttk.Label(f, text=f"{field}:").grid(row=i, column=1, sticky=tk.NW, padx=5, pady=5)
            
            height = 3
            if field == "Details": height = 8
            if field == "URLs" or field == "Awards": height = 5
            
            entry = tk.Text(f, width=60, height=height, font=("Segoe UI", 10))
            entry.grid(row=i, column=2, sticky=tk.EW, padx=5, pady=5)
            self.fields[field] = entry
            
        self._set_bio_button_state(tk.DISABLED)

    def run_scraping(self):
        try:
            from services.db import PerformerDB
            db = PerformerDB()
            self._db_data = db.get_performer_by_id(self.stash_id)
            self._stash_context = db.get_performer_context(self.stash_id)
            db.close()
        except Exception as e:
            messagebox.showerror("Erreur", f"Impossible de lire la DB: {e}")
            return

        if not self._db_data:
            messagebox.showerror("Erreur", "Performer non trouv√© dans la base.")
            return

        # Fusionner les donn√©es de la DB avec celles de la Phase 1 pour le scraping
temp_data_for_scraping="***MASKED***"
        temp_data_for_scraping.update(self.phase1_data)

        performer_name = temp_data_for_scraping.get("name", "")
        known_urls = temp_data_for_scraping.get("urls", [])

        self.progress_frame.pack(fill=tk.X, padx=10, pady=2, before=self.fields["Trivia"].master)
        self.progress_bar.start(10)
        self.progress_label.config(text=f"Scraping en cours pour {performer_name}...")
        self._set_bio_button_state(tk.DISABLED)

        def _do_scraping():
            try:
                from services.phase2_scraper import Phase2ScraperService
                from services.phase2_merger import Phase2Merger

                scraper = Phase2ScraperService()
                results = scraper.scrape(performer_name, known_urls=known_urls,
                                         progress_callback=lambda s, m: self.after(0, self.progress_label.config, {'text': f"[{s}] {m}"}))
                self._scraped_results = results

                merger = Phase2Merger()
                # Le merger doit utiliser les donn√©es √† jour (DB + Phase 1)
                self._merged_data = merger.merge(temp_data_for_scraping, results)
                
                self.after(0, lambda: self._set_bio_button_state(tk.NORMAL))
                self.after(0, lambda: self._show_wizard(self._merged_data, self._stash_context, temp_data_for_scraping))
            except Exception as e:
                err_msg = str(e)  # Capturer le message d'erreur imm√©diatement
                self.after(0, lambda: messagebox.showerror("Erreur scraping", err_msg))
            finally:
                self.after(0, self._hide_progress)

        threading.Thread(target=_do_scraping, daemon=True).start()

    def _set_bio_button_state(self, state):
        if hasattr(self, "header_buttons") and "üßô Lancer l'assistant Bio IA" in self.header_buttons:
            self.header_buttons["üßô Lancer l'assistant Bio IA"].config(state=state)

    def run_bio_wizard(self):
        """Lance l'assistant Bio en passant toutes les donn√©es accumul√©es."""
        if not self._db_data:
            messagebox.showwarning("Donn√©es manquantes", "Veuillez d'abord 'Scraper les sources'.")
            return

        # Cr√©er un contexte de donn√©es consolid√© pour le wizard
        # Ordre de fusion: DB < Phase 1 < Phase 2
        combined_data = copy.deepcopy(self._db_data)
        combined_data.update(self.phase1_data)
        combined_data.update(self.phase2_data)

        checked_fields = [f for f, var in self.field_checkboxes.items() if var.get()]
        
        wizard = BioWizard(
            parent=self,
            db_data=combined_data, # Utilise les donn√©es les plus √† jour
            stash_ctx=self._stash_context,
            merged_data=self._merged_data, # Donn√©es scrap√©es/fusionn√©es Phase 2
            scraped_results=self._scraped_results or [],
            checked_fields=checked_fields
        )
        
        if wizard.final_bio:
            self._update_field_display("Details", wizard.final_bio)
            self.phase2_data['details'] = wizard.final_bio # Sauvegarde la bio pour l'injection finale
            messagebox.showinfo("Biographie Pr√©par√©e", "La biographie a √©t√© mise √† jour et est pr√™te pour l'injection finale.")

    def _hide_progress(self):
        self.progress_bar.stop()
        self.progress_frame.pack_forget()

    def _show_wizard(self, merged_data: dict, stash_context: dict, db_data: dict):
        from gui.phase2_field_wizard import Phase2FieldWizard
        checked_fields = [f for f, var in self.field_checkboxes.items() if var.get()]
        wizard = Phase2FieldWizard(self.winfo_toplevel(), merged_data, stash_context, db_data, self._scraped_results, checked_fields)
        if wizard.result:
            self._apply_phase2_results(wizard.result)

    def _apply_phase2_results(self, result: dict):
        """Applique les r√©sultats du wizard Phase 2 en m√©moire, SANS injection DB."""
        # 'result' contient les donn√©es valid√©es du wizard
        # On met √† jour l'UI et on stocke dans self.phase2_data
        
        # Details (bio)
        if result.get("details"):
            self.phase2_data["details"] = result["details"]
            self._update_field_display("Details", result["details"])

        # Tattoos
        if result.get("tattoos"):
            tattoo_str = "; ".join(f"{t['position']} ({t['description']})" if t.get('description') else t['position'] for t in result["tattoos"])
            self.phase2_data["tattoos"] = tattoo_str
            self._update_field_display("Tattoos", tattoo_str)

        # Piercings
        if result.get("piercings"):
            piercing_str = "; ".join(f"{p['position']} ({p['description']})" if p.get('description') else p['position'] for p in result["piercings"])
            self.phase2_data["piercings"] = piercing_str
            self._update_field_display("Piercings", piercing_str)

        # Tags
        if result.get("tags"):
            import re
            tags_raw = result["tags"]
            tags = set([t.strip() for t in re.split(r'[ ,]+', tags_raw) if t.strip()])
            self.phase2_data["tags"] = list(tags)
            self._update_field_display("Tags", ", ".join(tags))

        # URLs
        if result.get("urls"):
            urls_raw = result["urls"]
            if isinstance(urls_raw, str): urls = set([u.strip() for u in urls_raw.splitlines() if u.strip()])
            elif isinstance(urls_raw, dict): urls = set([u.strip() for u in urls_raw.values() if u.strip()])
            else: urls = set([u.strip() for u in urls_raw if u.strip()])
            self.phase2_data["urls"] = list(urls)
            self._update_field_display("URLs", "\n".join(urls))

        # Awards et Trivia ‚Üí trait√©s diff√©remment car ce sont des custom fields
        custom_fields_updates = []
        if result.get("awards"):
            custom_fields_updates.extend([{"type": "award", "value": line.strip()} for line in result["awards"] if line.strip()])
        if result.get("trivia"):
            custom_fields_updates.extend([{"type": "trivia", "value": line.strip()} for line in result["trivia"].split("\n") if line.strip()])
        if custom_fields_updates:
            self.phase2_data['customfields'] = custom_fields_updates
        
        messagebox.showinfo("‚úÖ Donn√©es pr√©par√©es", "Les modifications de la Phase 2 ont √©t√© pr√©par√©es et sont pr√™tes pour la validation finale.")

    def _update_field_display(self, field_name: str, value: str):
        entry = self.fields.get(field_name)
        if entry and isinstance(entry, tk.Text):
            entry.delete("1.0", tk.END)
            if value:
                entry.insert("1.0", value)
            self.field_checkboxes.get(field_name, tk.BooleanVar()).set(True)


============================================================
[19/83] gui\phase1_conflict_dialog.py
------------------------------------------------------------
"""
Phase1ConflictDialog ‚Äî Dialogue de r√©solution des conflits Phase 1.
Affiche confirmations, nouveaux et conflits pour chaque champ coch√©.
"""
import tkinter as tk
from tkinter import ttk


# Code couleur par statut
STATUS_COLORS = {
    "confirmed": "#2ecc71",  # vert
    "new": "#3498db",        # bleu
    "conflict": "#e74c3c",   # rouge
    "empty": "#95a5a6",      # gris
}

STATUS_ICONS = {
    "confirmed": "‚úÖ",
    "new": "üÜï",
    "conflict": "‚ö†Ô∏è",
    "empty": "‚¨ú",
}


class Phase1ConflictDialog(tk.Toplevel):
    """
    Dialogue modal montrant le r√©sultat du scraping Phase 1.
    Pour chaque champ :
    - Confirm√© (vert) ‚Üí DB == source
    - Nouveau (bleu) ‚Üí DB vide, suggestion disponible
    - Conflit (rouge) ‚Üí DB ‚â† sources ‚Üí l'utilisateur choisit
    - Vide (gris) ‚Üí rien trouv√©
    """

    def __init__(self, parent, performer_name: str, merge_result: dict, db_mapping: dict):
        super().__init__(parent)
        self.performer_name = performer_name
        self.title(f"üîç R√©sultat scraping Phase 1 pour {performer_name}")
        self.merge_result = merge_result
        self.db_mapping = db_mapping
        self.result = None  # Dict final si valid√©

        self.geometry("800x600")
        self.minsize(750, 500)
        self.transient(parent)
        self.grab_set()

        # Variables de s√©lection par champ
        self.selections = {}

        self._build_ui()
        self.wait_window()

    def _build_ui(self):
        # ‚îÄ‚îÄ Compteurs en haut ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        header = ttk.Frame(self, padding=10)
        header.pack(fill=tk.X)

        counts = {"confirmed": 0, "new": 0, "conflict": 0, "empty": 0}
        for info in self.merge_result.values():
            counts[info["status"]] = counts.get(info["status"], 0) + 1

        summary = (
            f"‚úÖ Confirm√©s: {counts['confirmed']}  |  "
            f"üÜï Nouveaux: {counts['new']}  |  "
            f"‚ö†Ô∏è Conflits: {counts['conflict']}  |  "
            f"‚¨ú Vides: {counts['empty']}"
        )
        ttk.Label(header, text=summary, font=("Segoe UI", 11, "bold")).pack(anchor=tk.W)

        # ‚îÄ‚îÄ Zone scrollable ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        main = ttk.Frame(self)
        main.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        canvas = tk.Canvas(main, highlightthickness=0)
        scrollbar = ttk.Scrollbar(main, orient=tk.VERTICAL, command=canvas.yview)
        self.scroll_frame = ttk.Frame(canvas)

        self.scroll_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        canvas.create_window((0, 0), window=self.scroll_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        canvas.bind_all("<MouseWheel>",
                        lambda e: canvas.yview_scroll(int(-1 * (e.delta / 120)), "units"))

        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # ‚îÄ‚îÄ Lignes par champ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        for field_name, info in self.merge_result.items():
            self._build_field_row(field_name, info)

        # ‚îÄ‚îÄ Boutons ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        btn_frame = ttk.Frame(self, padding=10)
        btn_frame.pack(fill=tk.X)

        ttk.Button(btn_frame, text="‚úÖ Appliquer et continuer",
                   command=self._apply).pack(side=tk.RIGHT, padx=5)
        ttk.Button(btn_frame, text="‚ùå Annuler",
                   command=self.destroy).pack(side=tk.RIGHT, padx=5)

    def _build_field_row(self, field_name: str, info: dict):
        """Construire une ligne pour un champ."""
        status = info["status"]
        db_value = info.get("db_value") or ""
        scraped_values = info.get("scraped_values", {})
        suggestion = info.get("suggestion") or ""

        # Cadre principal du champ
        frame = ttk.Frame(self.scroll_frame, padding=(5, 3))
        frame.pack(fill=tk.X, padx=5, pady=2)

        # Ic√¥ne + Nom du champ
        icon = STATUS_ICONS.get(status, "")
        ttk.Label(frame, text=f"{icon} {field_name}", width=20, anchor=tk.W,
                  font=("Segoe UI", 10, "bold")).pack(side=tk.LEFT, padx=(0, 10))

        if status == "confirmed":
            # Tout va bien ‚Äî afficher simplement la valeur
            ttk.Label(frame, text=f"‚úì {db_value}",
                      font=("Segoe UI", 10)).pack(side=tk.LEFT, fill=tk.X, expand=True)
            self.selections[field_name] = tk.StringVar(value=db_value)
            
            # Afficher les sources qui confirment la valeur
            if scraped_values:
                sources = ", ".join(k.upper() for k in scraped_values.keys())
                ttk.Label(frame, text=f"[{sources}]", font=("Segoe UI", 8, "italic")).pack(side=tk.LEFT, padx=5)

        elif status == "empty":
            # Rien trouv√©
            ttk.Label(frame, text="(aucune donn√©e)",
                      font=("Segoe UI", 10, "italic")).pack(side=tk.LEFT)
            self.selections[field_name] = tk.StringVar(value="")

        elif status == "new":
            # Nouvelle valeur ‚Äî proposer la suggestion modifiable
            var = tk.StringVar(value=suggestion)
            self.selections[field_name] = var
            ttk.Label(frame, text="DB: (vide) ‚Üí",
                      font=("Segoe UI", 9, "italic")).pack(side=tk.LEFT, padx=(0, 5))
            entry = ttk.Entry(frame, textvariable=var, width=50)
            entry.pack(side=tk.LEFT, fill=tk.X, expand=True)
            # Source info
            sources = ", ".join(scraped_values.keys())
            ttk.Label(frame, text=f"[{sources}]",
                      font=("Segoe UI", 8)).pack(side=tk.LEFT, padx=5)

        elif status == "conflict":
            # Conflit ‚Äî radio buttons pour choisir
            self._build_conflict_section(frame, field_name, db_value, scraped_values, suggestion)

    def _build_conflict_section(self, parent, field_name, db_value, scraped_values, suggestion):
        """Construire la section de r√©solution de conflit."""
        # Sous-frame pour les options
        conflict_frame = ttk.Frame(parent)
        conflict_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        var = tk.StringVar(value=suggestion)
        self.selections[field_name] = var

        # Option 1 : garder la valeur DB
        ttk.Radiobutton(
            conflict_frame,
            text=f"DB: {db_value}",
            variable=var,
            value=db_value
        ).pack(anchor=tk.W)

        # Options : valeurs scrap√©es
        for source, val in scraped_values.items():
            ttk.Radiobutton(
                conflict_frame,
                text=f"{source.upper()}: {val}",
                variable=var,
                value=val
            ).pack(anchor=tk.W)

    def _apply(self):
        """Collecter les s√©lections et fermer."""
        self.result = {}
        for field_name, var in self.selections.items():
            val = var.get().strip()
            if val:
                self.result[field_name] = val
        self.destroy()


============================================================
[20/83] gui\phase2_field_wizard.py
------------------------------------------------------------
"""
Phase2FieldWizard ‚Äî Dialogue pas-√†-pas pour la r√©solution des champs Phase 2.
Pr√©sente un champ √† la fois avec le contexte Stash.
"""
import tkinter as tk
from tkinter import ttk


# Ordre des pages du wizard
WIZARD_PAGES = [
    ("awards", "üèÜ AWARDS"),
    ("trivia", "üìù TRIVIA"),
    ("tattoos", "üé® TATTOOS"),
    ("piercings", "üíâ PIERCINGS"),
    ("tags", "üè∑Ô∏è TAGS"),
    ("urls", "üîó URLs"),
    ("details", "üìñ DETAILS (Bio)"),
]


class Phase2FieldWizard(tk.Toplevel):
    """
    Wizard pas-√†-pas : une page par champ Phase 2.
    Chaque page affiche le contexte Stash + les donn√©es scrap√©es.
    """

    def __init__(self, parent, merged_data: dict, stash_context: dict,
                 db_data: dict, scraped_results: list[dict] = None, checked_fields: list[str] | None = None):
        super().__init__(parent)
        self.title("üìã Wizard Phase 2 ‚Äî R√©solution pas-√†-pas")
        self.merged_data = merged_data
        self.stash_ctx = stash_context
        self.db_data = db_data
        self.scraped_results = scraped_results or []  # Stocker les r√©sultats bruts
        self.checked_fields = checked_fields or []   # ‚Üê AJOUTER
        self.result = None
        self.generated_bio = None

        self.geometry("950x700")
        self.minsize(900, 600)
        self.transient(parent)
        self.grab_set()

        # √âtat du wizard
        self.current_page = 0
        self.selections = {}

        # Construire les cadres
        self._build_shell()
        self._show_page(0)
        self.wait_window()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # STRUCTURE PRINCIPALE
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_shell(self):
        """Cr√©er la coquille du wizard (header, zone contenu, boutons nav)."""
        # ‚îÄ‚îÄ Header ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        self.header_frame = ttk.Frame(self, padding=10)
        self.header_frame.pack(fill=tk.X)

        self.page_title = ttk.Label(self.header_frame, text="",
                                     font=("Segoe UI", 14, "bold"))
        self.page_title.pack(side=tk.LEFT)

        self.page_counter = ttk.Label(self.header_frame, text="",
                                       font=("Segoe UI", 10))
        self.page_counter.pack(side=tk.RIGHT)

        # Progress bar
        self.progress = ttk.Progressbar(self, maximum=len(WIZARD_PAGES), length=400)
        self.progress.pack(fill=tk.X, padx=10, pady=(0, 5))

        # ‚îÄ‚îÄ Zone contenu scrollable ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        content_wrapper = ttk.Frame(self)
        content_wrapper.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        self.canvas = tk.Canvas(content_wrapper, highlightthickness=0)
        scrollbar = ttk.Scrollbar(content_wrapper, orient=tk.VERTICAL, command=self.canvas.yview)
        self.content_frame = ttk.Frame(self.canvas)

        self.content_frame.bind(
            "<Configure>",
            lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all"))
        )
        self.canvas_window = self.canvas.create_window((0, 0), window=self.content_frame, anchor="nw")
        self.canvas.configure(yscrollcommand=scrollbar.set)
        self.canvas.bind_all("<MouseWheel>",
                             lambda e: self.canvas.yview_scroll(int(-1 * (e.delta / 120)), "units"))

        # Resize canvas window width
        self.canvas.bind("<Configure>",
                         lambda e: self.canvas.itemconfig(self.canvas_window, width=e.width))

        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # ‚îÄ‚îÄ Boutons navigation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        nav_frame = ttk.Frame(self, padding=10)
        nav_frame.pack(fill=tk.X)

        self.btn_prev = ttk.Button(nav_frame, text="‚óÄ Pr√©c√©dent", command=self._prev_page)
        self.btn_prev.pack(side=tk.LEFT, padx=5)

        ttk.Button(nav_frame, text="‚ùå Annuler", command=self.destroy).pack(side=tk.LEFT, padx=5)

        self.btn_next = ttk.Button(nav_frame, text="Suivant ‚ñ∂", command=self._next_page)
        self.btn_next.pack(side=tk.RIGHT, padx=5)

        self.btn_skip = ttk.Button(nav_frame, text="‚è≠ Passer", command=self._skip_page)
        self.btn_skip.pack(side=tk.RIGHT, padx=5)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # NAVIGATION
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _prev_page(self):
        if self.current_page > 0:
            self._save_current()
            self.current_page -= 1
            self._show_page(self.current_page)

    def _next_page(self):
        self._save_current()
        if self.current_page < len(WIZARD_PAGES) - 1:
            self.current_page += 1
            self._show_page(self.current_page)
        else:
            self._finish()

    def _skip_page(self):
        """Passer sans sauvegarder le champ courant."""
        if self.current_page < len(WIZARD_PAGES) - 1:
            self.current_page += 1
            self._show_page(self.current_page)
        else:
            self._finish()

    def _show_page(self, idx):
        """Afficher la page √† l'index donn√©."""
        field_key, title = WIZARD_PAGES[idx]

        # MAJ header
        self.page_title.config(text=title)
        self.page_counter.config(text=f"√âtape {idx + 1} / {len(WIZARD_PAGES)}")
        self.progress["value"] = idx + 1

        # MAJ boutons
        self.btn_prev.config(state=tk.NORMAL if idx > 0 else tk.DISABLED)
        self.btn_next.config(text="‚úÖ Terminer" if idx == len(WIZARD_PAGES) - 1 else "Suivant ‚ñ∂")

        # Vider le contenu
        for w in self.content_frame.winfo_children():
            w.destroy()

        # Construire la page selon le champ
        builder = {
            "details": self._page_details,
            "awards": self._page_awards,
            "trivia": self._page_trivia,
            "tattoos": self._page_body_art,
            "piercings": self._page_body_art,
            "tags": self._page_tags,
            "urls": self._page_urls,
        }
        builder_fn = builder.get(field_key, lambda k: None)
        builder_fn(field_key)

        # Scroll en haut
        self.canvas.yview_moveto(0)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CONTEXTE STASH (affich√© sur chaque page)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _add_stash_context(self, parent):
        """Ajouter un panneau de contexte Stash."""
        ctx = self.stash_ctx
        if not ctx:
            return

        frame = ttk.LabelFrame(parent, text="üìä Contexte Stash", padding=8)
        frame.pack(fill=tk.X, padx=5, pady=5)

        info_parts = []
        if ctx.get("scene_count"):
            info_parts.append(f"üé¨ {ctx['scene_count']} sc√®nes")
        if ctx.get("studios"):
            studios_str = ", ".join(ctx["studios"][:10])
            if len(ctx["studios"]) > 10:
                studios_str += f" (+{len(ctx['studios']) - 10})"
            info_parts.append(f"üè¢ Studios: {studios_str}")
        if ctx.get("groups"):
            groups_str = ", ".join(ctx["groups"][:8])
            if len(ctx["groups"]) > 8:
                groups_str += f" (+{len(ctx['groups']) - 8})"
            info_parts.append(f"üìÄ Groups: {groups_str}")
        if ctx.get("collaborators"):
            top5 = [f"{c['name']} ({c['count']})" for c in ctx["collaborators"][:5]]
            info_parts.append(f"üë• Top collabs: {', '.join(top5)}")

        for part in info_parts:
            ttk.Label(frame, text=part, font=("Segoe UI", 9), wraplength=800).pack(
                anchor=tk.W, pady=1)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: DETAILS (Bio)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_details(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("details", {})
        by_source = data.get("by_source", {})
        fused = data.get("fused")
        db_bio = self.db_data.get("details") or self.db_data.get("bio") or ""

        # Contexte Stash
        self._add_stash_context(parent)

        # Valeur Stash actuelle
        if db_bio:
            stash_frame = ttk.LabelFrame(parent, text="üìã Bio actuelle (Stash)", padding=8)
            stash_frame.pack(fill=tk.X, padx=5, pady=5)
            stash_text = tk.Text(stash_frame, height=4, width=80, font=("Segoe UI", 9),
                                 wrap=tk.WORD, state=tk.DISABLED)
            stash_text.pack(fill=tk.X)
            stash_text.config(state=tk.NORMAL)
            stash_text.insert("1.0", db_bio)
            stash_text.config(state=tk.DISABLED)

        # Choix de source
        self._details_choice_frame = ttk.LabelFrame(parent, text="üîÑ Choisir la bio", padding=8)
        self._details_choice_frame.pack(fill=tk.X, padx=5, pady=5)

        options = {}
        if db_bio:
            options["stash"] = db_bio

        for source, text in by_source.items():
            options[source] = text

        if fused:
            options["_fused_"] = fused

        # Ajouter la bio IA si d√©j√† g√©n√©r√©e (persistance)
        if self.generated_bio:
            options["_ia_"] = self.generated_bio

        if not options:
            ttk.Label(self._details_choice_frame, text="Aucune bio disponible",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W)
            return

        # R√©cup√©rer la s√©lection pr√©c√©dente ou d√©faut
        prev = self.selections.get("details", {}).get("_choice")
        
        # S√©lection par d√©faut : IA si dispo, sinon Stash, sinon premi√®re source
        default = prev
        if not default:
            default = "_ia_" if self.generated_bio else ("stash" if db_bio else list(options.keys())[0])

        self._details_var = tk.StringVar(value=default)
        self._details_options = options

        for key, text in options.items():
            label = key.upper() if key != "_fused_" else "FUSION"
            length = len(text)
            ttk.Radiobutton(self._details_choice_frame, text=f"{label} ({length} car.)",
                           variable=self._details_var, value=key).pack(anchor=tk.W, padx=5, pady=2)

        # Preview
        self._details_preview = tk.Text(parent, height=8, width=80,
                                         font=("Segoe UI", 9), wrap=tk.WORD)
        self._details_preview.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)

        def update(*_):
            self._details_preview.delete("1.0", tk.END)
            self._details_preview.insert("1.0", options.get(self._details_var.get(), ""))

        self._details_var.trace_add("write", update)
        update()

        # NOUVEAU : Bouton g√©n√©ration IA
        gen_frame = ttk.LabelFrame(parent, text="‚ú® G√©n√©ration IA", padding=8)
        gen_frame.pack(fill=tk.X, padx=5, pady=5)

        self._bio_status = ttk.Label(gen_frame, text="Pr√™t (Auto)", font=("Segoe UI", 9, "italic"))
        self._bio_status.pack(side=tk.LEFT, padx=10)

        ttk.Button(
            gen_frame,
            text="üîÑ R√©g√©n√©rer (Gemini/Ollama)",
            command=self._run_bio_generation
        ).pack(side=tk.RIGHT, padx=5)

        # Lancement automatique si pas encore de bio IA
        if not self.generated_bio:
            self.after(500, self._run_bio_generation)
        else:
            self._bio_status.config(text="‚úÖ Bio IA d√©j√† disponible")

    def _run_bio_generation(self):
        """Lance la g√©n√©ration IA en thread."""
        import threading
        import copy
        self._bio_status.config(text="üöÄ G√©n√©ration auto en cours (Gemini > Ollama)...")

        def t():
            from services.bio_generator import BioGenerator
            gen = BioGenerator()

            # Utiliser les vrais champs coch√©s (Phase 1 + Phase 2 disponibles)
            all_checked = (
                self.checked_fields if self.checked_fields
                else list(self.merged_data.keys())
            )
            # Ajouter tous les champs Phase 1 potentiellement utiles.
            # Le g√©n√©rateur d√©cidera d'utiliser les specs techniques (Taille/Poids...) uniquement si les infos sont limit√©es.
            all_checked.extend(["Name", "Birthdate", "Height", "Weight", "Measurements", "Fake Tits", "Hair Color", "Eye Color", "Ethnicity", "Country", "Aliases", "Career Length"])
            
            # S'assurer que Awards et URLs sont coch√©s s'ils existent dans les donn√©es fusionn√©es
            all_checked.extend(["Awards", "URLs"])
            
            # Utiliser les donn√©es fusionn√©es, mais mettre √† jour avec les s√©lections utilisateur
            # faites dans les onglets pr√©c√©dents (puisque Details est maintenant √† la fin)
            effective_merged = copy.deepcopy(self.merged_data)
            if "awards" in self.selections:
                effective_merged.setdefault("awards", {})["merged"] = self.selections["awards"]["value"]
            
            # AJOUT : Injecter les URLs valid√©es (onglet 6) pour que l'IA connaisse les bons r√©seaux sociaux
            if "urls" in self.selections:
                effective_merged.setdefault("urls", {})["merged"] = self.selections["urls"]["value"]

            ctx = gen.build_context_from_v2(
                db_data=self.db_data,
                stash_ctx=self.stash_ctx,
                scraped_results=self.scraped_results,  # Passer les r√©sultats bruts
                merged_data=effective_merged,
                checked_fields=all_checked,
            )
            bio = gen.generate(ctx)

            def update():
                if bio:
                    self.generated_bio = bio
                    # Mise √† jour UI seulement si on est encore sur la page Details
current_key="***MASKED***"
                    if hasattr(self, "_details_options") and current_key == "details":
                        if "_ia_" not in self._details_options:
                            self._details_options["_ia_"] = bio
                            ttk.Radiobutton(
                                self._details_choice_frame,
                                text=f"ü§ñ IA GENERATED ({len(bio)} car.)",
                                variable=self._details_var,
                                value="_ia_"
                            ).pack(anchor=tk.W, padx=5, pady=2)
                        self._details_var.set("_ia_") # S√©lectionne et met √† jour la preview via trace
                    self._bio_status.config(text=f"‚úÖ Bio g√©n√©r√©e ({len(bio)} car.)")
                else:
                    self._bio_status.config(
                        text="‚ùå √âchec ‚Äî V√©rifier .gemini_key ou Ollama actif")

            self.after(0, update)

        threading.Thread(target=t, daemon=True).start()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: AWARDS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_awards(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("awards", {})
        merged = data.get("merged", [])
        sources = data.get("sources", {})

        self._add_stash_context(parent)

        if not merged:
            ttk.Label(parent, text="Aucun award trouv√©",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        # Info sources
        info_frame = ttk.LabelFrame(parent, text="üìä Sources", padding=8)
        info_frame.pack(fill=tk.X, padx=5, pady=5)
        source_info = "  ".join(f"[{s}: {len(a)}]" for s, a in sources.items() if a)
        ttk.Label(info_frame, text=source_info, font=("Segoe UI", 9)).pack(anchor=tk.W)

        # Liste cochable
        list_frame = ttk.LabelFrame(parent, text=f"üèÜ Awards trouv√©s ({len(merged)})", padding=8)
        list_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        prev = self.selections.get("awards", {}).get("_items")
        self._award_vars = []
        for i, award in enumerate(merged):
            checked = prev[i] if prev and i < len(prev) else True
            var = tk.BooleanVar(value=checked)
            self._award_vars.append((var, award))
            ttk.Checkbutton(list_frame, text=award, variable=var).pack(anchor=tk.W, padx=5)

        # Boutons tout cocher/d√©cocher
        btn_row = ttk.Frame(list_frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda: [v.set(True) for v, _ in self._award_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda: [v.set(False) for v, _ in self._award_vars]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: TRIVIA
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_trivia(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("trivia", {})
        by_source = data.get("by_source", {})

        self._add_stash_context(parent)

        if not by_source:
            ttk.Label(parent, text="Aucun trivia trouv√©",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        choice_frame = ttk.LabelFrame(parent, text="üìù Sources Trivia", padding=8)
        choice_frame.pack(fill=tk.X, padx=5, pady=5)

        prev = self.selections.get("trivia", {}).get("_choice")
        default = prev or list(by_source.keys())[0]
        self._trivia_var = tk.StringVar(value=default)
        self._trivia_sources = by_source

        for source, text in by_source.items():
            preview = text[:80] + "..." if len(text) > 80 else text
            ttk.Radiobutton(choice_frame, text=f"{source.upper()} ‚Äî {preview}",
                           variable=self._trivia_var, value=source).pack(anchor=tk.W, padx=5, pady=2)

        self._trivia_preview = tk.Text(parent, height=6, width=80,
                                        font=("Segoe UI", 9), wrap=tk.WORD)
        self._trivia_preview.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)

        def update(*_):
            self._trivia_preview.delete("1.0", tk.END)
            self._trivia_preview.insert("1.0", by_source.get(self._trivia_var.get(), ""))

        self._trivia_var.trace_add("write", update)
        update()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: TATTOOS / PIERCINGS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_body_art(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get(field_key, {})
        merged = data.get("merged", [])
        db_value = data.get("db_value", "")

        self._add_stash_context(parent)

        # Valeur actuelle Stash
        if db_value:
            stash_frame = ttk.LabelFrame(parent, text=f"üìã {field_key.title()} actuels (Stash)",
                                          padding=8)
            stash_frame.pack(fill=tk.X, padx=5, pady=5)
            ttk.Label(stash_frame, text=str(db_value), font=("Segoe UI", 9),
                      wraplength=800).pack(anchor=tk.W)

        if not merged:
            ttk.Label(parent, text=f"Aucun {field_key} trouv√©",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        list_frame = ttk.LabelFrame(parent, text=f"R√©sultat ({len(merged)} entr√©es)", padding=8)
        list_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        prev = self.selections.get(field_key, {}).get("_items")
        var_attr = f"_{field_key}_vars"
        vars_list = []
        for i, item in enumerate(merged):
            pos = item.get("position", "?")
            desc = item.get("description", "")
            label = f"{pos}" + (f" ({desc})" if desc else "")
            checked = prev[i] if prev and i < len(prev) else True
            var = tk.BooleanVar(value=checked)
            vars_list.append((var, item))
            ttk.Checkbutton(list_frame, text=label, variable=var).pack(anchor=tk.W, padx=5)

        setattr(self, var_attr, vars_list)

        btn_row = ttk.Frame(list_frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda vl=vars_list: [v.set(True) for v, _ in vl]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda vl=vars_list: [v.set(False) for v, _ in vl]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: TAGS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_tags(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("tags", {})
        merged = data.get("merged", [])
        sources = data.get("sources", {})
        stash_tags = self.db_data.get("tags", [])

        self._add_stash_context(parent)

        # Tags actuels Stash
        if stash_tags:
            stash_frame = ttk.LabelFrame(parent, text=f"üìã Tags actuels Stash ({len(stash_tags)})",
                                          padding=8)
            stash_frame.pack(fill=tk.X, padx=5, pady=5)
            ttk.Label(stash_frame, text=", ".join(stash_tags[:30]),
                      font=("Segoe UI", 9), wraplength=800).pack(anchor=tk.W)

        if not merged:
            ttk.Label(parent, text="Aucun tag scrap√©",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        # Info sources
        info = "  ".join(f"[{s}: {len(t)}]" for s, t in sources.items() if t)
        ttk.Label(parent, text=f"Union : {len(merged)} tags ‚Äî {info}",
                  font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, padx=10, pady=5)

        list_frame = ttk.LabelFrame(parent, text="üè∑Ô∏è Tags scrap√©s", padding=8)
        list_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        prev = self.selections.get("tags", {}).get("_items")
        self._tag_vars = []
        for i, tag in enumerate(merged):
            # Marquer si d√©j√† dans Stash
            in_stash = tag.lower() in [t.lower() for t in stash_tags]
            label = f"{'‚úì ' if in_stash else ''}{tag}"
            checked = prev[i] if prev and i < len(prev) else True
            var = tk.BooleanVar(value=checked)
            self._tag_vars.append((var, tag))
            ttk.Checkbutton(list_frame, text=label, variable=var).pack(anchor=tk.W, padx=5)

        btn_row = ttk.Frame(list_frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda: [v.set(True) for v, _ in self._tag_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda: [v.set(False) for v, _ in self._tag_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="Nouveaux seuls",
                   command=lambda: self._select_new_tags_only(stash_tags)).pack(side=tk.LEFT, padx=2)

    def _select_new_tags_only(self, stash_tags):
        """Cocher uniquement les tags qui ne sont PAS d√©j√† dans Stash."""
        stash_lower = {t.lower() for t in stash_tags}
        for var, tag in self._tag_vars:
            var.set(tag.lower() not in stash_lower)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PAGE: URLs
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _page_urls(self, field_key):
        parent = self.content_frame
        data = self.merged_data.get("urls", {})
        merged = data.get("merged", {})
        stash_urls = self.db_data.get("urls", [])

        self._add_stash_context(parent)

        # URLs actuelles Stash
        if stash_urls:
            stash_frame = ttk.LabelFrame(parent, text=f"üìã URLs actuelles Stash ({len(stash_urls)})",
                                          padding=8)
            stash_frame.pack(fill=tk.X, padx=5, pady=5)
            for url in stash_urls[:15]:
                ttk.Label(stash_frame, text=url, font=("Segoe UI", 9)).pack(anchor=tk.W)

        if not merged:
            ttk.Label(parent, text="Aucune URL scrap√©e",
                      font=("Segoe UI", 10, "italic")).pack(anchor=tk.W, padx=10, pady=10)
            return

        list_frame = ttk.LabelFrame(parent, text=f"üîó URLs scrap√©es ({len(merged)})", padding=8)
        list_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        prev = self.selections.get("urls", {}).get("_items")
        self._url_vars = []
        for i, (key, url) in enumerate(sorted(merged.items())):
            checked = True
            if prev:
                checked = prev.get(key, True)
            var = tk.BooleanVar(value=checked)
            self._url_vars.append((var, key, url))
            row = ttk.Frame(list_frame)
            row.pack(fill=tk.X, padx=5, pady=1)
            ttk.Checkbutton(row, variable=var).pack(side=tk.LEFT)
            ttk.Label(row, text=f"{key}:", width=12, anchor=tk.W,
                      font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
            ttk.Label(row, text=url, font=("Segoe UI", 9)).pack(side=tk.LEFT, fill=tk.X)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # SAUVEGARDE / FINITION
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _save_current(self):
        """Sauvegarder la s√©lection de la page courante."""
field_key="***MASKED***"

        if field_key == "details":
            if hasattr(self, "_details_var"):
                choice = self._details_var.get()
                text = self._details_options.get(choice, "")
                self.selections["details"] = {"_choice": choice, "value": text}

        elif field_key == "awards":
            if hasattr(self, "_award_vars"):
                items = [v.get() for v, _ in self._award_vars]
                selected = [a for v, a in self._award_vars if v.get()]
                self.selections["awards"] = {"_items": items, "value": selected}

        elif field_key == "trivia":
            if hasattr(self, "_trivia_var"):
                choice = self._trivia_var.get()
                text = self._trivia_sources.get(choice, "")
                self.selections["trivia"] = {"_choice": choice, "value": text}

        elif field_key in ("tattoos", "piercings"):
            var_attr = f"_{field_key}_vars"
            if hasattr(self, var_attr):
                vars_list = getattr(self, var_attr)
                items = [v.get() for v, _ in vars_list]
                selected = [item for v, item in vars_list if v.get()]
                self.selections[field_key] = {"_items": items, "value": selected}

        elif field_key == "tags":
            if hasattr(self, "_tag_vars"):
                items = [v.get() for v, _ in self._tag_vars]
                selected = [t for v, t in self._tag_vars if v.get()]
                self.selections["tags"] = {"_items": items, "value": selected}

        elif field_key == "urls":
            if hasattr(self, "_url_vars"):
                items = {k: v.get() for v, k, _ in self._url_vars}
                selected = {k: u for v, k, u in self._url_vars if v.get()}
                self.selections["urls"] = {"_items": items, "value": selected}

    def _finish(self):
        """Collecter toutes les s√©lections et fermer."""
        self._save_current()

        self.result = {}

        # Details
        det = self.selections.get("details", {})
        self.result["details"] = det.get("value")

        # Awards
        aw = self.selections.get("awards", {})
        self.result["awards"] = aw.get("value", [])

        # Trivia
        tr = self.selections.get("trivia", {})
        self.result["trivia"] = tr.get("value")

        # Tattoos
        tt = self.selections.get("tattoos", {})
        self.result["tattoos"] = tt.get("value", [])

        # Piercings
        pi = self.selections.get("piercings", {})
        self.result["piercings"] = pi.get("value", [])

        # Tags
        tg = self.selections.get("tags", {})
        self.result["tags"] = tg.get("value", [])

        # URLs
        ur = self.selections.get("urls", {})
        self.result["urls"] = ur.get("value", {})

        self.destroy()


============================================================
[21/83] gui\phase2_merge_dialog.py
------------------------------------------------------------
"""
Phase2MergeDialog ‚Äî Fen√™tre modale de r√©solution des r√©sultats Phase 2.
Chaque champ a son propre mode d'affichage et de s√©lection.
"""
import tkinter as tk
from tkinter import ttk


class Phase2MergeDialog(tk.Toplevel):
    """
    Dialogue de fusion pour les champs Phase 2.
    Affiche les r√©sultats fusionn√©s et permet √† l'utilisateur
    de choisir/modifier avant application.
    """

    def __init__(self, parent, merged_data: dict):
        super().__init__(parent)
        self.title("üìã R√©sultats scraping Phase 2")
        self.merged_data = merged_data
        self.result = None  # Dict final si l'utilisateur valide

        # Configurer la fen√™tre
        self.geometry("900x700")
        self.minsize(800, 600)
        self.transient(parent)
        self.grab_set()

        # Variables de s√©lection
        self.selections = {}

        self._build_ui()
        self.wait_window()

    def _build_ui(self):
        # ‚îÄ‚îÄ Frame principal avec scroll ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        main = ttk.Frame(self, padding=10)
        main.pack(fill=tk.BOTH, expand=True)

        # Canvas + Scrollbar pour le contenu
        canvas = tk.Canvas(main, highlightthickness=0)
        scrollbar = ttk.Scrollbar(main, orient=tk.VERTICAL, command=canvas.yview)
        self.scroll_frame = ttk.Frame(canvas)

        self.scroll_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        canvas.create_window((0, 0), window=self.scroll_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        # Mousewheel scroll
        canvas.bind_all("<MouseWheel>",
                        lambda e: canvas.yview_scroll(int(-1 * (e.delta / 120)), "units"))

        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # ‚îÄ‚îÄ Sections par champ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        self._build_awards_section()
        self._build_trivia_section()
        self._build_details_section()
        self._build_body_art_section("tattoos", "üé® TATTOOS")
        self._build_body_art_section("piercings", "üíâ PIERCINGS")
        self._build_tags_section()
        self._build_urls_section()

        # ‚îÄ‚îÄ Boutons d'action ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        btn_frame = ttk.Frame(self, padding=10)
        btn_frame.pack(fill=tk.X)

        ttk.Button(btn_frame, text="‚úÖ Appliquer tout",
                   command=self._apply_all).pack(side=tk.RIGHT, padx=5)
        ttk.Button(btn_frame, text="‚ùå Annuler",
                   command=self.destroy).pack(side=tk.RIGHT, padx=5)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # AWARDS ‚Äî liste cochable
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_awards_section(self):
        data = self.merged_data.get("awards", {})
        merged = data.get("merged", [])
        sources = data.get("sources", {})

        frame = self._section_frame("üèÜ AWARDS")

        if not merged:
            ttk.Label(frame, text="Aucun award trouv√©").pack(anchor=tk.W)
            self.selections["awards"] = []
            return

        # Compteur par source
        source_info = "  ".join(f"[{s}: {len(a)}]" for s, a in sources.items() if a)
        ttk.Label(frame, text=source_info, font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, pady=(0, 5))

        # Liste cochable
        self.award_vars = []
        for award in merged:
            var = tk.BooleanVar(value=True)
            self.award_vars.append((var, award))
            ttk.Checkbutton(frame, text=award, variable=var).pack(anchor=tk.W, padx=10)

        # Boutons
        btn_row = ttk.Frame(frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout cocher",
                   command=lambda: [v.set(True) for v, _ in self.award_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Tout d√©cocher",
                   command=lambda: [v.set(False) for v, _ in self.award_vars]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TRIVIA ‚Äî s√©lecteur de source
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_trivia_section(self):
        data = self.merged_data.get("trivia", {})
        by_source = data.get("by_source", {})
        suggestion = data.get("suggestion")

        frame = self._section_frame("üìù TRIVIA")

        if not by_source:
            ttk.Label(frame, text="Aucun trivia trouv√©").pack(anchor=tk.W)
            self.selections["trivia"] = None
            return

        # Radio buttons pour chaque source
        self.trivia_var = tk.StringVar(value=list(by_source.keys())[0] if suggestion is None else
                                       next((k for k, v in by_source.items() if v == suggestion), ""))
        
        for source, text in by_source.items():
            preview = text[:100] + "..." if len(text) > 100 else text
            ttk.Radiobutton(frame, text=f"{source.upper()} ‚Äî {preview}",
                           variable=self.trivia_var, value=source).pack(anchor=tk.W, padx=10, pady=2)

        # Zone de pr√©visualisation
        self.trivia_preview = tk.Text(frame, height=4, width=80, font=("Segoe UI", 9), wrap=tk.WORD)
        self.trivia_preview.pack(fill=tk.X, padx=10, pady=5)
        
        # MAJ pr√©visualisation quand la s√©lection change
        def update_preview(*_):
            src = self.trivia_var.get()
            self.trivia_preview.delete("1.0", tk.END)
            self.trivia_preview.insert("1.0", by_source.get(src, ""))
        
        self.trivia_var.trace_add("write", update_preview)
        update_preview()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # DETAILS (Bio) ‚Äî radio source unique ou fusion
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_details_section(self):
        data = self.merged_data.get("details", {})
        by_source = data.get("by_source", {})
        fused = data.get("fused")

        frame = self._section_frame("üìñ DETAILS (Bio)")

        if not by_source:
            ttk.Label(frame, text="Aucune bio trouv√©e").pack(anchor=tk.W)
            self.selections["details"] = None
            return

        # Options
        self.details_var = tk.StringVar(value="freeones" if "freeones" in by_source else list(by_source.keys())[0])

        for source, text in by_source.items():
            length = len(text)
            ttk.Radiobutton(frame, text=f"{source.upper()} ({length} car.)",
                           variable=self.details_var, value=source).pack(anchor=tk.W, padx=10, pady=2)

        if fused:
            ttk.Radiobutton(frame, text=f"Fusion toutes sources ({len(fused)} car.)",
                           variable=self.details_var, value="_fused_").pack(anchor=tk.W, padx=10, pady=2)

        # Zone de pr√©visualisation
        self.details_preview = tk.Text(frame, height=6, width=80, font=("Segoe UI", 9), wrap=tk.WORD)
        self.details_preview.pack(fill=tk.X, padx=10, pady=5)

        def update_preview(*_):
            src = self.details_var.get()
            self.details_preview.delete("1.0", tk.END)
            if src == "_fused_":
                self.details_preview.insert("1.0", fused or "")
            else:
                self.details_preview.insert("1.0", by_source.get(src, ""))

        self.details_var.trace_add("write", update_preview)
        update_preview()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TATTOOS / PIERCINGS ‚Äî liste √©ditable
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_body_art_section(self, field: str, title: str):
        data = self.merged_data.get(field, {})
        merged = data.get("merged", [])

        frame = self._section_frame(title)

        if not merged:
            ttk.Label(frame, text=f"Aucun {field} trouv√©").pack(anchor=tk.W)
            self.selections[field] = []
            return

        # Info strat√©gie
        ttk.Label(frame, text=f"Merge auto (structur√© > flat) ‚Üí {len(merged)} entr√©es uniques",
                  font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, pady=(0, 5))

        # Liste cochable
        vars_list = []
        for item in merged:
            pos = item.get("position", "?")
            desc = item.get("description", "")
            label = f"{pos}" + (f" ({desc})" if desc else "")
            var = tk.BooleanVar(value=True)
            vars_list.append((var, item))
            ttk.Checkbutton(frame, text=label, variable=var).pack(anchor=tk.W, padx=10)

        setattr(self, f"{field}_vars", vars_list)

        # Boutons
        btn_row = ttk.Frame(frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda vl=vars_list: [v.set(True) for v, _ in vl]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda vl=vars_list: [v.set(False) for v, _ in vl]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TAGS ‚Äî liste filtrable
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_tags_section(self):
        data = self.merged_data.get("tags", {})
        merged = data.get("merged", [])
        sources = data.get("sources", {})

        frame = self._section_frame("üè∑Ô∏è TAGS")

        if not merged:
            ttk.Label(frame, text="Aucun tag trouv√©").pack(anchor=tk.W)
            self.selections["tags"] = []
            return

        # Compteur
        source_info = "  ".join(f"[{s}: {len(t)}]" for s, t in sources.items() if t)
        ttk.Label(frame, text=f"Union : {len(merged)} tags ‚Äî {source_info}",
                  font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, pady=(0, 5))

        # Filtre
        filter_frame = ttk.Frame(frame)
        filter_frame.pack(fill=tk.X, padx=10, pady=2)
        ttk.Label(filter_frame, text="Filtrer:").pack(side=tk.LEFT)
        self.tag_filter_var = tk.StringVar()
        filter_entry = ttk.Entry(filter_frame, textvariable=self.tag_filter_var, width=30)
        filter_entry.pack(side=tk.LEFT, padx=5)

        # Tags dans un frame scrollable
        tag_canvas = tk.Canvas(frame, height=150, highlightthickness=0)
        tag_scroll = ttk.Scrollbar(frame, orient=tk.VERTICAL, command=tag_canvas.yview)
        self.tag_inner = ttk.Frame(tag_canvas)

        self.tag_inner.bind("<Configure>",
                            lambda e: tag_canvas.configure(scrollregion=tag_canvas.bbox("all")))
        tag_canvas.create_window((0, 0), window=self.tag_inner, anchor="nw")
        tag_canvas.configure(yscrollcommand=tag_scroll.set)

        tag_scroll.pack(side=tk.RIGHT, fill=tk.Y)
        tag_canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=10)

        self.tag_vars = []
        for tag in merged:
            var = tk.BooleanVar(value=True)
            self.tag_vars.append((var, tag))
            ttk.Checkbutton(self.tag_inner, text=tag, variable=var).pack(anchor=tk.W)

        # Boutons
        btn_row = ttk.Frame(frame)
        btn_row.pack(anchor=tk.W, pady=5)
        ttk.Button(btn_row, text="‚úì Tout",
                   command=lambda: [v.set(True) for v, _ in self.tag_vars]).pack(side=tk.LEFT, padx=2)
        ttk.Button(btn_row, text="‚úó Rien",
                   command=lambda: [v.set(False) for v, _ in self.tag_vars]).pack(side=tk.LEFT, padx=2)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # URLs ‚Äî tableau
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _build_urls_section(self):
        data = self.merged_data.get("urls", {})
        merged = data.get("merged", {})

        frame = self._section_frame("üîó URLs")

        if not merged:
            ttk.Label(frame, text="Aucune URL trouv√©e").pack(anchor=tk.W)
            self.selections["urls"] = {}
            return

        ttk.Label(frame, text=f"{len(merged)} URLs agr√©g√©es",
                  font=("Segoe UI", 9, "italic")).pack(anchor=tk.W, pady=(0, 5))

        self.url_vars = []
        for key, url in sorted(merged.items()):
            var = tk.BooleanVar(value=True)
            self.url_vars.append((var, key, url))
            row = ttk.Frame(frame)
            row.pack(fill=tk.X, padx=10, pady=1)
            ttk.Checkbutton(row, variable=var).pack(side=tk.LEFT)
            ttk.Label(row, text=f"{key}:", width=12, anchor=tk.W,
                      font=("Segoe UI", 9, "bold")).pack(side=tk.LEFT)
            ttk.Label(row, text=url, font=("Segoe UI", 9)).pack(side=tk.LEFT, fill=tk.X)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Helpers
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _section_frame(self, title: str) -> ttk.Frame:
        """Cr√©er un cadre de section avec titre."""
        sep = ttk.Separator(self.scroll_frame, orient=tk.HORIZONTAL)
        sep.pack(fill=tk.X, padx=5, pady=(10, 5))

        lbl = ttk.Label(self.scroll_frame, text=title, 
                        font=("Segoe UI", 11, "bold"))
        lbl.pack(anchor=tk.W, padx=10)

        frame = ttk.Frame(self.scroll_frame, padding=(10, 5))
        frame.pack(fill=tk.X, padx=5)
        return frame

    def _apply_all(self):
        """Collecter toutes les s√©lections et fermer."""
        self.result = {}

        # Awards
        if hasattr(self, 'award_vars'):
            self.result["awards"] = [a for v, a in self.award_vars if v.get()]
        else:
            self.result["awards"] = []

        # Trivia
        if hasattr(self, 'trivia_var'):
            src = self.trivia_var.get()
            by_source = self.merged_data.get("trivia", {}).get("by_source", {})
            self.result["trivia"] = by_source.get(src)
        else:
            self.result["trivia"] = None

        # Details
        if hasattr(self, 'details_var'):
            src = self.details_var.get()
            details_data = self.merged_data.get("details", {})
            if src == "_fused_":
                self.result["details"] = details_data.get("fused")
            else:
                self.result["details"] = details_data.get("by_source", {}).get(src)
        else:
            self.result["details"] = None

        # Tattoos
        if hasattr(self, 'tattoos_vars'):
            self.result["tattoos"] = [item for v, item in self.tattoos_vars if v.get()]
        else:
            self.result["tattoos"] = []

        # Piercings
        if hasattr(self, 'piercings_vars'):
            self.result["piercings"] = [item for v, item in self.piercings_vars if v.get()]
        else:
            self.result["piercings"] = []

        # Tags
        if hasattr(self, 'tag_vars'):
            self.result["tags"] = [t for v, t in self.tag_vars if v.get()]
        else:
            self.result["tags"] = []

        # URLs
        if hasattr(self, 'url_vars'):
            self.result["urls"] = {k: u for v, k, u in self.url_vars if v.get()}
        else:
            self.result["urls"] = {}

        self.destroy()


============================================================
[22/83] main.py
------------------------------------------------------------
from gui.launcher import start_launcher

if __name__ == "__main__":
    start_launcher()


============================================================
[23/83] rapport_Ollama_20260225-022044_part01.txt
------------------------------------------------------------
===== RAPPORT PERTINENT : E:\Ollama =====
Fichiers inclus (extensions): .bat, .cfg, .conf, .csv, .env, .ini, .json, .jsonc, .md, .ps1, .py, .rst, .toml, .txt, .yaml, .yml
Toujours inclus (noms): .editorconfig, .gitignore, Dockerfile, LICENSE, Makefile, Pipfile, Pipfile.lock, README, README.md, pyproject.toml, requirements-dev.txt, requirements.txt, setup.cfg, setup.py
Dossiers exclus: .cache, .coverage, .env, .git, .idea, .mypy_cache, .pytest_cache, .ruff_cache, .venv, .vs, .vscode, __pycache__, backup, backups, build, dist, env, log, logs, node_modules, sauvegarde, sauvegardes, site-packages, temp, tmp, venv
Extensions exclues: .7z, .avi, .bmp, .bz2, .db, .dll, .dylib, .exe, .gif, .gz, .ico, .jpeg, .jpg, .log, .mdb, .mkv, .mov, .mp3, .mp4, .otf, .png, .pyd, .rar, .so, .sqlite, .svg, .ttf, .wav, .webp, .woff, .woff2, .xz, .zip

===== ARBORESCENCE FILTR√âE =====
E:\Ollama
+--- blobs
+--- lib
|   \--- ollama
|       +--- blobs
|       +--- cuda_v12
|       +--- cuda_v13
|       +--- manifests
|       |   \--- registry.ollama.ai
|       |       \--- library
|       |           +--- deepseek-r1
|       |           +--- deepseek-v3.1
|       |           +--- dolphin-llama3
|       |           +--- llama3.1
|       |           +--- llava
|       |           +--- moondream
|       |           \--- qwen3-coder
|       +--- rocm
|       |   \--- rocblas
|       |       \--- library
|       |           \--- TensileManifest.txt
|       \--- vulkan
+--- manifests
+--- models
|   +--- blobs
|   \--- manifests
|       \--- registry.ollama.ai
|           \--- library
|               +--- bakllava
|               +--- codellama
|               +--- deepseek-coder-v2
|               +--- dolphin-llama3
|               +--- gpt-oss
|               +--- llama3
|               +--- llama3.2
|               +--- llava
|               +--- llava-llama3
|               +--- mistral
|               +--- moondream
|               +--- nous-hermes2
|               \--- qwen2.5-coder
\--- rapport_pertinent_20260225-022044
    \--- rapport_Ollama_20260225-022044_part01.txt

===== CONTENU DES FICHIERS PERTINENTS (SECRETS MASQU√âS) =====
Total fichiers : 2
------------------------------------------------------------


============================================================
[1/2] lib\ollama\rocm\rocblas\library\TensileManifest.txt
------------------------------------------------------------
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1151.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.co


============================================================
[2/2] rapport_pertinent_20260225-022044\rapport_Ollama_20260225-022044_part01.txt
------------------------------------------------------------
===== RAPPORT PERTINENT : E:\Ollama =====
Fichiers inclus (extensions): .bat, .cfg, .conf, .csv, .env, .ini, .json, .jsonc, .md, .ps1, .py, .rst, .toml, .txt, .yaml, .yml
Toujours inclus (noms): .editorconfig, .gitignore, Dockerfile, LICENSE, Makefile, Pipfile, Pipfile.lock, README, README.md, pyproject.toml, requirements-dev.txt, requirements.txt, setup.cfg, setup.py
Dossiers exclus: .cache, .coverage, .env, .git, .idea, .mypy_cache, .pytest_cache, .ruff_cache, .venv, .vs, .vscode, __pycache__, backup, backups, build, dist, env, log, logs, node_modules, sauvegarde, sauvegardes, site-packages, temp, tmp, venv
Extensions exclues: .7z, .avi, .bmp, .bz2, .db, .dll, .dylib, .exe, .gif, .gz, .ico, .jpeg, .jpg, .log, .mdb, .mkv, .mov, .mp3, .mp4, .otf, .png, .pyd, .rar, .so, .sqlite, .svg, .ttf, .wav, .webp, .woff, .woff2, .xz, .zip

===== ARBORESCENCE FILTR√âE =====
E:\Ollama
+--- blobs
+--- lib
|   \--- ollama
|       +--- blobs
|       +--- cuda_v12
|       +--- cuda_v13
|       +--- manifests
|       |   \--- registry.ollama.ai
|       |       \--- library
|       |           +--- deepseek-r1
|       |           +--- deepseek-v3.1
|       |           +--- dolphin-llama3
|       |           +--- llama3.1
|       |           +--- llava
|       |           +--- moondream
|       |           \--- qwen3-coder
|       +--- rocm
|       |   \--- rocblas
|       |       \--- library
|       |           \--- TensileManifest.txt
|       \--- vulkan
+--- manifests
+--- models
|   +--- blobs
|   \--- manifests
|       \--- registry.ollama.ai
|           \--- library
|               +--- bakllava
|               +--- codellama
|               +--- deepseek-coder-v2
|               +--- dolphin-llama3
|               +--- gpt-oss
|               +--- llama3
|               +--- llama3.2
|               +--- llava
|               +--- llava-llama3
|               +--- mistral
|               +--- moondream
|               +--- nous-hermes2
|               \--- qwen2.5-coder
\--- rapport_pertinent_20260225-022044
    \--- rapport_Ollama_20260225-022044_part01.txt

===== CONTENU DES FICHIERS PERTINENTS (SECRETS MASQU√âS) =====
Total fichiers : 2
------------------------------------------------------------


============================================================
[1/2] lib\ollama\rocm\rocblas\library\TensileManifest.txt
------------------------------------------------------------
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1151.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_lazy_gfx1030.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.dat
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_fallback_gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1030.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1100.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1101.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1102.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\Kernels.so-000-gfx1151.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906-xnack-.hsaco
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HS_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_4xi8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_Ailk_BjlkC_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_I8I_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_AlikC_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_DD_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_ZZ_Contraction_l_AlikC_Bljk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Alik_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1100.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_BB_HPA_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_HPA_Contraction_l_Ailk_Bjlk_Cijk_Dijk_gfx1101.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_CC_Contraction_l_Alik_Bjlk_Cijk_Dijk_gfx906.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_HH_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1030.co
C:\j\workspace\K_Installer_release_rocm-rel-6.2\roclibs\rocBLAS\rocBLAS\build\release\Tensile\library\TensileLibrary_Type_SS_Contraction_l_Ailk_Bljk_Cijk_Dijk_gfx1102.co


============================================================
[24/83] rapport_Ollama_20260225-031519.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  3:15:19,22

=== NVIDIA GPU Status ===
Wed Feb 25 03:15:19 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 63%   59C    P2    48W / 170W |   5934MiB /  6144MiB |     18%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     11788    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[25/83] rapport_Ollama_20260225-031832.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  3:18:32,50

=== NVIDIA GPU Status ===
Wed Feb 25 03:18:32 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 63%   59C    P2    49W / 170W |   5942MiB /  6144MiB |     49%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     11788    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[26/83] rapport_Ollama_20260225-033018.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  3:30:18,98

=== NVIDIA GPU Status ===
Wed Feb 25 03:30:19 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 83%   77C    P2   158W / 170W |   5791MiB /  6144MiB |     80%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     11788    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[27/83] rapport_Ollama_20260225-034134.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  3:41:34,87

=== NVIDIA GPU Status ===
Wed Feb 25 03:41:35 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 85%   78C    P2   160W / 170W |   5838MiB /  6144MiB |     89%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     11788    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[28/83] rapport_Ollama_20260225-035640.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  3:56:40,90

=== NVIDIA GPU Status ===
Wed Feb 25 03:56:41 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
|  0%   50C    P8    15W / 170W |   3549MiB /  6144MiB |     18%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     11788    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[29/83] rapport_Ollama_20260225-043734.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  4:37:34,41

=== NVIDIA GPU Status ===
Wed Feb 25 04:37:34 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
|  0%   50C    P8    15W / 170W |   3675MiB /  6144MiB |     25%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[30/83] rapport_Ollama_20260225-044350.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  4:43:51,04

=== NVIDIA GPU Status ===
Wed Feb 25 04:43:51 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 64%   66C    P2   156W / 170W |   5902MiB /  6144MiB |     89%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[31/83] rapport_Ollama_20260225-044628.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  4:46:28,85

=== NVIDIA GPU Status ===
Wed Feb 25 04:46:29 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 80%   76C    P2   160W / 170W |   5882MiB /  6144MiB |     97%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[32/83] rapport_Ollama_20260225-044939.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  4:49:39,25

=== NVIDIA GPU Status ===
Wed Feb 25 04:49:39 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 84%   78C    P2    73W / 170W |   5875MiB /  6144MiB |     87%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[33/83] rapport_Ollama_20260225-052435.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  5:24:35,70

=== NVIDIA GPU Status ===
Wed Feb 25 05:24:36 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 85%   77C    P2   152W / 170W |   5907MiB /  6144MiB |     84%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     11884    C+G   ..._8wekyb3d8bbwe\Photos.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A     26548    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[34/83] rapport_Ollama_20260225-053701.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  5:37:02,01

=== NVIDIA GPU Status ===
Wed Feb 25 05:37:02 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 85%   77C    P2   146W / 170W |   5884MiB /  6144MiB |     83%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A     26548    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[35/83] rapport_Ollama_20260225-055741.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  5:57:41,27

=== NVIDIA GPU Status ===
Wed Feb 25 05:57:41 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
|  0%   47C    P8    15W / 170W |   3628MiB /  6144MiB |     27%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A     26548    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[36/83] rapport_Ollama_20260225-060228.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  6:02:28,62

=== NVIDIA GPU Status ===
Wed Feb 25 06:02:28 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
|  0%   49C    P8    15W / 170W |   3640MiB /  6144MiB |     16%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |
|    0   N/A  N/A      4828    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A      4844    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      5868    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      6292    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      7640    C+G   ...tqa\iCloud\iCloudHome.exe    N/A      |
|    0   N/A  N/A      8000    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      8188    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A      8196    C+G   ...wekyb3d8bbwe\Video.UI.exe    N/A      |
|    0   N/A  N/A      9776    C+G   ...Kaspersky 21.24\avpui.exe    N/A      |
|    0   N/A  N/A     10388    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     10804    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     11120    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     11564    C+G   ...yb3d8bbwe\M365Copilot.exe    N/A      |
|    0   N/A  N/A     12140    C+G   ...underbird\thunderbird.exe    N/A      |
|    0   N/A  N/A     12396    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     13064    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     13300    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     14476    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     14732      C   ...thon\Python312\python.exe    N/A      |
|    0   N/A  N/A     15292    C+G   ...ersky VPN 5.24\ksdeui.exe    N/A      |
|    0   N/A  N/A     16920    C+G   ...tigravity\Antigravity.exe    N/A      |
|    0   N/A  N/A     19872    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     20520    C+G   ...800.70\msedgewebview2.exe    N/A      |
|    0   N/A  N/A     21020    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A     26548    C+G   ...8bbwe\Notepad\Notepad.exe    N/A      |
+-----------------------------------------------------------------------------+

=== Ollama Models List ===


============================================================
[37/83] rapport_Ollama_20260225-061045.txt
------------------------------------------------------------
Report generated on 2026-02-25 at  6:10:45,11

=== NVIDIA GPU Status ===
Wed Feb 25 06:10:45 2026       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 526.86       Driver Version: 526.86       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |
| 32%   50C    P8    16W / 170W |   3642MiB /  6144MiB |     23%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4584    C+G   ...icrosoft VS Code\Code.exe    N/A      |


============================================================
[40/83] README.md
------------------------------------------------------------
# StashMaster V2

Gestion modulaire des m√©tadonn√©es performers (phase 1/phase 2).

- Phase 1 : m√©tadonn√©es usuelles (Nom, D√©sambigu√Øsation, Alias, Date de naissance, Date du d√©c√®s, Pays, Ethnicit√©, Couleur des cheveux, Couleur des yeux, Taille, Poids, Mensurations, Faux seins, Dur√©e de carri√®re)
- Phase 2 : champs avanc√©s (Bio, Trivia, Awards, Tattoos, Piercings, Tags, URLs, Details)

Structure pr√™te pour extensions (group, scene, etc.).


============================================================
[41/83] requirements.txt
------------------------------------------------------------
# tkinter is part of stdlib, not a pip package
sv_ttk
requests
pillow
loguru
pydantic
pyyaml
lxml
beautifulsoup4
google-generativeai


============================================================
[42/83] services\__init__.py
------------------------------------------------------------


============================================================
[43/83] services\bio_generator.py
------------------------------------------------------------
"""
BioGenerator ‚Äî G√©n√®re une bio professionnelle via IA √† partir des donn√©es V2.
Utilise Gemini (API REST) ou Ollama (fallback local).
"""
import os
import json
import urllib.request

GEMINI_MODEL = "gemini-2.0-flash"  # Flash = moins cher, suffisant
GEMINI_API_URL="***MASKED***"

SYSTEM_PROMPT = """Tu es un r√©dacteur expert pour une base de donn√©es de films pour adultes.
Ton objectif est de r√©diger une biographie structur√©e et professionnelle en FRAN√áAIS (Qu√©bec) pour l'artiste, bas√©e sur les faits fournis.

Respecte scrupuleusement le format suivant (avec les √©mojis et le gras) :

### [Nom] : [Titre accrocheur]

**Introduction**
[Paragraphe introductif : identit√©, dates cl√©s, pseudonymes, r√©sum√© de carri√®re.]

**üìÖ Origines et Premiers Pas**
[D√©tails sur les origines, la jeunesse, et l'entr√©e dans l'industrie.]

**üèÜ Carri√®re et Filmographie**
[Parcours professionnel, studios majeurs, √©volution, sc√®nes notables.]

**üí° Faits Int√©ressants & Vie Personnelle**
[Vie hors cam√©ra, personnalit√©, anecdotes, r√©seaux sociaux.]

**üëó Apparence et Style**
[Attributs physiques, style, tatouages, piercings.]

**üèÜ Prix et Distinctions**
[R√©compenses et nominations. Si aucune, mentionner la popularit√©.]

**Conclusion rapide**
[Phrase de conclusion sur l'h√©ritage de l'artiste.]

R√®gles imp√©ratives :
- N'invente AUCUN fait. Base-toi uniquement sur les donn√©es fournies (contexte Stash, donn√©es scrap√©es).
- Si une information est manquante pour une section, r√©dige une phrase g√©n√©rale ou passe bri√®vement, mais garde la section.
- Tu DOIS inclure TOUTES les 7 sections (Introduction, Origines, Carri√®re, Faits, Apparence, Prix, Conclusion).
- Le ton doit √™tre informatif, fluide et agr√©able √† lire (style encyclop√©dique/journalistique).
- Utilise le fran√ßais standard du Qu√©bec.
"""


class BioGenerator:
    def __init__(self, gemini_key: str | None = None, ollama_url: str = "http://localhost:11434"):
        self.gemini_key = gemini_key or self._load_gemini_key()
        self.ollama_url = ollama_url

    def _load_gemini_key(self) -> str | None:
        # Chercher le .gemini_key √† la racine du projet
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
key_path="***MASKED***"

        if os.path.exists(key_path):
            with open(key_path, 'r') as f:
                return f.read().strip()
        
        # Fallback pour le chemin d'origine si n√©cessaire (peut √™tre retir√© plus tard)
        legacy_path = r"F:\V2\.gemini_key"
        if os.path.exists(legacy_path):
             with open(legacy_path, 'r') as f:
                return f.read().strip()

        return None

    def build_context_from_v2(
        self,
        db_data: dict,
        stash_ctx: dict,
        scraped_results: list[dict],
        merged_data: dict,
        checked_fields: list[str],
    ) -> dict:
        """
        Construit le contexte de g√©n√©ration depuis les donn√©es V2.
        
        Args:
            db_data:         donn√©es DB du performer (V2 PerformerDB)
            stash_ctx:       contexte Stash (sc√®nes, studios, collabs)
            scraped_results: r√©sultats bruts des extracteurs Phase 2
            merged_data:     r√©sultats fusionn√©s Phase2Merger
            checked_fields:  champs coch√©s par l'utilisateur
        """
        ctx = {
            "name":      db_data.get("name", "Unknown"),
            "birthdate": db_data.get("birthdate") if "Birthdate" in checked_fields else None,
            "details":   db_data.get("details"),
            "mini_bios": [],
            "trivia":    [],
            "scene_count":    stash_ctx.get("scene_count", 0),
            "studios":        stash_ctx.get("studios", [])[:10],
            "collaborators":  [c["name"] for c in stash_ctx.get("collaborators", [])[:5]],
            "awards":         [],
            "socials":        [],
            "career_length":  None,
        }

        # Bios scrap√©es (Details)
        if "Details" in checked_fields:
            for src, bio_text in merged_data.get("details", {}).get("by_source", {}).items():
                if bio_text and len(bio_text) > 50:
                    ctx["mini_bios"].append(f"[{src.upper()}] {bio_text[:600]}")

        # D√©tection "Infos limit√©es" : Si aucune bio textuelle n'est trouv√©e, on autorise plus de specs techniques
        limited_info_mode = (len(ctx["mini_bios"]) == 0)

        # Helper pour r√©cup√©rer une valeur (DB > Merged/Scraped)
        def get_val(key):
            v = db_data.get(key)
            if not v and isinstance(merged_data, dict):
                v = merged_data.get(key)
            
            # Fallback : Chercher dans les r√©sultats bruts du scraping
            if not v and scraped_results:
                for res in scraped_results:
                    if res.get(key):
                        v = res.get(key)
                        break
            return v

        # Career Length (Phase 1)
        if "Career Length" in checked_fields:
            val = get_val("career_length")
            if val: ctx["career_length"] = val

        # 1. Origines & Apparence de base (Toujours utile pour la narration)
        base_specs = []
        if "Ethnicity" in checked_fields:
            val = get_val("ethnicity")
            if val: base_specs.append(f"Ethnicity: {val}")
        if "Country" in checked_fields:
            val = get_val("country")
            if val: base_specs.append(f"Country: {val}")
        if "Hair Color" in checked_fields:
            val = get_val("hair_color")
            if val:
                if isinstance(val, list):
                    val = val[0] if val else ""
                val = str(val).split(",")[0].strip() # Garder couleur principale
                if val: base_specs.append(f"Hair: {val}")
        if "Eye Color" in checked_fields:
            val = get_val("eye_color")
            if val: base_specs.append(f"Eyes: {val}")
        
        if base_specs:
            ctx["trivia"].append("Appearance/Origins: " + ", ".join(base_specs))

        # 2. Specs Techniques (Seulement si infos limit√©es pour √©viter l'effet "fiche technique")
        specs = []
        tech_map = {
            "Height":       ("height", "Height"),
            "Weight":       ("weight", "Weight"),
            "Measurements": ("measurements", "Measurements"),
            "Fake Tits":    ("fake_tits", "Fake Tits"),
        }
        
        if limited_info_mode:
            for field, (db_key, label) in tech_map.items():
                if field in checked_fields and db_data.get(db_key):
                    specs.append(f"{label}: {db_data[db_key]}")
                if field in checked_fields:
                    val = get_val(db_key)
                    if val: specs.append(f"{label}: {val}")
            
            if specs:
                ctx["trivia"].append("Physical Stats (Use to flesh out bio if needed): " + ", ".join(specs))

        # Awards (depuis merged_data)
        if "Awards" in checked_fields:
            awards = merged_data.get("awards", {}).get("merged", [])
            if awards:
                ctx["awards"] = awards  # Stocker la liste compl√®te pour le prompt

        # Aliases
        if "Aliases" in checked_fields and db_data.get("aliases"):
            aliases = db_data["aliases"]
            if isinstance(aliases, list):
                aliases = ", ".join(aliases)
            ctx["trivia"].append(f"Aliases: {aliases}")

        # URLs (Socials)
        if "URLs" in checked_fields:
            urls_data = merged_data.get("urls", {}).get("merged", {})
            # Filtrer pour ne garder que les r√©seaux sociaux principaux
social_keys="***MASKED***"
            found_socials = []
            for k, v in urls_data.items():
                if any(s in k.lower() for s in social_keys):
                    # On envoie "Twitter: http..." pour que l'IA puisse extraire le handle si elle veut
                    found_socials.append(f"{k} ({v})")
            if found_socials:
                ctx["socials"] = found_socials

        return ctx

    def build_prompt(self, ctx: dict) -> str:
        """Construit le prompt utilisateur depuis le contexte."""
        parts = [f"Performer: {ctx['name']}"]

        if ctx.get("birthdate"):
            parts.append(f"Born: {ctx['birthdate']}")

        if ctx.get("career_length"):
            parts.append(f"Years Active: {ctx['career_length']}")

        if ctx.get("trivia"):
            parts.append("\nKnown facts:")
            parts.extend(f"  - {t}" for t in ctx["trivia"])

        if ctx.get("scene_count"):
            parts.append(f"\nStash scenes: {ctx['scene_count']}")

        if ctx.get("studios"):
            parts.append(f"Studios worked with: {', '.join(ctx['studios'][:8])}")

        if ctx.get("collaborators"):
            parts.append(f"Frequent collaborators: {', '.join(ctx['collaborators'])}")

        if ctx.get("awards"):
            parts.append("\nAwards & Nominations:")
            # On envoie les 20 premiers pour donner de la mati√®re √† la section 'Prix'
            parts.append("; ".join(ctx["awards"][:20]))

        if ctx.get("socials"):
            parts.append(f"\nSocial Media: {', '.join(ctx['socials'])}")

        if ctx.get("mini_bios"):
            parts.append("\nSource bios for reference:")
            for bio in ctx["mini_bios"][:3]:  # max 3 sources
                parts.append(f"  {bio}")

        if ctx.get("details"):
            parts.append(f"\nCurrent Stash bio: {ctx['details'][:400]}")

        parts.append("\nR√©dige la biographie en fran√ßais (Qu√©bec) selon le mod√®le.")
        return "\n".join(parts)

    def generate(self, ctx: dict) -> str | None:
        """G√©n√®re la bio. Essaie Gemini d'abord, Ollama en fallback."""
        prompt = self.build_prompt(ctx)

        if self.gemini_key:
            result = self._call_gemini(prompt)
            if result:
                return result.strip()

        return self._call_ollama(prompt)

    def _call_gemini(self, user_prompt: str) -> str | None:
        url = GEMINI_API_URL.format(model=GEMINI_MODEL, key=self.gemini_key)
        payload = {
            "system_instruction": {"parts": [{"text": SYSTEM_PROMPT}]},
            "contents": [{"parts": [{"text": user_prompt}]}],
            "generationConfig": {
                "temperature": 0.7,
                "maxOutputTokens": 300,
            }
        }
        try:
            data = json.dumps(payload).encode()
            req = urllib.request.Request(url, data=data,
                                         headers={"Content-Type": "application/json"})
            with urllib.request.urlopen(req, timeout=20) as resp:
                result = json.loads(resp.read())
            return result["candidates"][0]["content"]["parts"][0]["text"]
        except Exception as e:
            print(f"[BioGenerator] Gemini error: {e}")
            return None

    def _call_ollama(self, user_prompt: str, model: str = "dolphin-llama3") -> str | None:
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user",   "content": user_prompt},
            ],
            "stream": False,
            "options": {"temperature": 0.7, "num_predict": 300}
        }
        try:
            data = json.dumps(payload).encode()
            req = urllib.request.Request(
                f"{self.ollama_url}/api/chat", data=data,
                headers={"Content-Type": "application/json"}
            )
            with urllib.request.urlopen(req, timeout=30) as resp:
                result = json.loads(resp.read())
            return result["message"]["content"]
        except Exception as e:
            print(f"[BioGenerator] Ollama error: {e}")
            return None


============================================================
[44/83] services\db.py
------------------------------------------------------------
import sqlite3

DB_PATH = r"H:\Stash\stash-go.sqlite"

class PerformerDB:
    def __init__(self, db_path=DB_PATH):
        self.conn = sqlite3.connect(db_path)
        self.conn.row_factory = sqlite3.Row

    def get_performer_by_id(self, performer_id):
        cur = self.conn.cursor()
        cur.execute("SELECT * FROM performers WHERE id=?", (performer_id,))
        row = cur.fetchone()
        if not row:
            return None
        data = dict(row)
        # Extraire les alias
        try:
            cur.execute("SELECT alias FROM performer_aliases WHERE performer_id=?", (performer_id,))
            aliases_rows = cur.fetchall()
            aliases = [r['alias'] for r in aliases_rows]
            print(f"DEBUG: Found aliases for ID {performer_id}: {aliases}")
        except Exception as e:
            print(f"DEBUG: Error fetching aliases: {e}")
            aliases = []
        data["aliases"] = aliases
        # Extraire les URLs
        try:
            cur.execute("SELECT url FROM performer_urls WHERE performer_id=?", (performer_id,))
            urls_rows = cur.fetchall()
            urls = [r['url'] for r in urls_rows]
            data["urls"] = urls
        except Exception as e:
            print(f"DEBUG: Error fetching URLs: {e}")
            data["urls"] = []

        # Extraire les tags
        try:
            # Jointure avec la table tags pour obtenir les noms
            query = """
                SELECT t.name 
                FROM tags t
                JOIN performers_tags pt ON pt.tag_id = t.id
                WHERE pt.performer_id = ?
            """
            cur.execute(query, (performer_id,))
            tags_rows = cur.fetchall()
            tags = [r['name'] for r in tags_rows]
            data["tags"] = tags
        except Exception as e:
            print(f"DEBUG: Error fetching tags: {e}")
            data["tags"] = []

        # Extraire la biographie (d√©tails)
        data["bio"] = data.get("details", "")

        return data

    def get_performer_context(self, performer_id):
        """Extraire le contexte Stash complet d'un performer."""
        cur = self.conn.cursor()
        ctx = {"groups": [], "studios": [], "collaborators": [], "scene_count": 0}

        # Nombre de sc√®nes
        try:
            cur.execute(
                "SELECT COUNT(*) as cnt FROM performers_scenes WHERE performer_id=?",
                (performer_id,),
            )
            row = cur.fetchone()
            ctx["scene_count"] = row["cnt"] if row else 0
        except Exception:
            pass

        # Groups (DVDs) via performers_scenes ‚Üí groups_scenes ‚Üí groups
        try:
            cur.execute(
                """
                SELECT DISTINCT g.name
                FROM groups g
                JOIN groups_scenes gs ON gs.group_id = g.id
                JOIN performers_scenes ps ON ps.scene_id = gs.scene_id
                WHERE ps.performer_id = ?
                ORDER BY g.name
                """,
                (performer_id,),
            )
            ctx["groups"] = [r["name"] for r in cur.fetchall()]
        except Exception:
            pass

        # Studios via scenes
        try:
            cur.execute(
                """
                SELECT DISTINCT st.name
                FROM studios st
                JOIN scenes s ON s.studio_id = st.id
                JOIN performers_scenes ps ON ps.scene_id = s.id
                WHERE ps.performer_id = ?
                ORDER BY st.name
                """,
                (performer_id,),
            )
            ctx["studios"] = [r["name"] for r in cur.fetchall()]
        except Exception:
            pass

        # Top collaborateurs
        try:
            cur.execute(
                """
                SELECT p2.name, COUNT(*) as cnt
                FROM performers_scenes ps1
                JOIN performers_scenes ps2 ON ps1.scene_id = ps2.scene_id
                JOIN performers p2 ON p2.id = ps2.performer_id
                WHERE ps1.performer_id = ? AND ps2.performer_id != ?
                GROUP BY p2.id
                ORDER BY cnt DESC
                LIMIT 20
                """,
                (performer_id, performer_id),
            )
            ctx["collaborators"] = [
                {"name": r["name"], "count": r["cnt"]} for r in cur.fetchall()
            ]
        except Exception:
            pass

        return ctx

    def get_known_performers(self):
        """Retourne une liste de tous les noms de performers connus."""
        cur = self.conn.cursor()
        cur.execute("SELECT name FROM performers ORDER BY name")
        rows = cur.fetchall()
        return [r['name'] for r in rows]

    def close(self):
        self.conn.close()

    def inject_performer_metadata(self, performer_id: int, updates: dict) -> None:
        """
        Met √† jour les champs Phase 2 d'un performer.
        G√®re : details, tattoos, piercings, awards, trivia, tags, urls.
        """
        cur = self.conn.cursor()

        # Champs texte directs
        DIRECT = {"details", "tattoos", "piercings", "trivia", "death_date", "awards"}
        direct = {k: v for k, v in updates.items() if k in DIRECT and v is not None}
        if direct:
            set_clause = ", ".join(f"{k}=?" for k in direct)
            vals = list(direct.values()) + [performer_id]
            cur.execute(
                f"UPDATE performers SET {set_clause}, updated_at=datetime('now') WHERE id=?",
                vals
            )

        # URLs ‚Üí table performer_urls
        for url in updates.get("urls", []):
            cur.execute(
                "SELECT COUNT(*) FROM performer_urls WHERE performer_id=? AND url=?",
                (performer_id, url)
            )
            if cur.fetchone()[0] == 0:
                cur.execute(
                    "SELECT COALESCE(MAX(position),-1)+1 FROM performer_urls WHERE performer_id=?",
                    (performer_id,)
                )
                pos = cur.fetchone()[0]
                cur.execute(
                    "INSERT INTO performer_urls (performer_id, position, url) VALUES (?,?,?)",
                    (performer_id, pos, url)
                )

        # Tags ‚Üí table performers_tags (get-or-create)
        for tag_name in updates.get("tags", []):
            cur.execute("SELECT id FROM tags WHERE name=?", (tag_name,))
            row = cur.fetchone()
            tag_id = row[0] if row else None
            if not tag_id:
                cur.execute(
                    "INSERT INTO tags (name, created_at, updated_at, ignore_auto_tag) "
                    "VALUES (?,datetime('now'),datetime('now'),0)",
                    (tag_name,)
                )
                tag_id = cur.lastrowid
            cur.execute(
                "SELECT COUNT(*) FROM performers_tags WHERE performer_id=? AND tag_id=?",
                (performer_id, tag_id)
            )
            if cur.fetchone()[0] == 0:
                cur.execute(
                    "INSERT INTO performers_tags (performer_id, tag_id) VALUES (?,?)",
                    (performer_id, tag_id)
                )

        self.conn.commit()

class GroupDB:
    def __init__(self, db_path=DB_PATH):
        self.conn = sqlite3.connect(db_path)
        self.conn.row_factory = sqlite3.Row

    def get_group_by_id(self, group_id):
        cur = self.conn.cursor()
        cur.execute("SELECT g.*, s.name as studio_name FROM groups g LEFT JOIN studios s ON g.studio_id = s.id WHERE g.id=?", (group_id,))
        row = cur.fetchone()
        if not row:
            return None
        data = dict(row)

        # Extraire les URLs
        try:
            cur.execute("SELECT url FROM group_urls WHERE group_id=?", (group_id,))
            urls_rows = cur.fetchall()
            urls = [r['url'] for r in urls_rows]
            data["urls"] = urls
        except Exception as e:
            print(f"DEBUG: Error fetching group URLs: {e}")
            data["urls"] = []

        # Extraire les tags (√âtiquettes)
        try:
            query = """
                SELECT t.name 
                FROM tags t
                JOIN groups_tags gt ON gt.tag_id = t.id
                WHERE gt.group_id = ?
            """
            cur.execute(query, (group_id,))
            tags_rows = cur.fetchall()
            tags = [r['name'] for r in tags_rows]
            data["tags"] = tags
        except Exception as e:
            print(f"DEBUG: Error fetching group tags: {e}")
            data["tags"] = []

        return data

    def get_group_scenes(self, group_id: int) -> list[dict]:
        """
        R√©cup√®re les sc√®nes associ√©es √† un groupe, y compris leurs URLs.
        """
        cur = self.conn.cursor()
        query = """
            SELECT
                s.id AS scene_id,
                gs.scene_index,
                s.title AS scene_title,
                GROUP_CONCAT(su.url) AS existing_urls
            FROM
                groups_scenes gs
            JOIN
                scenes s ON gs.scene_id = s.id
            LEFT JOIN
                scene_urls su ON s.id = su.scene_id
            WHERE
                gs.group_id = ?
            GROUP BY
                s.id, gs.scene_index, s.title
            ORDER BY
                gs.scene_index
        """
        cur.execute(query, (group_id,))
        rows = cur.fetchall()
        
        scenes = []
        for row in rows:
            scene = dict(row)
            scene['existing_urls'] = scene['existing_urls'].split(',') if scene['existing_urls'] else []
            scenes.append(scene)
        return scenes

    def inject_scene_urls(self, scene_urls_to_inject: list[dict]) -> None:
        """
        Injecte les URLs de sc√®nes dans la base de donn√©es.
        Args:
            scene_urls_to_inject: Liste de dicts avec {'scene_id': int, 'url': str, 'source': str}
        """
        cur = self.conn.cursor()
        for item in scene_urls_to_inject:
            scene_id = item['scene_id']
            url = item['url']
            
            # V√©rifier si l'URL existe d√©j√† pour cette sc√®ne
            cur.execute(
                "SELECT COUNT(*) FROM scene_urls WHERE scene_id=? AND url=?",
                (scene_id, url)
            )
            if cur.fetchone()[0] == 0:
                cur.execute(
                    "INSERT INTO scene_urls (scene_id, url) VALUES (?,?)",
                    (scene_id, url)
                )
        self.conn.commit()


    def close(self):
        self.conn.close()


============================================================
[45/83] services\extractors\__init__.py
------------------------------------------------------------


============================================================
[46/83] services\extractors\babepedia.py
------------------------------------------------------------
"""
Extracteur Babepedia ‚Äî source pour bio courte, tattoos/piercings
avec parsing am√©lior√©, et tags.
"""
import re

from services.extractors.base import BaseExtractor
from utils.body_art_parser import parse_body_art


class BabepediaExtractor(BaseExtractor):
    SOURCE_NAME = "babepedia"

    def build_url(self, performer_name: str) -> str | None:
        """Construire l'URL Babepedia depuis le nom."""
        # Babepedia utilise le nom with espaces ‚Üí underscores
        slug = self._normalize_name_underscore(performer_name)
        return f"https://www.babepedia.com/babe/{slug}"

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)

        tree = self._fetch_tree(url)
        if tree is None:
            return data

        # ‚îÄ‚îÄ Phase 1 ‚Äî M√©tadonn√©es ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["name"] = self._get_text(tree, '//h1/text()') or self._get_text(tree, '//h1//text()')
        
        aliases_text = self._get_stat(tree, "Aliases") or self._get_stat(tree, "Also known as")
        if aliases_text:
            data["aliases"] = [a.strip() for a in aliases_text.split(',') if a.strip()]

        data["birthdate"] = self._get_stat(tree, "Born") or self._get_stat(tree, "Birthday")
        data["country"] = self._get_stat(tree, "Birthplace")
        data["ethnicity"] = self._get_stat(tree, "Ethnicity")
        data["hair_color"] = self._get_stat(tree, "Hair color") or self._get_stat(tree, "Hair")
        data["eye_color"] = self._get_stat(tree, "Eye color") or self._get_stat(tree, "Eyes")
        data["height"] = self._get_stat(tree, "Height")
        data["weight"] = self._get_stat(tree, "Weight")
        data["measurements"] = self._get_stat(tree, "Measurements")
        data["fake_tits"] = self._get_stat(tree, "Boobs")

        # ‚îÄ‚îÄ Bio / Details ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        bio_xpaths = [
            '//p[@id="biotext"]/text()',
            '//div[@id="bio"]//text()',
            '//div[contains(@class,"bio")]//text()',
        ]
        for xpath in bio_xpaths:
            texts = tree.xpath(xpath)
            if texts:
                bio = " ".join(t.strip() for t in texts if t.strip())
                if len(bio) > 20:
                    data["details"] = bio
                    break

        # ‚îÄ‚îÄ Tattoos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tattoo_text = self._get_stat(tree, "Tattoos")
        data["tattoos"] = parse_body_art(tattoo_text) if tattoo_text else []

        # ‚îÄ‚îÄ Piercings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        piercing_text = self._get_stat(tree, "Piercings")
        data["piercings"] = parse_body_art(piercing_text) if piercing_text else []

# ‚îÄ‚îÄ Tags ‚Äî strat√©gie multi-fallback ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tags = []

        # Strat√©gie 1 : meta keywords (le plus fiable)
        meta_kw = tree.xpath('//meta[@name="keywords"]/@content')
        if meta_kw and meta_kw[0].strip():
            tags = [t.strip() for t in meta_kw[0].split(',')
                    if t.strip() and len(t.strip()) > 2]

        # Strat√©gie 2 : liens /tag/ ou /category/ dans la page
        if not tags:
            raw = tree.xpath(
                '//a[contains(@href,"/tag/") or contains(@href,"/category/")]/text()'
            )
            tags = [t.strip() for t in raw if t.strip() and len(t.strip()) > 2]

        # Strat√©gie 3 : balises sp√©cifiques Babepedia
        if not tags:
            raw = tree.xpath('//span[@class="tag"]/text() | //div[@class="tags"]//text()')
            tags = [t.strip() for t in raw if t.strip() and len(t.strip()) > 2]

        data["tags"] = list(dict.fromkeys(tags))[:50]  # d√©dupliquer, limiter √† 50

        # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["urls"]["babepedia"] = url

        # Extraire liens vers d'autres databases
        links = tree.xpath('//a[contains(@href, "http")]/@href')
        for link in links:
            link = link.strip()
            if "iafd.com" in link:
                data["urls"]["iafd"] = link
            elif "freeones.com" in link:
                data["urls"]["freeones"] = link
            elif "thenude" in link:
                data["urls"]["thenude"] = link
            elif "twitter.com" in link or "x.com" in link:
                data["urls"]["twitter"] = link
            elif "instagram.com" in link:
                data["urls"]["instagram"] = link

        return data

    def _get_stat(self, tree, label: str) -> str | None:
        """Extraire une stat de Babepedia (format tableau)."""
        xpaths = [
            f'//td[contains(text(), "{label}")]/following-sibling::td[1]',
            f'//span[contains(text(), "{label}")]/following-sibling::span[1]',
            f'//li[contains(., "{label}")]',
            f'//div[contains(@class,"stat")]//*[contains(text(), "{label}")]/..',
        ]
        for xpath in xpaths:
            val = self._get_text(tree, xpath)
            if val:
                # Nettoyer le label du texte si pr√©sent
                val = re.sub(rf'^{label}\s*[:]\s*', '', val, flags=re.I).strip()
                if val:
                    return val
        return None


============================================================
[47/83] services\extractors\base.py
------------------------------------------------------------
import re
import subprocess
from abc import ABC, abstractmethod

from lxml import html as lxml_html

from services.scrape_cache import ScrapeCache


class BaseExtractor(ABC):
    """
    Base commune √† tous les extracteurs de donn√©es performer.
    Chaque sous-classe impl√©mente extract_from_url() qui retourne
    un dict unifi√© Phase 2.
    """

    SOURCE_NAME: str = "base"
    HEADERS = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept-Language": "en-US,en;q=0.9",
    }
    TIMEOUT = 15

    # ‚îÄ‚îÄ M√©thode principale ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    @abstractmethod
    def extract_from_url(self, url: str) -> dict:
        """
        Extraire les donn√©es Phase 1 + Phase 2 depuis une URL.
        
        Retourne un dict unifi√© avec champs Phase 1 (m√©tadonn√©es)
        et Phase 2 (awards, trivia, details, tattoos, piercings, tags, urls).
        """
        ...

    def build_url(self, performer_name: str) -> str | None:
        """
        Construire l'URL d'un performer √† partir de son nom.
        √Ä surcharger dans les sous-classes.
        Retourne None si pas de logique de construction disponible.
        """
        return None

    # ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _empty_result(self, url: str) -> dict:
        """Retourner un r√©sultat vide avec les m√©tadonn√©es source."""
        return {
            "_source": self.SOURCE_NAME,
            "_url": url,
            # Phase 1 ‚Äî m√©tadonn√©es
            "name": None,
            "aliases": [],
            "birthdate": None,
            "death_date": None,
            "country": None,
            "ethnicity": None,
            "hair_color": None,
            "eye_color": None,
            "height": None,
            "weight": None,
            "measurements": None,
            "fake_tits": None,
            "career_length": None,
            # Phase 2 ‚Äî champs avanc√©s
            "awards": [],
            "trivia": None,
            "details": None,
            "tattoos": [],
            "piercings": [],
            "tags": [],
            "urls": {},
        }

    def _fetch_tree(self, url: str) -> lxml_html.HtmlElement | None:
        """
        T√©l√©charger une page HTML et retourner l'arbre lxml.
        Utilise le ScrapeCache si disponible.
        """
        try:
            command = ['curl.exe', '-L', '-s', '-A', self.HEADERS['User-Agent'], url]
            # Ex√©cuter curl et capturer la sortie binaire brute
            result = subprocess.run(command, capture_output=True, check=True, timeout=self.TIMEOUT)
            
            # Laisser lxml analyser le contenu binaire, il auto-d√©tectera l'encodage
            return lxml_html.fromstring(result.stdout)
        except subprocess.CalledProcessError as e:
            # D√©coder stderr pour l'impression, avec un fallback s√ªr
            error_message = e.stderr.decode('utf-8', errors='replace') if e.stderr else ''
            print(f"[{self.SOURCE_NAME}] Erreur fetch {url}: {error_message}")
            return None
        except Exception as e:
            print(f"[{self.SOURCE_NAME}] Erreur fetch {url}: {e}")
            return None

    def _get_text(self, tree: lxml_html.HtmlElement, xpath: str) -> str | None:
        """Extraire le texte d'un n≈ìud via XPath."""
        nodes = tree.xpath(xpath)
        if nodes:
            if isinstance(nodes[0], str):
                return nodes[0].strip() or None
            text = nodes[0].text_content().strip()
            return text if text else None
        return None

    def _get_texts(self, tree: lxml_html.HtmlElement, xpath: str) -> list[str]:
        """Extraire une liste de textes via XPath."""
        nodes = tree.xpath(xpath)
        result = []
        for n in nodes:
            if isinstance(n, str):
                t = n.strip()
            else:
                t = n.text_content().strip()
            if t:
                result.append(t)
        return result

    @staticmethod
    def _normalize_name(name: str) -> str:
        """Normaliser un nom pour construction d'URL (espaces ‚Üí tirets, lowercase)."""
        name = name.strip().lower()
        name = re.sub(r'[^a-z0-9\s-]', '', name)
        name = re.sub(r'\s+', '-', name)
        return name

    @staticmethod
    def _normalize_name_plus(name: str) -> str:
        """Normaliser un nom pour URL FreeOnes (espaces ‚Üí +)."""
        return name.strip().replace(' ', '+')

    @staticmethod
    def _normalize_name_underscore(name: str) -> str:
        """Normaliser un nom pour URL (espaces ‚Üí _)."""
        return name.strip().replace(' ', '_')


============================================================
[48/83] services\extractors\dvd\__init__.py
------------------------------------------------------------


============================================================
[49/83] services\extractors\dvd\adultempire_dvd.py
------------------------------------------------------------
"""
AdultEmpireDVDExtractor ‚Äî Source secondaire (P2).
Extrait aussi les URLs de clips/sc√®nes pour la Phase 2.
"""
import re
from services.extractors.dvd.base_dvd import BaseExtractorDVD


class AdultEmpireDVDExtractor(BaseExtractorDVD):
    SOURCE_NAME = "adultdvdempire"

    # Cookie requis pour acc√®s
    COOKIES = "ageConfirmed=true"

    def _fetch_tree(self, url: str):
        """Override pour injecter le cookie age."""
        cached = self.cache.get(url)
        if cached:
            from lxml import html as lxml_html
            return lxml_html.fromstring(cached)
        try:
            import subprocess
            from lxml import html as lxml_html
            result = subprocess.run(
                ["curl", "-sL", "--max-time", "15",
                 "-H", f"Cookie: {self.COOKIES}", url],
                capture_output=True, timeout=20
            )
            html = result.stdout.decode("utf-8", errors="replace")
            if html and len(html) > 500:
                self.cache.set(url, html)
                return lxml_html.fromstring(html)
        except Exception as e:
            print(f"[adultdvdempire] Fetch error: {e}")
        return None

    def search_urls(self, title: str, studio: str = "") -> list[str]:
        slug = re.sub(r'[^a-z0-9]+', '-', title.lower()).strip('-')
        return [
            f"https://www.adultdvdempire.com/{slug}.html",
            f"https://www.adultdvdempire.com/dvd/{slug}/",
        ]

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)
        tree = self._fetch_tree(url)
        if tree is None:
            return data

        data["title"] = (self._get_text(tree, '//h1[@class="title"]/text()') or 
                        self._get_text(tree, '//h1/text()'))

        data["studio"] = self._get_text(tree, '//a[contains(@href,"/studio/")]/text()')

        # Date ‚Äî format "Sep 22 2008"
        data["date"] = (self._get_text(tree, '//li[contains(text(),"Released")]/text()') or 
                       self._get_text(tree, '//*[contains(@class,"release-date")]/text()'))

        # Duration ‚Äî format "1 hrs. 55 mins."
        data["duration_raw"] = (self._get_text(tree, '//li[contains(text(),"Run Time")]/text()') or 
                                self._get_text(tree, '//*[contains(@class,"run-time")]/text()'))

        data["director"] = self._get_text(tree, '//a[contains(@href,"/director/")]/text()')

        # Synopsis ‚Äî strip "Show More"
        desc_nodes = tree.xpath('//*[contains(@class,"synopsis")]//text()')
        if desc_nodes:
            desc = " ".join(t.strip() for t in desc_nodes if t.strip()
                            and "show more" not in t.lower())
            data["description"] = desc[:2000] or None

        # Covers
        data["front_cover_url"] = (self._get_text(tree, '//img[contains(@class,"cover-front")]/@src') or 
                                   self._get_text(tree, '//div[@id="front-cover"]//img/@src'))
        data["back_cover_url"]  = (self._get_text(tree, '//img[contains(@class,"cover-back")]/@src') or 
                                   self._get_text(tree, '//div[@id="back-cover"]//img/@src'))

        # ‚îÄ‚îÄ PHASE 2 : extraction URLs sc√®nes via liens /clip/ ‚îÄ‚îÄ‚îÄ
        clip_links = tree.xpath('//a[contains(@href, "/clip/")]/@href')
        seen = set()
        for i, href in enumerate(clip_links):
            if href in seen:
                continue
            seen.add(href)
            scene_url = href if href.startswith("http") else f"https://www.adultdvdempire.com{href}"
            data["scenes"].append({
                "index": i + 1,
                "title": None,
                "url_adultdvdempire": scene_url,
            })

        return data


============================================================
[50/83] services\extractors\dvd\base_dvd.py
------------------------------------------------------------
"""
BaseExtractorDVD ‚Äî Base commune pour tous les extracteurs de groupes/DVD.
Similaire √† BaseExtractor (performer) mais adapt√© aux m√©tadonn√©es group.
"""
from abc import ABC, abstractmethod
import re
import subprocess
from lxml import html as lxml_html
from services.scrape_cache import ScrapeCache


class BaseExtractorDVD(ABC):
    SOURCE_NAME = "unknown"

    def __init__(self):
        self.cache = ScrapeCache()

    def _fetch_tree(self, url: str):
        """Fetch HTML avec cache. Retourne lxml tree ou None."""
        cached = self.cache.get(url)
        if cached:
            return lxml_html.fromstring(cached)
        try:
            result = subprocess.run(
                ["curl", "-sL", "--max-time", "15", url],
                capture_output=True, timeout=20
            )
            html = result.stdout.decode("utf-8", errors="replace")
            if html and len(html) > 500:
                self.cache.set(url, html)
                return lxml_html.fromstring(html)
        except Exception as e:
            print(f"[{self.SOURCE_NAME}] Fetch error: {e}")
        return None

    def _get_text(self, tree, xpath: str) -> str | None:
        nodes = tree.xpath(xpath)
        if nodes:
            text = nodes[0] if isinstance(nodes[0], str) else nodes[0].text_content()
            return text.strip() or None
        return None

    def _empty_result(self, url: str) -> dict:
        """Retourne un dict vide standard pour un group."""
        return {
            "_source": self.SOURCE_NAME,
            "_url": url,
            # M√©tadonn√©es group (Phase 1)
            "title": None,
            "aliases": None,
            "date": None,
            "studio": None,
            "director": None,
            "duration_raw": None,   # cha√Æne brute ‚Äî conversion dans merger
            "description": None,
            "tags": [],
            "front_cover_url": None,
            "back_cover_url": None,
            # Sc√®nes extraites du DVD (Phase 2)
            "scenes": [],   # list[dict] avec cl√©s : index, title, url_source
        }

    @abstractmethod
    def extract_from_url(self, url: str) -> dict:
        """Scraper une URL de group et retourner le dict unifi√©."""
        ...

    def search_urls(self, title: str, studio: str = "") -> list[str]:
        """
        Recherche de l'URL du group sur la source.
        √Ä impl√©menter dans chaque sous-classe.
        Retourne une liste d'URLs candidates (max 5).
        """
        return []


============================================================
[51/83] services\extractors\dvd\data18_dvd.py
------------------------------------------------------------
"""
Data18DVDExtractor ‚Äî Source prioritaire (P1) pour les m√©tadonn√©es group.
Extrait aussi les URLs de sc√®nes individuelles pour la Phase 2.
"""
import re
from services.extractors.dvd.base_dvd import BaseExtractorDVD


class Data18DVDExtractor(BaseExtractorDVD):
    SOURCE_NAME = "data18"

    def search_urls(self, title: str, studio: str = "") -> list[str]:
        """G√©n√®re des URLs candidates data18 pour un titre DVD."""
        slug = re.sub(r'[^a-z0-9]+', '-', title.lower()).strip('-')
        candidates = [
            f"https://www.data18.com/movies/{slug}",
            f"https://www.data18.com/content/{slug}",
            f"https://www.data18.com/movies/{slug}-dvd",
        ]
        
        # RESTAURATION DE LA RECHERCHE ACTIVE
        # Le site data18.com a un endpoint de recherche qui peut retourner une liste de r√©sultats
        try:
            search_query = title.strip().replace(' ', '+')
            search_url = f"https://www.data18.com/search?k={search_query}"
            
            tree = self._fetch_tree(search_url)
            if tree is not None:
                # R√©cup√©rer tous les liens qui ressemblent √† des films/DVDs
                # Pattern: /movies/ID-titre ou /content/ID-titre
                links = tree.xpath('//a[contains(@href, "/movies/")]/@href')
                links += tree.xpath('//a[contains(@href, "/content/")]/@href')
                
                # Aussi chercher dans les blocs gen11/gen12 typiques de data18
                links += tree.xpath('//div[contains(@class,"gen")]//a/@href')

                for link in links:
                    # Filtrer les faux positifs (scenes, tags, sites, etc.)
                    if any(x in link for x in ["/scenes/", "/tags/", "/sites/", "/pornstars/", "/studios/"]):
                        continue
                        
                    full_link = link if link.startswith("http") else f"https://www.data18.com{link}"
                    
                    # √âviter les doublons
                    if full_link not in candidates:
                        # Priorit√© : ins√©rer au d√©but si le titre est tr√®s proche (TODO)
                        # Pour l'instant on ajoute √† la fin
                        candidates.append(full_link)
        except Exception as e:
            print(f"[Data18] Search error: {e}")

        return candidates

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)
        tree = self._fetch_tree(url)
        if tree is None:
            return data

        # Titre
        data["title"] = self._get_text(tree, '//h1[@itemprop="name"]/text()') or \
                        self._get_text(tree, '//h1/text()')

        # Date ‚Äî plusieurs fallbacks
        date_raw = self._get_text(tree, '//span[contains(text(),"Release")]/following-sibling::text()[1]')
        if not date_raw:
            date_raw = self._get_text(tree, '//div[@class="gen-wrap"]//span[contains(@class,"date")]/text()')
        data["date"] = date_raw

        # Studio
        data["studio"] = self._get_text(tree, '//a[@itemprop="publisher"]/text()') or \
                         self._get_text(tree, '//span[@itemprop="publisher"]/text()')

        # Director
        data["director"] = self._get_text(tree, '//span[@itemprop="director"]/text()')

        # Duration ‚Äî format [HH:MM:SS]
        dur = self._get_text(tree, '//span[@itemprop="duration"]/text()') or \
              self._get_text(tree, '//*[contains(text(),"Run Time")]/following-sibling::text()[1]')
        data["duration_raw"] = dur

        # Description / Synopsis
        desc_nodes = tree.xpath('//div[@class="boxdesc"]//text()')
        if desc_nodes:
            desc = " ".join(t.strip() for t in desc_nodes if t.strip())
            # Couper avant <ul> ou <br><br>
            data["description"] = desc[:2000] if desc else None

        # Covers
        data["front_cover_url"] = self._get_text(tree, '//img[@id="frontbox"]/@src') or \
                                   self._get_text(tree, '//img[contains(@alt,"front")]/@src')
        data["back_cover_url"]  = self._get_text(tree, '//img[@id="backbox"]/@src') or \
                                   self._get_text(tree, '//img[contains(@alt,"back")]/@src')

        # Tags ‚Äî 3 listes : categories, themes, genres
        tags = []
        for xpath in [
            '//a[contains(@href,"/categories/")]/text()',
            '//a[contains(@href,"/themes/")]/text()',
            '//a[contains(@href,"/genres/")]/text()',
        ]:
            tags += [t.strip() for t in tree.xpath(xpath) if t.strip()]
        data["tags"] = list(dict.fromkeys(tags))  # d√©dupliqu√©, ordre pr√©serv√©

        # ‚îÄ‚îÄ PHASE 2 : extraction URLs sc√®nes individuelles ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # data18 liste les sc√®nes avec des liens /content/SCENE_ID
        scene_links = tree.xpath('//div[contains(@class,"scene")]//a[contains(@href,"/content/")]/@href')
        if not scene_links:
            scene_links = tree.xpath('//a[contains(@href,"/content/") and contains(@href,"/scene")]/@href')

        for i, href in enumerate(scene_links):
            scene_url = href if href.startswith("http") else f"https://www.data18.com{href}"
            data["scenes"].append({
                "index": i + 1,
                "title": None,      # sera enrichi si besoin
                "url_data18": scene_url,
            })

        return data


============================================================
[52/83] services\extractors\dvd\iafd_dvd.py
------------------------------------------------------------
"""
IafdDVDExtractor ‚Äî Extracteur IAFD pour les titres DVDs/Groups.
Source P2 (prioritaire pour DVDs classiques US, cast fiable, dates pr√©cises).
"""
import re
from services.extractors.dvd.base_dvd import BaseExtractorDVD
from utils.duration import parse_duration_to_seconds


class IafdDVDExtractor(BaseExtractorDVD):
    SOURCE_NAME = "iafd_dvd"
    BASE_URL = "https://www.iafd.com/title.rme/title="

    def build_url_from_title(self, title: str, year: str | None = None) -> str:
        """Construire l'URL de recherche IAFD depuis un titre."""
        slug = re.sub(r'[^a-z0-9]+', '-', title.lower()).strip('-')
        url = f"{self.BASE_URL}{slug}"
        if year:
            url += f"/year={year}"
        return url

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)

        tree = self._fetch_tree(url)
        if tree is None:
            return data

        # ‚îÄ‚îÄ Titre ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["title"] = self._get_text(tree, '//h1[@itemprop="name"]/text()')
        if not data["title"]:
            data["title"] = self._get_text(tree, '//h1/text()')

        # ‚îÄ‚îÄ Date / Ann√©e ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["date"] = (self._get_stat(tree, "Release Date") or 
                       self._get_stat(tree, "Year"))

        # ‚îÄ‚îÄ Studio ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["studio"] = (self._get_text(
            tree, '//p[@class="subheading"]/a[1]/text()'
        ) or self._get_stat(tree, "Studio"))

        # ‚îÄ‚îÄ R√©alisateur ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["director"] = (self._get_text(
            tree, '//p[b[contains(text(),"Director")]]/a/text()'
        ) or self._get_stat(tree, "Director"))

        # ‚îÄ‚îÄ Dur√©e ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        raw_duration = (self._get_stat(tree, "Running Time") or 
                       self._get_stat(tree, "Duration"))
        if raw_duration:
            data["duration"] = raw_duration
            secs = parse_duration_to_seconds(raw_duration)
            if secs:
                data["_duration_seconds"] = secs

        # ‚îÄ‚îÄ Description / Synopsis ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["description"] = (self._get_text(
            tree, '//div[@class="synopsis"]//text()'
        ) or self._get_text(
            tree, '//div[contains(@class,"description")]//text()'
        ))

        # ‚îÄ‚îÄ Aliases / Titres alternatifs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        aliases_text = (self._get_stat(tree, "Alternate Titles") or 
                       self._get_stat(tree, "AKA"))
        if aliases_text:
            data["aliases"] = [a.strip() for a in aliases_text.split(',') if a.strip()]

        # ‚îÄ‚îÄ Tags / Cat√©gories ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tags = tree.xpath('//div[@id="genres"]//a/text()')
        if not tags:
            tags = tree.xpath('//a[contains(@href,"/genre/")]/text()')
        data["tags"] = [t.strip() for t in tags if t.strip()]

        # ‚îÄ‚îÄ Sc√®nes (liste index√©e) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["scenes"] = self._extract_scenes(tree)

        # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["urls"] = {"iafd_dvd": url}

        return data

    def _extract_scenes(self, tree) -> list[dict]:
        """
        Extraire la liste des sc√®nes du DVD depuis la page IAFD.
        Retourne : [{"index": int, "title": str | None, "performers": [str]}]
        """
        scenes = []

        # IAFD liste les sc√®nes dans un tableau ou une liste ordonn√©e
        # Structure typique : <div id="sceneinfo"> ou <table class="w100">
        scene_rows = tree.xpath(
            '//div[contains(@id,"scene")] | //tr[contains(@class,"scene")]'
        )

        for i, row in enumerate(scene_rows, start=1):
            scene = {"index": i, "title": None, "performers": [], "url": None}

            # Titre de sc√®ne
            title_nodes = row.xpath('.//b/text() | .//strong/text()')
            if title_nodes:
                scene["title"] = title_nodes[0].strip()

            # Performers
            perf_links = row.xpath('.//a[contains(@href,"/person.rme/")]/text()')
            scene["performers"] = [p.strip() for p in perf_links if p.strip()]

            # URL directe de la sc√®ne (si disponible)
            scene_link = row.xpath('.//a[contains(@href,"/scene.rme/")]/@href')
            if scene_link:
                scene["url"] = "https://www.iafd.com" + scene_link[0]

            if scene["title"] or scene["performers"]:
                scenes.append(scene)

        return scenes

    def _get_stat(self, tree, label: str) -> str | None:
        """Extraire une valeur depuis les tableaux info IAFD."""
        xpaths = [
            f'//td[contains(text(),"{label}")]/following-sibling::td[1]',
            f'//b[contains(text(),"{label}")]/parent::*/following-sibling::*[1]',
            f'//p[contains(text(),"{label}")]/following-sibling::p[1]',
            f'//span[normalize-space(text())="{label}"]/following-sibling::span[1]',
        ]
        for xpath in xpaths:
            val = self._get_text(tree, xpath)
            if val:
                return val.strip()
        return None


============================================================
[53/83] services\extractors\dvd\jeedoo_dvd.py
------------------------------------------------------------
"""
JedeeDVDExtractor ‚Äî Extracteur Jeedoo pour productions europ√©ennes.
Source P4 (priorit√© basse ‚Äî sp√©cialis√© europe).
"""
from services.extractors.dvd.base_dvd import BaseExtractorDVD


class JedeeDVDExtractor(BaseExtractorDVD):
    SOURCE_NAME = "jeedoo"
    BASE_URL = "https://www.jeedoo.com/"

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)
        tree = self._fetch_tree(url)
        if tree is None:
            return data

        data["title"] = self._get_text(tree, '//h1/text()')
        data["date"] = self._get_text(tree,
            '//span[contains(@class,"date")]/text() | '
            '//p[contains(text(),"Ann√©e")]/following-sibling::p[1]/text()')
        data["studio"] = self._get_text(tree,
            '//a[contains(@href,"/studio/")]/text()')
        data["director"] = self._get_text(tree,
            '//p[contains(text(),"R√©alisateur")]/following-sibling::p[1]/text()')
        data["duration"] = self._get_text(tree,
            '//p[contains(text(),"Dur√©e")]/following-sibling::p[1]/text()')
        data["description"] = self._get_text(tree,
            '//div[contains(@class,"description")]//text()')

        tags = tree.xpath('//a[contains(@href,"/categorie/")]/text()')
        data["tags"] = [t.strip() for t in tags if t.strip()]

        data["scenes"] = []  # Jeedoo ne liste pas les sc√®nes individuellement
        data["urls"] = {"jeedoo": url}

        return data


============================================================
[54/83] services\extractors\freeones.py
------------------------------------------------------------
"""
Extracteur FreeOnes ‚Äî source primaire pour trivia et bio.
S√©pare details / trivia / awards contrairement au V1.
"""
import re
import html as html_std

from services.extractors.base import BaseExtractor
from utils.body_art_parser import parse_body_art


# Regex multi-ceremonies pour extraire les awards du texte
AWARD_PATTERN = re.compile(
    r'((?:\d{4}\s+)?(?:AVN|XBIZ|XRCO|NightMoves|XCritic|AEBN|AdultFilmDatabase'
    r'|Fans of Adult|TEA|GayVN|Grabby|Urban X|CAVR|Inked)[^\n.;]*)',
    re.I
)


class FreeonesExtractor(BaseExtractor):
    SOURCE_NAME = "freeones"

    def build_url(self, performer_name: str) -> str | None:
        """Construire l'URL FreeOnes depuis le nom."""
        slug = self._normalize_name(performer_name)
        return f"https://www.freeones.com/{slug}/bio"

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)

        tree = self._fetch_tree(url)
        if tree is None:
            return data

        # ‚îÄ‚îÄ Phase 1 ‚Äî M√©tadonn√©es ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["name"] = self._get_text(tree, '//h1/text()') or self._get_text(tree, '//h1//text()')
        
        aliases_el = self._get_stat(tree, "Aliases") or self._get_stat(tree, "Also Known As")
        if aliases_el:
            data["aliases"] = [a.strip() for a in aliases_el.split(',') if a.strip()]

        data["birthdate"] = self._get_stat(tree, "Date of Birth") or self._get_stat(tree, "Birthday")
        data["country"] = self._get_stat(tree, "Place of Birth") or self._get_stat(tree, "Birthplace")
        data["ethnicity"] = self._get_stat(tree, "Ethnicity")
        data["hair_color"] = self._get_stat(tree, "Hair Color") or self._get_stat(tree, "Hair")
        data["eye_color"] = self._get_stat(tree, "Eye Color") or self._get_stat(tree, "Eyes")
        data["height"] = self._get_stat(tree, "Height")
        data["weight"] = self._get_stat(tree, "Weight")

        # Ethnie : Essayer de r√©cup√©rer via les liens si le texte simple √©choue
        if not data["ethnicity"]:
            eth_links = tree.xpath('//a[contains(@href, "/ethnicity/")]/text()')
            if eth_links:
                data["ethnicity"] = eth_links[0].strip()

        data["measurements"] = self._get_stat(tree, "Measurements")
        
        # Si pas de measurements, essayer de construire depuis Bust/Waist/Hip
        if not data["measurements"]:
            bust = self._get_stat(tree, "Bust")
            waist = self._get_stat(tree, "Waist")
            hip = self._get_stat(tree, "Hip")
            if bust and waist and hip:
                data["measurements"] = f"{bust}-{waist}-{hip}"
        
        # Normalisation Mensurations (Conversion CM -> Pouces si n√©cessaire)
        if data["measurements"]:
            try:
                # Si les valeurs semblent √™tre en cm (toutes > 50), on convertit
                parts = re.findall(r'\d+', data["measurements"])
                if len(parts) == 3 and all(int(x) > 50 for x in parts):
                    imperial = [str(round(int(x) / 2.54)) for x in parts]
                    data["measurements"] = "-".join(imperial)
            except Exception:
                pass

        data["fake_tits"] = self._get_stat(tree, "Boobs") or self._get_stat(tree, "Enhanced")
        
        # Career Length : Combiner Start/End si Years Active est vide
        data["career_length"] = self._get_stat(tree, "Career Start") or self._get_stat(tree, "Years Active")
        if not data["career_length"]:
            start = self._get_stat(tree, "Career Start") or self._get_stat(tree, "Started")
            end = self._get_stat(tree, "Career End") or self._get_stat(tree, "Ended") or "Present"
            if start:
                data["career_length"] = f"{start}-{end}"

        # ‚îÄ‚îÄ Bio / Details ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        bio_xpaths = [
            '//div[@data-test="biography"]//text()[normalize-space()]',
            '//div[contains(@class,"biography")]//text()[normalize-space()]',
            '//div[@class="bio"]//text()[normalize-space()]',
        ]
        for xpath in bio_xpaths:
            bio_texts = tree.xpath(xpath)
            if bio_texts:
                bio = " ".join(t.strip() for t in bio_texts if t.strip())
                if len(bio) > 20:
                    data["details"] = bio
                    break

        # ‚îÄ‚îÄ Trivia / Additional Information ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        trivia_xpaths = [
            (
                "//p[normalize-space(text())='Additional Information']"
                "/following-sibling::div[contains(@class,'hide-on-edit')]"
                "//text()[normalize-space()]"
            ),
            (
                "//h3[contains(text(),'Additional')]"
                "/following-sibling::div//text()[normalize-space()]"
            ),
            (
                "//div[contains(@class,'additional')]"
                "//text()[normalize-space()]"
            ),
        ]
        for xpath in trivia_xpaths:
            trivia_texts = tree.xpath(xpath)
            if trivia_texts:
                raw = " ".join(t.strip() for t in trivia_texts if t.strip())
                if len(raw) > 10:
                    data["trivia"] = html_std.unescape(raw)
                    break

        # ‚îÄ‚îÄ Awards (regex depuis texte bio + trivia) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        combined = (data.get("details") or "") + " " + (data.get("trivia") or "")
        if combined.strip():
            matches = AWARD_PATTERN.findall(combined)
            data["awards"] = [m.strip() for m in matches if m.strip()]

        # ‚îÄ‚îÄ Tattoos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tattoo_text = self._get_stat(tree, "Tattoos")
        data["tattoos"] = parse_body_art(tattoo_text) if tattoo_text else []

        # ‚îÄ‚îÄ Piercings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        piercing_text = self._get_stat(tree, "Piercings")
        data["piercings"] = parse_body_art(piercing_text) if piercing_text else []

        # ‚îÄ‚îÄ Tags (cat√©gories) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tag_xpaths = [
            "//a[contains(@href,'/category/')]//text()",
            "//a[contains(@href,'/tag/')]//text()",
            "//div[contains(@class,'tag')]//a/text()",
        ]
        for xpath in tag_xpaths:
            tags_raw = self._get_texts(tree, xpath)
            if tags_raw:
                data["tags"] = [t.strip() for t in tags_raw if t.strip() and len(t.strip()) > 1]
                break

        # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["urls"]["freeones"] = url

        # Extraire les liens vers databases ext√©rieures
        links = tree.xpath('//a[contains(@class,"link") or contains(@class,"database")]/@href')
        for link in links:
            link = link.strip()
            if "iafd.com" in link:
                data["urls"]["iafd"] = link
            elif "babepedia.com" in link:
                data["urls"]["babepedia"] = link
            elif "thenude" in link:
                data["urls"]["thenude"] = link
            elif "twitter.com" in link or "x.com" in link:
                data["urls"]["twitter"] = link
            elif "instagram.com" in link:
                data["urls"]["instagram"] = link

        return data

    def _get_stat(self, tree, label: str) -> str | None:
        """Extraire une stat depuis la page FreeOnes."""
        label_slug = label.lower().replace(" ", "-")
        xpaths = [
            f'//span[contains(text(), "{label}")]/following-sibling::span[1]',
            f'//p[contains(@data-test, "{label.lower()}")]',
            f'//p[contains(@data-test, "{label_slug}")]',
            f'//div[contains(@data-test, "{label.lower()}")]',
            f'//div[contains(@data-test, "{label_slug}")]',
            f'//a[contains(@data-test, "{label_slug}")]',
            f'//td[contains(text(), "{label}")]/following-sibling::td[1]',
            f'//*[contains(text(), "{label}")]/following-sibling::*[1]',
        ]
        for xpath in xpaths:
            val = self._get_text(tree, xpath)
            if val:
                return val
        return None


============================================================
[55/83] services\extractors\iafd.py
------------------------------------------------------------
"""
Extracteur IAFD ‚Äî source primaire pour les awards.
"""
import re

from services.extractors.base import BaseExtractor
from utils.body_art_parser import parse_body_art


# Lignes parasites √† ignorer dans les awards
AWARD_SKIP = re.compile(r'^([-‚Äì‚Äî]+|nomin[√©e]e?d?|winner|won|year|award)$', re.I)

# Regex √©tendue multi-ceremonies pour fallback texte
AWARD_PATTERN = re.compile(
    r'(\d{4}\s+)?(AVN|XBIZ|XRCO|NightMoves|XCritic|AEBN|AdultFilmDatabase'
    r'|Fans of Adult Media|TEA|GayVN|Grabby|Urban X|CAVR|Inked)[^.\n]*',
    re.I
)


class IafdExtractor(BaseExtractor):
    SOURCE_NAME = "iafd"

    def build_url(self, performer_name: str) -> str | None:
        """Construire l'URL IAFD depuis le nom."""
        slug = self._normalize_name(performer_name)
        return f"https://www.iafd.com/person.rme/perfid={slug}/gender=f/{slug}.htm"

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)

        tree = self._fetch_tree(url)
        if tree is None:
            return data

        # ‚îÄ‚îÄ Phase 1 ‚Äî M√©tadonn√©es ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["name"] = self._get_text(tree, '//h1/text()')
        
        aliases_text = self._get_stat(tree, "Aliases") or self._get_stat(tree, "AKA")
        if aliases_text:
            data["aliases"] = [a.strip() for a in aliases_text.split(',') if a.strip()]

        data["birthdate"] = self._get_stat(tree, "Birthday") or self._get_stat(tree, "Date of Birth")
        data["death_date"] = self._get_stat(tree, "Death")
        data["ethnicity"] = self._get_stat(tree, "Ethnicity") or self._get_stat(tree, "Race")
        data["country"] = self._get_stat(tree, "Birthplace") or self._get_stat(tree, "Nationality")
        data["hair_color"] = self._get_stat(tree, "Hair Color")
        data["eye_color"] = self._get_stat(tree, "Eye Color")
        
        height_text = self._get_stat(tree, "Height")
        if height_text:
            data["height"] = height_text
        
        weight_text = self._get_stat(tree, "Weight")
        if weight_text:
            data["weight"] = weight_text
            
        data["measurements"] = self._get_stat(tree, "Measurements")
        data["fake_tits"] = self._get_stat(tree, "Boobs") or self._get_stat(tree, "Breast")
        
        career_years = self._get_stat(tree, "Years Active")
        if career_years:
            data["career_length"] = career_years
        else:
            start = self._get_stat(tree, "Start") or self._get_stat(tree, "Career Start")
            end = self._get_stat(tree, "End") or self._get_stat(tree, "Career End") or "Present"
            if start:
                data["career_length"] = f"{start} - {end}"

        # ‚îÄ‚îÄ Phase 2 ‚Äî Awards ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["awards"] = self._extract_awards(tree)

        # ‚îÄ‚îÄ Bio / Details ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        bio_text = self._get_text(tree, '//div[@id="bio"]')
        if not bio_text:
            bio_text = self._get_text(tree, '//div[contains(@class,"biodata")]')
        data["details"] = bio_text

        # ‚îÄ‚îÄ Tattoos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tattoo_text = self._get_stat(tree, "Tattoos")
        data["tattoos"] = parse_body_art(tattoo_text) if tattoo_text else []

        # ‚îÄ‚îÄ Piercings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        piercing_text = self._get_stat(tree, "Piercings")
        data["piercings"] = parse_body_art(piercing_text) if piercing_text else []

        # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["urls"]["iafd"] = url

        # Extraire les liens vers d'autres databases
        db_links = tree.xpath('//a[contains(@href, "http")]/@href')
        for link in db_links:
            link = link.strip()
            if "freeones.com" in link:
                data["urls"]["freeones"] = link
            elif "babepedia.com" in link:
                data["urls"]["babepedia"] = link
            elif "thenude.com" in link or "thenude.eu" in link:
                data["urls"]["thenude"] = link

        return data

    def _extract_awards(self, tree) -> list[str]:
        """Parser les awards depuis div#awards de IAFD."""
        awards_divs = tree.xpath("//div[@id='awards']")
        if not awards_divs:
            # Essayer un s√©lecteur alternatif
            awards_divs = tree.xpath("//div[contains(@class,'award')]")
        
        if not awards_divs:
            return []

        div = awards_divs[0]
        # Ins√©rer des sauts de ligne avant chaque <br>
        for br in div.xpath(".//br"):
            br.tail = "\n" + (br.tail or "")

        awards_text = div.text_content()
        raw_lines = [line.strip() for line in awards_text.splitlines() if line.strip()]
        
        # Filtrer les lignes parasites
        result = []
        current_year = ""

        for line in raw_lines:
            # D√©tection de l'ann√©e (ex: 2012) pour l'associer √† l'award
            if re.match(r'^\d{4}$', line):
                current_year = line
                continue

            if AWARD_SKIP.match(line):
                continue
            if len(line) < 4:
                continue
            
            # Ajouter l'ann√©e devant la ligne si elle n'y est pas d√©j√†
            if current_year and not line.startswith(current_year):
                result.append(f"{current_year} {line}")
            else:
                result.append(line)

        return result

    def _get_stat(self, tree, label: str) -> str | None:
        """Extraire une stat depuis le tableau IAFD (label ‚Üí valeur)."""
        # Essayer plusieurs formats de tableau bio IAFD
        xpaths = [
            f'//p[contains(text(), "{label}")]/following-sibling::p[1]',
            f'//td[contains(text(), "{label}")]/following-sibling::td[1]',
            f'//b[contains(text(), "{label}")]/parent::*/following-sibling::*[1]',
            f'//span[contains(text(), "{label}")]/following-sibling::span[1]',
        ]
        for xpath in xpaths:
            val = self._get_text(tree, xpath)
            if val:
                return val
        return None


============================================================
[56/83] services\extractors\thenude.py
------------------------------------------------------------
"""
Extracteur TheNude ‚Äî source pour bios studio contextualis√©es.
"""
import re

from services.extractors.base import BaseExtractor
from utils.body_art_parser import parse_body_art


class ThenudeExtractor(BaseExtractor):
    SOURCE_NAME = "thenude"

    def build_url(self, performer_name: str) -> str | None:
        """Construire l'URL TheNude depuis le nom."""
        slug = self._normalize_name_underscore(performer_name)
        return f"https://www.thenude.com/models/{slug}.htm"

    def extract_from_url(self, url: str) -> dict:
        data = self._empty_result(url)

        tree = self._fetch_tree(url)
        if tree is None:
            return data

        # ‚îÄ‚îÄ Phase 1 ‚Äî M√©tadonn√©es ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["name"] = self._get_text(tree, '//h1/text()')
        
        aliases_text = self._get_stat(tree, "Aliases") or self._get_stat(tree, "AKA")
        if aliases_text:
            data["aliases"] = [a.strip() for a in aliases_text.split(',') if a.strip()]

        data["birthdate"] = self._get_stat(tree, "Born") or self._get_stat(tree, "Date of Birth")
        data["country"] = self._get_stat(tree, "Birthplace") or self._get_stat(tree, "Country")
        data["ethnicity"] = self._get_stat(tree, "Ethnicity")
        data["hair_color"] = self._get_stat(tree, "Hair")
        data["eye_color"] = self._get_stat(tree, "Eyes")
        data["height"] = self._get_stat(tree, "Height")
        data["weight"] = self._get_stat(tree, "Weight")
        data["measurements"] = self._get_stat(tree, "Measurements")
        data["fake_tits"] = self._get_stat(tree, "Boobs")

        # ‚îÄ‚îÄ Bio principale ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        bio_xpaths = [
            '//p[@class="description"]/text()',
            '//div[@class="description"]//text()',
            '//div[contains(@class,"bio")]//text()',
        ]
        for xpath in bio_xpaths:
            texts = tree.xpath(xpath)
            if texts:
                bio = " ".join(t.strip() for t in texts if t.strip())
                if len(bio) > 20:
                    data["details"] = bio
                    break

        # ‚îÄ‚îÄ Bios studio (trivia) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        studio_bios = self._extract_studio_bios(tree)
        if studio_bios:
            data["trivia"] = "\n\n".join(studio_bios)

        # ‚îÄ‚îÄ Tattoos ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tattoo_text = self._get_stat(tree, "Tattoos")
        data["tattoos"] = parse_body_art(tattoo_text) if tattoo_text else []

        # ‚îÄ‚îÄ Piercings ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        piercing_text = self._get_stat(tree, "Piercings")
        data["piercings"] = parse_body_art(piercing_text) if piercing_text else []

        # ‚îÄ‚îÄ Tags (keywords) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        tag_xpaths = [
            '//div[@class="keywords"]//a/text()',
            '//meta[@name="keywords"]/@content',
            '//div[contains(@class,"tag")]//a/text()',
        ]
        for xpath in tag_xpaths:
            tags_raw = tree.xpath(xpath)
            if tags_raw:
                # Si c'est un meta keywords, splitter par virgule
                if len(tags_raw) == 1 and ',' in tags_raw[0]:
                    tags_raw = [t.strip() for t in tags_raw[0].split(',')]
                data["tags"] = [t.strip() for t in tags_raw 
                                if t.strip() and len(t.strip()) > 1]
                break

        # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        data["urls"]["thenude"] = url

        # Extraire les liens vers databases
        links = tree.xpath('//a[contains(@href, "http")]/@href')
        for link in links:
            link = link.strip()
            if "iafd.com" in link:
                data["urls"]["iafd"] = link
            elif "freeones.com" in link:
                data["urls"]["freeones"] = link
            elif "babepedia.com" in link:
                data["urls"]["babepedia"] = link

        return data

    def _extract_studio_bios(self, tree) -> list[str]:
        """Extraire les bios par studio (ex: 'BRAZZERS biography')."""
        studio_bios = []
        
        # Chercher les headers contenant "biography"
        bio_headers = tree.xpath(
            '//*[contains(translate(text(),"BIOGRAPHY","biography"), "biography")]'
        )
        
        for h in bio_headers:
            title = h.text_content().strip()
            if title.lower() == "biography":
                continue

            # Le contenu suit le header
            content_node = h.getnext()
            if content_node is not None:
                content = content_node.text_content().strip()
                if content and len(content) > 20:
                    studio_bios.append(f"[{title}]\n{content}")

        return studio_bios

    def _get_stat(self, tree, label: str) -> str | None:
        """Extraire une stat depuis la page TheNude."""
        xpaths = [
            f'//span[contains(text(), "{label}")]/following-sibling::span[1]',
            f'//td[contains(text(), "{label}")]/following-sibling::td[1]',
            f'//li[contains(., "{label}")]',
            f'//div[contains(@class,"stat")]//*[contains(text(), "{label}")]/..',
        ]
        for xpath in xpaths:
            val = self._get_text(tree, xpath)
            if val:
                # Nettoyer le label du texte si pr√©sent
                val = re.sub(rf'^{label}\s*[:]\s*', '', val, flags=re.I).strip()
                if val:
                    return val
        return None


============================================================
[57/83] services\group_phase1_merger.py
------------------------------------------------------------
"""
GroupPhase1Merger ‚Äî Fusionne les m√©tadonn√©es DVD/Group scrap√©es avec la DB.
"""

GROUP_FIELDS = {
    "Title": "name",
    "Aliases": "aliases",
    "Date": "date",
    "Studio": "studio",
    "Director": "director",
    "Duration": "duration",
    "Description": "description",
    "Tags": "tags",
    "URLs": "urls",
}

class GroupPhase1Merger:
    def merge(self, db_data: dict, scraped_results: list[dict], checked_fields: list[str]) -> dict:
        result = {}
        for field in checked_fields:
db_key="***MASKED***"
            if not db_key: continue
            
            db_val = db_data.get(db_key)
            if field == "Studio" and db_data.get("studio_name"):
                db_val = db_data["studio_name"]

            scraped_vals = {}
            for r in scraped_results:
                val = r.get(db_key.replace("name", "title") if db_key == "name" else db_key)
                if val:
                    scraped_vals[r["_source"]] = val
            
            if not scraped_vals:
                status = "empty"
                suggestion = db_val
            elif not db_val:
                status = "new"
                suggestion = self._pick_best(scraped_vals)
            else:
                # Priorit√© Data18 : si data18 est pr√©sent et diff√®re de DB, c'est un conflit (m√™me si d'autres sources matchent)
                data18_val = scraped_vals.get("data18")
                if data18_val and str(data18_val).lower().strip() != str(db_val).lower().strip():
                    status = "conflict"
                    suggestion = data18_val
                elif self._matches(db_val, scraped_vals):
                    status = "confirmed"
                    suggestion = db_val
                else:
                    status = "conflict"
                    suggestion = self._pick_best(scraped_vals)

            result[field] = {
                "status": status,
                "db_value": db_val,
                "scraped_values": scraped_vals,
                "suggestion": suggestion
            }
        return result

    def _matches(self, db_val, scraped_vals):
        db_s = str(db_val).lower().strip()
        for v in scraped_vals.values():
            if str(v).lower().strip() == db_s:
                return True
        return False

    def _pick_best(self, scraped_vals):
        # Priorit√© simple : data18 > iafd_dvd > adultdvdempire
        for src in ["data18", "iafd_dvd", "adultdvdempire"]:
            if src in scraped_vals:
                return scraped_vals[src]
        return list(scraped_vals.values())[0]


============================================================
[58/83] services\group_phase1_scraper.py
------------------------------------------------------------
"""
GroupPhase1ScraperService ‚Äî Orchestre le scraping des m√©tadonn√©es DVD/Group.
"""
from services.extractors.dvd.data18_dvd import Data18DVDExtractor
from services.extractors.dvd.adultempire_dvd import AdultEmpireDVDExtractor
from services.extractors.dvd.iafd_dvd import IafdDVDExtractor
from services.extractors.dvd.jeedoo_dvd import JedeeDVDExtractor

class GroupPhase1ScraperService:
    def __init__(self):
        self.extractors = {
            "data18": Data18DVDExtractor(),
            "adultdvdempire": AdultEmpireDVDExtractor(),
            "iafd_dvd": IafdDVDExtractor(),
            "jeedoo": JedeeDVDExtractor(),
        }

    def scrape(self, title: str, year: str = None, known_urls: list = None, progress_callback=None) -> list[dict]:
        results = []
        known_urls = known_urls or []

        # 1. Utiliser les URLs connues
        for url in known_urls:
            extractor = self._find_extractor_for_url(url)
            if extractor:
                if progress_callback:
                    progress_callback(extractor.SOURCE_NAME, "Fetching from known URL...")
                res = extractor.extract_from_url(url)
                if res and res.get("title"):
                    results.append(res)

        # 2. Si Data18 n'est pas trouv√© dans les URLs connues, essayer de deviner l'URL
        has_data18 = any(r.get("_source") == "data18" for r in results)
        if not has_data18:
            data18 = self.extractors["data18"]
            candidates = data18.search_urls(title)
            # Pas de boucle ici si search_urls ne fait que des guesses simples qui ont peu de chance de marcher
            # Mais on essaie quand m√™me les guesses simples
            for url in candidates:
                if progress_callback:
                    progress_callback("data18", f"Trying candidate: {url}...")
                res = data18.extract_from_url(url)
                if res and res.get("title"):
                    results.append(res)
                    break # Found one valid Data18 page

        # 3. Si pas de r√©sultat ou pour compl√©ter, essayer IAFD par titre
        has_iafd = any(r.get("_source") == "iafd_dvd" for r in results)
        if not has_iafd:
            # Essayer IAFD par titre
            iafd = self.extractors["iafd_dvd"]
            url = iafd.build_url_from_title(title, year)
            if progress_callback:
                progress_callback("iafd_dvd", f"Searching for {title}...")
            res = iafd.extract_from_url(url)
            if res and res.get("title"):
                results.append(res)

        return results

    def _find_extractor_for_url(self, url: str):
        if "data18.com" in url: return self.extractors["data18"]
        if "adultdvdempire.com" in url: return self.extractors["adultdvdempire"]
        if "iafd.com" in url: return self.extractors["iafd_dvd"]
        if "jeedoo.com" in url: return self.extractors["jeedoo"]
        return None


============================================================
[59/83] services\group_phase2_merger.py
------------------------------------------------------------
"""
GroupPhase2Merger ‚Äî Associe les URLs scrap√©es aux sc√®nes Stash par index.

Strat√©gie de matching :
  1. Exact sur scene_index (num√©ro de sc√®ne dans le DVD)
  2. Fallback : comparaison de titre normalis√©
"""


class GroupPhase2Merger:

    def merge(
        self,
        stash_scenes: list[dict],
        scraped_scene_urls: dict[int, dict[str, str]]
    ) -> list[dict]:
        """
        Associe les URLs scrap√©es √† chaque sc√®ne Stash.

        Args:
            stash_scenes:       liste de sc√®nes DB (scene_id, scene_index, scene_title, existing_urls)
            scraped_scene_urls: {scene_index: {"data18": url, "adultdvdempire": url}}

        Returns:
            Liste de dicts enrichis avec "new_urls" et "status"
        """
        result = []

        for scene in stash_scenes:
            scene_id    = scene["scene_id"]
            scene_index = scene.get("scene_index")
            scene_title = scene.get("scene_title", f"Sc√®ne {scene_id}")
            existing    = scene.get("existing_urls", [])

            # Chercher par index exact
            new_urls = scraped_scene_urls.get(scene_index, {})

            # Filtrer les URLs d√©j√† pr√©sentes
            truly_new = {
                src: url for src, url in new_urls.items()
                if url and url not in existing
            }

            # D√©terminer le statut
            if not new_urls:
                status = "no_match"
            elif not truly_new:
                status = "already_present"
            elif existing:
                status = "partial"
            else:
                status = "new"

            result.append({
                "scene_id":      scene_id,
                "scene_index":   scene_index,
                "scene_title":   scene_title,
                "existing_urls": existing,
                "new_urls":      truly_new,
                "status":        status,
            })

        return result


============================================================
[60/83] services\group_phase2_scraper.py
------------------------------------------------------------
"""
GroupPhase2ScraperService ‚Äî Scraping des URLs de sc√®nes individuelles pour un Group.
"""
from services.extractors.dvd.data18_dvd import Data18DVDExtractor
from services.extractors.dvd.adultempire_dvd import AdultEmpireDVDExtractor

class GroupPhase2ScraperService:
    def __init__(self):
        self.extractors = {
            "data18": Data18DVDExtractor(),
            "adultdvdempire": AdultEmpireDVDExtractor(),
        }

    def scrape(self, group_data: dict, progress_callback=None) -> dict[int, dict[str, str]]:
        """
        Retourne : { scene_index: { "source": "url" } }
        """
        all_scene_urls = {} # {index: {source: url}}

        urls = group_data.get("urls", [])
        for url in urls:
            extractor = None
            if "data18.com" in url: extractor = self.extractors["data18"]
            elif "adultdvdempire.com" in url: extractor = self.extractors["adultdvdempire"]
            
            if extractor:
                if progress_callback:
                    progress_callback(extractor.SOURCE_NAME, f"Scraping scene URLs...")
                
                res = extractor.extract_from_url(url)
                scenes = res.get("scenes", [])
                for s in scenes:
                    idx = s.get("index")
                    s_url = s.get("url")
                    if idx and s_url:
                        if idx not in all_scene_urls:
                            all_scene_urls[idx] = {}
                        all_scene_urls[idx][extractor.SOURCE_NAME] = s_url

        return all_scene_urls


============================================================
[61/83] services\phase1_merger.py
------------------------------------------------------------
"""
Phase1Merger ‚Äî Compare les donn√©es scrap√©es avec la DB pour les champs Phase 1.
Identifie les confirmations et les conflits.
G√®re la fusion et la normalisation des ALIAS.
"""

# Ordre de priorit√© des sources pour les champs simples
SOURCE_PRIORITY = ["iafd", "freeones", "babepedia", "thenude"]

# Champs Phase 1 avec leur cl√© DB
PHASE1_FIELDS = {
    "Name": "name",
    "Aliases": "aliases",
    "Birthdate": "birthdate",
    "Deathdate": "death_date",
    "Country": "country",
    "Ethnicity": "ethnicity",
    "Hair Color": "hair_color",
    "Eye Color": "eye_color",
    "Height": "height",
    "Weight": "weight",
    "Measurements": "measurements",
    "Fake Tits": "fake_tits",
    "Career Length": "career_length",
}


class Phase1Merger:
    def merge(self, db_data: dict, scraped_results: list[dict], 
              checked_fields: list[str]) -> dict:
        """
        Fusionner les donn√©es pour les champs coch√©s.
        Logique sp√©ciale pour les ALIASES.
        """
        result = {}
        
        for field_name in checked_fields:
db_key="***MASKED***"
            if not db_key:
                continue

            # --- Logique sp√©ciale pour les ALIASES ---
            if db_key == 'aliases':
                result[field_name] = self._merge_aliases(db_data, scraped_results)
                continue

            # --- Logique g√©n√©rale pour les autres champs ---
            db_value = self._normalize_value(db_data.get(db_key))
            
            scraped_values = {}
            for r in scraped_results:
                source = r.get("_source", "?")
                val = self._normalize_value(r.get(db_key))
                if val:
                    scraped_values[source] = val
            
            if not scraped_values:
                result[field_name] = self._build_result("empty", db_value, scraped_values, db_value)
            elif not db_value:
                suggestion = self._pick_best(scraped_values)
                result[field_name] = self._build_result("new", None, scraped_values, suggestion)
            elif self._matches_any(db_value, scraped_values):
                result[field_name] = self._build_result("confirmed", db_value, scraped_values, db_value)
            else:
                suggestion = self._pick_best(scraped_values)
                result[field_name] = self._build_result("conflict", db_value, scraped_values, suggestion)
        
        return result

    def _normalize_alias(self, alias: str) -> str | None:
        """Normalise une cha√Æne d'alias (lowercase, strip)."""
        if not isinstance(alias, str):
            return None
        normalized = alias.lower().strip()
        return normalized if normalized else None

    def _merge_aliases(self, db_data: dict, scraped_results: list[dict]) -> dict:
        """Fusionne, normalise et d√©doublonne les alias de toutes les sources."""
db_key="***MASKED***"
        db_aliases = db_data.get(db_key) or []
        
        final_aliases = set()
        
        # 1. Ajouter les alias de la DB
        for alias in db_aliases:
            norm_alias = self._normalize_alias(alias)
            if norm_alias:
                final_aliases.add(norm_alias)
        
        # 2. Ajouter les alias des sources scrap√©es
        scraped_values_dict = {}
        for r in scraped_results:
            source_name = r.get('_source', '?')
            source_aliases = r.get(db_key)
            if source_aliases:
                scraped_values_dict[source_name] = source_aliases
                for alias in source_aliases:
                    norm_alias = self._normalize_alias(alias)
                    if norm_alias:
                        final_aliases.add(norm_alias)
        
        # 3. D√©terminer le statut de la fusion
        db_set = {self._normalize_alias(a) for a in db_aliases if self._normalize_alias(a)}
        status = "empty"
        if final_aliases:
            if not db_set:
                status = "new"
            elif db_set == final_aliases:
                status = "confirmed"
            else:
                status = "conflict"  # 'conflict' indique qu'il y a des changements/ajouts

        # 4. Construire le r√©sultat final pour l'UI
        return self._build_result(
            status,
            db_value=", ".join(sorted(list(db_set))),
            scraped_values={k: ", ".join(v) for k, v in scraped_values_dict.items()},
            suggestion=", ".join(sorted(list(final_aliases)))
        )

    def _build_result(self, status, db_value, scraped_values, suggestion):
        """Helper pour construire le dictionnaire de r√©sultat."""
        return {
            "status": status,
            "db_value": db_value,
            "scraped_values": scraped_values,
            "suggestion": suggestion,
        }

    def _normalize_value(self, val) -> str | None:
        """Normaliser une valeur simple pour comparaison."""
        if val is None:
            return None
        # Ne g√®re plus les listes ici, elles sont trait√©es par _merge_aliases
        val = str(val).strip()
        return val if val else None

    def _matches_any(self, db_value: str, scraped_values: dict) -> bool:
        """V√©rifier si la valeur DB correspond √† au moins une source."""
        db_lower = db_value.lower().strip()
        for val in scraped_values.values():
            if val and val.lower().strip() == db_lower:
                return True
        return False

    def _pick_best(self, scraped_values: dict) -> str:
        """Choisir la meilleure valeur selon la priorit√© des sources."""
        for source in SOURCE_PRIORITY:
            if source in scraped_values and scraped_values[source]:
                return scraped_values[source]
        # Fallback : premi√®re valeur non-None
        for val in scraped_values.values():
            if val:
                return val
        return ""


============================================================
[62/83] services\phase2_merger.py
------------------------------------------------------------
"""
Phase2Merger ‚Äî fusionne les r√©sultats de scraping Phase 2
en donn√©es pr√™tes pour l'interface de r√©solution.
"""


class Phase2Merger:
    """
    Fusionne les r√©sultats de scraping Phase 2 en donn√©es pr√™tes
    pour l'interface Phase2MergeDialog.
    
    Chaque champ a sa propre strat√©gie de fusion.
    """

    def merge(self, db_data: dict, scraped_results: list[dict]) -> dict:
        """
        Fusionner les donn√©es DB + scraping pour tous les champs Phase 2.
        
        Args:
            db_data: Donn√©es actuelles du performer depuis Stash DB
            scraped_results: Liste de dicts retourn√©s par les extracteurs
            
        Returns:
            Dict avec les r√©sultats fusionn√©s par champ
        """
        return {
            "awards":    self._merge_awards(db_data, scraped_results),
            "trivia":    self._merge_trivia(db_data, scraped_results),
            "details":   self._merge_details(db_data, scraped_results),
            "tattoos":   self._merge_body_art("tattoos", db_data, scraped_results),
            "piercings": self._merge_body_art("piercings", db_data, scraped_results),
            "tags":      self._merge_tags(db_data, scraped_results),
            "urls":      self._merge_urls(db_data, scraped_results),
        }

    # ‚îÄ‚îÄ AWARDS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _merge_awards(self, db, scraped):
        """
        IAFD = source primaire (liste structur√©e).
        Autres sources = regex fallback depuis bio/trivia.
        R√©sultat : union d√©dupliqu√©e, IAFD en premier.
        """
        seen = set()
        result = []

        # 1. IAFD en premier (plus fiable)
        for r in scraped:
            if r.get("_source") == "iafd":
                for award in (r.get("awards") or []):
key="***MASKED***"
                    if key not in seen:
                        seen.add(key)
                        result.append(award)

        # 2. Autres sources en d√©duplication
        for r in scraped:
            if r.get("_source") != "iafd":
                for award in (r.get("awards") or []):
key="***MASKED***"
                    if key not in seen:
                        seen.add(key)
                        result.append(award)

        return {
            "db_value":  db.get("awards", []),
            "merged":    result,
            "sources":   {r["_source"]: r.get("awards", []) for r in scraped},
            "strategy":  "union_iafd_first"
        }

    # ‚îÄ‚îÄ TRIVIA ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _merge_trivia(self, db, scraped):
        """
        FreeOnes "Additional Information" est la meilleure source.
        TheNude bios studio = contexte suppl√©mentaire.
        Babepedia peut avoir quelques lignes.
        ‚Üí Proposer les 3 s√©par√©ment pour que l'utilisateur choisisse/combine.
        """
        by_source = {}
        for r in scraped:
            trivia = r.get("trivia")
            if trivia:
                by_source[r["_source"]] = trivia

        # S√©lection automatique par priorit√©
        best = None
        TRIVIA_PRIORITY = ["freeones", "thenude", "babepedia"]
        for src in TRIVIA_PRIORITY:
            if src in by_source:
                best = by_source[src]
                break

        return {
            "db_value":   db.get("trivia"),
            "by_source":  by_source,
            "suggestion": best,
            "strategy":   "best_source_priority"
        }

    # ‚îÄ‚îÄ BIO / DETAILS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _merge_details(self, db, scraped):
        """
        FreeOnes = bio narrative la plus compl√®te.
        TheNude = bios studio (contexte professionnel).
        Babepedia = court texte.
        ‚Üí Option 1 : FreeOnes seule
        ‚Üí Option 2 : Toutes sources concat√©n√©es (s√©parateurs clairs)
        ‚Üí Option 3 : DB actuelle
        """
        by_source = {}
        for r in scraped:
            detail = r.get("details")
            if detail and len(detail) > 50:
                by_source[r["_source"]] = detail

        # Construire option "fusion" ordonn√©e
        DETAIL_ORDER = ["freeones", "babepedia", "thenude", "boobpedia"]
        fused_parts = []
        for src in DETAIL_ORDER:
            if src in by_source:
                fused_parts.append(f"[Source: {src.upper()}]\n{by_source[src]}")

        fused = "\n\n---\n\n".join(fused_parts) if fused_parts else None

        return {
            "db_value":  db.get("details"),
            "by_source": by_source,
            "fused":     fused,
            "strategy":  "user_choice"
        }

    # ‚îÄ‚îÄ TATTOOS / PIERCINGS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _merge_body_art(self, field: str, db, scraped):
        """
        Hi√©rarchie : IAFD/FreeOnes (structur√©) > Babepedia/TheNude (flat).
        Union d√©dupliqu√©e par (position, description).
        Les entr√©es position="multiple" ne sont gard√©es que si aucune
        entr√©e structur√©e n'existe pour ce champ.
        """
        structured = []
        flat = []
        seen = set()

        QUALITY_ORDER = ["iafd", "freeones", "thenude", "babepedia"]
        for source in QUALITY_ORDER:
            for r in scraped:
                if r.get("_source") != source:
                    continue
                for item in (r.get(field) or []):
                    pos  = (item.get("position") or "").lower().strip()
                    desc = (item.get("description") or "").lower().strip()
key="***MASKED***"
                    if key in seen:
                        continue
                    seen.add(key)
                    if pos == "multiple":
                        flat.append(item)
                    else:
                        structured.append(item)

        # Utiliser flat uniquement si aucune entr√©e structur√©e
        merged = structured if structured else flat

        return {
            "db_value":  db.get(field, ""),
            "merged":    merged,
            "sources":   {r["_source"]: r.get(field, []) for r in scraped},
            "strategy":  "structured_priority"
        }

    # ‚îÄ‚îÄ TAGS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _merge_tags(self, db, scraped):
        """
        Union de toutes les sources, d√©duplification insensible √† la casse.
        """
        seen = set()
        merged = []
        for r in scraped:
            for tag in (r.get("tags") or []):
key="***MASKED***"
                if key and key not in seen:
                    seen.add(key)
                    merged.append(tag.strip().title())
        merged.sort()
        return {
            "db_value": db.get("tags", []),
            "merged":   merged,
            "sources":  {r["_source"]: r.get("tags", []) for r in scraped},
            "strategy": "union_all"
        }

    # ‚îÄ‚îÄ URLs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _merge_urls(self, db, scraped):
        """
        Agr√©ger databases + social_media de toutes les sources.
        Priorit√© : freeones > iafd > babepedia > thenude (pour ordre d'affichage).
        """
        URL_PRIORITY = ["freeones", "iafd", "babepedia", "thenude", "boobpedia"]
        merged = {}
        for source in reversed(URL_PRIORITY):
            for r in scraped:
                if r.get("_source") != source:
                    continue
                for key, url in (r.get("urls") or {}).items():
                    if url:
                        merged[key] = url
        return {
            "db_value": db.get("urls", []),
            "merged":   merged,
            "strategy": "priority_merge"
        }


============================================================
[63/83] services\phase2_scraper.py
------------------------------------------------------------
"""
Phase2ScraperService ‚Äî orchestre les 4 extracteurs et g√®re le cache.
"""
from services.scrape_cache import ScrapeCache
from services.extractors.iafd import IafdExtractor
from services.extractors.freeones import FreeonesExtractor
from services.extractors.thenude import ThenudeExtractor
from services.extractors.babepedia import BabepediaExtractor


class Phase2ScraperService:
    """
    Lance le scraping Phase 2 sur toutes les sources disponibles.
    
    Utilise le ScrapeCache pour √©viter le double scraping.
    Construit les URLs automatiquement si non disponibles.
    """

    def __init__(self):
        self.extractors = [
            IafdExtractor(),
            FreeonesExtractor(),
            ThenudeExtractor(),
            BabepediaExtractor(),
        ]

    def scrape(
        self,
        performer_name: str,
        known_urls: list[str] | None = None,
        progress_callback=None,
    ) -> list[dict]:
        """
        Scraper toutes les sources pour un performer.
        
        Args:
            performer_name: Nom du performer
            known_urls: URLs d√©j√† connues (depuis DB Stash)
            progress_callback: Callback(source_name, status) pour le progr√®s
            
        Returns:
            Liste de dicts Phase 2 (un par source r√©ussie)
        """
        results = []
        known_urls = known_urls or []

        # Mapper les URLs connues par source
        url_map = self._map_urls_to_sources(known_urls)

        for i, extractor in enumerate(self.extractors):
            source = extractor.SOURCE_NAME
            
            if progress_callback:
                progress_callback(source, f"Scraping {source}...")

            # D√©terminer l'URL √† utiliser
            url = url_map.get(source)
            if not url:
                url = extractor.build_url(performer_name)
            
            if not url:
                print(f"[Phase2Scraper] Pas d'URL pour {source}, skip")
                continue

            try:
                # V√©rifier le cache d'abord
                cached = ScrapeCache.get(url)
                if cached:
                    print(f"[Phase2Scraper] Cache hit pour {source}: {url}")
                    results.append(cached)
                    continue

                # Scraper
                print(f"[Phase2Scraper] Scraping {source}: {url}")
                data = extractor.extract_from_url(url)
                
                if data:
                    # Stocker en cache
                    ScrapeCache.set(url, data)
                    results.append(data)
                    print(f"[Phase2Scraper] {source} OK ‚Äî "
                          f"awards:{len(data.get('awards',[]))}, "
                          f"tags:{len(data.get('tags',[]))}, "
                          f"tattoos:{len(data.get('tattoos',[]))}")
                          
            except Exception as e:
                print(f"[Phase2Scraper] Erreur {source}: {e}")
                if progress_callback:
                    progress_callback(source, f"Erreur: {e}")

        if progress_callback:
            progress_callback("done", f"Scraping termin√© ‚Äî {len(results)} sources")

        return results

    def _map_urls_to_sources(self, urls: list[str]) -> dict[str, str]:
        """Mapper les URLs connues aux noms de source."""
        url_map = {}
        for url in urls:
            url_lower = url.lower()
            if "iafd.com" in url_lower:
                url_map["iafd"] = url
            elif "freeones.com" in url_lower:
                url_map["freeones"] = url
            elif "thenude.com" in url_lower or "thenude.eu" in url_lower:
                url_map["thenude"] = url
            elif "babepedia.com" in url_lower:
                url_map["babepedia"] = url
        return url_map


============================================================
[64/83] services\scrape_cache.py
------------------------------------------------------------
"""
Cache en m√©moire des r√©sultats de scraping pour √©viter le double
appel r√©seau entre Phase 1 et Phase 2.
"""


class ScrapeCache:
    """
    Cache class-level partag√© entre toutes les instances.
    Stocke les r√©sultats de scraping par URL.
    """
    _data: dict[str, dict] = {}

    @classmethod
    def set(cls, url: str, data: dict):
        """Stocker le r√©sultat de scraping pour une URL."""
        cls._data[url] = data

    @classmethod
    def get(cls, url: str) -> dict | None:
        """R√©cup√©rer le r√©sultat de scraping pour une URL, ou None."""
        return cls._data.get(url)

    @classmethod
    def has(cls, url: str) -> bool:
        """V√©rifier si une URL est en cache."""
        return url in cls._data

    @classmethod
    def clear(cls):
        """Vider tout le cache."""
        cls._data.clear()

    @classmethod
    def size(cls) -> int:
        """Nombre d'entr√©es en cache."""
        return len(cls._data)


============================================================
[65/83] start.bat
------------------------------------------------------------
@echo off
REM Optimized launcher: checks venv, dependencies, installs if missing, generates a report, and then runs main.py

REM Set venv directory name
set VENV_DIR=.venv

REM Check if venv exists and is valid
if not exist %VENV_DIR%\Scripts\activate.bat (
    echo Creating virtual environment...
    python -m venv %VENV_DIR%
    if %errorlevel% neq 0 (
        echo ERROR: Failed to create virtual environment. Please ensure Python is installed and in your PATH.
        pause
        exit /b 1
    )
)

REM Activate venv
call %VENV_DIR%\Scripts\activate.bat

REM Check and install dependencies if needed
echo Upgrading pip and installing requirements from requirements.txt...
python -m pip install --upgrade pip >nul
pip install -r requirements.txt

REM --- System Hardware and AI Report ---

REM Verify for NVIDIA GPU and Ollama, and generate a report
echo Generating system report...
for /f %%i in ('powershell -Command "Get-Date -format 'yyyyMMdd-HHmmss'"') do set TIMESTAMP=%%i
set REPORT_FILE=rapport_Ollama_%TIMESTAMP%.txt

(
    echo Report generated on %DATE% at %TIME%
    echo.
) > %REPORT_FILE%

REM Check for NVIDIA GPU
nvidia-smi >nul 2>&1
if %errorlevel% neq 0 (
    echo WARNING: NVIDIA GPU not found or nvidia-smi is not in your PATH.
    (
        echo === NVIDIA GPU Status ===
        echo NVIDIA GPU not found or nvidia-smi command failed.
    ) >> %REPORT_FILE%
) else (
    echo NVIDIA GPU detected.
    (
        echo === NVIDIA GPU Status ===
        nvidia-smi
    ) >> %REPORT_FILE%
)

REM Check for Ollama
where ollama >nul 2>&1
if %errorlevel% neq 0 (
    echo WARNING: Ollama not found. Please ensure it is installed and in your PATH.
    (
        echo.
        echo === Ollama Status ===
        echo Ollama not found.
    ) >> %REPORT_FILE%
) else (
    echo Ollama detected.
    (
        echo.
        echo === Ollama Models List ===
        ollama list
    ) >> %REPORT_FILE%
)

echo Report saved to %REPORT_FILE%
echo.

REM --- Launching Application ---
echo Starting the main application...
python main.py

pause


============================================================
[66/83] structure_bdd.md
------------------------------------------------------------
# Documentation de la base de donn√©es

Fichier : stash-go.sqlite

## Table `schema_migrations`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| version | uint64 | False | False | None |
| dirty | bool | False | False | None |


## Table `tags`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| name | varchar(255) | False | False | None |
| created_at | datetime | True | False | None |
| updated_at | datetime | True | False | None |
| ignore_auto_tag | boolean | True | False | '0' |
| description | TEXT | False | False | None |
| image_blob | varchar(255) | False | False | None |
| favorite | boolean | True | False | '0' |
| sort_name | varchar(255) | False | False | None |


## Table `sqlite_sequence`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| name |  | False | False | None |
| seq |  | False | False | None |


## Table `performer_stash_ids`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| performer_id | INTEGER | False | False | None |
| endpoint | varchar(255) | False | False | None |
| stash_id | varchar(36) | False | False | None |
| updated_at | datetime | True | False | '1970-01-01T00:00:00Z' |


## Table `studio_stash_ids`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| studio_id | INTEGER | False | False | None |
| endpoint | varchar(255) | False | False | None |
| stash_id | varchar(36) | False | False | None |
| updated_at | datetime | True | False | '1970-01-01T00:00:00Z' |


## Table `tags_relations`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| parent_id | INTEGER | False | True | None |
| child_id | INTEGER | False | True | None |


## Table `folders`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| path | varchar(255) | True | False | None |
| parent_folder_id | INTEGER | False | False | None |
| mod_time | datetime | True | False | None |
| created_at | datetime | True | False | None |
| updated_at | datetime | True | False | None |
| zip_file_id | INTEGER | False | False | None |


## Table `files`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| basename | varchar(255) | True | False | None |
| zip_file_id | INTEGER | False | False | None |
| parent_folder_id | INTEGER | True | False | None |
| size | INTEGER | True | False | None |
| mod_time | datetime | True | False | None |
| created_at | datetime | True | False | None |
| updated_at | datetime | True | False | None |


## Table `files_fingerprints`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| file_id | INTEGER | True | True | None |
| type | varchar(255) | True | True | None |
| fingerprint | BLOB | True | True | None |


## Table `video_files`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| file_id | INTEGER | True | True | None |
| duration | float | True | False | None |
| video_codec | varchar(255) | True | False | None |
| format | varchar(255) | True | False | None |
| audio_codec | varchar(255) | True | False | None |
| width | tinyint | True | False | None |
| height | tinyint | True | False | None |
| frame_rate | float | True | False | None |
| bit_rate | INTEGER | True | False | None |
| interactive | boolean | True | False | '0' |
| interactive_speed | INT | False | False | None |


## Table `video_captions`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| file_id | INTEGER | True | True | None |
| language_code | varchar(255) | True | True | None |
| filename | varchar(255) | True | False | None |
| caption_type | varchar(255) | True | True | None |


## Table `image_files`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| file_id | INTEGER | True | True | None |
| format | varchar(255) | True | False | None |
| width | tinyint | True | False | None |
| height | tinyint | True | False | None |


## Table `images_files`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| image_id | INTEGER | True | True | None |
| file_id | INTEGER | True | True | None |
| primary | boolean | True | False | None |


## Table `galleries_files`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| gallery_id | INTEGER | True | True | None |
| file_id | INTEGER | True | True | None |
| primary | boolean | True | False | None |


## Table `scenes_files`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| scene_id | INTEGER | True | True | None |
| file_id | INTEGER | True | True | None |
| primary | boolean | True | False | None |


## Table `performers_scenes`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| performer_id | INTEGER | False | True | None |
| scene_id | INTEGER | False | True | None |


## Table `scene_markers_tags`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| scene_marker_id | INTEGER | False | True | None |
| tag_id | INTEGER | False | True | None |


## Table `scenes_tags`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| scene_id | INTEGER | False | True | None |
| tag_id | INTEGER | False | True | None |


## Table `groups_scenes`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| group_id | INTEGER | False | True | None |
| scene_id | INTEGER | False | True | None |
| scene_index | tinyint | False | False | None |


## Table `performers_images`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| performer_id | INTEGER | False | True | None |
| image_id | INTEGER | False | True | None |


## Table `images_tags`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| image_id | INTEGER | False | True | None |
| tag_id | INTEGER | False | True | None |


## Table `scene_stash_ids`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| scene_id | INTEGER | True | True | None |
| endpoint | varchar(255) | True | True | None |
| stash_id | varchar(36) | True | False | None |
| updated_at | datetime | True | False | '1970-01-01T00:00:00Z' |


## Table `scenes_galleries`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| scene_id | INTEGER | True | True | None |
| gallery_id | INTEGER | True | True | None |


## Table `galleries_images`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| gallery_id | INTEGER | True | True | None |
| image_id | INTEGER | True | True | None |
| cover | BOOLEAN | True | False | 0 |


## Table `performers_galleries`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| performer_id | INTEGER | True | True | None |
| gallery_id | INTEGER | True | True | None |


## Table `galleries_tags`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| gallery_id | INTEGER | True | True | None |
| tag_id | INTEGER | True | True | None |


## Table `performers_tags`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| performer_id | INTEGER | True | True | None |
| tag_id | INTEGER | True | True | None |


## Table `tag_aliases`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| tag_id | INTEGER | True | True | None |
| alias | varchar(255) | True | True | None |


## Table `studio_aliases`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| studio_id | INTEGER | True | True | None |
| alias | varchar(255) | True | True | None |


## Table `performer_aliases`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| performer_id | INTEGER | True | True | None |
| alias | varchar(255) | True | True | None |


## Table `galleries_chapters`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| title | varchar(255) | True | False | None |
| image_index | INTEGER | True | False | None |
| gallery_id | INTEGER | True | False | None |
| created_at | datetime | True | False | None |
| updated_at | datetime | True | False | None |


## Table `blobs`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| checksum | varchar(255) | True | True | None |
| blob | BLOB | False | False | None |


## Table `scene_urls`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| scene_id | INTEGER | True | True | None |
| position | INTEGER | True | True | None |
| url | varchar(255) | True | True | None |


## Table `scene_markers`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| title | VARCHAR(255) | True | False | None |
| seconds | FLOAT | True | False | None |
| primary_tag_id | INTEGER | True | False | None |
| scene_id | INTEGER | True | False | None |
| created_at | DATETIME | True | False | None |
| updated_at | DATETIME | True | False | None |
| end_seconds | FLOAT | False | False | None |


## Table `studios`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| name | VARCHAR(255) | True | False | None |
| parent_id | INTEGER | False | False | NULL |
| created_at | DATETIME | True | False | None |
| updated_at | DATETIME | True | False | None |
| details | TEXT | False | False | None |
| rating | TINYINT | False | False | None |
| ignore_auto_tag | BOOLEAN | True | False | FALSE |
| image_blob | VARCHAR(255) | False | False | None |
| favorite | boolean | True | False | '0' |


## Table `saved_filters`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| name | varchar(510) | True | False | None |
| mode | varchar(255) | True | False | None |
| find_filter | BLOB | False | False | None |
| object_filter | BLOB | False | False | None |
| ui_options | BLOB | False | False | None |


## Table `image_urls`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| image_id | INTEGER | True | True | None |
| position | INTEGER | True | True | None |
| url | varchar(255) | True | True | None |


## Table `images`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| title | varchar(255) | False | False | None |
| rating | tinyint | False | False | None |
| studio_id | INTEGER | False | False | None |
| o_counter | tinyint | True | False | 0 |
| organized | boolean | True | False | '0' |
| created_at | datetime | True | False | None |
| updated_at | datetime | True | False | None |
| date | date | False | False | None |
| code | TEXT | False | False | None |
| photographer | TEXT | False | False | None |
| details | TEXT | False | False | None |
| date_precision | TINYINT | False | False | None |


## Table `gallery_urls`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| gallery_id | INTEGER | True | True | None |
| position | INTEGER | True | True | None |
| url | varchar(255) | True | True | None |


## Table `galleries`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| folder_id | INTEGER | False | False | None |
| title | varchar(255) | False | False | None |
| date | date | False | False | None |
| details | TEXT | False | False | None |
| studio_id | INTEGER | False | False | None |
| rating | tinyint | False | False | None |
| organized | boolean | True | False | '0' |
| created_at | datetime | True | False | None |
| updated_at | datetime | True | False | None |
| code | TEXT | False | False | None |
| photographer | TEXT | False | False | None |
| date_precision | TINYINT | False | False | None |


## Table `scenes`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| title | varchar(255) | False | False | None |
| details | TEXT | False | False | None |
| date | date | False | False | None |
| rating | tinyint | False | False | None |
| studio_id | INTEGER | False | False | None |
| organized | boolean | True | False | '0' |
| created_at | datetime | True | False | None |
| updated_at | datetime | True | False | None |
| code | TEXT | False | False | None |
| director | TEXT | False | False | None |
| resume_time | float | True | False | 0 |
| play_duration | float | True | False | 0 |
| cover_blob | varchar(255) | False | False | None |
| date_precision | TINYINT | False | False | None |


## Table `group_urls`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| group_id | INTEGER | True | True | None |
| position | INTEGER | True | True | None |
| url | varchar(255) | True | True | None |


## Table `groups`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| name | varchar(255) | True | False | None |
| aliases | varchar(255) | False | False | None |
| duration | INTEGER | False | False | None |
| date | date | False | False | None |
| rating | tinyint | False | False | None |
| studio_id | INTEGER | False | False | None |
| director | varchar(255) | False | False | None |
| description | TEXT | False | False | None |
| created_at | datetime | True | False | None |
| updated_at | datetime | True | False | None |
| front_image_blob | varchar(255) | False | False | None |
| back_image_blob | varchar(255) | False | False | None |
| date_precision | TINYINT | False | False | None |


## Table `groups_tags`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| group_id | INTEGER | True | True | None |
| tag_id | INTEGER | True | True | None |


## Table `performer_urls`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| performer_id | INTEGER | True | True | None |
| position | INTEGER | True | True | None |
| url | varchar(255) | True | True | None |


## Table `performers`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| name | varchar(255) | True | False | None |
| disambiguation | varchar(255) | False | False | None |
| gender | varchar(20) | False | False | None |
| birthdate | date | False | False | None |
| ethnicity | varchar(255) | False | False | None |
| country | varchar(255) | False | False | None |
| eye_color | varchar(255) | False | False | None |
| height | INT | False | False | None |
| measurements | varchar(255) | False | False | None |
| fake_tits | varchar(255) | False | False | None |
| career_length | varchar(255) | False | False | None |
| tattoos | varchar(255) | False | False | None |
| piercings | varchar(255) | False | False | None |
| favorite | boolean | True | False | '0' |
| created_at | datetime | True | False | None |
| updated_at | datetime | True | False | None |
| details | TEXT | False | False | None |
| death_date | date | False | False | None |
| hair_color | varchar(255) | False | False | None |
| weight | INTEGER | False | False | None |
| rating | tinyint | False | False | None |
| ignore_auto_tag | boolean | True | False | '0' |
| image_blob | varchar(255) | False | False | None |
| penis_length | float | False | False | None |
| circumcised | varchar[10] | False | False | None |
| birthdate_precision | TINYINT | False | False | None |
| death_date_precision | TINYINT | False | False | None |


## Table `studios_tags`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| studio_id | INTEGER | True | True | None |
| tag_id | INTEGER | True | True | None |


## Table `scenes_view_dates`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| scene_id | INTEGER | True | False | None |
| view_date | datetime | True | False | None |


## Table `scenes_o_dates`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| scene_id | INTEGER | True | False | None |
| o_date | datetime | True | False | None |


## Table `groups_relations`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| containing_id | INTEGER | True | True | None |
| sub_id | INTEGER | True | True | None |
| order_index | INTEGER | True | False | None |
| description | varchar(255) | False | False | None |


## Table `performer_custom_fields`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| performer_id | INTEGER | True | True | None |
| field | varchar(64) | True | True | None |
| value | BLOB | True | False | None |


## Table `studio_urls`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| studio_id | INTEGER | True | True | None |
| position | INTEGER | True | True | None |
| url | varchar(255) | True | True | None |


## Table `tag_stash_ids`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| tag_id | INTEGER | False | False | None |
| endpoint | varchar(255) | False | False | None |
| stash_id | varchar(36) | False | False | None |
| updated_at | datetime | True | False | '1970-01-01T00:00:00Z' |


## Table `sqlite_stat1`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| tbl |  | False | False | None |
| idx |  | False | False | None |
| stat |  | False | False | None |


## Table `sqlite_stat4`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| tbl |  | False | False | None |
| idx |  | False | False | None |
| neq |  | False | False | None |
| nlt |  | False | False | None |
| ndlt |  | False | False | None |
| sample |  | False | False | None |



============================================================
[67/83] test_data18_search.py
------------------------------------------------------------
from services.extractors.dvd.data18_dvd import Data18DVDExtractor

def test_search():
    extractor = Data18DVDExtractor()
    title = "Gangbanged 7"
    print(f"Searching for: {title}")
    
    # Try DDG
    search_query = f"site:data18.com {title}".replace(' ', '+')
    search_url = f"https://html.duckduckgo.com/html/?q={search_query}"
    print(f"Fetching DDG: {search_url}")
    
    tree = extractor._fetch_tree(search_url)
    if tree is None:
        print("Fetch returned None")
    else:
        print("Fetch successful")
        # DDG result links are in a.result__url or a.result__a
        links = tree.xpath('//a[contains(@class, "result__a")]/@href')
        print(f"Found {len(links)} links")
        for link in links:
            if "data18.com/movies/" in link or "data18.com/content/" in link:
                print(f"Match: {link}")

if __name__ == "__main__":
    test_search()


============================================================
[68/83] test_integration.py
------------------------------------------------------------
"""Tests d'int√©gration rapides ‚Äî √† lancer avant livraison."""
import sys
import os

# Add project root to path to allow imports
sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

def test_imports():
    """V√©rifier que tous les modules importent sans erreur."""
    modules = [
        "services.bio_generator",
        "services.phase2_scraper",
        "services.phase2_merger",
        "services.scrape_cache",
        # "services.group_phase1_scraper", # These don't exist
        # "services.group_phase1_merger",
        # "services.group_phase2_scraper",
        "services.group_phase2_merger",
        "services.extractors.iafd",
        "services.extractors.freeones",
        "services.extractors.babepedia",
        "services.extractors.thenude",
        "services.extractors.dvd.data18_dvd",
        "services.extractors.dvd.adultempire_dvd",
        "services.extractors.dvd.iafd_dvd",
        "utils.body_art_parser",
        "utils.duration",
        "gui.group_frame",
        "gui.phase2_field_wizard",
        "gui.phase2_merge_dialog",
        "gui.group_phase1",
        "gui.group_phase2",
    ]
    errors = []
    for mod in modules:
        try:
            __import__(mod)
            print(f"  ‚úÖ {mod}")
        except Exception as e:
            print(f"  ‚ùå {mod} ‚Üí {e}")
            errors.append(mod)
    return errors

def test_body_art_parser():
    """V√©rifier le parseur body art."""
    from utils.body_art_parser import parse_body_art
    assert parse_body_art("left wrist (tribal); right arm") == [
        {"position": "left wrist", "description": "tribal"},
        {"position": "right arm", "description": None},
    ]
    assert parse_body_art("None") == []
    assert parse_body_art("Yes - multiple") == [{"position": "multiple", "description": None}]
    print("  ‚úÖ body_art_parser")

def test_duration():
    """V√©rifier le parseur de dur√©es."""
    from utils.duration import parse_duration_to_seconds, format_duration
    assert parse_duration_to_seconds("01:55:32") == 6932
    assert parse_duration_to_seconds("115") == 6900
    assert parse_duration_to_seconds("1 hrs. 55 mins.") == 6900
    assert format_duration(6932) == "01:55:32"
    print("  ‚úÖ duration")

if __name__ == "__main__":
    print("=== Tests d'integration V2 ===\\n")
    print("Imports :")
    errs = test_imports()
    print("\\nUtilitaires :")
    test_body_art_parser()
    test_duration()
    if not errs:
        print("\\n‚úÖ Tous les tests passent")
    else:
        print(f"\\n‚ùå {len(errs)} modules en erreur : {errs}")
    sys.exit(0 if not errs else 1)


============================================================
[69/83] tests\__init__.py
------------------------------------------------------------


============================================================
[70/83] tests\test_db.py
------------------------------------------------------------
import unittest
from services.db import PerformerDB

class TestDB(unittest.TestCase):
    def setUp(self):
        self.db = PerformerDB()

    def tearDown(self):
        self.db.close()

    def test_get_known_performers(self):
        performers = self.db.get_known_performers()
        self.assertIsInstance(performers, list)
        if performers:
            self.assertIsInstance(performers[0], str)

if __name__ == '__main__':
    unittest.main()


============================================================
[71/83] tests\test_performer_fields.py
------------------------------------------------------------
import unittest
from gui.performer_frame import PerformerFrame
import tkinter as tk

class TestPerformerFields(unittest.TestCase):
    def setUp(self):
        self.root = tk.Tk()
        # On passe un stash_id de test
        self.frame = PerformerFrame(self.root, "1")

    def tearDown(self):
        self.root.destroy()

    def test_phase1_fields(self):
        # Le PerformerFrame d√©marre en Phase 1
        phase1_frame = self.frame.current_frame
        
        # V√©rifier que les champs de la Phase 1 sont bien cr√©√©s
        for field in phase1_frame.fields_list:
            self.assertIn(field, phase1_frame.fields)
            self.assertIsNotNone(phase1_frame.fields[field])

    def test_phase2_fields(self):
        # Naviguer vers la Phase 2
        self.frame.goto_phase2()
        phase2_frame = self.frame.current_frame

        # V√©rifier que les champs de la Phase 2 sont bien cr√©√©s
        for field in phase2_frame.fields_list:
            self.assertIn(field, phase2_frame.fields)
            self.assertIsNotNone(phase2_frame.fields[field])

if __name__ == "__main__":
    unittest.main()


============================================================
[72/83] utils\__init__.py
------------------------------------------------------------


============================================================
[73/83] utils\audit_markers.csv
------------------------------------------------------------
Ôªøid,title,duration,markers,points,tags,overlaps,short,long,exact_dupes,penalty
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",966.67,6,0,2,0,0,0,0,0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",800.35,6,0,2,0,0,1,0,1
179,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc5",915.47,5,0,2,0,0,0,0,0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",1138.83,8,0,2,0,0,0,0,0
181,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc1",955.18,2,0,1,0,0,0,0,0
182,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc5",336.28,1,0,1,0,0,1,0,1
183,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc3,986.29,3,0,1,0,0,0,0,0
186,"Sai Tai Tiger in Frauen Knast, Teufelsbrut Hinter Gittern! Sc2",1004.31,1,0,1,0,0,0,0,0
187,Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc1,669.9,1,0,1,0,0,1,0,1
188,Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc4,523.91,2,0,1,0,0,0,0,0
190,"Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc7",444.5,3,0,2,0,0,1,0,1
191,Aderes Quin in StepMom Gets Double Dick,2797.73,11,0,2,0,0,0,0,0
192,Alejandra Rico in Que Rico!,1346.01,4,0,2,0,0,0,0,0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",1367.91,5,0,2,0,0,0,0,0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",2422.35,7,0,2,0,0,0,0,0
195,"Athenea Rose in Safe Cracked, Holes Filled",2387.01,6,0,4,0,0,1,0,1
196,Ava Devine in Milf Asian Cummouth Facial,1971.46,8,0,2,0,0,0,0,0
197,"Barbie Sins in Anal Domination, 6on1 DAP",2676.75,11,0,4,0,0,0,0,0
198,Barbie Sins in DP Bandits! Sc4,2951.67,8,0,1,0,0,0,0,0
199,Blanche Bradburry in 10 Guy Anal Showdown,5188.42,16,0,3,0,0,0,0,0
200,Blanche Bradburry in Gangbang Anal Blitz,2098.1,6,0,1,0,0,0,0,0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,4003.86,10,0,4,0,0,0,0,0
202,Blanche Bradburry in Rough DAP Gangbang,3178.32,6,0,2,0,0,0,0,0
203,Blanche Bradburry in Triple Penetration Madness,3097.07,8,0,2,0,0,0,0,0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3392.67,5,0,1,0,0,0,0,0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3460.38,7,0,2,0,0,0,0,0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3478.7,7,0,2,0,0,0,0,0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",4584.67,6,0,3,0,0,0,0,0
208,Cherry Kiss in DP Bandits! Sc2,2380.63,5,0,1,0,0,0,0,0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3294.08,17,0,6,0,0,0,0,0
210,Destiny Mira in Put My Back Into It,1176.67,2,0,1,0,0,0,0,0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",6115.7,16,0,4,0,0,0,0,0
212,Jolee Love in Craving For Cocks,3458.3,7,0,3,0,0,0,0,0
213,Jolee Love in DP Bandits! 2 Sc3,3020.64,8,0,3,0,0,0,0,0
214,Jolee Love in Hardcore DAP Creampie,2874.07,5,0,2,0,0,0,0,0
215,,2020.04,3,0,1,0,0,0,0,0
216,,1843.83,2,0,1,0,0,0,0,0
217,,2426.39,4,0,2,0,0,0,0,0
218,,2378.98,2,0,1,0,0,0,0,0
219,,2209.99,12,0,3,0,0,0,0,0
220,,2484.12,3,0,1,0,0,0,0,0
221,,1027.97,6,0,3,0,0,0,0,0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",3097.41,18,0,3,0,0,0,0,0
223,,2142.37,14,0,4,0,0,0,0,0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2381.19,10,0,3,0,0,0,0,0
225,,2856.41,9,0,2,0,0,0,0,0
226,,2213.08,14,0,4,0,0,0,0,0
227,Adriana Chechik in Horny Housewives 6 Sc2,2073.71,8,0,2,0,0,0,0,0
228,Adira Allure in Airtight Diva Sc4,1848.07,4,0,2,0,0,0,0,0
229,Alexis Tae in Gangbang Sluts Sc1,2470.0,15,0,3,0,0,0,0,0
230,Amirah Adara in DP Bandits! Sc1,3162.37,8,0,1,0,0,0,0,0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,1838.44,11,0,4,0,0,0,0,0
232,Anai Loves in My stepmom,1847.97,4,0,2,0,0,0,0,0
233,Angela White in Angela's Airtight DP,2710.14,7,0,3,0,0,1,0,1
234,Anissa Kate in Love Everything About Her,2003.57,4,0,2,0,0,0,0,0
235,Anissa Kate in Sizziling Double Penetration Delight,1963.0,7,0,3,0,0,0,0,0
236,"Anissa Kate, Olivia Del Rio in Personal Guide Chapter 1",1572.27,1,0,1,0,0,0,0,0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",2276.54,6,0,2,0,0,0,0,0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,2911.74,11,0,3,0,0,0,0,0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",1808.53,10,0,2,0,0,0,0,0
240,Assh Lee in All Over That Cock,1501.5,2,0,1,0,0,0,0,0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",1811.95,5,0,1,0,0,0,0,0
242,Baby Gemini in Ricky's Room Blowbang,930.94,7,0,1,0,0,0,0,0
243,Barbie Sins in DAP with Creampie,2322.6,5,0,1,0,0,0,0,0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4231.12,21,0,2,0,0,0,0,0
245,Belinha Baracho in Intense 5on1 Gangbang,3285.92,14,0,3,0,0,0,0,0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3969.33,8,0,1,0,0,0,0,0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",4620.67,6,0,3,0,0,0,0,0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",3686.05,6,0,3,0,0,0,0,0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",2988.67,8,0,3,0,0,0,0,0
250,,7096.8,15,0,3,0,0,0,0,0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",3115.78,7,0,1,0,0,0,0,0
252,Bonny Bon in Sexual Rage 2 Sc3,2024.6,4,0,1,0,0,1,0,1
253,Cali Caliente in The Gangbang Part IV,2273.1,12,0,3,0,0,0,0,0
254,Carla Morelli in Gangbang with 4 Cocks,2008.38,6,0,2,0,0,1,0,1
255,Carla Morelli in Hot for Teacher,1324.61,3,0,2,0,0,0,0,0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4683.35,16,0,4,0,0,0,0,0
257,Chloe Amour in Mon Amour Sc1,2140.95,14,0,3,0,0,0,0,0
258,Chloe Amour in Mon Amour Sc2,1705.6,4,0,2,0,0,0,0,0
259,Chloe Amour in Mon Amour Sc3,2999.48,12,0,4,0,0,0,0,0
260,"Chloe Amour, Jennifer White in Mon Amour Sc4",2322.44,4,0,2,0,0,0,0,0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,4135.34,17,0,4,0,0,0,0,0
262,"Cindy Starfall, Gaia in Swappers Sc1",1883.39,5,0,3,0,0,0,0,0
263,Cookie Cream in Asian 1st BBG Threesome & DP,1959.48,6,0,2,0,0,0,0,0
265,"Danielle Renee, MarsFoxxx in Group Bang",2928.73,16,0,4,0,0,0,0,0
266,Daria Glower in Die Haremsw√§chterin des √ñl Scheichs Sc6,872.11,2,0,1,0,0,1,0,1
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,3879.57,12,0,3,0,0,0,0,0
268,Anissa Kate in Les Retrouvailles Sc5,1655.17,10,0,2,0,0,0,0,0
269,Emmanuelle Noire in Busty Ebony Beauty,2046.87,10,0,3,0,0,0,0,0
270,Francesca Le in Lewood Gangbang Battle of the MILFs Sc1,313.95,1,0,1,0,0,0,0,0
271,Francesca Le in Lewood Gangbang Battle of the MILFs Sc2,322.69,2,0,1,0,0,0,0,0
272,Francesca Le in Lewood Gangbang Battle of the MILFs Sc3,351.55,1,0,1,0,0,1,0,1
273,Francesca Le in Lewood Gangbang Battle of the MILFs Sc4,353.15,2,0,1,0,0,0,0,0
274,Francesca Le in Lewood Gangbang Battle of the MILFs Sc5,372.77,2,0,2,0,0,0,0,0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,2935.1,18,0,3,0,0,0,0,0
276,"Gaia in Screw My Wife, Please 76 Sc2",970.13,1,0,1,0,0,0,0,0
277,Gaia in Throated 39 Sc5,1687.53,9,0,3,0,0,0,0,0
278,Hannah Jo in Thick Dick Threesome,2248.52,10,0,1,0,0,0,0,0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,4474.28,23,0,3,0,0,0,0,0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",1356.44,8,0,3,0,0,0,0,0
281,Jada Fire in Assault That Ass 8 Sc3,1772.08,6,0,3,0,0,0,0,0
282,Jada Fire in Throat Yogurt 2 Sc1,825.17,2,0,2,0,0,1,0,1
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,1741.89,5,0,1,0,0,0,0,0
284,Jasminy Villar in The Stepfather And His Four Friends,3601.9,16,0,4,0,0,0,0,0
285,Jena LaRose in Blacks On Blondes,2051.49,6,0,1,0,0,0,0,0
286,Jennifer White in Jennifer White Overload Sc1,3271.9,19,0,3,0,0,0,0,0
287,Jennifer White in Jennifer White Overload Sc2,3398.01,14,0,4,0,0,1,0,1
288,Jennifer White in Jennifer White Overload Sc3,2525.56,17,0,4,0,0,0,0,0
289,Jenny Pretinha in Fucked by 3 Huge Cocks,1525.5,2,0,2,0,0,0,0,0
290,Juelz Ventura in POV BBC Airtight Gangbang,2889.31,15,0,2,0,0,0,0,0
291,"Jureka Del Mar, Maylee Fun in Asian Hotties Work to Cure",927.16,1,0,1,0,0,0,0,0
292,Katalina Kyle in Ass Worship 18 Sc3,2497.1,12,0,3,0,0,0,0,0
293,Katalina Kyle in Takes Every Inch Of Manuel,1619.48,7,0,2,0,0,0,0,0
294,Katia Belinii in Swallowing 5 Big Loads,1602.46,8,0,3,0,0,0,0,0
295,Kayla Carera in Bride Bangers Sc1,1466.64,1,0,1,0,0,0,0,0
296,Kayla Carrera in Anal Integrity Sc1,2701.5,3,0,3,0,0,0,0,0
297,Kaylani Lei in Asian Fuck Machines Sc5,2666.77,5,0,3,0,0,0,0,0
298,Kazumi Squirts in BBC Orgy Room,4253.97,18,0,3,0,0,0,0,0
299,Kazumi Squirts in Gangbang With Piss and DP,3392.33,15,0,5,0,0,0,0,0
301,Kelly Oliveira in Assfucked 4on1 with DP,2982.93,6,0,2,0,0,0,0,0
302,Kelly Oliveira in First DP for Brazilian Teen,1941.68,4,0,2,0,0,0,0,0
303,Kelly Oliveira in Sexy Latina DAP 3on1,2731.42,5,0,3,0,0,0,0,0
304,"Kelly Oliveira, Veronica Leal in 5on2 orgy with DP",3354.17,5,0,3,0,0,0,0,0
305,Keri Sable in Cum Filled Asshole Overload 2 Sc1,2796.03,3,0,2,0,0,1,0,1
306,Kim XXX in Manga Total Vollgespritzt Sc1,1921.96,7,0,3,0,0,0,0,0
307,Kim XXX in Tiny Whore Kim XXX Stuffed,2071.48,5,0,2,0,0,0,0,0
308,Kira Thorn in Balls Deep 5on2,2984.69,7,0,2,0,0,0,0,0
309,Kitana Montana in Birthday Threeway,1767.27,6,0,2,0,0,0,0,0
310,Kitana Montana in Post,2356.53,10,0,3,0,0,0,0,0
311,Laura Fiorentino in 6on1 Swallow,3877.5,14,0,6,0,0,0,0,0
312,Lela Star in Assparade 54 Sc2,719.83,2,0,2,0,0,0,0,0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,2180.24,5,0,3,0,0,0,0,0
314,Lolly Ink in True Gonzo Sc5,1580.37,4,0,3,0,0,1,0,1
315,Luna Star in Double Stuffed,2003.93,8,0,2,0,0,0,0,0
316,Luna Star in Why She's A Pornstar,2068.02,7,0,3,0,0,0,0,0
317,Marilyn Johnson in Airtight Diva Sc1,1617.07,2,0,1,0,0,0,0,0
318,"Mariska, Tekohas in Cum Cum Gangbang Orgy",996.07,6,0,2,0,0,0,0,0
319,Maxine X in I'm Here for the Gang Bang! Sc1,2883.66,9,0,4,0,0,0,0,0
320,Maxine X in I'm Here for the Gang Bang! Sc2,3520.19,11,0,2,0,0,0,0,0
321,Megan Rain in 10 Cock Blowbang!,1505.33,2,0,2,0,0,1,0,1
322,Melissa Hot in Fucked by 4 Big Cocks,2871.21,8,0,3,0,0,0,0,0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",3110.92,7,0,1,0,0,0,0,0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1472.38,5,0,2,0,0,0,0,0
325,Mercedes Carrera in MILF Cumsluts Sc4,2974.8,10,0,3,0,0,0,0,0
326,Mia Trejsi in 100% Hell,3432.54,14,0,3,0,0,0,0,0
327,"Mia Trejsi, Ria Sunn in Angels Of Hardcore 3 Sc1",368.17,2,0,2,0,0,0,0,0
328,Mih Ninfetinha in 4on1 with DP,2697.69,12,0,3,0,0,0,0,0
329,Miss Teela in First Time 10 Gangbang,1702.4,9,0,2,0,0,0,0,0
330,Monika Fox in DP Fantasies 11 Sc3,2549.07,6,0,3,0,0,1,0,1
331,Natasha Teen in Pussy DAPTAP,2914.04,6,0,2,0,0,0,0,0
332,Nia Nacci in Cum Bang 15 Sc3,2274.57,10,0,3,0,0,1,0,1
333,Nia Nacci in We Fuck Black Girls 14 Sc5,1953.01,2,0,2,0,0,0,0,0
334,Nia Nacci in White Out 9 Sc2,3971.75,17,0,4,0,0,0,0,0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",2217.92,11,0,2,0,0,0,0,0
336,Nina Elle in Big Wet Milf Asses Sc2,1890.05,3,0,1,0,0,0,0,0
337,Nina Elle in Gang Bang Addiction Sc4,2890.12,7,0,2,0,0,0,0,0
338,Nina Elle in MILF Cumsluts Sc3,2316.06,2,0,2,0,0,1,0,1
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1654.47,5,0,4,0,0,0,0,0
340,Phoenix Marie in Ass Worship 13 Sc4,2550.74,8,0,1,0,0,0,0,0
341,Rachele Richey in Gangbang Audition,2679.13,8,0,2,0,0,0,0,0
342,Rose Lynn in Airtight Diva Sc3,1585.07,5,0,3,0,0,0,0,0
343,Sadie Summers in Gangbang Sluts Sc2,3098.0,19,0,4,0,0,0,0,0
344,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc2,692.67,4,0,1,0,0,0,0,0
345,"Sai Tai Tiger, Daria Glower in Die Haremsw√§chterin des √ñl Scheichs Sc5",912.0,1,0,1,0,0,0,0,0
346,"Sai Tai Tiger, Daria Glower, Valerie Hilton in Die Haremsw√§chterin des √ñl Scheichs Sc1",1574.35,1,0,1,0,0,1,0,1
347,Sandra Parker in 1st at GB Junkies,1725.71,8,0,2,0,0,0,0,0
348,Sandra Parker in Anal Driller 9 Sc3,1411.03,6,0,1,0,0,0,0,0
349,Sandra Parker in Analizator Sc4,1873.37,2,0,2,0,0,1,0,1
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1852.02,4,0,1,0,0,0,0,0
351,Sandra Parker in Double Stuffed 8 Sc1,1689.02,5,0,1,0,0,0,0,0
352,Sara Retali in BBC Piss Gangbang,2231.14,13,0,2,0,0,0,0,0
353,Sara Retali in Slut Cant Get Enough Gangbang,1841.15,9,0,4,0,0,0,0,0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1207.62,6,0,2,0,0,0,0,0
355,"Sara Retali, Sapphire Astrea in Spa Day Gone Wild",930.08,2,0,2,0,0,0,0,0
356,Sarai Minx in Big Tit Slut Milks Cock,1562.59,5,0,3,0,0,0,0,0
357,"Sheila Ortega, Sara Retali, Anny Star in 3 Latinas by the Pool",2443.6,5,0,3,0,0,0,0,0
358,Shyla Stylez in Anal Integrity Sc2,2097.2,4,0,3,0,0,0,0,0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,3149.11,14,0,2,0,0,0,0,0
360,Skin Diamond in Rump Raiders Sc3,1820.97,3,0,1,0,0,0,0,0
361,,18562.53,50,0,4,0,0,0,0,0
362,Summer Day in America Bukkake Live,1495.04,2,0,2,0,0,1,0,1
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1893.47,11,0,3,0,0,0,0,0
364,Summer Vixen in Gangbang Sluts Sc3,2318.0,14,0,3,0,0,0,0,0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,3808.68,15,0,4,0,0,0,0,0
366,Tekohas in Ass & BigTits,1987.33,6,0,3,0,0,0,0,0
367,Tekohas in Bareback Party in Stuttgart,3021.03,15,0,3,0,0,0,0,0
368,Thai Suzy in WeLoveBukkake 4,993.54,2,0,2,0,0,1,0,1
369,Tia Maria in Cum On Melon Tits,2922.37,8,0,2,0,0,0,0,0
370,Tia Maria in DPd By Two BWCs,1677.93,4,0,2,0,0,0,0,0
371,Tyra Ride in First BBC  DP Gangbang,2411.49,12,0,4,0,0,0,0,0
372,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc10,363.0,1,0,1,0,0,0,0,0
373,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc11,377.98,1,0,1,0,0,0,0,0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,3230.76,17,0,4,0,0,0,0,0
376,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc8,287.95,1,0,1,0,0,0,0,0
378,Veronica Leal in Domination Gangbang,3676.9,18,0,4,0,0,0,0,0
379,Vittoria Devine in DP Pee 5on1,4787.33,17,0,3,0,0,0,0,0
380,Vittoria Devine in Domination Gangbang,2075.61,8,0,4,0,0,0,0,0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,2631.01,4,0,1,0,0,0,0,0
382,Vit√≥ria Beatriz in Edjunior VideoGuru,892.0,2,0,1,0,0,0,0,0
383,Willow Ryder in I Love Anal 3 Sc3,2177.1,11,0,2,0,0,1,0,1
384,Yasmina Khan in Birthday Gangbang,1583.33,6,0,3,0,0,0,0,0
385,Yasmina Khan in Play with 4 Cocks at Once!,2459.47,11,0,1,0,0,0,0,0
386,AJ Applegate in Gangbang Me Sc1,3184.69,4,0,2,0,0,0,0,0
388,Adira Allure in Interracial Blowbang 24 Sc1,2018.72,8,0,4,0,0,1,0,1
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",1200.63,6,0,2,0,0,0,0,0
392,"Adriana Chechik, Gaia in Grease XXX A Parody Sc5",1245.2,2,0,1,0,0,1,0,1
394,Adrianna Luna in Praise The Load 7 Sc1,1590.98,6,0,4,0,0,1,0,1
395,Adrianna Luna in Slut Puppies 5 Sc5,1963.99,7,0,1,0,0,0,0,0
396,Aidra Fox in Gangbanged 7 Sc1,3703.02,7,0,3,0,0,0,0,0
397,Alena Croft in Blacks on Cougars 17 Sc1,1905.91,4,0,2,0,0,1,0,1
398,Alena Croft in Feeding Frenzy 12 Sc2,1668.84,4,0,3,0,0,1,0,1
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2074.42,8,0,3,0,0,0,0,0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",3823.94,9,0,3,0,0,0,0,0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2108.36,9,0,2,0,0,1,0,1
402,Alexis Ford in Gang Bang Addiction Sc3,3074.11,17,0,3,0,0,0,0,0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2406.19,8,0,3,0,0,2,0,2
404,Alexis Monroe in Gang Bang Addiction Sc5,3167.07,7,0,4,0,0,0,0,0
405,Alexis Texas in Gang Bang Addiction Sc1,1871.1,9,0,2,0,0,0,0,0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2099.5,12,0,2,0,0,0,0,0
407,Alina in Annegret Zugekleistert Sc4,1086.13,4,0,2,0,0,0,0,0
409,Alina Lopez in No Going Back Sc1,720.81,1,0,1,0,0,0,0,0
410,Alina Lopez in Perfectly Natural 19 Sc4,1335.45,2,0,1,0,0,0,0,0
411,Alina Lopez in Pussy is The Best Medicine 9 Sc5,643.23,1,0,1,0,0,0,0,0
413,Alina Lopez in Wet Food 9 Sc1,3173.84,17,0,2,0,0,1,0,1
415,"Alyssa Divine, Kiki Minaj, Victoria Pure, Victoria Summers in Backstage Bangers Sc1",1698.25,5,0,2,0,0,0,0,0
416,Amara Romani in Gangbang Auditions 31 Sc3,3535.03,12,0,4,0,0,0,0,0
418,Amber Deen in Dogfart Invades Europe! 4 Sc1,2030.69,4,0,2,0,0,1,0,1
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2825.64,11,0,3,0,0,0,0,0
420,Amelia Sadaat in White Dicks in Black Chics 3 Sc4,631.71,1,0,1,0,0,0,0,0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2221.59,9,0,2,0,0,1,0,1
422,Andi Anderson in Young Harlots Gang Bang Sc2,4059.46,23,0,4,0,0,0,0,0
423,"Angel Eyes, Jada Fire in Freak Nasty Sc1",681.83,3,0,2,0,0,0,0,0
424,Angela White in Going All Out with a Gangbang 2 Sc4,2827.82,8,0,4,0,0,0,0,0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3376.44,15,0,3,0,0,0,0,0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",1079.8,6,0,3,0,0,1,0,1
428,Alejandra Rico in Intense Latin Gangbang,1350.14,5,0,2,0,0,0,0,0
429,Alejandra Rico in Tons of Cum,1581.57,5,0,2,0,0,1,0,1
430,Alex Grey in A Dirty Submissive Slut For Cock,1393.96,3,0,1,0,0,1,0,1
431,Alexa Nova in GangBang Creampie 246,2005.99,7,0,1,0,0,0,0,0
432,Alexa Payne in Stepmom Seduction on the Kitchen Counter,2150.67,5,0,2,0,0,0,0,0
433,"Alexia Vendome, Sothy Hiko, Yasmine Lafitte in No Limits Anal and Oral Orgy",1317.99,4,0,1,0,0,0,0,0
434,Alexis Kay in GangBang Creampie 417,782.58,4,0,3,0,0,0,0,0
435,Alicia Ribeiro in Anal and Oral with 3 Big Black Cocks,2701.5,4,0,2,0,0,0,0,0
436,Alicia Trece in Rough Gangbang and Pee Play,2822.9,8,0,3,0,0,0,0,0
437,Aliyah Taylor in Gang Bang All Her Holes,1805.91,7,0,3,0,0,0,0,0
438,Allatra Hot in MILF Craving Hardcore Attention,1370.7,4,0,1,0,0,0,0,0
439,Alura Jenson in GangBang Creampie 240,2719.32,10,0,3,0,0,0,0,0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",3930.19,11,0,3,0,0,0,0,0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2893.35,5,0,3,0,0,0,0,0
442,Amari Anne in You Wanna Cheat Again,2416.03,6,0,2,0,0,0,0,0
443,Amirah Adara in Rough Gangbang Session,3034.87,2,0,1,0,0,0,0,0
444,Amy Reid in AllOut Blowbang Session,1026.51,2,0,2,0,0,1,0,1
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,3231.2,7,0,3,0,0,0,0,0
446,Ana J√∫lia in DP com a Mulata Cavala,2450.12,7,0,3,0,0,0,0,0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",1562.35,3,0,2,0,0,0,0,0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",5115.56,4,0,2,0,0,0,0,0
449,Angel Lima in Big Butt Airtight Show,2837.54,5,0,2,0,0,0,0,0
450,Angel Lima in Hardcore Brazilian Double Anal,3478.13,7,0,3,0,0,0,0,0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3114.73,8,0,2,0,0,0,0,0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",1476.81,6,0,2,0,0,0,0,0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3114.77,11,0,2,0,0,0,0,0
454,Angel Sins in Short Stuff Big Stuff,2863.71,2,0,1,0,0,0,0,0
455,"Angel Smalls, Anna De Ville, Barbie Sins, Jureka Del Mar, May Thai, Nathaly Cherie, Selvaggia in Messy Facial Compilation",479.91,1,0,1,0,0,1,0,1
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",4638.91,13,0,4,0,0,0,0,0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,2835.8,8,0,1,0,0,0,0,0
459,Ania Kinski in Kinky DP Session At The Clinic,2079.32,1,0,1,0,0,1,0,1
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,2198.79,8,0,2,0,0,0,0,0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",4163.26,11,0,3,0,0,0,0,0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",1977.47,3,0,2,0,0,0,0,0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",2405.01,4,0,1,0,0,0,0,0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3407.16,7,0,2,0,0,0,0,0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",1761.96,8,0,2,0,0,0,0,0
468,Anissa Kate in A Hot Surfer Threesome,1927.33,6,0,2,0,0,0,0,0
469,Anissa Kate in Hardcore Business Meeting,2364.29,6,0,3,0,0,0,0,0
470,Anissa Kate in Poolside DP with BBC and Cumshot Finish,3007.15,6,0,2,0,0,0,0,0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3045.74,8,0,3,0,0,0,0,0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3016.48,7,0,2,0,0,0,0,0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",2188.8,8,0,3,0,0,0,0,0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",2800.07,7,0,3,0,0,1,0,1
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",913.45,7,0,2,0,0,0,0,0
477,Anni Star in Lingerie Pleasure Premi√®re,841.89,2,0,2,0,0,0,0,0
479,April Snow in GangBang Creampie 232,2387.31,11,0,3,0,0,0,0,0
480,"Ariel Pure Magic, Zoey Reyes in Dominican Oil Twins",997.95,1,0,1,0,0,0,0,0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,2230.09,4,0,1,0,0,0,0,0
482,"Ashby Winter in Vogue 2, Part 5",2819.84,6,0,3,0,0,0,0,0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,945.82,5,0,2,0,0,0,0,0
484,Ashley Cumstar in Gangbang Party,685.52,4,0,2,0,0,0,0,0
485,Athenea Rose in 5on1 Hardcore Gangbang,3371.07,15,0,3,0,0,0,0,0
486,Athenea Rose in 7on1 DAP Gangbang,3957.23,12,0,5,0,0,0,0,0
487,Athenea Rose in Airtight 6on1 Destruction,3391.51,16,0,3,0,0,0,0,0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,3491.85,11,0,3,0,0,0,0,0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5211.6,22,0,5,0,0,0,0,0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,4281.95,14,0,5,0,0,0,0,0
491,Athenea Rose in Hardcore Interracial DAP,3057.12,2,0,1,0,0,0,0,0
492,Athenea Rose in Hecho en Medelln,3900.67,6,0,3,0,0,0,0,0
493,Athenea Rose in Intense Anal Destruction,1839.23,2,0,1,0,0,0,0,0
494,Athenea Rose in Loves Public Anal,2607.95,2,0,1,0,0,0,0,0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,1953.07,1,0,1,0,0,0,0,0
496,Athenea Rose in Playing with 3 BBC,2441.44,5,0,1,0,0,0,0,0
497,Athenea Rose in PremiumBukkake #1,1263.3,5,0,2,0,0,1,0,1
498,Athenea Rose in PremiumBukkake #2,1064.0,2,0,2,0,0,2,0,2
499,Athenea Rose in PremiumBukkake #3,2329.59,4,0,2,0,0,1,0,1
500,Athenea Rose in Sex Crazed Slut 4on1,2765.94,8,0,4,0,0,0,0,0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,3828.96,15,0,5,0,0,0,0,0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",3745.63,3,0,1,0,0,0,0,0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",4161.3,10,0,3,0,0,0,0,0
504,Aubrey Black in GangBang Creampie 225,1779.39,8,0,3,0,0,0,0,0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",4394.87,12,0,4,0,0,0,0,0
506,Avery Jane in Brutal 7on1 DAP Birthday,3513.58,10,0,5,0,0,0,0,0
507,Avery Jane in Milking Mike Adriano,2301.68,4,0,3,0,0,0,0,0
508,Avery Jane in Piss Soaked Backdoor Debut,4138.75,16,0,5,0,0,0,0,0
509,Avi Love in GangBang Creampie 216,2673.71,11,0,4,0,0,0,0,0
511,Baby Gemini in All About The Booty,1607.66,1,0,1,0,0,0,0,0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,2648.0,10,0,3,0,0,1,0,1
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",3220.68,6,0,2,0,0,0,0,0
514,Barbie Esm in Rough 4on1 DAP Fantasy,3077.02,12,0,3,0,0,0,0,0
515,Barbie Sins in Barbie Gets wet with 2 BBC,2173.76,6,0,2,0,0,0,0,0
517,"Barbie Sins in DAP, Piss and Power Play",3215.13,8,0,3,0,0,0,0,0
518,Barbie Sins in No Holes Barred Gonzo Assault,3180.83,14,0,4,0,0,0,0,0
519,Barbie Sins in Rough DAP & Swallow Madness,2896.44,12,0,4,0,0,0,0,0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",1603.11,4,0,2,0,0,0,0,0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",3022.73,9,0,1,0,0,0,0,0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",2763.36,12,0,2,0,0,0,0,0
931,"Sarai Minx, Savanah Storm in Messy Manicure Threesome",1798.08,6,0,2,0,0,0,0,0
1008,"Sai Tai Tiger, Salma De Nora in Die Haremsw√§chterin des √ñl Scheichs Sc4",1003.34,3,0,1,0,0,0,0,0
1020,,2733.12,8,0,1,0,0,0,0,0
1023,Sai Tai Tiger in Die Haremsw√§chterin die Verbannung Sc3,568.01,1,0,1,0,0,0,0,0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",1826.8,7,0,2,0,0,0,0,0
1030,Sai Tai Tiger in Die Haremsw√§chterin die Verbannung Sc2,973.26,4,0,1,0,0,0,0,0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",2183.45,6,0,1,0,0,0,0,0
1038,Alina Li in Asian Fuck Faces 3 Sc6,1208.25,2,0,2,0,0,2,0,2
1043,"Anni Star, CJ Miles in Glamorous Double Penetration",1859.5,4,0,2,0,0,0,0,0
1044,"Ania Kinski, Anissa Kate in Real Estate Gets Real Dirty",1707.04,4,0,2,0,0,1,0,1
1047,Ania Kinski in Home Alone Double Penetration,2581.06,10,0,3,0,0,0,0,0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",2046.91,7,0,3,0,0,0,0,0
1062,Six Bodies In Motion,4574.0,8,0,2,0,0,0,0,0
1063,,1618.58,5,0,2,0,0,0,0,0
1065,,2630.13,8,0,4,0,0,0,0,0
1068,,1027.87,5,0,3,0,0,0,0,0


============================================================
[74/83] utils\body_art_parser.py
------------------------------------------------------------
"""
Utilitaire partag√© pour parser les tattoos/piercings depuis n'importe quelle source.
G√®re les formats structur√©s ("wrist (tribal)") et flat ("multiple tattoos").
"""
import re


def parse_body_art(raw_text: str) -> list[dict]:
    """
    Parse un texte brut de tattoos/piercings en liste structur√©e.
    
    Retourne: [{"position": str, "description": str | None}]
    
    Exemples d'entr√©e:
        "left wrist (tribal); right arm (sleeve)" ‚Üí 2 entr√©es structur√©es
        "Yes - multiple tattoos" ‚Üí 1 entr√©e flat
        "None" ‚Üí []
    """
    items = []
    if not raw_text or raw_text.strip().lower() in ('unknown', 'no', 'n/a', 'none', ''):
        return items

    # Retirer le pr√©fixe "Yes - " ou "Yes," courant
    cleaned = re.sub(r'^Yes\s*[-,]?\s*', '', raw_text, flags=re.I).strip()
    if not cleaned:
        return items

    # S√©parer par point-virgule d'abord (plus fiable), sinon virgule
    if ';' in cleaned:
        parts = cleaned.split(';')
    else:
        parts = cleaned.split(',')

    for part in parts:
        part = part.strip()
        if not part or part.lower() in ('unknown', 'no', 'n/a', 'none'):
            continue

        # Tenter de parser "position (description)"
        m = re.match(r'(.+?)\s*\((.+?)\)\s*$', part)
        if m:
            items.append({
                "position": m.group(1).strip(),
                "description": m.group(2).strip()
            })
        else:
            # Tenter "position - description" ou "position : description"
            m2 = re.match(r'(.+?)\s*[-‚Äì:]\s+(.+)', part)
            if m2 and len(m2.group(1)) < 30:
                items.append({
                    "position": m2.group(1).strip(),
                    "description": m2.group(2).strip()
                })
            else:
                items.append({
                    "position": part,
                    "description": None
                })

    return items


============================================================
[75/83] utils\cleanup_all.py
------------------------------------------------------------
from __future__ import annotations
import sqlite3
from collections import defaultdict, OrderedDict
from typing import Dict, List, Set, Tuple, Optional, Any

DB_PATH = r"H:\Stash\stash-go.sqlite"

# ============================================================
# CONFIGURATION DU NETTOYAGE
# ============================================================

# Dur√©e minimale globale : tout marqueur <= cette valeur (s) sera supprim√©
# Mettre √† 0.0 pour d√©sactiver.
GLOBAL_MIN_DURATION: float = 60.0

# R√®gles cibl√©es par tag (ind√©pendantes de la r√®gle globale)
# Format : (tag, dur√©e_max_ou_None_pour_tous)
TARGETED_DELETE_RULES: List[Tuple[str, Optional[float]]] = [
    ("69", None),  # Supprimer TOUS les marqueurs "69" (quelle que soit la dur√©e)
]

# ============================================================
conn = sqlite3.connect(DB_PATH)
cur = conn.cursor()

print("=== M√âNAGE COMPLET DES MARQUEURS ===\n")

# ---- 1. Chargement de tous les marqueurs ----
cur.execute("""
    SELECT m.id, m.seconds, m.end_seconds, GROUP_CONCAT(t.name), m.title
    FROM scene_markers m
    LEFT JOIN scene_markers_tags mt ON m.id = mt.scene_marker_id
    LEFT JOIN tags t ON mt.tag_id = t.id
    GROUP BY m.id
    ORDER BY m.scene_id, m.seconds
""")
all_markers: List[Any] = cur.fetchall()

# Construire un dict id -> marker data
marker_data: Dict[int, Dict[str, Any]] = {}
for row in all_markers:
    mid: int = int(row[0])
    start: float = float(row[1])
    end: float = float(row[2]) if row[2] else 0.0
    tags_str: str = str(row[3]) if row[3] else ""
    title: str = str(row[4]) if row[4] else ""
    tags: Set[str] = set(tags_str.split(",")) if tags_str else set()
    if title:
        tags.add(title)
    marker_data[mid] = {"start": start, "end": end, "tags": tags}

# ---- 2. Grouper les IDs par sc√®ne (ordonn√©s par seconds) ----
cur.execute("SELECT scene_id, id FROM scene_markers ORDER BY scene_id, seconds")
scene_groups: Dict[int, List[int]] = OrderedDict()
for row in cur.fetchall():
    sid: int = int(row[0])
    mid: int = int(row[1])
    if sid not in scene_groups:
        scene_groups[sid] = []
    scene_groups[sid].append(mid)

# ---- 3. R√®gles FUSION + CONTENANCE ----
markers_to_delete: Set[int] = set()
markers_to_update: Dict[int, float] = {}
fusions_count: int = 0
contained_count: int = 0

for sid, mids in scene_groups.items():
    for i in range(len(mids)):
        id1: int = mids[i]
        if id1 not in marker_data:
            continue
        d1 = marker_data[id1]
        start1: float = d1["start"]
        end1: float = d1["end"]
        tags1: Set[str] = d1["tags"]

        for j in range(i + 1, len(mids)):
            id2: int = mids[j]
            if id2 not in marker_data or id2 in markers_to_delete:
                continue
            d2 = marker_data[id2]
            start2: float = d2["start"]
            end2: float = d2["end"]
            tags2: Set[str] = d2["tags"]

            # Doit avoir le m√™me tag unique
            if tags1 != tags2 or len(tags1) != 1:
                if start2 >= end1:
                    break
                continue

            gap: float = start2 - end1

            # R√®gle FUSION : gap < 10s
            if gap < 10:
                new_end: float = max(end1, end2 if end2 else start2)
                markers_to_update[id1] = new_end
                markers_to_delete.add(id2)
                end1 = new_end
                d1["end"] = new_end
                fusions_count += 1
                continue

            # Arr√™t si hors zone
            if start2 >= end1:
                break

            # R√®gle CONTENANCE
            if start1 < end2 and start2 < end1:
                if start1 <= start2 and end1 >= end2 and end2 > 0:
                    markers_to_delete.add(id2)
                    contained_count += 1
                elif start2 <= start1 and end2 >= end1 and end1 > 0:
                    markers_to_delete.add(id1)
                    contained_count += 1

# ---- 4. R√®gle GLOBALE : dur√©e minimale ----
global_short_count: int = 0
if GLOBAL_MIN_DURATION > 0:
    for mid, d in marker_data.items():
        if mid in markers_to_delete:
            continue
        m_dur: float = d["end"] - d["start"] if d["end"] > d["start"] else 0.0
        if m_dur <= GLOBAL_MIN_DURATION:
            markers_to_delete.add(mid)
            global_short_count += 1
            # Annuler la fusion si ce marqueur √©tait source d'une mise √† jour
            if mid in markers_to_update:
                del markers_to_update[mid]

# ---- 5. R√®gles CIBL√âES par tag ----
targeted_deletes: Dict[str, int] = {}
for mid, d in marker_data.items():
    if mid in markers_to_delete:
        continue
    m_dur = d["end"] - d["start"] if d["end"] > d["start"] else 0.0
    m_tags: Set[str] = d["tags"]
    for tag_rule, max_dur in TARGETED_DELETE_RULES:
        if tag_rule in m_tags:
            if max_dur is None or m_dur <= max_dur:
                markers_to_delete.add(mid)
                rule_key: str = f"{tag_rule} (<= {max_dur}s)" if max_dur else tag_rule
                targeted_deletes[rule_key] = targeted_deletes.get(rule_key, 0) + 1
                break

# ---- 6. Bilan ----
total_updates: int = len(markers_to_update)
total_deletes: int = len(markers_to_delete)

print(f"Bilan du nettoyage identifi√© :\n")
print(f"  [FUSION]     {fusions_count} marqueurs fusionn√©s")
print(f"  [CONTENANCE] {contained_count} marqueurs contenus dans un autre")
if GLOBAL_MIN_DURATION > 0:
    print(f"  [DUR√âE ‚â§{int(GLOBAL_MIN_DURATION)}s] {global_short_count} marqueurs trop courts (toutes cat√©gories)")
for rule, count in targeted_deletes.items():
    print(f"  [CIBL√â]      {count} marqueurs '{rule}'")
print(f"\n  TOTAL : {total_updates} UPDATE(s), {total_deletes} DELETE(s)\n")

if not markers_to_delete and not markers_to_update:
    print("[OK] Aucune action n√©cessaire. Base d√©j√† propre !")
    conn.close()
    exit()

confirm: str = input("Voulez-vous appliquer tous ces changements ? (OUI/non) : ")

if confirm == "OUI":
    try:
        for mid, new_end in markers_to_update.items():
            cur.execute("UPDATE scene_markers SET end_seconds = ? WHERE id = ?", (new_end, mid))

        ids_to_del: List[Tuple[int]] = [(mid,) for mid in markers_to_delete]
        cur.executemany("DELETE FROM scene_markers_tags WHERE scene_marker_id = ?", ids_to_del)
        cur.executemany("DELETE FROM scene_markers WHERE id = ?", ids_to_del)

        conn.commit()
        print(f"\n[OK] {total_updates} mise(s) √† jour et {total_deletes} suppression(s) appliqu√©es avec succ√®s.")
    except Exception as e:
        print(f"\n[ERREUR] √âchec : {e}")
        conn.rollback()
else:
    print("[INFO] Nettoyage annul√©.")

conn.close()


============================================================
[76/83] utils\cleanup_specific.py
------------------------------------------------------------
import sqlite3

DB_PATH = r"H:\Stash\stash-go.sqlite"

def main():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()

    print("=== NETTOYAGE CIBL√â DES MARQUEURS ===\n")

    # 1. Identifier les marqueurs "69" (via Tag ou Titre)
    cur.execute("""
        SELECT DISTINCT m.id 
        FROM scene_markers m
        LEFT JOIN scene_markers_tags mt ON m.id = mt.scene_marker_id
        LEFT JOIN tags t ON mt.tag_id = t.id
        WHERE m.title = '69' OR t.name = '69'
    """)
    ids_69 = [r[0] for r in cur.fetchall()]

    # 2. Identifier les marqueurs "Anal" <= 60s (via Tag ou Titre)
    cur.execute("""
        SELECT DISTINCT m.id 
        FROM scene_markers m
        LEFT JOIN scene_markers_tags mt ON m.id = mt.scene_marker_id
        LEFT JOIN tags t ON mt.tag_id = t.id
        WHERE (m.title = 'Anal' OR t.name = 'Anal')
          AND ((m.end_seconds - m.seconds) <= 60 
               OR m.end_seconds IS NULL 
               OR m.end_seconds = 0)
    """)
    ids_anal_short = [r[0] for r in cur.fetchall()]

    total_ids = set(ids_69) | set(ids_anal_short)

    print(f"Bilan avant suppression :")
    print(f"  - Marqueurs '69' trouv√©s : {len(ids_69)}")
    print(f"  - Marqueurs 'Anal' (<= 60s) trouv√©s : {len(ids_anal_short)}")
    print(f"  - Total unique √† supprimer : {len(total_ids)}")

    if not total_ids:
        print("\n[!] Aucun marqueur ne correspond aux crit√®res. Fin du script.")
        conn.close()
        return

    confirm = input("\n√ätes-vous s√ªr de vouloir SUPPRIMER ces marqueurs d√©finitivement ? (OUI/non) : ")
    
    if confirm == "OUI":
        try:
            ids_tuple = [(int(mid),) for mid in total_ids]
            
            # Supprimer les liens tags
            cur.executemany("DELETE FROM scene_markers_tags WHERE scene_marker_id = ?", ids_tuple)
            # Supprimer les marqueurs
            cur.executemany("DELETE FROM scene_markers WHERE id = ?", ids_tuple)
            
            conn.commit()
            print(f"\n[OK] {len(total_ids)} marqueurs ont √©t√© supprim√©s avec succ√®s.")
        except Exception as e:
            print(f"\n[ERREUR] √âchec de la suppression : {e}")
            conn.rollback()
    else:
        print("\n[INFO] Op√©ration annul√©e.")

    conn.close()

if __name__ == "__main__":
    main()


============================================================
[77/83] utils\customfield_utils.py
------------------------------------------------------------
# Utilitaire pour injecter des customfields performer

def inject_customfields(db, performer_id, customfields):
    """
    Injecte une liste de customfields pour un performer.
    customfields = [
        {"type": "award", "value": "..."},
        {"type": "trivia", "value": "..."},
        {"type": "tattoo", "value": "..."},
        {"type": "piercing", "value": "..."},
    ]
    """
    cur = db.conn.cursor()
    for cf in customfields:
        cur.execute(
            "INSERT INTO performer_customfields (performer_id, type, value) VALUES (?, ?, ?)",
            (performer_id, cf["type"], cf["value"])
        )
    db.conn.commit()


============================================================
[78/83] utils\duration.py
------------------------------------------------------------
"""
Utilitaire de conversion de dur√©es pour les DVDs.
G√®re tous les formats rencontr√©s dans les sources DVD.
"""
import re
from typing import Optional


def parse_duration_to_seconds(raw: str) -> Optional[int]:
    """
    Convertit n'importe quelle repr√©sentation de dur√©e en secondes (INTEGER).

    Formats support√©s :
        '[01:55:32]'  ‚Üí data18
        '01:55:00'    ‚Üí adultdvdempire (apr√®s conversion interne)
        '115'         ‚Üí iafd (minutes brutes)
        '240 minutes' ‚Üí jeedoo
        '1 hrs. 55 mins.' ‚Üí adultdvdempire brut
    """
    if not raw:
        return None
    raw = str(raw).strip().strip("[]")

    # HH:MM:SS
    m = re.match(r'^(\d+):(\d{2}):(\d{2})$', raw)
    if m:
        return int(m.group(1)) * 3600 + int(m.group(2)) * 60 + int(m.group(3))

    # MM:SS
    m = re.match(r'^(\d+):(\d{2})$', raw)
    if m:
        return int(m.group(1)) * 60 + int(m.group(2))

    # "1 hrs. 55 mins." ou "1 hr 55 min"
    m = re.search(r'(\d+)\s*hr', raw, re.I)
    if m:
        hours = int(m.group(1))
        mins_m = re.search(r'(\d+)\s*min', raw, re.I)
        mins = int(mins_m.group(1)) if mins_m else 0
        return hours * 3600 + mins * 60

    # "240 minutes" ou "240 mins"
    m = re.search(r'(\d+)\s*min', raw, re.I)
    if m:
        return int(m.group(1)) * 60

    # Nombre seul ‚Üí consid√©r√© comme minutes (iafd)
    m = re.match(r'^(\d+)$', raw)
    if m:
        return int(m.group(1)) * 60

    return None


def format_duration(seconds: int) -> str:
    """Formate des secondes en HH:MM:SS pour affichage."""
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    return f"{h:02d}:{m:02d}:{s:02d}"


============================================================
[79/83] utils\list_short_markers.py
------------------------------------------------------------
import sqlite3
import csv

DB_PATH = r"H:\Stash\stash-go.sqlite"
OUTPUT_CSV = "short_markers.csv"

def main():
    print(f"Extraction des marqueurs <= 60s...")
    
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    
    query = """
    SELECT 
        s.id as scene_id, 
        s.title as scene_title, 
        m.id as marker_id, 
        m.title as marker_title, 
        m.seconds as start, 
        m.end_seconds as end
    FROM scene_markers m
    JOIN scenes s ON m.scene_id = s.id
    WHERE (m.end_seconds - m.seconds) <= 60 
       OR m.end_seconds IS NULL 
       OR m.end_seconds = 0
    ORDER BY s.id, m.seconds
    """
    
    cur.execute(query)
    rows = cur.fetchall()
    
    results = []
    for r in rows:
        scene_id, scene_title, m_id, m_title, start, end = r
        duration = (end - start) if (end and end > start) else 0
        results.append({
            "scene_id": scene_id,
            "scene_title": scene_title,
            "marker_id": m_id,
            "marker_title": m_title,
            "start": round(start, 2),
            "end": round(end, 2) if end else 0,
            "duration": round(duration, 2)
        })
    
    if results:
        with open(OUTPUT_CSV, mode='w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=results[0].keys())
            writer.writeheader()
            writer.writerows(results)
        print(f"[OK] {len(results)} marqueurs extraits dans {OUTPUT_CSV}")
    else:
        print("[!] Aucun marqueur de moins de 60s trouv√©.")
        
    conn.close()

if __name__ == "__main__":
    main()


============================================================
[80/83] utils\marker.py
------------------------------------------------------------
from __future__ import annotations
import sqlite3
import csv
from collections import defaultdict
from typing import Dict, List, Set, Tuple, Optional, Any

DB_PATH = r"H:\Stash\stash-go.sqlite"
CSV_REPORT = "audit_markers.csv"

conn = sqlite3.connect(DB_PATH)
cur = conn.cursor()

print("=== AUDIT AVANCE DES MARKERS ===\n")

# R√©cup√©rer toutes sc√®nes
cur.execute("""
    SELECT s.id, s.title, v.duration 
    FROM scenes s
    LEFT JOIN scenes_files sf ON s.id = sf.scene_id
    LEFT JOIN video_files v ON sf.file_id = v.file_id
    WHERE sf."primary" = 1 OR sf."primary" IS NULL
    GROUP BY s.id
""")
scenes: List[Any] = cur.fetchall()

tag_stats: Dict[str, Dict[str, int]] = defaultdict(lambda: {
    "total": 0,
    "overlaps": 0,
    "short": 0,
    "long": 0,
    "exact_dupes": 0
})

scene_results: List[Dict[str, Any]] = []
scene_scores: List[Tuple[Optional[str], int]] = []
markers_to_delete: Set[int] = set()
markers_to_update: Dict[int, float] = {}

for scene_row in scenes:
    scene_id: int = int(scene_row[0])
    title: Optional[str] = str(scene_row[1]) if scene_row[1] else None
    duration: float = float(scene_row[2]) if scene_row[2] else 0.0

    cur.execute("""
    SELECT m.id, m.title, m.seconds, m.end_seconds,
           GROUP_CONCAT(t.name)
    FROM scene_markers m
    LEFT JOIN scene_markers_tags mt ON m.id = mt.scene_marker_id
    LEFT JOIN tags t ON mt.tag_id = t.id
    WHERE m.scene_id = ?
    GROUP BY m.id
    """, (scene_id,))

    markers: List[Any] = cur.fetchall()
    if not markers:
        continue

    # Stats par sc√®ne
    markers_count: int = len(markers)
    unique_tags: Set[str] = set()
    scene_overlaps: int = 0
    scene_short: int = 0
    scene_long: int = 0
    scene_exact_dupes: int = 0
    scene_points: int = 0

    overlaps_list: List[Any] = []
    seen_ranges: Dict[Tuple[float, float], List[Set[str]]] = {}

    markers_sorted: List[Any] = sorted(markers, key=lambda x: x[2])

    for i in range(len(markers_sorted)):
        _row1 = markers_sorted[i]
        id1: int = int(_row1[0])
        m_title1: str = str(_row1[1]) if _row1[1] else ""
        start1: float = float(_row1[2])
        end1: float = float(_row1[3]) if _row1[3] else 0.0
        tags1_str: str = str(_row1[4]) if _row1[4] else ""

        tags1: Set[str] = set(tags1_str.split(",")) if tags1_str else set()
        if m_title1:
            tags1.add(m_title1)

        unique_tags.update(tags1)
        duration_marker: float = end1 - start1 if end1 else 0.0

        # Detection points sans dur√©e
        if not end1 or end1 <= start1:
            scene_points += 1

        # Detection doublons exacts (m√™me plage)
        time_range: Tuple[float, float] = (round(start1, 2), round(end1, 2) if end1 else 0.0)
        if time_range in seen_ranges:
            for prev_tags in seen_ranges[time_range]:
                common: Set[str] = tags1.intersection(prev_tags)
                if common:
                    scene_exact_dupes += len(common)
                    for t in common:
                        tag_stats[t]["exact_dupes"] += 1

        if time_range not in seen_ranges:
            seen_ranges[time_range] = []
        seen_ranges[time_range].append(tags1)

        # Stats tags
        for tag in tags1:
            tag_stats[tag]["total"] += 1
            if duration_marker > 0 and duration_marker < 3:
                tag_stats[tag]["short"] += 1
                scene_short += 1
            if duration and duration_marker > duration * 0.2:
                tag_stats[tag]["long"] += 1
                scene_long += 1

        # Check overlaps / Fusion / Redondance
        for j in range(i + 1, len(markers_sorted)):
            _row2 = markers_sorted[j]
            id2: int = int(_row2[0])
            m_title2: str = str(_row2[1]) if _row2[1] else ""
            start2: float = float(_row2[2])
            end2: float = float(_row2[3]) if _row2[3] else 0.0
            tags2_str: str = str(_row2[4]) if _row2[4] else ""

            tags2: Set[str] = set(tags2_str.split(",")) if tags2_str else set()
            if m_title2:
                tags2.add(m_title2)

            # --- LOGIQUE DE FUSION (GAP < 10s) ---
            if tags1 == tags2 and len(tags1) == 1:
                gap: float = start2 - (end1 if end1 else start1)
                if gap < 10:
                    new_end: float = max(end1 if end1 else start1, end2 if end2 else start2)
                    markers_to_update[id1] = new_end
                    markers_to_delete.add(id2)
                    end1 = new_end

            # Arr√™t de la boucle j si on d√©passe la zone de collision
            if start2 >= (end1 if end1 else start1):
                break

            # --- LOGIQUE DE CHEVAUCHEMENT / OVERLAP ---
            if start1 < (end2 if end2 else start2) and start2 < (end1 if end1 else start1):
                common2: Set[str] = tags1.intersection(tags2)
                if common2:
                    scene_overlaps += len(common2)
                    overlaps_list.append((id1, id2, start1, end1, start2, end2, list(common2)))
                    for tag in common2:
                        tag_stats[tag]["overlaps"] += 1

                    # --- LOGIQUE DE SUPPRESSION (CONTENU DANS) ---
                    if tags1 == tags2 and len(tags1) == 1:
                        # B est dans A
                        if start1 <= start2 and (end1 >= end2 if end1 and end2 else False):
                            markers_to_delete.add(id2)
                        # A est dans B
                        elif start2 <= start1 and (end2 >= end1 if end1 and end2 else False):
                            markers_to_delete.add(id1)

    scene_penalty: int = (scene_overlaps * 2) + scene_short + scene_long + (scene_exact_dupes * 5) + scene_points

    res: Dict[str, Any] = {
        "id": scene_id,
        "title": title,
        "duration": duration,
        "markers": markers_count,
        "points": scene_points,
        "tags": len(unique_tags),
        "overlaps": scene_overlaps,
        "short": scene_short,
        "long": scene_long,
        "exact_dupes": scene_exact_dupes,
        "penalty": scene_penalty
    }
    scene_results.append(res)
    scene_scores.append((title, scene_penalty))

    if scene_penalty > 0:
        display_title: str = title[:60] if title else "Sans titre"
        print(f"SCENE: {display_title}...")
        print(f"  - Dur√©e: {duration}s | Marqueurs: {markers_count} (Points: {scene_points}) | Tags: {len(unique_tags)}")
        print(f"  - Probl√®mes: Overlaps={scene_overlaps}, Courts={scene_short}, Longs={scene_long}, Doublons Plage={scene_exact_dupes}")
        print(f"  - Score P√©nalit√©: {scene_penalty}")
        print("-" * 30)

# Export CSV
with open(CSV_REPORT, mode='w', newline='', encoding='utf-8-sig') as f:
    if scene_results:
        writer = csv.DictWriter(f, fieldnames=list(scene_results[0].keys()))
        writer.writeheader()
        writer.writerows(scene_results)

print(f"\n[OK] Rapport d√©taill√© export√© dans: {CSV_REPORT}")

# Classement sc√®nes
print("\n=== TOP 20 SCENES PROBLEMATIQUES ===\n")
scene_scores.sort(key=lambda x: x[1], reverse=True)
top20: List[Tuple[Optional[str], int]] = scene_scores[:20]
for s in top20:
    if s[1] > 0:
        print(f"{s[1]:>4} pts | {s[0]}")

# Audit Tags
print("\n=== SCORE DE COHERENCE PAR TAG ===\n")
tag_audit: List[Tuple[str, int, Dict[str, int]]] = []
for tag, data in tag_stats.items():
    score: int = max(0, 100 - (data["overlaps"] * 3) - data["short"] - data["long"] - (data["exact_dupes"] * 10))
    tag_audit.append((tag, score, data))

tag_audit.sort(key=lambda x: x[1])
top_tags: List[Tuple[str, int, Dict[str, int]]] = tag_audit[:15]
for tag, score, data in top_tags:
    print(f"{tag[:20]:<20} | Score: {score:>3}/100 | {data['total']} total, {data['overlaps']} over, {data['exact_dupes']} dupes")

# --- SUPPRESSION / MISE A JOUR DES DOUBLONS ET FUSIONS ---
if markers_to_delete or markers_to_update:
    print(f"\n[!] ALERT: Travail de nettoyage identifi√© :")
    if markers_to_delete:
        print(f"  - {len(markers_to_delete)} marqueurs √† SUPPRIMER (doublons/fusions)")
    if markers_to_update:
        print(f"  - {len(markers_to_update)} marqueurs √† METTRE √Ä JOUR (fusions de dur√©e)")

    confirm: str = input(f"\nVoulez-vous appliquer ces changements √† la base de donn√©es ? (oui/non) : ")

    if confirm.lower() in ['oui', 'y', 'yes', 'o']:
        try:
            # 1. Mises √† jour (Fusions)
            for mid, new_end in markers_to_update.items():
                cur.execute("UPDATE scene_markers SET end_seconds = ? WHERE id = ?", (new_end, mid))

            # 2. Suppressions
            ids_to_del: List[Tuple[int]] = [(mid,) for mid in markers_to_delete]
            cur.executemany("DELETE FROM scene_markers WHERE id = ?", ids_to_del)
            cur.executemany("DELETE FROM scene_markers_tags WHERE scene_marker_id = ?", ids_to_del)

            conn.commit()
            print(f"[OK] Modifications appliqu√©es avec succ√®s.")
        except Exception as e:
            print(f"[ERREUR] √âchec des modifications : {e}")
            conn.rollback()
    else:
        print("[INFO] Nettoyage annul√©.")

conn.close()

============================================================
[81/83] utils\meta_tag_utils.py
------------------------------------------------------------
# Utilitaires pour la normalisation et la propagation des tags de m√©tadonn√©es

COLORED_HAIR_TAGS = {
    "Blue": "BlueHair",
    "Green": "GreenHair",
    "Grey": "GreyHair",
    "Pink": "PinkHair",
    "Purple": "PurpleHair",
    "Red": "RedHair",
    "White": "WhiteHair"
}
NATURAL_HAIR_TAGS = {
    "Black": "BlackHair",
    "Blond": "BlondHair",
    "Brown": "BrownHair",
    "Redhead": "RedHead"
}
HAIR_TAGS = {**COLORED_HAIR_TAGS, **NATURAL_HAIR_TAGS}

NATIONALITY_TAGS = {
    "Cubaine": "Cuban",
    "Dominicaine": "Dominican",
    "Colombienne": "Colombian",
    "Thai": "Thai",
    "V√©n√©zu√©lienne": "Venezuelan",
    "Brasilian": "Brazilian",
    "Mexicaine": "Mexican"
}

ETHNY_TAGS = {
    "Asian": "Asian",
    "Ebony": "Ebony",
    "Latina": "Latina"
}


def normalize_tag(value):
    """Normalise une valeur de m√©tadonn√©e pour correspondre √† un tag."""
    v = value.strip().capitalize()
    return v


def meta_to_tags(meta):
    """Transforme les m√©tadonn√©es en tags selon les r√®gles."""
    tags = []
    # Couleur de cheveux
    hair = meta.get("Hair Color", "")
    for color in [c.strip() for c in hair.split(",") if c.strip()]:
        tag = HAIR_TAGS.get(normalize_tag(color))
        if tag:
            tags.append(tag)
    # Nationalit√©
    nat = meta.get("Country", "")
    tag = NATIONALITY_TAGS.get(normalize_tag(nat))
    if tag:
        tags.append(tag)
    # Ethnie
    ethny = meta.get("Ethnicity", "")
    tag = ETHNY_TAGS.get(normalize_tag(ethny))
    if tag:
        tags.append(tag)
    # MILF
    import datetime
    birthdate = meta.get("Birthdate", "")
    if birthdate:
        try:
            birth = datetime.datetime.strptime(birthdate, "%Y-%m-%d")
            age = (datetime.datetime.now() - birth).days // 365
            if age >= 35:
                tags.append("MILF")
        except Exception:
            pass
    # BigTits
    measurements = meta.get("Measurements", "")
    bust = None
    hips = None
    if measurements:
        # Format attendu: 36-24-38
        parts = [p.strip() for p in measurements.split("-")]
        if len(parts) == 3:
            try:
                bust = int(parts[0])
            except Exception:
                pass
            try:
                hips = int(parts[2])
            except Exception:
                pass
    # BigTits
    if bust is not None and bust >= 36:
        tags.append("BigTits")
    # BigButt
    if hips is not None and hips >= 38:
        tags.append("BigButt")
    # Bimbo
    if "BigTits" in tags and "BigButt" in tags:
        tags.append("Bimbo")
    return tags


def propagate_tags_to_scenes(db, performer_id, tags):
    """Ajoute/supprime les tags de couleur de cheveux, nationalit√©, ethnie sur toutes les sc√®nes de l'artiste."""
    # R√©cup√©rer toutes les sc√®nes de l'artiste
    cur = db.conn.cursor()
    cur.execute("SELECT scene_id FROM performers_scenes WHERE performer_id=?", (performer_id,))
    scene_ids = [r[0] for r in cur.fetchall()]
    for scene_id in scene_ids:
        # R√©cup√©rer les tags actuels
        cur.execute("SELECT t.name FROM tags t JOIN scenes_tags st ON st.tag_id = t.id WHERE st.scene_id=?", (scene_id,))
        scene_tags = [r[0] for r in cur.fetchall()]
        # Ajouter les tags manquants
        for tag in tags:
            if tag not in scene_tags:
                # Ajout du tag √† la sc√®ne
                cur.execute("SELECT id FROM tags WHERE name=?", (tag,))
                row = cur.fetchone()
                if row:
                    tag_id = row[0]
                    cur.execute("INSERT INTO scenes_tags (scene_id, tag_id) VALUES (?,?)", (scene_id, tag_id))
        # Supprimer les tags erron√©s
        valid_tags = set(HAIR_TAGS.values()) | set(NATIONALITY_TAGS.values()) | set(ETHNY_TAGS.values()) | {"MILF", "BigTits", "BigButt", "Bimbo"}
        for tag in scene_tags:
            if tag in valid_tags and tag not in tags:
                cur.execute("SELECT id FROM tags WHERE name=?", (tag,))
                row = cur.fetchone()
                if row:
                    tag_id = row[0]
                    cur.execute("DELETE FROM scenes_tags WHERE scene_id=? AND tag_id=?", (scene_id, tag_id))
    db.conn.commit()


============================================================
[82/83] utils\short_markers.csv
------------------------------------------------------------
Ôªøscene_id,scene_title,marker_id,marker_title,start,end,duration
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",23,BlowJob,16.0,28.0,12.0
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",24,BlowJob,246.0,266.0,20.0
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",28,Cumshot,382.0,402.0,20.0
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",30,Cumshot,590.0,612.0,22.0
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",31,Cumshot,718.0,750.0,32.0
177,"Anna Gold, Jazzy Jay, Sai Tai Tiger in Schlamm Schlacht Sc2",26,BlowJob,816.0,838.0,22.0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",42,Cumshot,134.0,170.0,36.0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",36,BlowJob,276.0,286.0,10.0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",37,BlowJob,338.0,372.0,34.0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",40,BlowJob,476.0,526.0,50.0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",39,BlowJob,720.0,732.0,12.0
178,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc2",44,Cumshot,778.0,796.0,18.0
179,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc5",68,BlowJob,176.0,218.0,42.0
179,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc5",69,BlowJob,358.0,370.0,12.0
179,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc5",70,BlowJob,656.0,666.0,10.0
179,"Anna Gold, Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc5",71,BlowJob,718.0,744.0,26.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",52,BlowJob,184.0,188.0,4.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",47,BlowJob,416.0,444.0,28.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",48,BlowJob,504.0,540.0,36.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",54,BlowJob,582.0,590.0,8.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",60,Cumshot,622.0,634.0,12.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",61,Cumshot,658.0,664.0,6.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",49,BlowJob,814.0,832.0,18.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",50,BlowJob,878.0,902.0,24.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",58,Cumshot,888.0,910.0,22.0
180,"Anna Gold, Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc3",51,BlowJob,972.0,1002.0,30.0
181,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc1",64,BlowJob,104.0,124.0,20.0
181,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc1",62,BlowJob,144.0,156.0,12.0
181,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc1",63,BlowJob,206.0,236.0,30.0
181,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc1",65,BlowJob,876.0,922.0,46.0
181,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc1",66,Cumshot,896.0,932.0,36.0
182,"Mya Diamond in Frauen Knast, Teufelsbrut Hinter Gittern! Sc5",77,Cumshot,280.0,306.0,26.0
183,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc3,82,BlowJob,92.0,142.0,50.0
183,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc3,83,BlowJob,196.0,208.0,12.0
183,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc3,86,Anal,696.0,724.0,28.0
183,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc3,84,BlowJob,848.0,860.0,12.0
186,"Sai Tai Tiger in Frauen Knast, Teufelsbrut Hinter Gittern! Sc2",80,Anal,346.0,370.0,24.0
186,"Sai Tai Tiger in Frauen Knast, Teufelsbrut Hinter Gittern! Sc2",81,Anal,608.0,620.0,12.0
188,Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc4,93,Cumshot,140.0,150.0,10.0
188,Sai Tai Tiger in The Very Best Of Sperma Gangbang Sc4,94,Cumshot,208.0,234.0,26.0
190,"Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc7",103,Cumshot,160.0,194.0,34.0
190,"Sai Tai Tiger, Salma De Nora in The Very Best Of Sperma Gangbang Sc7",104,Cumshot,246.0,258.0,12.0
191,Aderes Quin in StepMom Gets Double Dick,110,BlowJob,744.0,756.0,12.0
191,Aderes Quin in StepMom Gets Double Dick,112,BlowJob,1092.0,1102.0,10.0
191,Aderes Quin in StepMom Gets Double Dick,113,BlowJob,1144.0,1168.0,24.0
191,Aderes Quin in StepMom Gets Double Dick,114,BlowJob,1212.0,1264.0,52.0
191,Aderes Quin in StepMom Gets Double Dick,117,BlowJob,1878.0,1918.0,40.0
191,Aderes Quin in StepMom Gets Double Dick,122,Grabbing Boobs,2572.0,2592.0,20.0
191,Aderes Quin in StepMom Gets Double Dick,123,Grabbing Boobs,2692.0,2746.0,54.0
192,Alejandra Rico in Que Rico!,1950,BlowJob,154.0,210.0,56.0
192,Alejandra Rico in Que Rico!,1952,BlowJob,836.0,876.0,40.0
192,Alejandra Rico in Que Rico!,1957,Cumshot,1310.0,1346.0,36.0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",2434,Gangbang,146.0,160.0,14.0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",2427,BlowJob,550.0,564.0,14.0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",2428,BlowJob,1118.0,1158.0,40.0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",2429,BlowJob,1228.0,1258.0,30.0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",2436,Grabbing Boobs,1310.0,1328.0,18.0
193,"Alexa Flexy, Monika Fox in Balls Deep Russia Ritual",2433,BlowJob,1340.0,1354.0,14.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1869,Grabbing Boobs,418.0,444.0,26.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1870,Grabbing Boobs,488.0,524.0,36.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1871,Grabbing Boobs,972.0,992.0,20.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1878,Anal,1480.0,1506.0,26.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1879,Anal,1556.0,1570.0,14.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1880,Anal,1636.0,1692.0,56.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1881,Anal,1980.0,2010.0,30.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1875,BlowJob,2046.0,2068.0,22.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1882,Anal,2230.0,2268.0,38.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1883,Cumshot,2358.0,2398.0,40.0
194,"Athenea Rose in Caught Mid Robbery, DP Ensues",1877,BlowJob,2360.0,2402.0,42.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1884,Grabbing Boobs,464.0,502.0,38.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1885,Grabbing Boobs,542.0,570.0,28.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1886,Grabbing Boobs,740.0,774.0,34.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1892,Gangbang,968.0,984.0,16.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1887,Grabbing Boobs,1706.0,1752.0,46.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1894,Anal,1832.0,1842.0,10.0
195,"Athenea Rose in Safe Cracked, Holes Filled",1896,Anal,2248.0,2280.0,32.0
196,Ava Devine in Milf Asian Cummouth Facial,3663,Grabbing Boobs,6.0,26.0,20.0
196,Ava Devine in Milf Asian Cummouth Facial,3665,BlowJob,418.0,466.0,48.0
196,Ava Devine in Milf Asian Cummouth Facial,3670,BlowJob,508.0,520.0,12.0
196,Ava Devine in Milf Asian Cummouth Facial,3666,BlowJob,810.0,834.0,24.0
196,Ava Devine in Milf Asian Cummouth Facial,3667,BlowJob,1030.0,1054.0,24.0
196,Ava Devine in Milf Asian Cummouth Facial,3668,BlowJob,1222.0,1242.0,20.0
196,Ava Devine in Milf Asian Cummouth Facial,3671,Anal,1374.0,1414.0,40.0
196,Ava Devine in Milf Asian Cummouth Facial,3669,BlowJob,1424.0,1436.0,12.0
196,Ava Devine in Milf Asian Cummouth Facial,3672,Anal,1612.0,1666.0,54.0
196,Ava Devine in Milf Asian Cummouth Facial,3673,Anal,1764.0,1806.0,42.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3644,BlowJob,204.0,246.0,42.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3649,Pissing,318.0,324.0,6.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3657,Gangbang,700.0,726.0,26.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3646,BlowJob,794.0,820.0,26.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3658,Gangbang,1004.0,1022.0,18.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3659,Gangbang,1176.0,1186.0,10.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3647,BlowJob,1370.0,1400.0,30.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3660,Gangbang,1508.0,1546.0,38.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3652,Anal,1818.0,1858.0,40.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3653,Anal,1938.0,1952.0,14.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3654,Anal,2032.0,2054.0,22.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3655,Anal,2110.0,2134.0,24.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3656,Anal,2218.0,2268.0,50.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3661,Cumshot,2470.0,2528.0,58.0
197,"Barbie Sins in Anal Domination, 6on1 DAP",3662,Cumshot,2604.0,2620.0,16.0
198,Barbie Sins in DP Bandits! Sc4,3701,BlowJob,1008.0,1016.0,8.0
198,Barbie Sins in DP Bandits! Sc4,3705,Anal,1554.0,1564.0,10.0
198,Barbie Sins in DP Bandits! Sc4,3697,BlowJob,1964.0,1994.0,30.0
198,Barbie Sins in DP Bandits! Sc4,3707,Anal,2146.0,2164.0,18.0
198,Barbie Sins in DP Bandits! Sc4,3698,BlowJob,2612.0,2632.0,20.0
198,Barbie Sins in DP Bandits! Sc4,3709,Cumshot,2798.0,2822.0,24.0
198,Barbie Sins in DP Bandits! Sc4,3700,BlowJob,2816.0,2852.0,36.0
198,Barbie Sins in DP Bandits! Sc4,3710,Cumshot,2892.0,2924.0,32.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3621,BlowJob,584.0,630.0,46.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3600,Anal,680.0,698.0,18.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3601,Anal,742.0,780.0,38.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3629,BlowJob,816.0,862.0,46.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3603,Anal,1258.0,1278.0,20.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3630,Cumshot,1344.0,1378.0,34.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3605,Anal,1722.0,1744.0,22.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3606,Anal,1800.0,1858.0,58.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3607,Anal,1924.0,1944.0,20.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3609,Anal,2728.0,2780.0,52.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3610,Anal,2862.0,2900.0,38.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3635,Cumshot,2912.0,2922.0,10.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3611,Anal,2940.0,2974.0,34.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3612,Anal,3030.0,3064.0,34.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3624,BlowJob,3158.0,3172.0,14.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3636,Cumshot,3168.0,3174.0,6.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3625,BlowJob,3454.0,3476.0,22.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3613,Anal,3496.0,3548.0,52.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3639,Gangbang,3530.0,3570.0,40.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3626,BlowJob,3544.0,3558.0,14.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3614,Anal,3604.0,3618.0,14.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3640,Gangbang,3678.0,3692.0,14.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3627,BlowJob,3682.0,3694.0,12.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3615,Anal,3764.0,3796.0,32.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3616,Anal,4008.0,4024.0,16.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3617,Anal,4168.0,4224.0,56.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3643,Gangbang,4306.0,4336.0,30.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3618,Anal,4332.0,4344.0,12.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3619,Anal,4382.0,4400.0,18.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3628,BlowJob,4560.0,4602.0,42.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3632,Cumshot,4784.0,4796.0,12.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3637,Cumshot,4816.0,4822.0,6.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3633,Cumshot,4888.0,4896.0,8.0
199,Blanche Bradburry in 10 Guy Anal Showdown,3620,Anal,4922.0,4948.0,26.0
200,Blanche Bradburry in Gangbang Anal Blitz,3588,BlowJob,126.0,154.0,28.0
200,Blanche Bradburry in Gangbang Anal Blitz,3595,69,274.0,300.0,26.0
200,Blanche Bradburry in Gangbang Anal Blitz,3589,BlowJob,346.0,372.0,26.0
200,Blanche Bradburry in Gangbang Anal Blitz,3590,BlowJob,468.0,494.0,26.0
200,Blanche Bradburry in Gangbang Anal Blitz,3596,Anal,742.0,758.0,16.0
200,Blanche Bradburry in Gangbang Anal Blitz,3591,BlowJob,780.0,792.0,12.0
200,Blanche Bradburry in Gangbang Anal Blitz,3597,Anal,792.0,812.0,20.0
200,Blanche Bradburry in Gangbang Anal Blitz,3592,BlowJob,994.0,1010.0,16.0
200,Blanche Bradburry in Gangbang Anal Blitz,3593,BlowJob,1868.0,1882.0,14.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3714,Anal,1066.0,1080.0,14.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3715,Anal,1158.0,1176.0,18.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3725,BlowJob,1282.0,1296.0,14.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3726,BlowJob,1340.0,1368.0,28.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3727,BlowJob,1514.0,1530.0,16.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3729,BlowJob,1782.0,1792.0,10.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3731,Gangbang,2082.0,2098.0,16.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3717,Anal,2116.0,2154.0,38.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3732,Gangbang,2144.0,2160.0,16.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3718,Anal,2206.0,2246.0,40.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3733,Gangbang,2214.0,2226.0,12.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3719,Anal,2282.0,2292.0,10.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3720,Anal,2450.0,2480.0,30.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3721,Anal,2828.0,2860.0,32.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3730,BlowJob,2982.0,3012.0,30.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3723,Anal,3106.0,3122.0,16.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3734,Cumshot,3832.0,3854.0,22.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3735,Cumshot,3890.0,3928.0,38.0
201,Blanche Bradburry in Hardcore 8√óDAP Ordeal,3738,Cumshot,3958.0,3992.0,34.0
202,Blanche Bradburry in Rough DAP Gangbang,3800,Grabbing Boobs,4.0,34.0,30.0
202,Blanche Bradburry in Rough DAP Gangbang,3802,BlowJob,1230.0,1246.0,16.0
202,Blanche Bradburry in Rough DAP Gangbang,3806,Anal,1296.0,1308.0,12.0
202,Blanche Bradburry in Rough DAP Gangbang,3807,Anal,1340.0,1362.0,22.0
202,Blanche Bradburry in Rough DAP Gangbang,3803,BlowJob,1376.0,1418.0,42.0
202,Blanche Bradburry in Rough DAP Gangbang,3808,Anal,1514.0,1562.0,48.0
202,Blanche Bradburry in Rough DAP Gangbang,3805,BlowJob,3020.0,3034.0,14.0
202,Blanche Bradburry in Rough DAP Gangbang,3812,Anal,3122.0,3150.0,28.0
203,Blanche Bradburry in Triple Penetration Madness,3739,Grabbing Boobs,130.0,190.0,60.0
203,Blanche Bradburry in Triple Penetration Madness,3740,Grabbing Boobs,276.0,298.0,22.0
203,Blanche Bradburry in Triple Penetration Madness,3741,Grabbing Boobs,384.0,404.0,20.0
203,Blanche Bradburry in Triple Penetration Madness,3742,Grabbing Boobs,602.0,622.0,20.0
203,Blanche Bradburry in Triple Penetration Madness,3745,Anal,922.0,956.0,34.0
203,Blanche Bradburry in Triple Penetration Madness,3751,BlowJob,1486.0,1516.0,30.0
203,Blanche Bradburry in Triple Penetration Madness,3752,BlowJob,2220.0,2250.0,30.0
203,Blanche Bradburry in Triple Penetration Madness,3748,Anal,2804.0,2818.0,14.0
203,Blanche Bradburry in Triple Penetration Madness,3743,Grabbing Boobs,2830.0,2848.0,18.0
203,Blanche Bradburry in Triple Penetration Madness,3749,Anal,2888.0,2912.0,24.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3674,Anal,354.0,370.0,16.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3675,Anal,448.0,460.0,12.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3676,Anal,582.0,602.0,20.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3677,Anal,662.0,674.0,12.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3688,BlowJob,1382.0,1442.0,60.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3679,Anal,1546.0,1558.0,12.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3690,BlowJob,1640.0,1666.0,26.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3678,Anal,1692.0,1740.0,48.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3691,BlowJob,1836.0,1870.0,34.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3692,BlowJob,2214.0,2230.0,16.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3681,Anal,2618.0,2642.0,24.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3693,BlowJob,2672.0,2706.0,34.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3682,Anal,2686.0,2710.0,24.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3683,Anal,2798.0,2812.0,14.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3684,Anal,2858.0,2904.0,46.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3685,Anal,2938.0,2960.0,22.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3686,Anal,3208.0,3226.0,18.0
204,"Blanche Bradburry, Jessie Volt, Sandra Luberc in Raw Anal Lineup",3687,Anal,3284.0,3310.0,26.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3753,Grabbing Boobs,422.0,482.0,60.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3754,Anal,1222.0,1244.0,22.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3755,Anal,1286.0,1346.0,60.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3757,Anal,1356.0,1390.0,34.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3758,Anal,1482.0,1498.0,16.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3765,BlowJob,1822.0,1838.0,16.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3762,BlowJob,2180.0,2192.0,12.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3763,BlowJob,2368.0,2386.0,18.0
205,"Blanche Bradburry, Linda Sweet, Timea Bella in Rough Double Anal Assault",3764,BlowJob,3150.0,3160.0,10.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3813,Grabbing Boobs,16.0,50.0,34.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3814,Anal,382.0,392.0,10.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3824,Anal,970.0,980.0,10.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3825,Anal,1196.0,1206.0,10.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3826,BlowJob,1412.0,1422.0,10.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3827,BlowJob,1500.0,1554.0,54.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3828,BlowJob,1988.0,2006.0,18.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3815,Anal,2186.0,2204.0,18.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3829,BlowJob,2304.0,2314.0,10.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3816,Anal,2384.0,2416.0,32.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3817,Anal,2520.0,2552.0,32.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3818,Anal,2622.0,2682.0,60.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3819,Anal,2716.0,2730.0,14.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3830,BlowJob,2744.0,2760.0,16.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3820,Anal,2798.0,2826.0,28.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3821,Anal,2924.0,2938.0,14.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3822,Anal,2972.0,3018.0,46.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3823,Anal,3166.0,3178.0,12.0
206,"Blanche Bradburry, Nathaly Cherie, Vicktoria Redd in Kreme Porno",3831,BlowJob,3418.0,3442.0,24.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3878,Grabbing Boobs,98.0,136.0,38.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3879,Anal,164.0,188.0,24.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3880,Anal,646.0,676.0,30.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3881,Anal,708.0,720.0,12.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3884,BlowJob,1926.0,1964.0,38.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3885,BlowJob,4178.0,4192.0,14.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3886,BlowJob,4296.0,4324.0,28.0
207,"Blanche Bradburry, Nicky Dream in Double Anal Blond Bombshell",3887,Cumshot,4512.0,4526.0,14.0
208,Cherry Kiss in DP Bandits! Sc2,3867,BlowJob,518.0,568.0,50.0
208,Cherry Kiss in DP Bandits! Sc2,3873,Anal,696.0,750.0,54.0
208,Cherry Kiss in DP Bandits! Sc2,3868,BlowJob,698.0,732.0,34.0
208,Cherry Kiss in DP Bandits! Sc2,3869,BlowJob,798.0,812.0,14.0
208,Cherry Kiss in DP Bandits! Sc2,3870,BlowJob,852.0,874.0,22.0
208,Cherry Kiss in DP Bandits! Sc2,3875,Anal,1120.0,1142.0,22.0
208,Cherry Kiss in DP Bandits! Sc2,3871,BlowJob,1936.0,1968.0,32.0
208,Cherry Kiss in DP Bandits! Sc2,3872,BlowJob,2342.0,2358.0,16.0
208,Cherry Kiss in DP Bandits! Sc2,3877,Cumshot,2346.0,2362.0,16.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3768,Grabbing Boobs,6.0,56.0,50.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3772,Gangbang,156.0,168.0,12.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3780,Anal,172.0,228.0,56.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3790,BlowJob,256.0,286.0,30.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3781,Anal,276.0,302.0,26.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3791,BlowJob,494.0,506.0,12.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3792,BlowJob,626.0,658.0,32.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3774,Gangbang,782.0,816.0,34.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3782,Anal,882.0,900.0,18.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3775,Gangbang,888.0,916.0,28.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3776,Gangbang,956.0,978.0,22.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3777,Gangbang,1178.0,1192.0,14.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3793,BlowJob,1232.0,1274.0,42.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3784,Anal,1276.0,1292.0,16.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3796,Pissing,1332.0,1370.0,38.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3794,BlowJob,1394.0,1406.0,12.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3778,Gangbang,1406.0,1424.0,18.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3769,Grabbing Boobs,1574.0,1602.0,28.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3798,Titjob,1590.0,1606.0,16.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3785,Anal,1718.0,1734.0,16.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3779,Gangbang,1762.0,1804.0,42.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3787,Anal,2226.0,2242.0,16.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3797,Pissing,2296.0,2334.0,38.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3770,Grabbing Boobs,2442.0,2478.0,36.0
209,"Claudia Garcia, Jennifer Mendez in 5on2 Bootylicious Blowout",3771,Grabbing Boobs,3176.0,3214.0,38.0
210,Destiny Mira in Put My Back Into It,1899,BlowJob,288.0,312.0,24.0
210,Destiny Mira in Put My Back Into It,1900,BlowJob,612.0,662.0,50.0
210,Destiny Mira in Put My Back Into It,1901,Anal,960.0,1004.0,44.0
210,Destiny Mira in Put My Back Into It,1902,Anal,1048.0,1058.0,10.0
210,Destiny Mira in Put My Back Into It,1903,Anal,1114.0,1146.0,32.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3832,BlowJob,1516.0,1572.0,56.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3834,BlowJob,1826.0,1856.0,30.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3843,Gangbang,1978.0,1992.0,14.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3836,BlowJob,2228.0,2264.0,36.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3837,BlowJob,2522.0,2540.0,18.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3849,Pissing,2804.0,2816.0,12.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3838,BlowJob,2834.0,2844.0,10.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3852,Anal,3442.0,3452.0,10.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3839,BlowJob,3472.0,3484.0,12.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3853,Anal,3600.0,3618.0,18.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3854,Anal,3660.0,3710.0,50.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3855,Anal,3746.0,3762.0,16.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3857,Anal,4076.0,4088.0,12.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3850,Pissing,4250.0,4268.0,18.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3847,Gangbang,4724.0,4744.0,20.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3848,Gangbang,4776.0,4816.0,40.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3859,Anal,4820.0,4842.0,22.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3840,BlowJob,4828.0,4876.0,48.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3862,Cumshot,5800.0,5814.0,14.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3863,Cumshot,5856.0,5906.0,50.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3864,Cumshot,5950.0,5994.0,44.0
211,"Jolee Love in 9on1 Wet Frenzy, DAP",3865,Cumshot,6046.0,6076.0,30.0
212,Jolee Love in Craving For Cocks,1905,BlowJob,972.0,1000.0,28.0
212,Jolee Love in Craving For Cocks,1906,BlowJob,1134.0,1144.0,10.0
212,Jolee Love in Craving For Cocks,1907,BlowJob,1206.0,1230.0,24.0
212,Jolee Love in Craving For Cocks,1912,Anal,1804.0,1820.0,16.0
212,Jolee Love in Craving For Cocks,1908,BlowJob,2194.0,2212.0,18.0
212,Jolee Love in Craving For Cocks,1915,Anal,2540.0,2590.0,50.0
212,Jolee Love in Craving For Cocks,1909,BlowJob,2746.0,2798.0,52.0
212,Jolee Love in Craving For Cocks,1917,DP,3182.0,3186.0,4.0
213,Jolee Love in DP Bandits! 2 Sc3,1918,Grabbing Boobs,1030.0,1050.0,20.0
213,Jolee Love in DP Bandits! 2 Sc3,1929,Anal,1470.0,1508.0,38.0
213,Jolee Love in DP Bandits! 2 Sc3,1920,BlowJob,1526.0,1544.0,18.0
213,Jolee Love in DP Bandits! 2 Sc3,1930,Anal,1606.0,1640.0,34.0
213,Jolee Love in DP Bandits! 2 Sc3,1921,BlowJob,1820.0,1848.0,28.0
213,Jolee Love in DP Bandits! 2 Sc3,1926,BlowJob,1914.0,1948.0,34.0
213,Jolee Love in DP Bandits! 2 Sc3,1935,Cumshot,1952.0,1958.0,6.0
213,Jolee Love in DP Bandits! 2 Sc3,1923,BlowJob,2360.0,2416.0,56.0
213,Jolee Love in DP Bandits! 2 Sc3,1924,BlowJob,2452.0,2468.0,16.0
213,Jolee Love in DP Bandits! 2 Sc3,1933,Anal,2654.0,2690.0,36.0
213,Jolee Love in DP Bandits! 2 Sc3,1934,Anal,2758.0,2788.0,30.0
213,Jolee Love in DP Bandits! 2 Sc3,1925,BlowJob,2890.0,2928.0,38.0
214,Jolee Love in Hardcore DAP Creampie,1941,BlowJob,370.0,382.0,12.0
214,Jolee Love in Hardcore DAP Creampie,1944,BlowJob,1780.0,1794.0,14.0
214,Jolee Love in Hardcore DAP Creampie,1945,BlowJob,2056.0,2078.0,22.0
214,Jolee Love in Hardcore DAP Creampie,1947,Pissing,2628.0,2656.0,28.0
215,,2140,Anal,188.0,208.0,20.0
215,,2141,Anal,722.0,742.0,20.0
215,,2144,BlowJob,818.0,834.0,16.0
215,,2145,BlowJob,894.0,932.0,38.0
215,,2143,Anal,1452.0,1464.0,12.0
215,,2146,BlowJob,1640.0,1652.0,12.0
216,,2150,Anal,162.0,188.0,26.0
216,,2151,Anal,222.0,260.0,38.0
216,,2152,Anal,388.0,408.0,20.0
216,,2153,Anal,602.0,620.0,18.0
216,,2154,Cumshot,1830.0,1844.0,14.0
217,,2159,BlowJob,692.0,740.0,48.0
217,,2157,Anal,1220.0,1238.0,18.0
217,,2160,BlowJob,1800.0,1852.0,52.0
217,,2161,BlowJob,2262.0,2278.0,16.0
218,,2162,BlowJob,486.0,504.0,18.0
218,,2165,Anal,788.0,842.0,54.0
218,,2166,Anal,1494.0,1536.0,42.0
218,,2167,Anal,2144.0,2170.0,26.0
218,,2168,Anal,2206.0,2236.0,30.0
218,,2163,BlowJob,2268.0,2280.0,12.0
218,,2169,Cumshot,2300.0,2330.0,30.0
218,,2164,BlowJob,2306.0,2312.0,6.0
219,,2171,BlowJob,66.0,86.0,20.0
219,,2181,BlowJob,444.0,468.0,24.0
219,,2186,Cumshot,450.0,496.0,46.0
219,,2189,Cumshot,528.0,534.0,6.0
219,,2172,BlowJob,584.0,638.0,54.0
219,,2173,BlowJob,682.0,706.0,24.0
219,,2182,BlowJob,1032.0,1038.0,6.0
219,,2190,Cumshot,1074.0,1112.0,38.0
219,,2174,BlowJob,1086.0,1120.0,34.0
219,,2191,Cumshot,1312.0,1324.0,12.0
219,,2176,BlowJob,1382.0,1428.0,46.0
219,,2194,Gangbang,1408.0,1422.0,14.0
219,,2177,BlowJob,1512.0,1532.0,20.0
219,,2178,BlowJob,1634.0,1690.0,56.0
219,,2179,BlowJob,1726.0,1742.0,16.0
219,,2183,BlowJob,1778.0,1820.0,42.0
219,,2192,Cumshot,1820.0,1832.0,12.0
219,,2180,BlowJob,2050.0,2090.0,40.0
219,,2184,BlowJob,2140.0,2172.0,32.0
219,,2193,Cumshot,2176.0,2186.0,10.0
220,,2196,BlowJob,662.0,678.0,16.0
220,,2198,BlowJob,688.0,698.0,10.0
220,,2197,BlowJob,716.0,726.0,10.0
220,,2199,BlowJob,1334.0,1356.0,22.0
220,,2201,Cumshot,2468.0,2478.0,10.0
221,,2202,BlowJob,114.0,158.0,44.0
221,,2203,BlowJob,332.0,382.0,50.0
221,,2205,Gangbang,388.0,398.0,10.0
221,,2206,Cumshot,670.0,696.0,26.0
221,,2204,BlowJob,732.0,772.0,40.0
221,,2210,Cumshot,806.0,834.0,28.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2017,Gangbang,218.0,274.0,56.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2058,BlowJob,256.0,272.0,16.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2018,Gangbang,358.0,376.0,18.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2020,Gangbang,576.0,634.0,58.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2063,Anal,586.0,600.0,14.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2059,BlowJob,592.0,602.0,10.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2064,Anal,746.0,772.0,26.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2065,Anal,822.0,860.0,38.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2021,Gangbang,982.0,994.0,12.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2030,Anal,1046.0,1080.0,34.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2052,Gangbang,1062.0,1116.0,54.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2031,Anal,1140.0,1150.0,10.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2067,Anal,1230.0,1242.0,12.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2023,Gangbang,1506.0,1542.0,36.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2024,Gangbang,1630.0,1678.0,48.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2069,Cumshot,1806.0,1834.0,28.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2034,BlowJob,1808.0,1838.0,30.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2032,Anal,2002.0,2016.0,14.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2055,Gangbang,2076.0,2086.0,10.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2057,Gangbang,2504.0,2534.0,30.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2073,Cumshot,2662.0,2704.0,42.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2074,Cumshot,2770.0,2780.0,10.0
222,"Adriana Chechik, Teanna Trump, Vicki Chase in Star Sc4",2036,BlowJob,2940.0,2988.0,48.0
223,,2212,Cumshot,112.0,122.0,10.0
223,,2215,BlowJob,208.0,218.0,10.0
223,,2218,BlowJob,258.0,274.0,16.0
223,,2216,BlowJob,352.0,406.0,54.0
223,,2220,BlowJob,466.0,488.0,22.0
223,,2231,Gangbang,800.0,812.0,12.0
223,,2222,BlowJob,868.0,884.0,16.0
223,,2228,Grabbing Boobs,1150.0,1178.0,28.0
223,,2223,BlowJob,1164.0,1174.0,10.0
223,,2229,Grabbing Boobs,1250.0,1298.0,48.0
223,,2232,Gangbang,1400.0,1436.0,36.0
223,,2233,Anal,1488.0,1504.0,16.0
223,,2225,BlowJob,1606.0,1616.0,10.0
223,,2230,Grabbing Boobs,2106.0,2130.0,24.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2350,BlowJob,106.0,112.0,6.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2357,BlowJob,310.0,332.0,22.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2235,BlowJob,408.0,444.0,36.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2236,BlowJob,770.0,818.0,48.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2241,BlowJob,886.0,898.0,12.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2244,Cumshot,888.0,894.0,6.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2363,Anal,1358.0,1394.0,36.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2245,Anal,1426.0,1438.0,12.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2353,BlowJob,1610.0,1618.0,8.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2246,Anal,1696.0,1716.0,20.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2243,Grabbing Boobs,1880.0,1928.0,48.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2242,BlowJob,2224.0,2232.0,8.0
224,"Adreena Winters, Ani Black Fox in Ani Black Fox im Spermaland Teil 2 Sc2",2355,BlowJob,2332.0,2340.0,8.0
225,,2247,Grabbing Boobs,26.0,56.0,30.0
225,,2250,BlowJob,746.0,758.0,12.0
225,,2251,BlowJob,806.0,822.0,16.0
225,,2256,Anal,1120.0,1132.0,12.0
225,,2257,Anal,1202.0,1216.0,14.0
225,,2258,Anal,1256.0,1284.0,28.0
225,,2252,BlowJob,1336.0,1356.0,20.0
225,,2259,Anal,1416.0,1430.0,14.0
225,,2260,Anal,1522.0,1544.0,22.0
225,,2249,Grabbing Boobs,1618.0,1646.0,28.0
225,,2253,BlowJob,1754.0,1796.0,42.0
225,,2261,Anal,1918.0,1950.0,32.0
225,,2263,Anal,2238.0,2268.0,30.0
225,,2254,BlowJob,2382.0,2406.0,24.0
225,,2264,Anal,2630.0,2656.0,26.0
225,,2265,Anal,2706.0,2756.0,50.0
225,,2255,BlowJob,2766.0,2814.0,48.0
226,,2266,Grabbing Boobs,26.0,44.0,18.0
226,,2267,Grabbing Boobs,236.0,264.0,28.0
226,,2268,Grabbing Boobs,300.0,320.0,20.0
226,,2281,Titjob,308.0,318.0,10.0
226,,2272,BlowJob,788.0,798.0,10.0
226,,2269,Grabbing Boobs,912.0,940.0,28.0
226,,2273,BlowJob,1074.0,1108.0,34.0
226,,2274,BlowJob,1448.0,1458.0,10.0
226,,2288,Anal,1476.0,1496.0,20.0
226,,2275,BlowJob,1502.0,1542.0,40.0
226,,2283,Cumshot,1600.0,1632.0,32.0
226,,2276,BlowJob,1636.0,1660.0,24.0
226,,2289,Anal,1694.0,1734.0,40.0
226,,2277,BlowJob,1736.0,1778.0,42.0
226,,2278,BlowJob,1878.0,1906.0,28.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2295,Gangbang,638.0,652.0,14.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2377,BlowJob,996.0,1010.0,14.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2298,Anal,1048.0,1064.0,16.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2388,Cumshot,1112.0,1118.0,6.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2382,Anal,1142.0,1160.0,18.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2378,BlowJob,1154.0,1174.0,20.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2379,BlowJob,1220.0,1256.0,36.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2299,Anal,1232.0,1278.0,46.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2385,Anal,1808.0,1818.0,10.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2293,BlowJob,1830.0,1844.0,14.0
227,Adriana Chechik in Horny Housewives 6 Sc2,2386,Anal,1924.0,1936.0,12.0
228,Adira Allure in Airtight Diva Sc4,2332,Grabbing Boobs,178.0,218.0,40.0
228,Adira Allure in Airtight Diva Sc4,2339,Cumshot,798.0,802.0,4.0
228,Adira Allure in Airtight Diva Sc4,2340,Cumshot,854.0,880.0,26.0
228,Adira Allure in Airtight Diva Sc4,2334,BlowJob,872.0,924.0,52.0
228,Adira Allure in Airtight Diva Sc4,2335,BlowJob,1108.0,1148.0,40.0
228,Adira Allure in Airtight Diva Sc4,2336,BlowJob,1184.0,1210.0,26.0
228,Adira Allure in Airtight Diva Sc4,2342,Cumshot,1808.0,1832.0,24.0
228,Adira Allure in Airtight Diva Sc4,2338,BlowJob,1824.0,1834.0,10.0
229,Alexis Tae in Gangbang Sluts Sc1,2533,BlowJob,796.0,828.0,32.0
229,Alexis Tae in Gangbang Sluts Sc1,2534,BlowJob,888.0,898.0,10.0
229,Alexis Tae in Gangbang Sluts Sc1,2524,Gangbang,912.0,936.0,24.0
229,Alexis Tae in Gangbang Sluts Sc1,2535,BlowJob,936.0,972.0,36.0
229,Alexis Tae in Gangbang Sluts Sc1,2536,BlowJob,1050.0,1102.0,52.0
229,Alexis Tae in Gangbang Sluts Sc1,2526,Gangbang,1266.0,1290.0,24.0
229,Alexis Tae in Gangbang Sluts Sc1,2527,Gangbang,1344.0,1384.0,40.0
229,Alexis Tae in Gangbang Sluts Sc1,2528,Gangbang,1538.0,1548.0,10.0
229,Alexis Tae in Gangbang Sluts Sc1,2529,Gangbang,1600.0,1656.0,56.0
229,Alexis Tae in Gangbang Sluts Sc1,2537,BlowJob,1696.0,1708.0,12.0
229,Alexis Tae in Gangbang Sluts Sc1,2530,Gangbang,2018.0,2072.0,54.0
229,Alexis Tae in Gangbang Sluts Sc1,2531,Gangbang,2134.0,2170.0,36.0
229,Alexis Tae in Gangbang Sluts Sc1,2539,Cumshot,2218.0,2256.0,38.0
230,Amirah Adara in DP Bandits! Sc1,2814,BlowJob,1308.0,1354.0,46.0
230,Amirah Adara in DP Bandits! Sc1,2823,Anal,1510.0,1528.0,18.0
230,Amirah Adara in DP Bandits! Sc1,2815,BlowJob,1786.0,1844.0,58.0
230,Amirah Adara in DP Bandits! Sc1,2825,Anal,1856.0,1870.0,14.0
230,Amirah Adara in DP Bandits! Sc1,2816,BlowJob,1914.0,1946.0,32.0
230,Amirah Adara in DP Bandits! Sc1,2817,BlowJob,2078.0,2104.0,26.0
230,Amirah Adara in DP Bandits! Sc1,5061,Anal,2086.0,2132.0,46.0
230,Amirah Adara in DP Bandits! Sc1,5062,Anal,2230.0,2284.0,54.0
230,Amirah Adara in DP Bandits! Sc1,2818,BlowJob,2560.0,2590.0,30.0
230,Amirah Adara in DP Bandits! Sc1,5065,Anal,2592.0,2606.0,14.0
230,Amirah Adara in DP Bandits! Sc1,2819,BlowJob,2840.0,2896.0,56.0
230,Amirah Adara in DP Bandits! Sc1,2827,Anal,2932.0,2948.0,16.0
230,Amirah Adara in DP Bandits! Sc1,2822,BlowJob,3002.0,3022.0,20.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2892,Grabbing Boobs,86.0,110.0,24.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2900,Gangbang,230.0,254.0,24.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2895,BlowJob,744.0,774.0,30.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2901,Gangbang,834.0,878.0,44.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2896,BlowJob,904.0,930.0,26.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2902,Gangbang,1280.0,1332.0,52.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2898,BlowJob,1282.0,1334.0,52.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2893,Grabbing Boobs,1370.0,1410.0,40.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2899,BlowJob,1474.0,1486.0,12.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2904,Cumshot,1630.0,1634.0,4.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2905,Cumshot,1728.0,1752.0,24.0
231,Ana Nova in 6 Black Sticks 1 White Trick Sc1,2906,Cumshot,1822.0,1832.0,10.0
232,Anai Loves in My stepmom,2908,Grabbing Boobs,668.0,710.0,42.0
232,Anai Loves in My stepmom,2916,Titjob,710.0,736.0,26.0
232,Anai Loves in My stepmom,2914,BlowJob,726.0,780.0,54.0
232,Anai Loves in My stepmom,2915,BlowJob,796.0,802.0,6.0
232,Anai Loves in My stepmom,2917,Anal,850.0,908.0,58.0
232,Anai Loves in My stepmom,2918,Anal,1060.0,1098.0,38.0
232,Anai Loves in My stepmom,2910,Grabbing Boobs,1208.0,1240.0,32.0
232,Anai Loves in My stepmom,2919,Anal,1446.0,1460.0,14.0
233,Angela White in Angela's Airtight DP,3062,Anal,488.0,516.0,28.0
233,Angela White in Angela's Airtight DP,3063,Anal,648.0,702.0,54.0
233,Angela White in Angela's Airtight DP,3069,Grabbing Boobs,660.0,686.0,26.0
233,Angela White in Angela's Airtight DP,3064,Anal,750.0,770.0,20.0
233,Angela White in Angela's Airtight DP,3065,Anal,1052.0,1094.0,42.0
233,Angela White in Angela's Airtight DP,3060,BlowJob,1104.0,1140.0,36.0
233,Angela White in Angela's Airtight DP,3070,Grabbing Boobs,1494.0,1516.0,22.0
233,Angela White in Angela's Airtight DP,3067,Anal,2158.0,2170.0,12.0
233,Angela White in Angela's Airtight DP,3068,Anal,2244.0,2264.0,20.0
233,Angela White in Angela's Airtight DP,3071,Cumshot,2480.0,2508.0,28.0
233,Angela White in Angela's Airtight DP,3072,Cumshot,2578.0,2612.0,34.0
233,Angela White in Angela's Airtight DP,3073,Cumshot,2656.0,2674.0,18.0
234,Anissa Kate in Love Everything About Her,3314,BlowJob,192.0,226.0,34.0
234,Anissa Kate in Love Everything About Her,3318,BlowJob,322.0,346.0,24.0
234,Anissa Kate in Love Everything About Her,3316,BlowJob,942.0,954.0,12.0
234,Anissa Kate in Love Everything About Her,3317,BlowJob,1358.0,1368.0,10.0
234,Anissa Kate in Love Everything About Her,3322,Anal,1676.0,1720.0,44.0
235,Anissa Kate in Sizziling Double Penetration Delight,3334,Grabbing Boobs,170.0,212.0,42.0
235,Anissa Kate in Sizziling Double Penetration Delight,3336,BlowJob,330.0,382.0,52.0
235,Anissa Kate in Sizziling Double Penetration Delight,3335,Grabbing Boobs,466.0,518.0,52.0
235,Anissa Kate in Sizziling Double Penetration Delight,3339,BlowJob,470.0,516.0,46.0
235,Anissa Kate in Sizziling Double Penetration Delight,3343,Anal,1102.0,1138.0,36.0
235,Anissa Kate in Sizziling Double Penetration Delight,3341,BlowJob,1192.0,1226.0,34.0
235,Anissa Kate in Sizziling Double Penetration Delight,3342,BlowJob,1536.0,1546.0,10.0
235,Anissa Kate in Sizziling Double Penetration Delight,3344,Anal,1564.0,1596.0,32.0
235,Anissa Kate in Sizziling Double Penetration Delight,3345,Anal,1658.0,1690.0,32.0
235,Anissa Kate in Sizziling Double Penetration Delight,3346,Cumshot,1876.0,1916.0,40.0
236,"Anissa Kate, Olivia Del Rio in Personal Guide Chapter 1",3406,Grabbing Boobs,328.0,386.0,58.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3432,BlowJob,468.0,518.0,50.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3433,BlowJob,784.0,798.0,14.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3438,Anal,884.0,928.0,44.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3439,Anal,964.0,988.0,24.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3440,Anal,1254.0,1274.0,20.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3441,Anal,1540.0,1550.0,10.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3442,Anal,1594.0,1606.0,12.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3436,BlowJob,1632.0,1644.0,12.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3437,BlowJob,2168.0,2194.0,26.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3434,BlowJob,2204.0,2256.0,52.0
237,"Anne Castro, Belinha Baracho, Claudia Bella in Anal Integrity Sc5",3444,Cumshot,2264.0,2274.0,10.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3888,Gangbang,64.0,98.0,34.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3891,BlowJob,556.0,588.0,32.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3892,BlowJob,814.0,858.0,44.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3893,BlowJob,906.0,956.0,50.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3894,BlowJob,1034.0,1052.0,18.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3895,BlowJob,1104.0,1152.0,48.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3896,BlowJob,1354.0,1374.0,20.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3899,Anal,2380.0,2414.0,34.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3900,Anal,2458.0,2476.0,18.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3901,Anal,2578.0,2610.0,32.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3902,Cumshot,2780.0,2812.0,32.0
238,Asa Akira in The Greatest Gangbangs Ever! Sc1,3903,Cumshot,2864.0,2898.0,34.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3905,BlowJob,114.0,132.0,18.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3906,BlowJob,368.0,412.0,44.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3912,Cumshot,376.0,402.0,26.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3908,BlowJob,620.0,654.0,34.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3909,BlowJob,734.0,754.0,20.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3910,BlowJob,870.0,906.0,36.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3911,BlowJob,1108.0,1116.0,8.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3914,Cumshot,1128.0,1136.0,8.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3917,Cumshot,1460.0,1492.0,32.0
239,"Asia Deville, Sai Tai Tiger in Sperma Gang Bang Sc6",3918,Cumshot,1574.0,1598.0,24.0
240,Assh Lee in All Over That Cock,3964,Anal,114.0,130.0,16.0
240,Assh Lee in All Over That Cock,3966,BlowJob,790.0,830.0,40.0
240,Assh Lee in All Over That Cock,3969,Cumshot,1436.0,1440.0,4.0
240,Assh Lee in All Over That Cock,3967,BlowJob,1470.0,1480.0,10.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3922,BlowJob,276.0,286.0,10.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3924,BlowJob,364.0,422.0,58.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3928,Anal,570.0,582.0,12.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3925,BlowJob,1124.0,1154.0,30.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3926,BlowJob,1298.0,1308.0,10.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3932,Anal,1314.0,1330.0,16.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3929,Anal,1372.0,1424.0,52.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3927,BlowJob,1434.0,1458.0,24.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3930,Anal,1568.0,1584.0,16.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3934,Anal,1610.0,1626.0,16.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3935,Anal,1706.0,1746.0,40.0
241,"August Taylor, Reena Sky in Busty Cops On Patrol 3 Sc4",3936,Cumshot,1768.0,1810.0,42.0
242,Baby Gemini in Ricky's Room Blowbang,3937,Gangbang,62.0,84.0,22.0
242,Baby Gemini in Ricky's Room Blowbang,3938,Gangbang,124.0,140.0,16.0
242,Baby Gemini in Ricky's Room Blowbang,3939,Gangbang,238.0,254.0,16.0
242,Baby Gemini in Ricky's Room Blowbang,3940,Gangbang,300.0,350.0,50.0
242,Baby Gemini in Ricky's Room Blowbang,3941,Gangbang,594.0,638.0,44.0
242,Baby Gemini in Ricky's Room Blowbang,3942,Gangbang,694.0,722.0,28.0
242,Baby Gemini in Ricky's Room Blowbang,3943,Gangbang,764.0,806.0,42.0
242,Baby Gemini in Ricky's Room Blowbang,3944,Gangbang,864.0,898.0,34.0
243,Barbie Sins in DAP with Creampie,3977,Anal,652.0,672.0,20.0
243,Barbie Sins in DAP with Creampie,3978,Anal,704.0,722.0,18.0
243,Barbie Sins in DAP with Creampie,3971,BlowJob,770.0,782.0,12.0
243,Barbie Sins in DAP with Creampie,3972,BlowJob,946.0,980.0,34.0
243,Barbie Sins in DAP with Creampie,3973,BlowJob,1390.0,1440.0,50.0
243,Barbie Sins in DAP with Creampie,3981,Anal,1498.0,1542.0,44.0
243,Barbie Sins in DAP with Creampie,3982,Anal,1624.0,1648.0,24.0
243,Barbie Sins in DAP with Creampie,3984,Anal,1888.0,1926.0,38.0
243,Barbie Sins in DAP with Creampie,3985,Anal,1968.0,2002.0,34.0
243,Barbie Sins in DAP with Creampie,3974,BlowJob,1970.0,2012.0,42.0
243,Barbie Sins in DAP with Creampie,3986,Anal,2172.0,2216.0,44.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4014,Gangbang,180.0,220.0,40.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4015,Gangbang,374.0,398.0,24.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4024,Anal,532.0,572.0,40.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4025,Anal,666.0,676.0,10.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4003,BlowJob,882.0,896.0,14.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4027,Anal,1158.0,1214.0,56.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4004,BlowJob,1216.0,1226.0,10.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4038,Cumshot,1312.0,1320.0,8.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4040,Cumshot,1334.0,1344.0,10.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4005,BlowJob,1360.0,1394.0,34.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4017,Gangbang,1488.0,1502.0,14.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4028,Anal,1642.0,1676.0,34.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4007,BlowJob,1678.0,1692.0,14.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4018,Gangbang,1792.0,1802.0,10.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4019,Gangbang,1878.0,1904.0,26.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4030,Anal,2288.0,2334.0,46.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4008,BlowJob,2304.0,2330.0,26.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4031,Anal,2514.0,2536.0,22.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4021,Gangbang,2638.0,2666.0,28.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4009,BlowJob,2642.0,2684.0,42.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4032,Anal,2646.0,2686.0,40.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4022,Gangbang,2812.0,2828.0,16.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4034,Anal,2848.0,2862.0,14.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4013,BlowJob,3046.0,3056.0,10.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4035,Anal,3554.0,3590.0,36.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4036,Anal,3672.0,3700.0,28.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4037,Anal,3978.0,3990.0,12.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4012,BlowJob,4124.0,4138.0,14.0
244,Belinha Baracho in Hardcore 5on1 Gangbang,4039,Cumshot,4200.0,4204.0,4.0
245,Belinha Baracho in Intense 5on1 Gangbang,4041,Grabbing Boobs,6.0,28.0,22.0
245,Belinha Baracho in Intense 5on1 Gangbang,4042,Grabbing Boobs,142.0,176.0,34.0
245,Belinha Baracho in Intense 5on1 Gangbang,4050,Anal,482.0,520.0,38.0
245,Belinha Baracho in Intense 5on1 Gangbang,4057,Gangbang,608.0,632.0,24.0
245,Belinha Baracho in Intense 5on1 Gangbang,4051,Anal,610.0,662.0,52.0
245,Belinha Baracho in Intense 5on1 Gangbang,4044,BlowJob,624.0,656.0,32.0
245,Belinha Baracho in Intense 5on1 Gangbang,4058,Gangbang,720.0,758.0,38.0
245,Belinha Baracho in Intense 5on1 Gangbang,4045,BlowJob,746.0,772.0,26.0
245,Belinha Baracho in Intense 5on1 Gangbang,4052,Anal,758.0,796.0,38.0
245,Belinha Baracho in Intense 5on1 Gangbang,4059,Gangbang,932.0,984.0,52.0
245,Belinha Baracho in Intense 5on1 Gangbang,4060,Gangbang,1212.0,1232.0,20.0
245,Belinha Baracho in Intense 5on1 Gangbang,4061,Gangbang,1362.0,1380.0,18.0
245,Belinha Baracho in Intense 5on1 Gangbang,4046,BlowJob,1686.0,1722.0,36.0
245,Belinha Baracho in Intense 5on1 Gangbang,4048,BlowJob,1992.0,2004.0,12.0
245,Belinha Baracho in Intense 5on1 Gangbang,4049,BlowJob,2044.0,2058.0,14.0
245,Belinha Baracho in Intense 5on1 Gangbang,4063,Gangbang,2282.0,2304.0,22.0
245,Belinha Baracho in Intense 5on1 Gangbang,4064,Gangbang,2350.0,2372.0,22.0
245,Belinha Baracho in Intense 5on1 Gangbang,4054,Anal,2858.0,2878.0,20.0
245,Belinha Baracho in Intense 5on1 Gangbang,4055,Anal,2994.0,3028.0,34.0
245,Belinha Baracho in Intense 5on1 Gangbang,4056,Anal,3184.0,3194.0,10.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3946,Anal,1076.0,1128.0,52.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3956,BlowJob,1256.0,1286.0,30.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3947,Anal,1294.0,1336.0,42.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3949,Anal,1996.0,2038.0,42.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3951,Anal,2370.0,2428.0,58.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3952,Anal,2478.0,2508.0,30.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3957,BlowJob,2996.0,3028.0,32.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3953,Anal,3056.0,3080.0,24.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3958,BlowJob,3092.0,3112.0,20.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3954,Anal,3256.0,3274.0,18.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3959,BlowJob,3312.0,3352.0,40.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3960,BlowJob,3394.0,3418.0,24.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3961,BlowJob,3638.0,3656.0,18.0
246,"Belinha Baracho, Eva Perez in Assfucked Together",3962,BlowJob,3816.0,3858.0,42.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3987,Cumshot,1354.0,1374.0,20.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3988,Gangbang,1708.0,1720.0,12.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3991,BlowJob,1926.0,1958.0,32.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3994,Anal,2090.0,2120.0,30.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3995,Anal,2276.0,2292.0,16.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3996,Anal,2328.0,2376.0,48.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3989,Gangbang,2706.0,2720.0,14.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3998,Anal,3830.0,3846.0,16.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3992,BlowJob,3834.0,3848.0,14.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3999,Anal,3878.0,3916.0,38.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",3990,Gangbang,3894.0,3908.0,14.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",4000,Anal,4006.0,4020.0,14.0
247,"Belinha Baracho, Polly Petrova in Fucked by Big Cocks",4001,Anal,4136.0,4146.0,10.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4065,Grabbing Boobs,26.0,44.0,18.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4066,Anal,232.0,264.0,32.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4067,Anal,478.0,502.0,24.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4068,Anal,966.0,1006.0,40.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4078,BlowJob,1160.0,1216.0,56.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4082,Cumshot,1596.0,1618.0,22.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4070,Anal,2034.0,2072.0,38.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4071,Anal,2138.0,2170.0,32.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4072,Anal,2210.0,2266.0,56.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4079,BlowJob,2484.0,2542.0,58.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4080,BlowJob,2690.0,2706.0,16.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4081,BlowJob,2864.0,2874.0,10.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4074,Anal,3228.0,3262.0,34.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4075,Anal,3312.0,3348.0,36.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4076,Anal,3396.0,3416.0,20.0
248,"Belinha Baracho, Sandy Cortez in 2on2 Orgy",4077,Anal,3456.0,3484.0,28.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4084,BlowJob,696.0,738.0,42.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4083,Grabbing Boobs,886.0,924.0,38.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4085,BlowJob,1008.0,1018.0,10.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4086,BlowJob,1050.0,1060.0,10.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4087,BlowJob,1206.0,1218.0,12.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4089,BlowJob,1932.0,1952.0,20.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4090,BlowJob,2688.0,2708.0,20.0
249,"Bella Reese, Diamond Kitty, Kayla Carrera in Boldly Girls Cumlouder",4091,Cumshot,2928.0,2978.0,50.0
250,,2302,Gangbang,368.0,380.0,12.0
250,,2303,Gangbang,426.0,456.0,30.0
250,,2304,Gangbang,588.0,620.0,32.0
250,,2306,Gangbang,1028.0,1058.0,30.0
250,,2324,Grabbing Boobs,1182.0,1200.0,18.0
250,,2326,Anal,1224.0,1256.0,32.0
250,,2307,Gangbang,1230.0,1252.0,22.0
250,,2308,Gangbang,1308.0,1354.0,46.0
250,,2309,Gangbang,1602.0,1614.0,12.0
250,,2310,Gangbang,1646.0,1696.0,50.0
250,,2311,Gangbang,1736.0,1760.0,24.0
250,,2327,Anal,1802.0,1836.0,34.0
250,,2312,Gangbang,1906.0,1922.0,16.0
250,,2328,Anal,2064.0,2084.0,20.0
250,,2313,Gangbang,2158.0,2188.0,30.0
250,,2329,Anal,3432.0,3450.0,18.0
250,,2330,Anal,3514.0,3532.0,18.0
250,,2319,BlowJob,5992.0,6014.0,22.0
250,,2320,BlowJob,6062.0,6098.0,36.0
250,,2325,Grabbing Boobs,6140.0,6184.0,44.0
250,,2321,BlowJob,6160.0,6186.0,26.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4094,BlowJob,976.0,996.0,20.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4101,Anal,1360.0,1398.0,38.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4102,Anal,1552.0,1596.0,44.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4095,BlowJob,1756.0,1778.0,22.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4103,Anal,1782.0,1830.0,48.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4104,Anal,1934.0,1980.0,46.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4096,BlowJob,2416.0,2436.0,20.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4097,BlowJob,2642.0,2680.0,38.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4098,BlowJob,2714.0,2726.0,12.0
251,"Bobbi Starr, Monica Santhiago in Anal Integrity Sc3",4107,Anal,2748.0,2764.0,16.0
252,Bonny Bon in Sexual Rage 2 Sc3,4109,Anal,570.0,610.0,40.0
252,Bonny Bon in Sexual Rage 2 Sc3,4110,Anal,724.0,758.0,34.0
252,Bonny Bon in Sexual Rage 2 Sc3,4111,Anal,930.0,952.0,22.0
252,Bonny Bon in Sexual Rage 2 Sc3,4112,Anal,986.0,1018.0,32.0
252,Bonny Bon in Sexual Rage 2 Sc3,4113,Anal,1112.0,1152.0,40.0
252,Bonny Bon in Sexual Rage 2 Sc3,4117,BlowJob,1120.0,1156.0,36.0
252,Bonny Bon in Sexual Rage 2 Sc3,4114,Anal,1450.0,1474.0,24.0
252,Bonny Bon in Sexual Rage 2 Sc3,4120,Cumshot,1952.0,1960.0,8.0
252,Bonny Bon in Sexual Rage 2 Sc3,4119,BlowJob,1962.0,1994.0,32.0
253,Cali Caliente in The Gangbang Part IV,4121,BlowJob,172.0,214.0,42.0
253,Cali Caliente in The Gangbang Part IV,4129,Gangbang,832.0,878.0,46.0
253,Cali Caliente in The Gangbang Part IV,4133,Anal,1014.0,1036.0,22.0
253,Cali Caliente in The Gangbang Part IV,4124,BlowJob,1214.0,1246.0,32.0
253,Cali Caliente in The Gangbang Part IV,4134,Anal,1342.0,1376.0,34.0
253,Cali Caliente in The Gangbang Part IV,4135,Anal,1460.0,1502.0,42.0
253,Cali Caliente in The Gangbang Part IV,4136,Anal,1562.0,1602.0,40.0
253,Cali Caliente in The Gangbang Part IV,4137,Anal,1668.0,1690.0,22.0
253,Cali Caliente in The Gangbang Part IV,4131,Gangbang,1756.0,1788.0,32.0
253,Cali Caliente in The Gangbang Part IV,4126,BlowJob,1844.0,1886.0,42.0
253,Cali Caliente in The Gangbang Part IV,4138,Anal,1922.0,1934.0,12.0
253,Cali Caliente in The Gangbang Part IV,4127,BlowJob,1956.0,1970.0,14.0
253,Cali Caliente in The Gangbang Part IV,4128,BlowJob,2048.0,2060.0,12.0
253,Cali Caliente in The Gangbang Part IV,4139,Cumshot,2118.0,2128.0,10.0
253,Cali Caliente in The Gangbang Part IV,4140,Cumshot,2206.0,2222.0,16.0
254,Carla Morelli in Gangbang with 4 Cocks,4142,BlowJob,862.0,902.0,40.0
254,Carla Morelli in Gangbang with 4 Cocks,4145,BlowJob,1556.0,1600.0,44.0
254,Carla Morelli in Gangbang with 4 Cocks,4143,BlowJob,1646.0,1668.0,22.0
254,Carla Morelli in Gangbang with 4 Cocks,4144,BlowJob,1722.0,1734.0,12.0
254,Carla Morelli in Gangbang with 4 Cocks,4146,Cumshot,1760.0,1796.0,36.0
255,Carla Morelli in Hot for Teacher,1936,Grabbing Boobs,98.0,140.0,42.0
255,Carla Morelli in Hot for Teacher,1937,BlowJob,226.0,274.0,48.0
255,Carla Morelli in Hot for Teacher,1939,Cumshot,1294.0,1308.0,14.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4152,Grabbing Boobs,552.0,594.0,42.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4154,BlowJob,752.0,766.0,14.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4164,Anal,1074.0,1104.0,30.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4156,BlowJob,1244.0,1258.0,14.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4165,Anal,1448.0,1470.0,22.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4166,Anal,1754.0,1782.0,28.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4157,BlowJob,1822.0,1880.0,58.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4167,Anal,2000.0,2022.0,22.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4158,BlowJob,2108.0,2150.0,42.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4173,Gangbang,2260.0,2274.0,14.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4160,BlowJob,2406.0,2454.0,48.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4169,Anal,3170.0,3206.0,36.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4174,Gangbang,3178.0,3188.0,10.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4153,Grabbing Boobs,3536.0,3568.0,32.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4155,BlowJob,3668.0,3686.0,18.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4162,BlowJob,3772.0,3812.0,40.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4171,Anal,3908.0,3922.0,14.0
256,Cherie DeVille in Lewood Gangbang Battle of the MILFs 2 Sc1,4163,BlowJob,3952.0,3962.0,10.0
257,Chloe Amour in Mon Amour Sc1,4181,BlowJob,292.0,302.0,10.0
257,Chloe Amour in Mon Amour Sc1,4182,BlowJob,504.0,546.0,42.0
257,Chloe Amour in Mon Amour Sc1,4183,BlowJob,618.0,650.0,32.0
257,Chloe Amour in Mon Amour Sc1,4189,BlowJob,906.0,920.0,14.0
257,Chloe Amour in Mon Amour Sc1,4185,BlowJob,1020.0,1054.0,34.0
257,Chloe Amour in Mon Amour Sc1,4190,DP,1100.0,1110.0,10.0
257,Chloe Amour in Mon Amour Sc1,4186,BlowJob,1110.0,1122.0,12.0
257,Chloe Amour in Mon Amour Sc1,4191,DP,1220.0,1248.0,28.0
257,Chloe Amour in Mon Amour Sc1,4192,DP,1292.0,1310.0,18.0
257,Chloe Amour in Mon Amour Sc1,4188,BlowJob,1568.0,1608.0,40.0
257,Chloe Amour in Mon Amour Sc1,4193,DP,1848.0,1868.0,20.0
257,Chloe Amour in Mon Amour Sc1,4179,Anal,1958.0,1974.0,16.0
257,Chloe Amour in Mon Amour Sc1,4198,Gangbang,2004.0,2028.0,24.0
257,Chloe Amour in Mon Amour Sc1,4196,Cumshot,2132.0,2140.0,8.0
258,Chloe Amour in Mon Amour Sc2,4200,BlowJob,372.0,382.0,10.0
258,Chloe Amour in Mon Amour Sc2,4201,BlowJob,614.0,644.0,30.0
258,Chloe Amour in Mon Amour Sc2,4202,BlowJob,738.0,794.0,56.0
259,Chloe Amour in Mon Amour Sc3,4205,Gangbang,164.0,190.0,26.0
259,Chloe Amour in Mon Amour Sc3,4206,Gangbang,242.0,260.0,18.0
259,Chloe Amour in Mon Amour Sc3,4207,Gangbang,442.0,456.0,14.0
259,Chloe Amour in Mon Amour Sc3,4208,Gangbang,548.0,578.0,30.0
259,Chloe Amour in Mon Amour Sc3,4215,BlowJob,918.0,974.0,56.0
259,Chloe Amour in Mon Amour Sc3,4209,Gangbang,944.0,980.0,36.0
259,Chloe Amour in Mon Amour Sc3,4210,Gangbang,1352.0,1372.0,20.0
259,Chloe Amour in Mon Amour Sc3,4228,DP,1392.0,1420.0,28.0
259,Chloe Amour in Mon Amour Sc3,4211,Gangbang,2184.0,2202.0,18.0
259,Chloe Amour in Mon Amour Sc3,4212,Gangbang,2468.0,2492.0,24.0
259,Chloe Amour in Mon Amour Sc3,4213,Gangbang,2804.0,2828.0,24.0
259,Chloe Amour in Mon Amour Sc3,4226,Cumshot,2960.0,2988.0,28.0
260,"Chloe Amour, Jennifer White in Mon Amour Sc4",151,BlowJob,230.0,244.0,14.0
260,"Chloe Amour, Jennifer White in Mon Amour Sc4",153,BlowJob,412.0,432.0,20.0
260,"Chloe Amour, Jennifer White in Mon Amour Sc4",157,Cumshot,2310.0,2324.0,14.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,159,Grabbing Boobs,308.0,348.0,40.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,161,BlowJob,784.0,820.0,36.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,172,BlowJob,872.0,908.0,36.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,174,Gangbang,994.0,1008.0,14.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,164,BlowJob,1610.0,1646.0,36.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,165,BlowJob,2016.0,2060.0,44.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,166,BlowJob,2132.0,2144.0,12.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,168,BlowJob,2498.0,2514.0,16.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,178,Anal,2508.0,2556.0,48.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,173,BlowJob,2654.0,2660.0,6.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,176,Gangbang,3258.0,3308.0,50.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,169,BlowJob,3414.0,3426.0,12.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,170,BlowJob,3466.0,3500.0,34.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,180,Anal,3720.0,3730.0,10.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,181,Anal,3814.0,3864.0,50.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,171,BlowJob,3856.0,3914.0,58.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,182,Cumshot,3944.0,3990.0,46.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,184,Cumshot,4024.0,4072.0,48.0
261,Christy Love in Lewood Gangbang Battle of the MILFs 3 Sc2,185,Cumshot,4110.0,4132.0,22.0
262,"Cindy Starfall, Gaia in Swappers Sc1",213,Grabbing Boobs,200.0,242.0,42.0
262,"Cindy Starfall, Gaia in Swappers Sc1",215,BlowJob,842.0,852.0,10.0
262,"Cindy Starfall, Gaia in Swappers Sc1",218,69,876.0,906.0,30.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,186,Grabbing Boobs,116.0,146.0,30.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,188,BlowJob,214.0,252.0,38.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,189,BlowJob,332.0,348.0,16.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,195,Anal,1414.0,1466.0,52.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,194,BlowJob,1860.0,1868.0,8.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,187,Grabbing Boobs,1868.0,1914.0,46.0
263,Cookie Cream in Asian 1st BBG Threesome & DP,196,Cumshot,1950.0,1956.0,6.0
265,"Danielle Renee, MarsFoxxx in Group Bang",235,Gangbang,230.0,262.0,32.0
265,"Danielle Renee, MarsFoxxx in Group Bang",224,BlowJob,380.0,392.0,12.0
265,"Danielle Renee, MarsFoxxx in Group Bang",236,Gangbang,396.0,428.0,32.0
265,"Danielle Renee, MarsFoxxx in Group Bang",243,Anal,416.0,426.0,10.0
265,"Danielle Renee, MarsFoxxx in Group Bang",226,BlowJob,648.0,700.0,52.0
265,"Danielle Renee, MarsFoxxx in Group Bang",238,Gangbang,684.0,706.0,22.0
265,"Danielle Renee, MarsFoxxx in Group Bang",244,Anal,814.0,842.0,28.0
265,"Danielle Renee, MarsFoxxx in Group Bang",245,Anal,920.0,930.0,10.0
265,"Danielle Renee, MarsFoxxx in Group Bang",228,BlowJob,942.0,968.0,26.0
265,"Danielle Renee, MarsFoxxx in Group Bang",246,Cumshot,1072.0,1092.0,20.0
265,"Danielle Renee, MarsFoxxx in Group Bang",229,BlowJob,1160.0,1206.0,46.0
265,"Danielle Renee, MarsFoxxx in Group Bang",241,Grabbing Boobs,1252.0,1282.0,30.0
265,"Danielle Renee, MarsFoxxx in Group Bang",230,BlowJob,1326.0,1354.0,28.0
265,"Danielle Renee, MarsFoxxx in Group Bang",239,Gangbang,1330.0,1350.0,20.0
265,"Danielle Renee, MarsFoxxx in Group Bang",247,Cumshot,1494.0,1504.0,10.0
265,"Danielle Renee, MarsFoxxx in Group Bang",248,Cumshot,1588.0,1594.0,6.0
265,"Danielle Renee, MarsFoxxx in Group Bang",231,BlowJob,1662.0,1704.0,42.0
265,"Danielle Renee, MarsFoxxx in Group Bang",232,BlowJob,1874.0,1916.0,42.0
265,"Danielle Renee, MarsFoxxx in Group Bang",242,Grabbing Boobs,1908.0,1942.0,34.0
265,"Danielle Renee, MarsFoxxx in Group Bang",233,BlowJob,2442.0,2464.0,22.0
265,"Danielle Renee, MarsFoxxx in Group Bang",240,Gangbang,2482.0,2508.0,26.0
266,Daria Glower in Die Haremsw√§chterin des √ñl Scheichs Sc6,250,BlowJob,782.0,804.0,22.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,251,Grabbing Boobs,84.0,114.0,30.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,254,BlowJob,590.0,614.0,24.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,265,Cumshot,1372.0,1376.0,4.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,257,BlowJob,1412.0,1428.0,16.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,252,Grabbing Boobs,1458.0,1512.0,54.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,269,Anal,2194.0,2208.0,14.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,260,BlowJob,2660.0,2676.0,16.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,261,BlowJob,2716.0,2730.0,14.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,271,Anal,2746.0,2798.0,52.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,262,BlowJob,3350.0,3404.0,54.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,263,BlowJob,3520.0,3530.0,10.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,264,BlowJob,3658.0,3694.0,36.0
267,Dee Williams in Lewood Gangbang Battle of the Milfs 4 Sc2,266,Cumshot,3696.0,3718.0,22.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1858,BlowJob,176.0,214.0,38.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1859,BlowJob,382.0,410.0,28.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1860,BlowJob,460.0,476.0,16.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1861,BlowJob,560.0,592.0,32.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1856,Grabbing Boobs,638.0,666.0,28.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1862,BlowJob,650.0,680.0,30.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1863,BlowJob,890.0,902.0,12.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1857,Grabbing Boobs,1016.0,1074.0,58.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1864,BlowJob,1064.0,1094.0,30.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1866,BlowJob,1310.0,1316.0,6.0
268,EPORNER.COM in [fUVDpObpmPW] Anissa Kate,1868,Cumshot,1572.0,1592.0,20.0
269,Emmanuelle Noire in Busty Ebony Beauty,132,Grabbing Boobs,214.0,236.0,22.0
269,Emmanuelle Noire in Busty Ebony Beauty,140,BlowJob,342.0,354.0,12.0
269,Emmanuelle Noire in Busty Ebony Beauty,141,BlowJob,482.0,486.0,4.0
269,Emmanuelle Noire in Busty Ebony Beauty,134,BlowJob,546.0,564.0,18.0
269,Emmanuelle Noire in Busty Ebony Beauty,133,Grabbing Boobs,574.0,608.0,34.0
269,Emmanuelle Noire in Busty Ebony Beauty,135,BlowJob,924.0,936.0,12.0
269,Emmanuelle Noire in Busty Ebony Beauty,143,Anal,1124.0,1184.0,60.0
269,Emmanuelle Noire in Busty Ebony Beauty,136,BlowJob,1172.0,1204.0,32.0
269,Emmanuelle Noire in Busty Ebony Beauty,144,Anal,1216.0,1234.0,18.0
269,Emmanuelle Noire in Busty Ebony Beauty,137,BlowJob,1242.0,1260.0,18.0
269,Emmanuelle Noire in Busty Ebony Beauty,145,Anal,1420.0,1430.0,10.0
269,Emmanuelle Noire in Busty Ebony Beauty,138,BlowJob,1614.0,1636.0,22.0
269,Emmanuelle Noire in Busty Ebony Beauty,139,BlowJob,1688.0,1700.0,12.0
269,Emmanuelle Noire in Busty Ebony Beauty,142,BlowJob,1726.0,1730.0,4.0
269,Emmanuelle Noire in Busty Ebony Beauty,146,Cumshot,1976.0,2002.0,26.0
269,Emmanuelle Noire in Busty Ebony Beauty,148,Cumshot,2040.0,2048.0,8.0
270,Francesca Le in Lewood Gangbang Battle of the MILFs Sc1,149,BlowJob,162.0,210.0,48.0
271,Francesca Le in Lewood Gangbang Battle of the MILFs Sc2,274,BlowJob,52.0,72.0,20.0
271,Francesca Le in Lewood Gangbang Battle of the MILFs Sc2,275,BlowJob,214.0,228.0,14.0
271,Francesca Le in Lewood Gangbang Battle of the MILFs Sc2,276,Anal,278.0,296.0,18.0
272,Francesca Le in Lewood Gangbang Battle of the MILFs Sc3,278,69,136.0,148.0,12.0
273,Francesca Le in Lewood Gangbang Battle of the MILFs Sc4,279,BlowJob,16.0,48.0,32.0
273,Francesca Le in Lewood Gangbang Battle of the MILFs Sc4,282,Anal,130.0,140.0,10.0
273,Francesca Le in Lewood Gangbang Battle of the MILFs Sc4,280,BlowJob,178.0,202.0,24.0
273,Francesca Le in Lewood Gangbang Battle of the MILFs Sc4,281,BlowJob,330.0,342.0,12.0
274,Francesca Le in Lewood Gangbang Battle of the MILFs Sc5,283,BlowJob,136.0,160.0,24.0
274,Francesca Le in Lewood Gangbang Battle of the MILFs Sc5,284,Grabbing Boobs,224.0,252.0,28.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,300,Gangbang,162.0,190.0,28.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,285,BlowJob,166.0,188.0,22.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,286,BlowJob,222.0,238.0,16.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,312,Cumshot,230.0,236.0,6.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,287,BlowJob,274.0,296.0,22.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,288,BlowJob,342.0,364.0,22.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,302,Gangbang,510.0,562.0,52.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,289,BlowJob,512.0,534.0,22.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,303,Gangbang,660.0,692.0,32.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,291,BlowJob,982.0,994.0,12.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,306,Gangbang,1230.0,1256.0,26.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,316,Anal,1468.0,1508.0,40.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,307,Gangbang,1480.0,1528.0,48.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,292,BlowJob,1514.0,1540.0,26.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,317,Anal,1560.0,1580.0,20.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,293,BlowJob,1582.0,1612.0,30.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,294,BlowJob,1670.0,1684.0,14.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,318,Anal,1684.0,1712.0,28.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,319,Anal,1764.0,1814.0,50.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,308,Gangbang,1814.0,1836.0,22.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,295,BlowJob,1920.0,1944.0,24.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,322,DP,2300.0,2324.0,24.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,310,Gangbang,2318.0,2350.0,32.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,311,Gangbang,2692.0,2706.0,14.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,299,BlowJob,2744.0,2750.0,6.0
275,Francesca Le in Lewood Gangbang Battle of the MILFs Sc6,298,BlowJob,2852.0,2872.0,20.0
276,"Gaia in Screw My Wife, Please 76 Sc2",323,BlowJob,184.0,230.0,46.0
277,Gaia in Throated 39 Sc5,324,Grabbing Boobs,254.0,298.0,44.0
277,Gaia in Throated 39 Sc5,326,BlowJob,350.0,362.0,12.0
277,Gaia in Throated 39 Sc5,332,BlowJob,378.0,406.0,28.0
277,Gaia in Throated 39 Sc5,335,Cumshot,586.0,596.0,10.0
277,Gaia in Throated 39 Sc5,334,BlowJob,702.0,752.0,50.0
277,Gaia in Throated 39 Sc5,329,BlowJob,1080.0,1140.0,60.0
277,Gaia in Throated 39 Sc5,330,BlowJob,1302.0,1356.0,54.0
277,Gaia in Throated 39 Sc5,325,Grabbing Boobs,1346.0,1382.0,36.0
277,Gaia in Throated 39 Sc5,331,BlowJob,1530.0,1568.0,38.0
278,Hannah Jo in Thick Dick Threesome,340,BlowJob,968.0,978.0,10.0
278,Hannah Jo in Thick Dick Threesome,341,BlowJob,1036.0,1046.0,10.0
278,Hannah Jo in Thick Dick Threesome,342,BlowJob,1140.0,1174.0,34.0
278,Hannah Jo in Thick Dick Threesome,343,BlowJob,1212.0,1230.0,18.0
278,Hannah Jo in Thick Dick Threesome,344,BlowJob,1364.0,1374.0,10.0
278,Hannah Jo in Thick Dick Threesome,345,BlowJob,1892.0,1910.0,18.0
278,Hannah Jo in Thick Dick Threesome,346,BlowJob,2038.0,2094.0,56.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,348,Grabbing Boobs,50.0,80.0,30.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,349,Grabbing Boobs,378.0,402.0,24.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,350,Grabbing Boobs,638.0,658.0,20.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,353,BlowJob,916.0,932.0,16.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,354,BlowJob,1060.0,1074.0,14.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,370,Anal,1084.0,1116.0,32.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,355,BlowJob,1124.0,1166.0,42.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,356,BlowJob,1414.0,1430.0,16.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,357,BlowJob,1514.0,1524.0,10.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,358,BlowJob,1726.0,1780.0,54.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,360,BlowJob,2150.0,2180.0,30.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,376,Gangbang,2574.0,2620.0,46.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,351,Grabbing Boobs,2706.0,2738.0,32.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,371,Anal,2750.0,2760.0,10.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,372,Anal,2814.0,2836.0,22.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,373,Anal,2892.0,2934.0,42.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,352,Grabbing Boobs,2916.0,2954.0,38.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,374,Anal,3048.0,3102.0,54.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,364,BlowJob,3074.0,3114.0,40.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,366,BlowJob,3646.0,3690.0,44.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,369,BlowJob,4148.0,4180.0,32.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,377,Cumshot,4236.0,4294.0,58.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,378,Cumshot,4354.0,4362.0,8.0
279,India Summer in Lewood Gangbang Battle of the MILFs 2 Sc2,379,Cumshot,4424.0,4466.0,42.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",380,BlowJob,266.0,294.0,28.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",385,BlowJob,312.0,318.0,6.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",386,Gangbang,336.0,346.0,10.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",382,BlowJob,442.0,474.0,32.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",387,Gangbang,488.0,514.0,26.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",5011,BlowJob,576.0,586.0,10.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",5012,BlowJob,744.0,792.0,48.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",383,BlowJob,834.0,846.0,12.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",388,Gangbang,864.0,894.0,30.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",5014,BlowJob,900.0,922.0,22.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",393,Cumshot,1224.0,1248.0,24.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",5015,BlowJob,1260.0,1300.0,40.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",400,Cumshot,1280.0,1302.0,22.0
280,"Jacky, Sai Tai Tiger in Sperma Gang Bang Sc2",401,Grabbing Boobs,1308.0,1332.0,24.0
281,Jada Fire in Assault That Ass 8 Sc3,404,BlowJob,338.0,372.0,34.0
281,Jada Fire in Assault That Ass 8 Sc3,405,BlowJob,416.0,470.0,54.0
281,Jada Fire in Assault That Ass 8 Sc3,408,Cumshot,472.0,476.0,4.0
281,Jada Fire in Assault That Ass 8 Sc3,406,BlowJob,912.0,942.0,30.0
281,Jada Fire in Assault That Ass 8 Sc3,407,BlowJob,1082.0,1124.0,42.0
281,Jada Fire in Assault That Ass 8 Sc3,409,Cumshot,1650.0,1654.0,4.0
282,Jada Fire in Throat Yogurt 2 Sc1,412,Cumshot,126.0,182.0,56.0
282,Jada Fire in Throat Yogurt 2 Sc1,413,Cumshot,788.0,826.0,38.0
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,417,BlowJob,886.0,928.0,42.0
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,418,BlowJob,980.0,1022.0,42.0
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,419,BlowJob,1116.0,1136.0,20.0
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,420,BlowJob,1350.0,1382.0,32.0
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,421,Anal,1522.0,1538.0,16.0
283,Jasmine Webb in We Fuck Black Girls 9 Sc3,422,Cumshot,1696.0,1738.0,42.0
284,Jasminy Villar in The Stepfather And His Four Friends,423,Anal,194.0,230.0,36.0
284,Jasminy Villar in The Stepfather And His Four Friends,434,BlowJob,386.0,390.0,4.0
284,Jasminy Villar in The Stepfather And His Four Friends,438,Gangbang,690.0,722.0,32.0
284,Jasminy Villar in The Stepfather And His Four Friends,439,Gangbang,756.0,786.0,30.0
284,Jasminy Villar in The Stepfather And His Four Friends,430,BlowJob,1148.0,1166.0,18.0
284,Jasminy Villar in The Stepfather And His Four Friends,425,Anal,1174.0,1196.0,22.0
284,Jasminy Villar in The Stepfather And His Four Friends,440,Gangbang,1350.0,1388.0,38.0
284,Jasminy Villar in The Stepfather And His Four Friends,447,DP,1648.0,1670.0,22.0
284,Jasminy Villar in The Stepfather And His Four Friends,448,DP,1840.0,1876.0,36.0
284,Jasminy Villar in The Stepfather And His Four Friends,441,Gangbang,1910.0,1944.0,34.0
284,Jasminy Villar in The Stepfather And His Four Friends,442,Gangbang,2006.0,2016.0,10.0
284,Jasminy Villar in The Stepfather And His Four Friends,444,Gangbang,2330.0,2382.0,52.0
284,Jasminy Villar in The Stepfather And His Four Friends,427,Anal,2368.0,2380.0,12.0
284,Jasminy Villar in The Stepfather And His Four Friends,431,BlowJob,2520.0,2532.0,12.0
284,Jasminy Villar in The Stepfather And His Four Friends,445,Gangbang,2532.0,2552.0,20.0
284,Jasminy Villar in The Stepfather And His Four Friends,449,Cumshot,3312.0,3366.0,54.0
284,Jasminy Villar in The Stepfather And His Four Friends,452,Cumshot,3384.0,3394.0,10.0
284,Jasminy Villar in The Stepfather And His Four Friends,433,BlowJob,3406.0,3458.0,52.0
284,Jasminy Villar in The Stepfather And His Four Friends,450,Cumshot,3422.0,3432.0,10.0
284,Jasminy Villar in The Stepfather And His Four Friends,451,Cumshot,3468.0,3492.0,24.0
285,Jena LaRose in Blacks On Blondes,453,BlowJob,206.0,246.0,40.0
285,Jena LaRose in Blacks On Blondes,461,Anal,492.0,548.0,56.0
285,Jena LaRose in Blacks On Blondes,462,Anal,602.0,626.0,24.0
285,Jena LaRose in Blacks On Blondes,463,Anal,702.0,716.0,14.0
285,Jena LaRose in Blacks On Blondes,458,BlowJob,750.0,782.0,32.0
285,Jena LaRose in Blacks On Blondes,464,Anal,764.0,800.0,36.0
285,Jena LaRose in Blacks On Blondes,465,Anal,892.0,934.0,42.0
285,Jena LaRose in Blacks On Blondes,454,BlowJob,968.0,990.0,22.0
285,Jena LaRose in Blacks On Blondes,459,BlowJob,1250.0,1276.0,26.0
285,Jena LaRose in Blacks On Blondes,456,BlowJob,1622.0,1660.0,38.0
285,Jena LaRose in Blacks On Blondes,457,BlowJob,1778.0,1804.0,26.0
285,Jena LaRose in Blacks On Blondes,467,Cumshot,2034.0,2052.0,18.0
286,Jennifer White in Jennifer White Overload Sc1,469,Grabbing Boobs,212.0,244.0,32.0
286,Jennifer White in Jennifer White Overload Sc1,470,Gangbang,414.0,446.0,32.0
286,Jennifer White in Jennifer White Overload Sc1,488,BlowJob,598.0,622.0,24.0
286,Jennifer White in Jennifer White Overload Sc1,472,Gangbang,656.0,672.0,16.0
286,Jennifer White in Jennifer White Overload Sc1,473,Gangbang,754.0,772.0,18.0
286,Jennifer White in Jennifer White Overload Sc1,489,BlowJob,816.0,838.0,22.0
286,Jennifer White in Jennifer White Overload Sc1,475,Gangbang,1224.0,1244.0,20.0
286,Jennifer White in Jennifer White Overload Sc1,490,BlowJob,1640.0,1664.0,24.0
286,Jennifer White in Jennifer White Overload Sc1,492,BlowJob,1826.0,1860.0,34.0
286,Jennifer White in Jennifer White Overload Sc1,478,Gangbang,1990.0,2026.0,36.0
286,Jennifer White in Jennifer White Overload Sc1,479,Gangbang,2094.0,2114.0,20.0
286,Jennifer White in Jennifer White Overload Sc1,481,Gangbang,2312.0,2342.0,30.0
286,Jennifer White in Jennifer White Overload Sc1,482,Gangbang,2394.0,2416.0,22.0
286,Jennifer White in Jennifer White Overload Sc1,494,Anal,2550.0,2570.0,20.0
286,Jennifer White in Jennifer White Overload Sc1,495,Anal,2608.0,2620.0,12.0
286,Jennifer White in Jennifer White Overload Sc1,484,Gangbang,2676.0,2696.0,20.0
286,Jennifer White in Jennifer White Overload Sc1,485,Gangbang,2732.0,2746.0,14.0
286,Jennifer White in Jennifer White Overload Sc1,486,Gangbang,2780.0,2798.0,18.0
286,Jennifer White in Jennifer White Overload Sc1,497,Anal,2934.0,2950.0,16.0
287,Jennifer White in Jennifer White Overload Sc2,498,Grabbing Boobs,84.0,114.0,30.0
287,Jennifer White in Jennifer White Overload Sc2,499,Grabbing Boobs,196.0,220.0,24.0
287,Jennifer White in Jennifer White Overload Sc2,508,Gangbang,712.0,752.0,40.0
287,Jennifer White in Jennifer White Overload Sc2,511,Cumshot,846.0,878.0,32.0
287,Jennifer White in Jennifer White Overload Sc2,517,Anal,1004.0,1042.0,38.0
287,Jennifer White in Jennifer White Overload Sc2,518,Anal,1186.0,1242.0,56.0
287,Jennifer White in Jennifer White Overload Sc2,519,Anal,1342.0,1364.0,22.0
287,Jennifer White in Jennifer White Overload Sc2,501,BlowJob,1644.0,1692.0,48.0
287,Jennifer White in Jennifer White Overload Sc2,502,BlowJob,1792.0,1812.0,20.0
287,Jennifer White in Jennifer White Overload Sc2,520,Anal,1970.0,2026.0,56.0
287,Jennifer White in Jennifer White Overload Sc2,521,Anal,2102.0,2162.0,60.0
287,Jennifer White in Jennifer White Overload Sc2,522,Anal,2270.0,2330.0,60.0
287,Jennifer White in Jennifer White Overload Sc2,504,BlowJob,2274.0,2312.0,38.0
287,Jennifer White in Jennifer White Overload Sc2,505,BlowJob,2588.0,2646.0,58.0
287,Jennifer White in Jennifer White Overload Sc2,509,Gangbang,2626.0,2640.0,14.0
287,Jennifer White in Jennifer White Overload Sc2,524,Anal,2722.0,2766.0,44.0
287,Jennifer White in Jennifer White Overload Sc2,506,BlowJob,2876.0,2918.0,42.0
287,Jennifer White in Jennifer White Overload Sc2,510,Gangbang,3104.0,3138.0,34.0
287,Jennifer White in Jennifer White Overload Sc2,514,Cumshot,3106.0,3140.0,34.0
287,Jennifer White in Jennifer White Overload Sc2,515,Cumshot,3196.0,3252.0,56.0
287,Jennifer White in Jennifer White Overload Sc2,513,Cumshot,3282.0,3294.0,12.0
288,Jennifer White in Jennifer White Overload Sc3,525,Grabbing Boobs,58.0,100.0,42.0
288,Jennifer White in Jennifer White Overload Sc3,539,BlowJob,130.0,156.0,26.0
288,Jennifer White in Jennifer White Overload Sc3,540,BlowJob,324.0,358.0,34.0
288,Jennifer White in Jennifer White Overload Sc3,541,BlowJob,400.0,428.0,28.0
288,Jennifer White in Jennifer White Overload Sc3,528,Gangbang,408.0,426.0,18.0
288,Jennifer White in Jennifer White Overload Sc3,542,BlowJob,470.0,484.0,14.0
288,Jennifer White in Jennifer White Overload Sc3,543,BlowJob,568.0,586.0,18.0
288,Jennifer White in Jennifer White Overload Sc3,545,BlowJob,756.0,768.0,12.0
288,Jennifer White in Jennifer White Overload Sc3,526,Grabbing Boobs,784.0,826.0,42.0
288,Jennifer White in Jennifer White Overload Sc3,530,Gangbang,810.0,862.0,52.0
288,Jennifer White in Jennifer White Overload Sc3,531,Gangbang,920.0,944.0,24.0
288,Jennifer White in Jennifer White Overload Sc3,552,Anal,934.0,990.0,56.0
288,Jennifer White in Jennifer White Overload Sc3,551,BlowJob,938.0,950.0,12.0
288,Jennifer White in Jennifer White Overload Sc3,547,BlowJob,1000.0,1010.0,10.0
288,Jennifer White in Jennifer White Overload Sc3,549,BlowJob,1268.0,1282.0,14.0
288,Jennifer White in Jennifer White Overload Sc3,554,Anal,1660.0,1692.0,32.0
288,Jennifer White in Jennifer White Overload Sc3,534,Gangbang,1738.0,1750.0,12.0
288,Jennifer White in Jennifer White Overload Sc3,535,Gangbang,1794.0,1838.0,44.0
288,Jennifer White in Jennifer White Overload Sc3,555,Anal,1840.0,1856.0,16.0
288,Jennifer White in Jennifer White Overload Sc3,550,BlowJob,1952.0,1982.0,30.0
288,Jennifer White in Jennifer White Overload Sc3,536,Gangbang,2042.0,2074.0,32.0
288,Jennifer White in Jennifer White Overload Sc3,557,Cumshot,2368.0,2388.0,20.0
288,Jennifer White in Jennifer White Overload Sc3,558,Cumshot,2438.0,2478.0,40.0
288,Jennifer White in Jennifer White Overload Sc3,538,Gangbang,2440.0,2464.0,24.0
289,Jenny Pretinha in Fucked by 3 Huge Cocks,560,Anal,374.0,408.0,34.0
289,Jenny Pretinha in Fucked by 3 Huge Cocks,559,BlowJob,450.0,466.0,16.0
289,Jenny Pretinha in Fucked by 3 Huge Cocks,561,Anal,696.0,720.0,24.0
289,Jenny Pretinha in Fucked by 3 Huge Cocks,562,Anal,858.0,868.0,10.0
289,Jenny Pretinha in Fucked by 3 Huge Cocks,563,Anal,1088.0,1100.0,12.0
290,Juelz Ventura in POV BBC Airtight Gangbang,565,BlowJob,2.0,18.0,16.0
290,Juelz Ventura in POV BBC Airtight Gangbang,581,Anal,52.0,70.0,18.0
290,Juelz Ventura in POV BBC Airtight Gangbang,582,Anal,192.0,202.0,10.0
290,Juelz Ventura in POV BBC Airtight Gangbang,583,Anal,238.0,282.0,44.0
290,Juelz Ventura in POV BBC Airtight Gangbang,567,BlowJob,376.0,410.0,34.0
290,Juelz Ventura in POV BBC Airtight Gangbang,568,BlowJob,472.0,514.0,42.0
290,Juelz Ventura in POV BBC Airtight Gangbang,569,BlowJob,628.0,642.0,14.0
290,Juelz Ventura in POV BBC Airtight Gangbang,570,BlowJob,674.0,692.0,18.0
290,Juelz Ventura in POV BBC Airtight Gangbang,572,BlowJob,1130.0,1148.0,18.0
290,Juelz Ventura in POV BBC Airtight Gangbang,580,BlowJob,1166.0,1190.0,24.0
290,Juelz Ventura in POV BBC Airtight Gangbang,588,Cumshot,1222.0,1226.0,4.0
290,Juelz Ventura in POV BBC Airtight Gangbang,589,Cumshot,1342.0,1354.0,12.0
290,Juelz Ventura in POV BBC Airtight Gangbang,573,BlowJob,1344.0,1358.0,14.0
290,Juelz Ventura in POV BBC Airtight Gangbang,585,Anal,1402.0,1416.0,14.0
290,Juelz Ventura in POV BBC Airtight Gangbang,590,Cumshot,1512.0,1522.0,10.0
290,Juelz Ventura in POV BBC Airtight Gangbang,594,Cumshot,1628.0,1640.0,12.0
290,Juelz Ventura in POV BBC Airtight Gangbang,575,BlowJob,1702.0,1738.0,36.0
290,Juelz Ventura in POV BBC Airtight Gangbang,576,BlowJob,1774.0,1790.0,16.0
290,Juelz Ventura in POV BBC Airtight Gangbang,586,Anal,1812.0,1846.0,34.0
290,Juelz Ventura in POV BBC Airtight Gangbang,587,Anal,1882.0,1912.0,30.0
290,Juelz Ventura in POV BBC Airtight Gangbang,577,BlowJob,2074.0,2088.0,14.0
290,Juelz Ventura in POV BBC Airtight Gangbang,592,Cumshot,2430.0,2468.0,38.0
290,Juelz Ventura in POV BBC Airtight Gangbang,578,BlowJob,2446.0,2478.0,32.0
290,Juelz Ventura in POV BBC Airtight Gangbang,595,Cumshot,2850.0,2884.0,34.0
290,Juelz Ventura in POV BBC Airtight Gangbang,579,BlowJob,2854.0,2868.0,14.0
291,"Jureka Del Mar, Maylee Fun in Asian Hotties Work to Cure",596,BlowJob,112.0,118.0,6.0
291,"Jureka Del Mar, Maylee Fun in Asian Hotties Work to Cure",597,BlowJob,130.0,186.0,56.0
292,Katalina Kyle in Ass Worship 18 Sc3,603,BlowJob,608.0,620.0,12.0
292,Katalina Kyle in Ass Worship 18 Sc3,604,BlowJob,674.0,684.0,10.0
292,Katalina Kyle in Ass Worship 18 Sc3,605,BlowJob,766.0,776.0,10.0
292,Katalina Kyle in Ass Worship 18 Sc3,606,BlowJob,824.0,834.0,10.0
292,Katalina Kyle in Ass Worship 18 Sc3,600,Grabbing Boobs,942.0,980.0,38.0
292,Katalina Kyle in Ass Worship 18 Sc3,607,BlowJob,1264.0,1300.0,36.0
292,Katalina Kyle in Ass Worship 18 Sc3,614,Anal,1310.0,1342.0,32.0
292,Katalina Kyle in Ass Worship 18 Sc3,615,Anal,1464.0,1480.0,16.0
292,Katalina Kyle in Ass Worship 18 Sc3,619,DP,1712.0,1740.0,28.0
292,Katalina Kyle in Ass Worship 18 Sc3,601,Grabbing Boobs,1898.0,1938.0,40.0
292,Katalina Kyle in Ass Worship 18 Sc3,620,DP,2018.0,2030.0,12.0
292,Katalina Kyle in Ass Worship 18 Sc3,613,BlowJob,2064.0,2112.0,48.0
292,Katalina Kyle in Ass Worship 18 Sc3,609,BlowJob,2174.0,2186.0,12.0
292,Katalina Kyle in Ass Worship 18 Sc3,610,BlowJob,2252.0,2276.0,24.0
292,Katalina Kyle in Ass Worship 18 Sc3,602,Grabbing Boobs,2328.0,2346.0,18.0
292,Katalina Kyle in Ass Worship 18 Sc3,611,BlowJob,2354.0,2396.0,42.0
292,Katalina Kyle in Ass Worship 18 Sc3,612,BlowJob,2436.0,2474.0,38.0
293,Katalina Kyle in Takes Every Inch Of Manuel,630,BlowJob,440.0,468.0,28.0
293,Katalina Kyle in Takes Every Inch Of Manuel,625,Grabbing Boobs,506.0,562.0,56.0
293,Katalina Kyle in Takes Every Inch Of Manuel,626,Grabbing Boobs,746.0,790.0,44.0
293,Katalina Kyle in Takes Every Inch Of Manuel,631,BlowJob,908.0,936.0,28.0
293,Katalina Kyle in Takes Every Inch Of Manuel,635,Anal,1028.0,1040.0,12.0
293,Katalina Kyle in Takes Every Inch Of Manuel,632,BlowJob,1124.0,1140.0,16.0
293,Katalina Kyle in Takes Every Inch Of Manuel,627,Grabbing Boobs,1156.0,1176.0,20.0
293,Katalina Kyle in Takes Every Inch Of Manuel,628,Grabbing Boobs,1484.0,1520.0,36.0
293,Katalina Kyle in Takes Every Inch Of Manuel,634,BlowJob,1544.0,1570.0,26.0
293,Katalina Kyle in Takes Every Inch Of Manuel,636,Cumshot,1550.0,1594.0,44.0
293,Katalina Kyle in Takes Every Inch Of Manuel,629,Grabbing Boobs,1578.0,1614.0,36.0
294,Katia Belinii in Swallowing 5 Big Loads,637,Grabbing Boobs,18.0,48.0,30.0
294,Katia Belinii in Swallowing 5 Big Loads,638,Grabbing Boobs,338.0,368.0,30.0
294,Katia Belinii in Swallowing 5 Big Loads,645,Anal,482.0,514.0,32.0
294,Katia Belinii in Swallowing 5 Big Loads,641,BlowJob,574.0,588.0,14.0
294,Katia Belinii in Swallowing 5 Big Loads,639,Grabbing Boobs,728.0,746.0,18.0
294,Katia Belinii in Swallowing 5 Big Loads,648,Cumshot,740.0,750.0,10.0
294,Katia Belinii in Swallowing 5 Big Loads,642,BlowJob,752.0,784.0,32.0
294,Katia Belinii in Swallowing 5 Big Loads,647,Anal,1100.0,1114.0,14.0
294,Katia Belinii in Swallowing 5 Big Loads,643,BlowJob,1132.0,1172.0,40.0
294,Katia Belinii in Swallowing 5 Big Loads,649,Cumshot,1208.0,1238.0,30.0
294,Katia Belinii in Swallowing 5 Big Loads,644,BlowJob,1262.0,1298.0,36.0
294,Katia Belinii in Swallowing 5 Big Loads,650,Cumshot,1520.0,1538.0,18.0
296,Kayla Carrera in Anal Integrity Sc1,653,Grabbing Boobs,108.0,154.0,46.0
296,Kayla Carrera in Anal Integrity Sc1,654,BlowJob,528.0,570.0,42.0
296,Kayla Carrera in Anal Integrity Sc1,657,Anal,1716.0,1770.0,54.0
296,Kayla Carrera in Anal Integrity Sc1,658,Anal,1848.0,1882.0,34.0
296,Kayla Carrera in Anal Integrity Sc1,660,Anal,2394.0,2404.0,10.0
296,Kayla Carrera in Anal Integrity Sc1,661,Anal,2516.0,2560.0,44.0
297,Kaylani Lei in Asian Fuck Machines Sc5,664,DP,912.0,918.0,6.0
297,Kaylani Lei in Asian Fuck Machines Sc5,666,BlowJob,1622.0,1650.0,28.0
297,Kaylani Lei in Asian Fuck Machines Sc5,667,BlowJob,1696.0,1728.0,32.0
297,Kaylani Lei in Asian Fuck Machines Sc5,665,DP,2096.0,2132.0,36.0
298,Kazumi Squirts in BBC Orgy Room,684,Cumshot,174.0,220.0,46.0
298,Kazumi Squirts in BBC Orgy Room,692,Gangbang,184.0,208.0,24.0
298,Kazumi Squirts in BBC Orgy Room,693,Gangbang,240.0,268.0,28.0
298,Kazumi Squirts in BBC Orgy Room,685,Cumshot,398.0,428.0,30.0
298,Kazumi Squirts in BBC Orgy Room,671,BlowJob,1230.0,1252.0,22.0
298,Kazumi Squirts in BBC Orgy Room,672,BlowJob,1380.0,1408.0,28.0
298,Kazumi Squirts in BBC Orgy Room,695,Gangbang,1398.0,1424.0,26.0
298,Kazumi Squirts in BBC Orgy Room,673,BlowJob,1526.0,1540.0,14.0
298,Kazumi Squirts in BBC Orgy Room,696,Gangbang,2264.0,2292.0,28.0
298,Kazumi Squirts in BBC Orgy Room,674,BlowJob,2386.0,2432.0,46.0
298,Kazumi Squirts in BBC Orgy Room,683,BlowJob,2838.0,2842.0,4.0
298,Kazumi Squirts in BBC Orgy Room,676,BlowJob,3054.0,3064.0,10.0
298,Kazumi Squirts in BBC Orgy Room,677,BlowJob,3234.0,3244.0,10.0
298,Kazumi Squirts in BBC Orgy Room,678,BlowJob,3346.0,3356.0,10.0
298,Kazumi Squirts in BBC Orgy Room,688,Cumshot,3490.0,3510.0,20.0
298,Kazumi Squirts in BBC Orgy Room,689,Cumshot,3560.0,3612.0,52.0
298,Kazumi Squirts in BBC Orgy Room,686,Cumshot,3644.0,3652.0,8.0
298,Kazumi Squirts in BBC Orgy Room,690,Cumshot,3696.0,3706.0,10.0
298,Kazumi Squirts in BBC Orgy Room,687,Cumshot,3830.0,3836.0,6.0
298,Kazumi Squirts in BBC Orgy Room,679,BlowJob,4166.0,4204.0,38.0
298,Kazumi Squirts in BBC Orgy Room,691,Cumshot,4204.0,4228.0,24.0
299,Kazumi Squirts in Gangbang With Piss and DP,704,Pissing,26.0,36.0,10.0
299,Kazumi Squirts in Gangbang With Piss and DP,708,BlowJob,208.0,246.0,38.0
299,Kazumi Squirts in Gangbang With Piss and DP,716,Cumshot,494.0,516.0,22.0
299,Kazumi Squirts in Gangbang With Piss and DP,717,Cumshot,914.0,918.0,4.0
299,Kazumi Squirts in Gangbang With Piss and DP,705,Pissing,940.0,990.0,50.0
299,Kazumi Squirts in Gangbang With Piss and DP,725,Anal,1512.0,1534.0,22.0
299,Kazumi Squirts in Gangbang With Piss and DP,700,Gangbang,1608.0,1620.0,12.0
299,Kazumi Squirts in Gangbang With Piss and DP,726,Anal,1622.0,1636.0,14.0
299,Kazumi Squirts in Gangbang With Piss and DP,701,Gangbang,1808.0,1818.0,10.0
299,Kazumi Squirts in Gangbang With Piss and DP,702,Gangbang,1866.0,1886.0,20.0
299,Kazumi Squirts in Gangbang With Piss and DP,714,BlowJob,1876.0,1894.0,18.0
299,Kazumi Squirts in Gangbang With Piss and DP,711,BlowJob,1962.0,1986.0,24.0
299,Kazumi Squirts in Gangbang With Piss and DP,727,Anal,2016.0,2046.0,30.0
299,Kazumi Squirts in Gangbang With Piss and DP,729,DP,2028.0,2048.0,20.0
299,Kazumi Squirts in Gangbang With Piss and DP,728,Anal,2218.0,2252.0,34.0
299,Kazumi Squirts in Gangbang With Piss and DP,703,Gangbang,2386.0,2402.0,16.0
299,Kazumi Squirts in Gangbang With Piss and DP,712,BlowJob,2418.0,2470.0,52.0
299,Kazumi Squirts in Gangbang With Piss and DP,715,BlowJob,2596.0,2614.0,18.0
299,Kazumi Squirts in Gangbang With Piss and DP,706,Pissing,2934.0,2968.0,34.0
299,Kazumi Squirts in Gangbang With Piss and DP,713,BlowJob,3258.0,3268.0,10.0
301,Kelly Oliveira in Assfucked 4on1 with DP,748,BlowJob,560.0,578.0,18.0
301,Kelly Oliveira in Assfucked 4on1 with DP,749,BlowJob,682.0,710.0,28.0
301,Kelly Oliveira in Assfucked 4on1 with DP,750,BlowJob,758.0,778.0,20.0
301,Kelly Oliveira in Assfucked 4on1 with DP,753,Titjob,760.0,776.0,16.0
301,Kelly Oliveira in Assfucked 4on1 with DP,756,Anal,1420.0,1450.0,30.0
301,Kelly Oliveira in Assfucked 4on1 with DP,757,Anal,1608.0,1636.0,28.0
301,Kelly Oliveira in Assfucked 4on1 with DP,752,BlowJob,1722.0,1754.0,32.0
301,Kelly Oliveira in Assfucked 4on1 with DP,759,Anal,2014.0,2040.0,26.0
301,Kelly Oliveira in Assfucked 4on1 with DP,760,Anal,2174.0,2184.0,10.0
301,Kelly Oliveira in Assfucked 4on1 with DP,761,Anal,2236.0,2292.0,56.0
301,Kelly Oliveira in Assfucked 4on1 with DP,763,Cumshot,2918.0,2922.0,4.0
301,Kelly Oliveira in Assfucked 4on1 with DP,764,Cumshot,2934.0,2970.0,36.0
301,Kelly Oliveira in Assfucked 4on1 with DP,745,Grabbing Boobs,2956.0,2976.0,20.0
302,Kelly Oliveira in First DP for Brazilian Teen,765,BlowJob,160.0,190.0,30.0
302,Kelly Oliveira in First DP for Brazilian Teen,766,BlowJob,270.0,312.0,42.0
302,Kelly Oliveira in First DP for Brazilian Teen,772,Anal,610.0,668.0,58.0
302,Kelly Oliveira in First DP for Brazilian Teen,773,Anal,702.0,724.0,22.0
302,Kelly Oliveira in First DP for Brazilian Teen,774,Anal,796.0,810.0,14.0
302,Kelly Oliveira in First DP for Brazilian Teen,775,Anal,978.0,990.0,12.0
302,Kelly Oliveira in First DP for Brazilian Teen,778,Grabbing Boobs,1762.0,1788.0,26.0
302,Kelly Oliveira in First DP for Brazilian Teen,770,BlowJob,1874.0,1900.0,26.0
302,Kelly Oliveira in First DP for Brazilian Teen,779,Cumshot,1902.0,1912.0,10.0
303,Kelly Oliveira in Sexy Latina DAP 3on1,782,Anal,766.0,812.0,46.0
303,Kelly Oliveira in Sexy Latina DAP 3on1,786,BlowJob,1590.0,1620.0,30.0
303,Kelly Oliveira in Sexy Latina DAP 3on1,781,Grabbing Boobs,2224.0,2250.0,26.0
303,Kelly Oliveira in Sexy Latina DAP 3on1,784,Anal,2538.0,2596.0,58.0
303,Kelly Oliveira in Sexy Latina DAP 3on1,787,Cumshot,2638.0,2698.0,60.0
304,"Kelly Oliveira, Veronica Leal in 5on2 orgy with DP",791,Anal,322.0,332.0,10.0
304,"Kelly Oliveira, Veronica Leal in 5on2 orgy with DP",793,BlowJob,960.0,990.0,30.0
304,"Kelly Oliveira, Veronica Leal in 5on2 orgy with DP",796,Cumshot,3154.0,3196.0,42.0
304,"Kelly Oliveira, Veronica Leal in 5on2 orgy with DP",797,Cumshot,3234.0,3274.0,40.0
305,Keri Sable in Cum Filled Asshole Overload 2 Sc1,806,Grabbing Boobs,2128.0,2154.0,26.0
305,Keri Sable in Cum Filled Asshole Overload 2 Sc1,804,Anal,2324.0,2384.0,60.0
305,Keri Sable in Cum Filled Asshole Overload 2 Sc1,802,BlowJob,2412.0,2428.0,16.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,807,Cumshot,236.0,252.0,16.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,809,Cumshot,266.0,270.0,4.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,813,BlowJob,670.0,680.0,10.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,814,BlowJob,848.0,874.0,26.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,816,Anal,1000.0,1044.0,44.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,817,Anal,1124.0,1134.0,10.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,818,Pissing,1224.0,1238.0,14.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,812,Grabbing Boobs,1616.0,1668.0,52.0
306,Kim XXX in Manga Total Vollgespritzt Sc1,815,BlowJob,1646.0,1700.0,54.0
307,Kim XXX in Tiny Whore Kim XXX Stuffed,819,Grabbing Boobs,144.0,164.0,20.0
307,Kim XXX in Tiny Whore Kim XXX Stuffed,820,BlowJob,184.0,238.0,54.0
307,Kim XXX in Tiny Whore Kim XXX Stuffed,821,BlowJob,316.0,332.0,16.0
307,Kim XXX in Tiny Whore Kim XXX Stuffed,822,BlowJob,672.0,686.0,14.0
307,Kim XXX in Tiny Whore Kim XXX Stuffed,823,BlowJob,1044.0,1096.0,52.0
308,Kira Thorn in Balls Deep 5on2,830,BlowJob,200.0,226.0,26.0
308,Kira Thorn in Balls Deep 5on2,831,BlowJob,508.0,526.0,18.0
308,Kira Thorn in Balls Deep 5on2,839,DP,790.0,794.0,4.0
308,Kira Thorn in Balls Deep 5on2,840,DP,958.0,976.0,18.0
308,Kira Thorn in Balls Deep 5on2,832,BlowJob,1032.0,1088.0,56.0
308,Kira Thorn in Balls Deep 5on2,841,Gangbang,1126.0,1142.0,16.0
308,Kira Thorn in Balls Deep 5on2,833,BlowJob,1128.0,1140.0,12.0
308,Kira Thorn in Balls Deep 5on2,834,BlowJob,1192.0,1206.0,14.0
308,Kira Thorn in Balls Deep 5on2,838,Anal,2728.0,2774.0,46.0
309,Kitana Montana in Birthday Threeway,849,Cumshot,116.0,126.0,10.0
309,Kitana Montana in Birthday Threeway,850,Cumshot,218.0,248.0,30.0
309,Kitana Montana in Birthday Threeway,843,BlowJob,260.0,272.0,12.0
309,Kitana Montana in Birthday Threeway,853,Anal,530.0,570.0,40.0
309,Kitana Montana in Birthday Threeway,854,Anal,916.0,930.0,14.0
309,Kitana Montana in Birthday Threeway,855,Anal,984.0,1004.0,20.0
309,Kitana Montana in Birthday Threeway,845,BlowJob,1032.0,1060.0,28.0
309,Kitana Montana in Birthday Threeway,856,Anal,1202.0,1226.0,24.0
309,Kitana Montana in Birthday Threeway,851,Cumshot,1672.0,1688.0,16.0
309,Kitana Montana in Birthday Threeway,848,BlowJob,1694.0,1706.0,12.0
310,Kitana Montana in Post,1836,BlowJob,176.0,198.0,22.0
310,Kitana Montana in Post,1842,Cumshot,290.0,294.0,4.0
310,Kitana Montana in Post,1838,BlowJob,630.0,640.0,10.0
310,Kitana Montana in Post,1852,DP,1036.0,1040.0,4.0
310,Kitana Montana in Post,1845,Anal,1058.0,1086.0,28.0
310,Kitana Montana in Post,1839,BlowJob,1212.0,1234.0,22.0
310,Kitana Montana in Post,1846,Anal,1422.0,1452.0,30.0
310,Kitana Montana in Post,1853,DP,1740.0,1772.0,32.0
310,Kitana Montana in Post,1848,Anal,1858.0,1904.0,46.0
310,Kitana Montana in Post,1854,DP,1886.0,1892.0,6.0
310,Kitana Montana in Post,1855,DP,1946.0,2006.0,60.0
310,Kitana Montana in Post,1849,Anal,1950.0,1960.0,10.0
310,Kitana Montana in Post,1850,Anal,2000.0,2028.0,28.0
310,Kitana Montana in Post,1851,Anal,2124.0,2162.0,38.0
310,Kitana Montana in Post,1841,BlowJob,2270.0,2274.0,4.0
310,Kitana Montana in Post,1843,Cumshot,2306.0,2350.0,44.0
311,Laura Fiorentino in 6on1 Swallow,861,BlowJob,290.0,320.0,30.0
311,Laura Fiorentino in 6on1 Swallow,870,Gangbang,704.0,748.0,44.0
311,Laura Fiorentino in 6on1 Swallow,874,Cumshot,968.0,972.0,4.0
311,Laura Fiorentino in 6on1 Swallow,880,Pissing,974.0,990.0,16.0
311,Laura Fiorentino in 6on1 Swallow,875,Cumshot,1042.0,1088.0,46.0
311,Laura Fiorentino in 6on1 Swallow,862,BlowJob,1084.0,1120.0,36.0
311,Laura Fiorentino in 6on1 Swallow,883,DP,2126.0,2130.0,4.0
311,Laura Fiorentino in 6on1 Swallow,873,Gangbang,2244.0,2270.0,26.0
311,Laura Fiorentino in 6on1 Swallow,881,Pissing,2322.0,2360.0,38.0
311,Laura Fiorentino in 6on1 Swallow,882,Pissing,2414.0,2442.0,28.0
311,Laura Fiorentino in 6on1 Swallow,876,Cumshot,2474.0,2496.0,22.0
311,Laura Fiorentino in 6on1 Swallow,864,Anal,2772.0,2794.0,22.0
311,Laura Fiorentino in 6on1 Swallow,866,Anal,2974.0,3004.0,30.0
311,Laura Fiorentino in 6on1 Swallow,867,Anal,3048.0,3106.0,58.0
311,Laura Fiorentino in 6on1 Swallow,868,Anal,3194.0,3254.0,60.0
311,Laura Fiorentino in 6on1 Swallow,869,Anal,3360.0,3406.0,46.0
311,Laura Fiorentino in 6on1 Swallow,858,Grabbing Boobs,3558.0,3582.0,24.0
311,Laura Fiorentino in 6on1 Swallow,877,Cumshot,3622.0,3632.0,10.0
311,Laura Fiorentino in 6on1 Swallow,879,Cumshot,3852.0,3856.0,4.0
312,Lela Star in Assparade 54 Sc2,884,Anal,94.0,112.0,18.0
312,Lela Star in Assparade 54 Sc2,885,BlowJob,166.0,178.0,12.0
312,Lela Star in Assparade 54 Sc2,887,Titjob,182.0,216.0,34.0
312,Lela Star in Assparade 54 Sc2,886,BlowJob,368.0,404.0,36.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,890,BlowJob,88.0,148.0,60.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,894,Grabbing Boobs,156.0,180.0,24.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,891,BlowJob,238.0,256.0,18.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,895,DP,980.0,992.0,12.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,892,BlowJob,1030.0,1040.0,10.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,896,Anal,1102.0,1144.0,42.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,897,Anal,1356.0,1396.0,40.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,898,Anal,1438.0,1450.0,12.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,899,Cumshot,2064.0,2094.0,30.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,893,BlowJob,2112.0,2166.0,54.0
313,Lena Coxx in Italiana Guarra se Come 6 Pollas,900,Cumshot,2136.0,2182.0,46.0
314,Lolly Ink in True Gonzo Sc5,907,Anal,994.0,1014.0,20.0
314,Lolly Ink in True Gonzo Sc5,903,Grabbing Boobs,1180.0,1200.0,20.0
314,Lolly Ink in True Gonzo Sc5,908,Cumshot,1486.0,1504.0,18.0
315,Luna Star in Double Stuffed,909,Grabbing Boobs,18.0,72.0,54.0
315,Luna Star in Double Stuffed,910,BlowJob,156.0,186.0,30.0
315,Luna Star in Double Stuffed,913,BlowJob,986.0,1010.0,24.0
315,Luna Star in Double Stuffed,917,Anal,1338.0,1356.0,18.0
315,Luna Star in Double Stuffed,914,BlowJob,1512.0,1534.0,22.0
315,Luna Star in Double Stuffed,919,Anal,1558.0,1574.0,16.0
315,Luna Star in Double Stuffed,915,BlowJob,1632.0,1684.0,52.0
316,Luna Star in Why She's A Pornstar,920,Grabbing Boobs,212.0,234.0,22.0
316,Luna Star in Why She's A Pornstar,921,Grabbing Boobs,314.0,340.0,26.0
316,Luna Star in Why She's A Pornstar,922,BlowJob,450.0,462.0,12.0
316,Luna Star in Why She's A Pornstar,923,BlowJob,502.0,536.0,34.0
316,Luna Star in Why She's A Pornstar,924,BlowJob,778.0,798.0,20.0
316,Luna Star in Why She's A Pornstar,925,BlowJob,1172.0,1186.0,14.0
316,Luna Star in Why She's A Pornstar,926,BlowJob,1686.0,1700.0,14.0
316,Luna Star in Why She's A Pornstar,927,BlowJob,1956.0,1976.0,20.0
317,Marilyn Johnson in Airtight Diva Sc1,930,BlowJob,578.0,590.0,12.0
317,Marilyn Johnson in Airtight Diva Sc1,935,BlowJob,616.0,632.0,16.0
317,Marilyn Johnson in Airtight Diva Sc1,931,BlowJob,668.0,682.0,14.0
317,Marilyn Johnson in Airtight Diva Sc1,932,BlowJob,878.0,890.0,12.0
317,Marilyn Johnson in Airtight Diva Sc1,933,BlowJob,1446.0,1456.0,10.0
317,Marilyn Johnson in Airtight Diva Sc1,937,Cumshot,1470.0,1474.0,4.0
317,Marilyn Johnson in Airtight Diva Sc1,939,Cumshot,1578.0,1602.0,24.0
318,"Mariska, Tekohas in Cum Cum Gangbang Orgy",940,BlowJob,172.0,218.0,46.0
318,"Mariska, Tekohas in Cum Cum Gangbang Orgy",946,Cumshot,502.0,510.0,8.0
318,"Mariska, Tekohas in Cum Cum Gangbang Orgy",942,BlowJob,592.0,610.0,18.0
318,"Mariska, Tekohas in Cum Cum Gangbang Orgy",943,BlowJob,856.0,866.0,10.0
318,"Mariska, Tekohas in Cum Cum Gangbang Orgy",944,BlowJob,904.0,960.0,56.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,950,BlowJob,164.0,182.0,18.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,953,BlowJob,2002.0,2038.0,36.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,955,Cumshot,2194.0,2236.0,42.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,949,Grabbing Boobs,2216.0,2234.0,18.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,956,Cumshot,2588.0,2598.0,10.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,958,Cumshot,2642.0,2648.0,6.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,959,Cumshot,2704.0,2730.0,26.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,960,Cumshot,2804.0,2844.0,40.0
319,Maxine X in I'm Here for the Gang Bang! Sc1,961,Cumshot,2876.0,2882.0,6.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,962,BlowJob,208.0,240.0,32.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,963,BlowJob,470.0,480.0,10.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,973,Anal,566.0,608.0,42.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,975,Anal,884.0,898.0,14.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,976,Anal,938.0,964.0,26.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,977,Anal,1006.0,1038.0,32.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,964,BlowJob,1064.0,1078.0,14.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,978,Anal,1314.0,1336.0,22.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,965,BlowJob,1392.0,1414.0,22.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,979,Anal,1566.0,1602.0,36.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,980,Anal,1648.0,1678.0,30.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,966,BlowJob,1810.0,1828.0,18.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,981,Anal,1860.0,1902.0,42.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,967,BlowJob,1990.0,2010.0,20.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,970,BlowJob,2090.0,2106.0,16.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,971,BlowJob,2168.0,2190.0,22.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,972,BlowJob,2262.0,2272.0,10.0
320,Maxine X in I'm Here for the Gang Bang! Sc2,968,BlowJob,2898.0,2912.0,14.0
321,Megan Rain in 10 Cock Blowbang!,986,Gangbang,548.0,578.0,30.0
321,Megan Rain in 10 Cock Blowbang!,987,Cumshot,1490.0,1506.0,16.0
322,Melissa Hot in Fucked by 4 Big Cocks,991,BlowJob,76.0,86.0,10.0
322,Melissa Hot in Fucked by 4 Big Cocks,998,Gangbang,94.0,126.0,32.0
322,Melissa Hot in Fucked by 4 Big Cocks,989,Anal,736.0,756.0,20.0
322,Melissa Hot in Fucked by 4 Big Cocks,992,BlowJob,954.0,1000.0,46.0
322,Melissa Hot in Fucked by 4 Big Cocks,993,BlowJob,1032.0,1042.0,10.0
322,Melissa Hot in Fucked by 4 Big Cocks,999,Gangbang,1726.0,1746.0,20.0
322,Melissa Hot in Fucked by 4 Big Cocks,1000,Gangbang,2142.0,2168.0,26.0
322,Melissa Hot in Fucked by 4 Big Cocks,1001,Gangbang,2204.0,2222.0,18.0
322,Melissa Hot in Fucked by 4 Big Cocks,1002,Gangbang,2254.0,2274.0,20.0
322,Melissa Hot in Fucked by 4 Big Cocks,1003,Gangbang,2350.0,2364.0,14.0
322,Melissa Hot in Fucked by 4 Big Cocks,997,BlowJob,2414.0,2432.0,18.0
322,Melissa Hot in Fucked by 4 Big Cocks,1004,Cumshot,2436.0,2448.0,12.0
322,Melissa Hot in Fucked by 4 Big Cocks,1005,Cumshot,2526.0,2538.0,12.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1015,Anal,782.0,792.0,10.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1014,BlowJob,1052.0,1090.0,38.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1016,Anal,1154.0,1176.0,22.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1006,BlowJob,1580.0,1596.0,16.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1007,BlowJob,1684.0,1696.0,12.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1008,BlowJob,1944.0,1954.0,10.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1009,BlowJob,2166.0,2190.0,24.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1010,BlowJob,2484.0,2514.0,30.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1019,Anal,2540.0,2568.0,28.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1011,BlowJob,2630.0,2662.0,32.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1012,BlowJob,2982.0,2998.0,16.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1013,BlowJob,3040.0,3072.0,32.0
323,"Melissa Moore, Morgan Lee in Orgy Masters 8 Sc2",1021,Cumshot,3064.0,3108.0,44.0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1028,Anal,350.0,380.0,30.0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1024,BlowJob,862.0,904.0,42.0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1029,Grabbing Boobs,1058.0,1084.0,26.0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1030,Grabbing Boobs,1256.0,1284.0,28.0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1025,BlowJob,1374.0,1410.0,36.0
324,"Melissa Stratton, Tru Kait in Hoo Milfy Ass",1027,BlowJob,1438.0,1446.0,8.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1031,Grabbing Boobs,182.0,218.0,36.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1041,Cumshot,684.0,698.0,14.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1033,BlowJob,962.0,976.0,14.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1034,BlowJob,1360.0,1392.0,32.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1036,BlowJob,2384.0,2412.0,28.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1037,BlowJob,2464.0,2476.0,12.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1038,BlowJob,2512.0,2558.0,46.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1039,BlowJob,2614.0,2632.0,18.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1042,Cumshot,2882.0,2892.0,10.0
325,Mercedes Carrera in MILF Cumsluts Sc4,1044,Cumshot,2918.0,2960.0,42.0
326,Mia Trejsi in 100% Hell,1050,Gangbang,94.0,126.0,32.0
326,Mia Trejsi in 100% Hell,1059,BlowJob,464.0,480.0,16.0
326,Mia Trejsi in 100% Hell,1051,Gangbang,674.0,688.0,14.0
326,Mia Trejsi in 100% Hell,1061,BlowJob,1318.0,1340.0,22.0
326,Mia Trejsi in 100% Hell,1047,Grabbing Boobs,1324.0,1370.0,46.0
326,Mia Trejsi in 100% Hell,1052,Gangbang,1332.0,1372.0,40.0
326,Mia Trejsi in 100% Hell,1053,Gangbang,1420.0,1434.0,14.0
326,Mia Trejsi in 100% Hell,1054,Gangbang,1626.0,1652.0,26.0
326,Mia Trejsi in 100% Hell,1062,BlowJob,1880.0,1892.0,12.0
326,Mia Trejsi in 100% Hell,1063,BlowJob,2308.0,2326.0,18.0
326,Mia Trejsi in 100% Hell,1055,Gangbang,2496.0,2510.0,14.0
326,Mia Trejsi in 100% Hell,1056,Gangbang,2762.0,2788.0,26.0
326,Mia Trejsi in 100% Hell,1057,Gangbang,2926.0,2938.0,12.0
326,Mia Trejsi in 100% Hell,1058,Gangbang,2972.0,2992.0,20.0
326,Mia Trejsi in 100% Hell,1048,Grabbing Boobs,3280.0,3338.0,58.0
326,Mia Trejsi in 100% Hell,1064,BlowJob,3318.0,3324.0,6.0
326,Mia Trejsi in 100% Hell,1049,Grabbing Boobs,3384.0,3426.0,42.0
326,Mia Trejsi in 100% Hell,1066,Cumshot,3416.0,3424.0,8.0
327,"Mia Trejsi, Ria Sunn in Angels Of Hardcore 3 Sc1",1067,Grabbing Boobs,0.0,20.0,20.0
327,"Mia Trejsi, Ria Sunn in Angels Of Hardcore 3 Sc1",1068,Gangbang,60.0,94.0,34.0
327,"Mia Trejsi, Ria Sunn in Angels Of Hardcore 3 Sc1",1070,Anal,192.0,230.0,38.0
327,"Mia Trejsi, Ria Sunn in Angels Of Hardcore 3 Sc1",1069,Gangbang,314.0,362.0,48.0
328,Mih Ninfetinha in 4on1 with DP,1074,BlowJob,946.0,988.0,42.0
328,Mih Ninfetinha in 4on1 with DP,1079,BlowJob,1024.0,1054.0,30.0
328,Mih Ninfetinha in 4on1 with DP,1080,BlowJob,1542.0,1550.0,8.0
328,Mih Ninfetinha in 4on1 with DP,1076,BlowJob,1848.0,1880.0,32.0
328,Mih Ninfetinha in 4on1 with DP,1084,Gangbang,1852.0,1912.0,60.0
328,Mih Ninfetinha in 4on1 with DP,1081,BlowJob,2004.0,2008.0,4.0
328,Mih Ninfetinha in 4on1 with DP,1077,BlowJob,2288.0,2332.0,44.0
328,Mih Ninfetinha in 4on1 with DP,1085,Cumshot,2516.0,2524.0,8.0
329,Miss Teela in First Time 10 Gangbang,1086,Gangbang,124.0,168.0,44.0
329,Miss Teela in First Time 10 Gangbang,1092,BlowJob,884.0,914.0,30.0
329,Miss Teela in First Time 10 Gangbang,1093,BlowJob,954.0,994.0,40.0
329,Miss Teela in First Time 10 Gangbang,1089,Gangbang,1180.0,1208.0,28.0
329,Miss Teela in First Time 10 Gangbang,1094,BlowJob,1214.0,1266.0,52.0
329,Miss Teela in First Time 10 Gangbang,1095,BlowJob,1314.0,1342.0,28.0
329,Miss Teela in First Time 10 Gangbang,1096,BlowJob,1422.0,1448.0,26.0
329,Miss Teela in First Time 10 Gangbang,1098,Cumshot,1608.0,1636.0,28.0
329,Miss Teela in First Time 10 Gangbang,1099,Cumshot,1658.0,1702.0,44.0
330,Monika Fox in DP Fantasies 11 Sc3,1101,Grabbing Boobs,172.0,204.0,32.0
330,Monika Fox in DP Fantasies 11 Sc3,1102,Grabbing Boobs,430.0,458.0,28.0
330,Monika Fox in DP Fantasies 11 Sc3,1103,Grabbing Boobs,952.0,978.0,26.0
330,Monika Fox in DP Fantasies 11 Sc3,1108,Cumshot,1428.0,1438.0,10.0
330,Monika Fox in DP Fantasies 11 Sc3,1110,Anal,2074.0,2104.0,30.0
330,Monika Fox in DP Fantasies 11 Sc3,1105,BlowJob,2330.0,2348.0,18.0
330,Monika Fox in DP Fantasies 11 Sc3,1112,Anal,2474.0,2490.0,16.0
330,Monika Fox in DP Fantasies 11 Sc3,1109,Cumshot,2520.0,2538.0,18.0
331,Natasha Teen in Pussy DAPTAP,1115,BlowJob,606.0,620.0,14.0
331,Natasha Teen in Pussy DAPTAP,1116,BlowJob,1064.0,1112.0,48.0
331,Natasha Teen in Pussy DAPTAP,1119,Gangbang,1176.0,1200.0,24.0
331,Natasha Teen in Pussy DAPTAP,1117,BlowJob,1178.0,1210.0,32.0
331,Natasha Teen in Pussy DAPTAP,1120,DP,1366.0,1396.0,30.0
331,Natasha Teen in Pussy DAPTAP,1118,BlowJob,2744.0,2772.0,28.0
332,Nia Nacci in Cum Bang 15 Sc3,1132,Cumshot,1060.0,1082.0,22.0
332,Nia Nacci in Cum Bang 15 Sc3,1123,BlowJob,1106.0,1152.0,46.0
332,Nia Nacci in Cum Bang 15 Sc3,1124,BlowJob,1234.0,1284.0,50.0
332,Nia Nacci in Cum Bang 15 Sc3,1126,BlowJob,1532.0,1566.0,34.0
332,Nia Nacci in Cum Bang 15 Sc3,1127,BlowJob,1598.0,1614.0,16.0
332,Nia Nacci in Cum Bang 15 Sc3,1128,BlowJob,1666.0,1696.0,30.0
332,Nia Nacci in Cum Bang 15 Sc3,1129,BlowJob,1742.0,1778.0,36.0
332,Nia Nacci in Cum Bang 15 Sc3,1130,BlowJob,1920.0,1930.0,10.0
333,Nia Nacci in We Fuck Black Girls 14 Sc5,1137,Grabbing Boobs,640.0,680.0,40.0
333,Nia Nacci in We Fuck Black Girls 14 Sc5,1138,Anal,1474.0,1532.0,58.0
333,Nia Nacci in We Fuck Black Girls 14 Sc5,1139,Anal,1606.0,1632.0,26.0
333,Nia Nacci in We Fuck Black Girls 14 Sc5,1140,Cumshot,1924.0,1952.0,28.0
334,Nia Nacci in White Out 9 Sc2,1142,BlowJob,84.0,134.0,50.0
334,Nia Nacci in White Out 9 Sc2,1155,Grabbing Boobs,1052.0,1104.0,52.0
334,Nia Nacci in White Out 9 Sc2,1144,BlowJob,1302.0,1354.0,52.0
334,Nia Nacci in White Out 9 Sc2,1145,BlowJob,1386.0,1402.0,16.0
334,Nia Nacci in White Out 9 Sc2,1161,Anal,1504.0,1530.0,26.0
334,Nia Nacci in White Out 9 Sc2,1156,Grabbing Boobs,1940.0,1978.0,38.0
334,Nia Nacci in White Out 9 Sc2,1152,Gangbang,1954.0,1976.0,22.0
334,Nia Nacci in White Out 9 Sc2,1158,Grabbing Boobs,2526.0,2558.0,32.0
334,Nia Nacci in White Out 9 Sc2,1147,BlowJob,2690.0,2718.0,28.0
334,Nia Nacci in White Out 9 Sc2,1148,BlowJob,2922.0,2938.0,16.0
334,Nia Nacci in White Out 9 Sc2,1159,Grabbing Boobs,3076.0,3118.0,42.0
334,Nia Nacci in White Out 9 Sc2,1160,Grabbing Boobs,3200.0,3224.0,24.0
334,Nia Nacci in White Out 9 Sc2,1149,BlowJob,3330.0,3358.0,28.0
334,Nia Nacci in White Out 9 Sc2,1162,Anal,3578.0,3594.0,16.0
334,Nia Nacci in White Out 9 Sc2,1153,Gangbang,3582.0,3620.0,38.0
334,Nia Nacci in White Out 9 Sc2,1163,Anal,3662.0,3680.0,18.0
334,Nia Nacci in White Out 9 Sc2,1151,BlowJob,3784.0,3792.0,8.0
334,Nia Nacci in White Out 9 Sc2,1164,Cumshot,3850.0,3860.0,10.0
334,Nia Nacci in White Out 9 Sc2,1165,Cumshot,3878.0,3890.0,12.0
334,Nia Nacci in White Out 9 Sc2,1166,Cumshot,3944.0,3970.0,26.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1167,BlowJob,264.0,314.0,50.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1177,Cumshot,334.0,370.0,36.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1168,BlowJob,468.0,488.0,20.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1175,BlowJob,686.0,696.0,10.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1169,BlowJob,742.0,764.0,22.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1170,BlowJob,810.0,846.0,36.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1180,Cumshot,876.0,892.0,16.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1171,BlowJob,934.0,988.0,54.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1181,Cumshot,978.0,996.0,18.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1187,Cumshot,1382.0,1388.0,6.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1173,BlowJob,1434.0,1444.0,10.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1182,Cumshot,1436.0,1446.0,10.0
335,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",1174,BlowJob,2022.0,2062.0,40.0
336,Nina Elle in Big Wet Milf Asses Sc2,1194,Anal,612.0,656.0,44.0
336,Nina Elle in Big Wet Milf Asses Sc2,1191,BlowJob,676.0,696.0,20.0
336,Nina Elle in Big Wet Milf Asses Sc2,1192,BlowJob,1496.0,1526.0,30.0
336,Nina Elle in Big Wet Milf Asses Sc2,1196,Cumshot,1842.0,1846.0,4.0
336,Nina Elle in Big Wet Milf Asses Sc2,1197,Cumshot,1862.0,1878.0,16.0
336,Nina Elle in Big Wet Milf Asses Sc2,1193,Grabbing Boobs,1870.0,1890.0,20.0
337,Nina Elle in Gang Bang Addiction Sc4,1199,BlowJob,762.0,790.0,28.0
337,Nina Elle in Gang Bang Addiction Sc4,1200,BlowJob,888.0,908.0,20.0
337,Nina Elle in Gang Bang Addiction Sc4,1206,Anal,908.0,938.0,30.0
337,Nina Elle in Gang Bang Addiction Sc4,1209,Gangbang,1070.0,1100.0,30.0
337,Nina Elle in Gang Bang Addiction Sc4,1201,BlowJob,1356.0,1370.0,14.0
337,Nina Elle in Gang Bang Addiction Sc4,1203,BlowJob,2248.0,2266.0,18.0
337,Nina Elle in Gang Bang Addiction Sc4,1208,Anal,2266.0,2284.0,18.0
337,Nina Elle in Gang Bang Addiction Sc4,1204,BlowJob,2300.0,2322.0,22.0
337,Nina Elle in Gang Bang Addiction Sc4,1205,BlowJob,2844.0,2866.0,22.0
338,Nina Elle in MILF Cumsluts Sc3,1212,Anal,494.0,510.0,16.0
338,Nina Elle in MILF Cumsluts Sc3,1213,Anal,730.0,752.0,22.0
338,Nina Elle in MILF Cumsluts Sc3,1214,Anal,990.0,1002.0,12.0
338,Nina Elle in MILF Cumsluts Sc3,1211,Grabbing Boobs,1058.0,1080.0,22.0
338,Nina Elle in MILF Cumsluts Sc3,1222,Titjob,1076.0,1100.0,24.0
338,Nina Elle in MILF Cumsluts Sc3,1223,Cumshot,1090.0,1110.0,20.0
338,Nina Elle in MILF Cumsluts Sc3,1226,Cumshot,1146.0,1154.0,8.0
338,Nina Elle in MILF Cumsluts Sc3,1215,Anal,1286.0,1312.0,26.0
338,Nina Elle in MILF Cumsluts Sc3,1219,Anal,1988.0,2022.0,34.0
338,Nina Elle in MILF Cumsluts Sc3,1224,Cumshot,2232.0,2240.0,8.0
338,Nina Elle in MILF Cumsluts Sc3,1228,Cumshot,2262.0,2314.0,52.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1232,BlowJob,170.0,184.0,14.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1229,Grabbing Boobs,254.0,286.0,32.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1235,BlowJob,644.0,666.0,22.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1241,Gangbang,662.0,682.0,20.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1230,Grabbing Boobs,1016.0,1038.0,22.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1239,Anal,1070.0,1094.0,24.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1240,Anal,1238.0,1266.0,28.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1236,BlowJob,1530.0,1558.0,28.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1237,BlowJob,1602.0,1624.0,22.0
339,"Olivia Del Rio, Shalina Devine in DP Fantasies 11 Sc1",1231,Grabbing Boobs,1610.0,1644.0,34.0
340,Phoenix Marie in Ass Worship 13 Sc4,1244,Anal,552.0,604.0,52.0
340,Phoenix Marie in Ass Worship 13 Sc4,1254,BlowJob,1034.0,1074.0,40.0
340,Phoenix Marie in Ass Worship 13 Sc4,1245,Anal,1036.0,1054.0,18.0
340,Phoenix Marie in Ass Worship 13 Sc4,1255,BlowJob,1142.0,1158.0,16.0
340,Phoenix Marie in Ass Worship 13 Sc4,1246,Anal,1332.0,1380.0,48.0
340,Phoenix Marie in Ass Worship 13 Sc4,1247,Anal,1466.0,1514.0,48.0
340,Phoenix Marie in Ass Worship 13 Sc4,1248,Anal,1556.0,1594.0,38.0
340,Phoenix Marie in Ass Worship 13 Sc4,1258,BlowJob,1674.0,1684.0,10.0
340,Phoenix Marie in Ass Worship 13 Sc4,1259,BlowJob,1748.0,1764.0,16.0
340,Phoenix Marie in Ass Worship 13 Sc4,1250,Anal,1894.0,1914.0,20.0
340,Phoenix Marie in Ass Worship 13 Sc4,1251,Anal,1952.0,1990.0,38.0
340,Phoenix Marie in Ass Worship 13 Sc4,1261,BlowJob,2038.0,2080.0,42.0
341,Rachele Richey in Gangbang Audition,1262,Grabbing Boobs,162.0,192.0,30.0
341,Rachele Richey in Gangbang Audition,1263,BlowJob,666.0,690.0,24.0
341,Rachele Richey in Gangbang Audition,1266,BlowJob,700.0,760.0,60.0
341,Rachele Richey in Gangbang Audition,1265,BlowJob,798.0,848.0,50.0
341,Rachele Richey in Gangbang Audition,1268,BlowJob,904.0,918.0,14.0
341,Rachele Richey in Gangbang Audition,1274,Anal,1064.0,1082.0,18.0
341,Rachele Richey in Gangbang Audition,1269,BlowJob,1110.0,1128.0,18.0
341,Rachele Richey in Gangbang Audition,1270,BlowJob,1290.0,1322.0,32.0
341,Rachele Richey in Gangbang Audition,1276,Anal,1428.0,1462.0,34.0
341,Rachele Richey in Gangbang Audition,1277,Anal,1508.0,1540.0,32.0
341,Rachele Richey in Gangbang Audition,1278,Anal,1584.0,1596.0,12.0
341,Rachele Richey in Gangbang Audition,1279,Anal,1654.0,1708.0,54.0
341,Rachele Richey in Gangbang Audition,1271,BlowJob,1700.0,1722.0,22.0
341,Rachele Richey in Gangbang Audition,1272,BlowJob,2534.0,2544.0,10.0
341,Rachele Richey in Gangbang Audition,1273,BlowJob,2584.0,2604.0,20.0
341,Rachele Richey in Gangbang Audition,1282,Cumshot,2666.0,2676.0,10.0
342,Rose Lynn in Airtight Diva Sc3,1283,Grabbing Boobs,212.0,268.0,56.0
342,Rose Lynn in Airtight Diva Sc3,1284,BlowJob,362.0,390.0,28.0
342,Rose Lynn in Airtight Diva Sc3,1285,BlowJob,428.0,464.0,36.0
342,Rose Lynn in Airtight Diva Sc3,1286,BlowJob,948.0,988.0,40.0
343,Sadie Summers in Gangbang Sluts Sc2,1309,Cumshot,542.0,586.0,44.0
343,Sadie Summers in Gangbang Sluts Sc2,1295,Gangbang,1166.0,1206.0,40.0
343,Sadie Summers in Gangbang Sluts Sc2,1296,Gangbang,1250.0,1292.0,42.0
343,Sadie Summers in Gangbang Sluts Sc2,1290,Grabbing Boobs,1608.0,1628.0,20.0
343,Sadie Summers in Gangbang Sluts Sc2,1297,Gangbang,1610.0,1644.0,34.0
343,Sadie Summers in Gangbang Sluts Sc2,1307,BlowJob,1688.0,1702.0,14.0
343,Sadie Summers in Gangbang Sluts Sc2,1298,Gangbang,1692.0,1712.0,20.0
343,Sadie Summers in Gangbang Sluts Sc2,1299,Gangbang,1780.0,1800.0,20.0
343,Sadie Summers in Gangbang Sluts Sc2,1300,Gangbang,1848.0,1900.0,52.0
343,Sadie Summers in Gangbang Sluts Sc2,1306,BlowJob,2132.0,2180.0,48.0
343,Sadie Summers in Gangbang Sluts Sc2,1302,Gangbang,2318.0,2338.0,20.0
343,Sadie Summers in Gangbang Sluts Sc2,1303,Gangbang,2378.0,2392.0,14.0
343,Sadie Summers in Gangbang Sluts Sc2,1292,Grabbing Boobs,2560.0,2602.0,42.0
343,Sadie Summers in Gangbang Sluts Sc2,1311,Cumshot,2782.0,2786.0,4.0
343,Sadie Summers in Gangbang Sluts Sc2,1312,Cumshot,2838.0,2842.0,4.0
343,Sadie Summers in Gangbang Sluts Sc2,1316,Cumshot,3022.0,3056.0,34.0
343,Sadie Summers in Gangbang Sluts Sc2,1317,Cumshot,3088.0,3098.0,10.0
344,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc2,1319,BlowJob,220.0,230.0,10.0
344,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc2,1320,BlowJob,302.0,316.0,14.0
344,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc2,1321,BlowJob,370.0,398.0,28.0
344,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc2,1323,BlowJob,636.0,662.0,26.0
344,Sai Tai Tiger in Die Haremsw√§chterin des √ñl Scheichs Sc2,1324,Grabbing Boobs,642.0,676.0,34.0
345,"Sai Tai Tiger, Daria Glower in Die Haremsw√§chterin des √ñl Scheichs Sc5",1325,69,46.0,64.0,18.0
345,"Sai Tai Tiger, Daria Glower in Die Haremsw√§chterin des √ñl Scheichs Sc5",1326,69,292.0,318.0,26.0
346,"Sai Tai Tiger, Daria Glower, Valerie Hilton in Die Haremsw√§chterin des √ñl Scheichs Sc1",1332,Cumshot,1534.0,1568.0,34.0
347,Sandra Parker in 1st at GB Junkies,1335,BlowJob,402.0,414.0,12.0
347,Sandra Parker in 1st at GB Junkies,1336,BlowJob,656.0,668.0,12.0
347,Sandra Parker in 1st at GB Junkies,1342,Anal,674.0,722.0,48.0
347,Sandra Parker in 1st at GB Junkies,1344,Anal,960.0,1006.0,46.0
347,Sandra Parker in 1st at GB Junkies,1338,BlowJob,1040.0,1056.0,16.0
347,Sandra Parker in 1st at GB Junkies,1339,BlowJob,1316.0,1330.0,14.0
347,Sandra Parker in 1st at GB Junkies,1346,Anal,1374.0,1430.0,56.0
347,Sandra Parker in 1st at GB Junkies,1340,BlowJob,1518.0,1528.0,10.0
347,Sandra Parker in 1st at GB Junkies,1347,Anal,1536.0,1554.0,18.0
347,Sandra Parker in 1st at GB Junkies,1348,Cumshot,1650.0,1658.0,8.0
348,Sandra Parker in Anal Driller 9 Sc3,1351,BlowJob,830.0,852.0,22.0
348,Sandra Parker in Anal Driller 9 Sc3,1352,BlowJob,942.0,998.0,56.0
348,Sandra Parker in Anal Driller 9 Sc3,1353,BlowJob,1040.0,1070.0,30.0
348,Sandra Parker in Anal Driller 9 Sc3,1354,BlowJob,1140.0,1198.0,58.0
348,Sandra Parker in Anal Driller 9 Sc3,1355,BlowJob,1320.0,1348.0,28.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1363,Anal,276.0,298.0,22.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1372,BlowJob,598.0,610.0,12.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1364,Anal,650.0,694.0,44.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1375,BlowJob,664.0,674.0,10.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1365,Anal,812.0,838.0,26.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1373,BlowJob,850.0,876.0,26.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1366,Anal,976.0,998.0,22.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1367,Anal,1034.0,1068.0,34.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1368,Anal,1192.0,1236.0,44.0
350,Sandra Parker in Cum Hungry Leave Full 2 Sc3,1369,Anal,1306.0,1330.0,24.0
351,Sandra Parker in Double Stuffed 8 Sc1,1377,BlowJob,632.0,662.0,30.0
351,Sandra Parker in Double Stuffed 8 Sc1,1383,Anal,692.0,702.0,10.0
351,Sandra Parker in Double Stuffed 8 Sc1,1384,Anal,764.0,796.0,32.0
351,Sandra Parker in Double Stuffed 8 Sc1,1378,BlowJob,888.0,916.0,28.0
351,Sandra Parker in Double Stuffed 8 Sc1,1385,Anal,958.0,980.0,22.0
351,Sandra Parker in Double Stuffed 8 Sc1,1379,BlowJob,974.0,996.0,22.0
351,Sandra Parker in Double Stuffed 8 Sc1,1380,BlowJob,1240.0,1290.0,50.0
351,Sandra Parker in Double Stuffed 8 Sc1,1387,Anal,1314.0,1346.0,32.0
351,Sandra Parker in Double Stuffed 8 Sc1,1381,BlowJob,1600.0,1658.0,58.0
351,Sandra Parker in Double Stuffed 8 Sc1,1389,Cumshot,1608.0,1612.0,4.0
351,Sandra Parker in Double Stuffed 8 Sc1,1390,Cumshot,1634.0,1686.0,52.0
352,Sara Retali in BBC Piss Gangbang,1391,Gangbang,40.0,92.0,52.0
352,Sara Retali in BBC Piss Gangbang,1398,BlowJob,110.0,136.0,26.0
352,Sara Retali in BBC Piss Gangbang,1392,Gangbang,192.0,204.0,12.0
352,Sara Retali in BBC Piss Gangbang,1399,BlowJob,194.0,214.0,20.0
352,Sara Retali in BBC Piss Gangbang,1400,BlowJob,360.0,406.0,46.0
352,Sara Retali in BBC Piss Gangbang,1393,Gangbang,610.0,656.0,46.0
352,Sara Retali in BBC Piss Gangbang,1409,Anal,748.0,792.0,44.0
352,Sara Retali in BBC Piss Gangbang,1394,Gangbang,792.0,818.0,26.0
352,Sara Retali in BBC Piss Gangbang,1402,BlowJob,830.0,868.0,38.0
352,Sara Retali in BBC Piss Gangbang,1404,BlowJob,1192.0,1216.0,24.0
352,Sara Retali in BBC Piss Gangbang,1395,Gangbang,1222.0,1258.0,36.0
352,Sara Retali in BBC Piss Gangbang,1396,Gangbang,1290.0,1318.0,28.0
352,Sara Retali in BBC Piss Gangbang,1405,BlowJob,1488.0,1500.0,12.0
352,Sara Retali in BBC Piss Gangbang,1410,Anal,1684.0,1718.0,34.0
352,Sara Retali in BBC Piss Gangbang,1397,Gangbang,1686.0,1732.0,46.0
352,Sara Retali in BBC Piss Gangbang,1406,BlowJob,1858.0,1910.0,52.0
352,Sara Retali in BBC Piss Gangbang,1411,Cumshot,2130.0,2142.0,12.0
352,Sara Retali in BBC Piss Gangbang,1413,Cumshot,2152.0,2180.0,28.0
352,Sara Retali in BBC Piss Gangbang,1412,Cumshot,2190.0,2196.0,6.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1414,BlowJob,122.0,158.0,36.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1418,Gangbang,136.0,174.0,38.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1424,Grabbing Boobs,300.0,326.0,26.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1419,Gangbang,420.0,432.0,12.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1420,Gangbang,804.0,838.0,34.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1425,Anal,1124.0,1178.0,54.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1421,Gangbang,1222.0,1248.0,26.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1426,Anal,1262.0,1302.0,40.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1416,BlowJob,1308.0,1360.0,52.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1422,Gangbang,1336.0,1350.0,14.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1427,Anal,1340.0,1366.0,26.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1423,Gangbang,1548.0,1598.0,50.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1428,Anal,1566.0,1582.0,16.0
353,Sara Retali in Slut Cant Get Enough Gangbang,1429,Cumshot,1754.0,1766.0,12.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1430,Gangbang,48.0,62.0,14.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1431,Gangbang,206.0,226.0,20.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1436,BlowJob,284.0,308.0,24.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1433,Gangbang,400.0,414.0,14.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1437,BlowJob,402.0,426.0,24.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1439,Anal,416.0,428.0,12.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1434,Gangbang,474.0,492.0,18.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1440,Anal,626.0,652.0,26.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1441,Anal,710.0,736.0,26.0
354,"Sara Retali, Nelson Mandingo in 4on1 BBC's Including DP",1438,BlowJob,964.0,986.0,22.0
355,"Sara Retali, Sapphire Astrea in Spa Day Gone Wild",1442,Grabbing Boobs,252.0,280.0,28.0
355,"Sara Retali, Sapphire Astrea in Spa Day Gone Wild",1443,BlowJob,778.0,802.0,24.0
356,Sarai Minx in Big Tit Slut Milks Cock,1445,Grabbing Boobs,278.0,334.0,56.0
356,Sarai Minx in Big Tit Slut Milks Cock,1447,BlowJob,508.0,564.0,56.0
356,Sarai Minx in Big Tit Slut Milks Cock,1449,BlowJob,584.0,626.0,42.0
356,Sarai Minx in Big Tit Slut Milks Cock,1446,Grabbing Boobs,652.0,684.0,32.0
356,Sarai Minx in Big Tit Slut Milks Cock,1451,Titjob,1472.0,1496.0,24.0
356,Sarai Minx in Big Tit Slut Milks Cock,1452,Cumshot,1476.0,1490.0,14.0
356,Sarai Minx in Big Tit Slut Milks Cock,1453,Cumshot,1504.0,1546.0,42.0
357,"Sheila Ortega, Sara Retali, Anny Star in 3 Latinas by the Pool",1460,Titjob,904.0,916.0,12.0
357,"Sheila Ortega, Sara Retali, Anny Star in 3 Latinas by the Pool",1461,Anal,1180.0,1236.0,56.0
357,"Sheila Ortega, Sara Retali, Anny Star in 3 Latinas by the Pool",1458,BlowJob,2230.0,2234.0,4.0
357,"Sheila Ortega, Sara Retali, Anny Star in 3 Latinas by the Pool",1459,BlowJob,2322.0,2330.0,8.0
358,Shyla Stylez in Anal Integrity Sc2,1470,Anal,1116.0,1158.0,42.0
358,Shyla Stylez in Anal Integrity Sc2,1466,BlowJob,1230.0,1266.0,36.0
358,Shyla Stylez in Anal Integrity Sc2,1467,Grabbing Boobs,1272.0,1294.0,22.0
358,Shyla Stylez in Anal Integrity Sc2,1471,Titjob,1284.0,1300.0,16.0
358,Shyla Stylez in Anal Integrity Sc2,1464,BlowJob,1470.0,1480.0,10.0
358,Shyla Stylez in Anal Integrity Sc2,1468,Grabbing Boobs,1502.0,1548.0,46.0
358,Shyla Stylez in Anal Integrity Sc2,1465,BlowJob,2014.0,2032.0,18.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1473,BlowJob,190.0,212.0,22.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1474,BlowJob,246.0,302.0,56.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1475,BlowJob,480.0,492.0,12.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1476,BlowJob,540.0,578.0,38.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1477,BlowJob,892.0,922.0,30.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1478,BlowJob,1258.0,1286.0,28.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1480,BlowJob,1788.0,1804.0,16.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1487,Anal,1840.0,1854.0,14.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1481,BlowJob,1912.0,1940.0,28.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1488,Anal,1954.0,1988.0,34.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1482,BlowJob,2140.0,2172.0,32.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1484,BlowJob,2488.0,2520.0,32.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1489,Anal,2542.0,2562.0,20.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1485,BlowJob,2642.0,2680.0,38.0
359,Sirena XXX in The Adventures of Potro and Pony S01E01,1486,BlowJob,2800.0,2820.0,20.0
360,Skin Diamond in Rump Raiders Sc3,1492,BlowJob,774.0,786.0,12.0
360,Skin Diamond in Rump Raiders Sc3,1493,BlowJob,826.0,868.0,42.0
360,Skin Diamond in Rump Raiders Sc3,1494,Anal,906.0,958.0,52.0
360,Skin Diamond in Rump Raiders Sc3,1495,Anal,1014.0,1062.0,48.0
361,,3538,Cumshot,1358.0,1362.0,4.0
361,,3564,Cumshot,1764.0,1778.0,14.0
361,,3453,Grabbing Boobs,2804.0,2838.0,34.0
361,,3540,Cumshot,2820.0,2852.0,32.0
361,,3541,Cumshot,3052.0,3104.0,52.0
361,,3454,Grabbing Boobs,3754.0,3810.0,56.0
361,,3518,BlowJob,3892.0,3902.0,10.0
361,,3455,Grabbing Boobs,4208.0,4230.0,22.0
361,,3456,Grabbing Boobs,4730.0,4772.0,42.0
361,,3457,Grabbing Boobs,4926.0,4974.0,48.0
361,,3582,Titjob,4930.0,4966.0,36.0
361,,3583,Titjob,5410.0,5424.0,14.0
361,,3570,Cumshot,5454.0,5510.0,56.0
361,,3459,Grabbing Boobs,5666.0,5714.0,48.0
361,,3571,Cumshot,6348.0,6366.0,18.0
361,,3547,Cumshot,6568.0,6576.0,8.0
361,,3573,Cumshot,7966.0,8012.0,46.0
361,,3460,Grabbing Boobs,8028.0,8078.0,50.0
361,,3584,Titjob,9490.0,9522.0,32.0
361,,3550,Cumshot,10168.0,10172.0,4.0
361,,3577,Cumshot,11414.0,11420.0,6.0
361,,3463,Grabbing Boobs,11842.0,11884.0,42.0
361,,3585,Titjob,11878.0,11898.0,20.0
361,,3523,BlowJob,11900.0,11912.0,12.0
361,,3464,Grabbing Boobs,11920.0,11942.0,22.0
361,,3586,Titjob,11936.0,11952.0,16.0
361,,3508,BlowJob,12116.0,12130.0,14.0
361,,3525,BlowJob,12148.0,12194.0,46.0
361,,3465,Grabbing Boobs,12158.0,12178.0,20.0
361,,3526,BlowJob,12236.0,12274.0,38.0
361,,3527,BlowJob,12324.0,12372.0,48.0
361,,3578,Cumshot,12354.0,12396.0,42.0
361,,3466,Grabbing Boobs,12384.0,12424.0,40.0
361,,3467,Grabbing Boobs,12642.0,12688.0,46.0
361,,3529,BlowJob,12704.0,12718.0,14.0
361,,3468,Grabbing Boobs,13206.0,13226.0,20.0
361,,3469,Grabbing Boobs,13326.0,13346.0,20.0
361,,3554,Cumshot,14380.0,14384.0,4.0
361,,3556,Cumshot,14534.0,14556.0,22.0
361,,3557,Cumshot,14758.0,14770.0,12.0
361,,3471,Grabbing Boobs,15488.0,15526.0,38.0
361,,3558,Cumshot,15776.0,15824.0,48.0
361,,3559,Cumshot,16422.0,16442.0,20.0
361,,3563,Cumshot,18522.0,18532.0,10.0
362,Summer Day in America Bukkake Live,1497,Gangbang,914.0,938.0,24.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1508,Gangbang,294.0,308.0,14.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1509,Gangbang,382.0,392.0,10.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1514,Cumshot,384.0,388.0,4.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1501,BlowJob,520.0,532.0,12.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1502,BlowJob,572.0,606.0,34.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1510,Gangbang,708.0,730.0,22.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1503,BlowJob,762.0,798.0,36.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1511,Gangbang,848.0,864.0,16.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1518,Anal,850.0,862.0,12.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1519,Anal,906.0,958.0,52.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1512,Gangbang,1014.0,1030.0,16.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1504,BlowJob,1106.0,1122.0,16.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1505,BlowJob,1310.0,1352.0,42.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1521,Anal,1388.0,1404.0,16.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1507,BlowJob,1592.0,1638.0,46.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1513,Gangbang,1646.0,1682.0,36.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1517,Cumshot,1698.0,1716.0,18.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1515,Cumshot,1750.0,1766.0,16.0
363,Summer Day in Interracial Double Penetration Gangbangs 2 Sc2,1516,Cumshot,1854.0,1886.0,32.0
364,Summer Vixen in Gangbang Sluts Sc3,1522,Gangbang,86.0,116.0,30.0
364,Summer Vixen in Gangbang Sluts Sc3,1523,Gangbang,158.0,198.0,40.0
364,Summer Vixen in Gangbang Sluts Sc3,1535,BlowJob,222.0,242.0,20.0
364,Summer Vixen in Gangbang Sluts Sc3,1538,Anal,230.0,252.0,22.0
364,Summer Vixen in Gangbang Sluts Sc3,1524,Gangbang,364.0,374.0,10.0
364,Summer Vixen in Gangbang Sluts Sc3,1536,BlowJob,368.0,424.0,56.0
364,Summer Vixen in Gangbang Sluts Sc3,1539,Anal,398.0,444.0,46.0
364,Summer Vixen in Gangbang Sluts Sc3,1525,Gangbang,424.0,448.0,24.0
364,Summer Vixen in Gangbang Sluts Sc3,1540,Anal,602.0,612.0,10.0
364,Summer Vixen in Gangbang Sluts Sc3,1526,Gangbang,688.0,706.0,18.0
364,Summer Vixen in Gangbang Sluts Sc3,1541,Anal,710.0,724.0,14.0
364,Summer Vixen in Gangbang Sluts Sc3,1542,Anal,858.0,900.0,42.0
364,Summer Vixen in Gangbang Sluts Sc3,1528,Gangbang,956.0,1014.0,58.0
364,Summer Vixen in Gangbang Sluts Sc3,1537,BlowJob,1206.0,1218.0,12.0
364,Summer Vixen in Gangbang Sluts Sc3,1529,Gangbang,1288.0,1338.0,50.0
364,Summer Vixen in Gangbang Sluts Sc3,1544,Anal,1378.0,1396.0,18.0
364,Summer Vixen in Gangbang Sluts Sc3,1530,Gangbang,1386.0,1400.0,14.0
364,Summer Vixen in Gangbang Sluts Sc3,1531,Gangbang,1462.0,1498.0,36.0
364,Summer Vixen in Gangbang Sluts Sc3,1548,DP,1484.0,1500.0,16.0
364,Summer Vixen in Gangbang Sluts Sc3,1545,Anal,1486.0,1504.0,18.0
364,Summer Vixen in Gangbang Sluts Sc3,1532,Gangbang,1692.0,1722.0,30.0
364,Summer Vixen in Gangbang Sluts Sc3,1533,Gangbang,1798.0,1826.0,28.0
364,Summer Vixen in Gangbang Sluts Sc3,1534,Gangbang,1874.0,1886.0,12.0
364,Summer Vixen in Gangbang Sluts Sc3,1549,Cumshot,2290.0,2318.0,28.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1550,Grabbing Boobs,266.0,290.0,24.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1551,Grabbing Boobs,462.0,500.0,38.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1554,BlowJob,996.0,1008.0,12.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1560,BlowJob,1086.0,1098.0,12.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1562,BlowJob,1182.0,1200.0,18.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1563,Gangbang,1402.0,1416.0,14.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1555,BlowJob,1468.0,1478.0,10.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1556,BlowJob,1700.0,1748.0,48.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1557,BlowJob,1908.0,1944.0,36.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1565,Anal,2236.0,2270.0,34.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1558,BlowJob,2356.0,2376.0,20.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1566,Anal,2420.0,2442.0,22.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1552,Grabbing Boobs,2472.0,2518.0,46.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1567,Anal,2738.0,2792.0,54.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1564,Gangbang,2874.0,2884.0,10.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1568,Anal,3006.0,3042.0,36.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1569,Anal,3112.0,3140.0,28.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1570,Anal,3284.0,3344.0,60.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1571,Anal,3388.0,3422.0,34.0
365,Syren De Mer in Lewood Gangbang Battle of the MILFs 3 Sc1,1561,BlowJob,3620.0,3628.0,8.0
366,Tekohas in Ass & BigTits,1573,BlowJob,206.0,234.0,28.0
366,Tekohas in Ass & BigTits,1574,BlowJob,344.0,404.0,60.0
366,Tekohas in Ass & BigTits,1576,Grabbing Boobs,432.0,452.0,20.0
366,Tekohas in Ass & BigTits,1577,Anal,658.0,690.0,32.0
366,Tekohas in Ass & BigTits,1575,BlowJob,972.0,982.0,10.0
366,Tekohas in Ass & BigTits,1578,Cumshot,1070.0,1080.0,10.0
367,Tekohas in Bareback Party in Stuttgart,1585,BlowJob,434.0,458.0,24.0
367,Tekohas in Bareback Party in Stuttgart,1581,Grabbing Boobs,850.0,876.0,26.0
367,Tekohas in Bareback Party in Stuttgart,1586,BlowJob,898.0,910.0,12.0
367,Tekohas in Bareback Party in Stuttgart,1587,BlowJob,1028.0,1040.0,12.0
367,Tekohas in Bareback Party in Stuttgart,1588,BlowJob,1090.0,1104.0,14.0
367,Tekohas in Bareback Party in Stuttgart,1582,Grabbing Boobs,1092.0,1114.0,22.0
367,Tekohas in Bareback Party in Stuttgart,1589,BlowJob,1184.0,1244.0,60.0
367,Tekohas in Bareback Party in Stuttgart,1596,Cumshot,1234.0,1240.0,6.0
367,Tekohas in Bareback Party in Stuttgart,1600,Cumshot,1250.0,1300.0,50.0
367,Tekohas in Bareback Party in Stuttgart,1590,BlowJob,1338.0,1348.0,10.0
367,Tekohas in Bareback Party in Stuttgart,1607,Anal,1460.0,1476.0,16.0
367,Tekohas in Bareback Party in Stuttgart,1591,BlowJob,2142.0,2162.0,20.0
367,Tekohas in Bareback Party in Stuttgart,1601,Cumshot,2232.0,2266.0,34.0
367,Tekohas in Bareback Party in Stuttgart,1602,Cumshot,2324.0,2360.0,36.0
367,Tekohas in Bareback Party in Stuttgart,1603,Cumshot,2450.0,2466.0,16.0
367,Tekohas in Bareback Party in Stuttgart,1592,BlowJob,2530.0,2548.0,18.0
367,Tekohas in Bareback Party in Stuttgart,1593,BlowJob,2656.0,2668.0,12.0
367,Tekohas in Bareback Party in Stuttgart,1595,BlowJob,2766.0,2792.0,26.0
367,Tekohas in Bareback Party in Stuttgart,1598,Cumshot,2780.0,2806.0,26.0
367,Tekohas in Bareback Party in Stuttgart,1605,Cumshot,2868.0,2872.0,4.0
368,Thai Suzy in WeLoveBukkake 4,1611,Cumshot,90.0,110.0,20.0
368,Thai Suzy in WeLoveBukkake 4,1612,Cumshot,974.0,994.0,20.0
369,Tia Maria in Cum On Melon Tits,1613,BlowJob,244.0,270.0,26.0
369,Tia Maria in Cum On Melon Tits,1614,BlowJob,338.0,348.0,10.0
369,Tia Maria in Cum On Melon Tits,1620,Grabbing Boobs,900.0,932.0,32.0
369,Tia Maria in Cum On Melon Tits,1615,BlowJob,1152.0,1186.0,34.0
369,Tia Maria in Cum On Melon Tits,1616,BlowJob,1248.0,1272.0,24.0
369,Tia Maria in Cum On Melon Tits,1621,Grabbing Boobs,1294.0,1320.0,26.0
369,Tia Maria in Cum On Melon Tits,1622,Grabbing Boobs,1702.0,1740.0,38.0
369,Tia Maria in Cum On Melon Tits,1623,Grabbing Boobs,2648.0,2708.0,60.0
369,Tia Maria in Cum On Melon Tits,1624,Cumshot,2896.0,2918.0,22.0
370,Tia Maria in DPd By Two BWCs,1626,BlowJob,236.0,254.0,18.0
370,Tia Maria in DPd By Two BWCs,1630,Cumshot,660.0,680.0,20.0
370,Tia Maria in DPd By Two BWCs,1633,Anal,920.0,962.0,42.0
370,Tia Maria in DPd By Two BWCs,1634,Anal,1034.0,1076.0,42.0
370,Tia Maria in DPd By Two BWCs,1628,BlowJob,1336.0,1350.0,14.0
370,Tia Maria in DPd By Two BWCs,1629,BlowJob,1384.0,1432.0,48.0
370,Tia Maria in DPd By Two BWCs,1636,Anal,1482.0,1500.0,18.0
370,Tia Maria in DPd By Two BWCs,1631,Cumshot,1628.0,1650.0,22.0
370,Tia Maria in DPd By Two BWCs,1632,Cumshot,1660.0,1668.0,8.0
371,Tyra Ride in First BBC  DP Gangbang,1651,Pissing,366.0,370.0,4.0
371,Tyra Ride in First BBC  DP Gangbang,1644,BlowJob,378.0,414.0,36.0
371,Tyra Ride in First BBC  DP Gangbang,1645,BlowJob,468.0,478.0,10.0
371,Tyra Ride in First BBC  DP Gangbang,1646,BlowJob,538.0,580.0,42.0
371,Tyra Ride in First BBC  DP Gangbang,1638,Gangbang,584.0,598.0,14.0
371,Tyra Ride in First BBC  DP Gangbang,1647,BlowJob,674.0,724.0,50.0
371,Tyra Ride in First BBC  DP Gangbang,1639,Gangbang,682.0,696.0,14.0
371,Tyra Ride in First BBC  DP Gangbang,1648,BlowJob,944.0,970.0,26.0
371,Tyra Ride in First BBC  DP Gangbang,1640,Gangbang,1012.0,1040.0,28.0
371,Tyra Ride in First BBC  DP Gangbang,1652,Pissing,1200.0,1204.0,4.0
371,Tyra Ride in First BBC  DP Gangbang,1654,Anal,1344.0,1356.0,12.0
371,Tyra Ride in First BBC  DP Gangbang,1655,Anal,1390.0,1412.0,22.0
371,Tyra Ride in First BBC  DP Gangbang,1657,Anal,1626.0,1662.0,36.0
371,Tyra Ride in First BBC  DP Gangbang,1649,BlowJob,1748.0,1766.0,18.0
371,Tyra Ride in First BBC  DP Gangbang,1658,Anal,1750.0,1782.0,32.0
371,Tyra Ride in First BBC  DP Gangbang,1642,Gangbang,1908.0,1936.0,28.0
371,Tyra Ride in First BBC  DP Gangbang,1659,Anal,1970.0,2016.0,46.0
371,Tyra Ride in First BBC  DP Gangbang,1660,Cumshot,2274.0,2320.0,46.0
371,Tyra Ride in First BBC  DP Gangbang,1661,Cumshot,2348.0,2372.0,24.0
371,Tyra Ride in First BBC  DP Gangbang,1650,BlowJob,2354.0,2364.0,10.0
372,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc10,1662,BlowJob,104.0,140.0,36.0
373,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc11,1663,BlowJob,40.0,54.0,14.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1678,Gangbang,192.0,202.0,10.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1679,Gangbang,434.0,474.0,40.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1665,BlowJob,552.0,582.0,30.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1680,Gangbang,560.0,576.0,16.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1666,BlowJob,732.0,752.0,20.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1681,Gangbang,884.0,914.0,30.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1667,BlowJob,888.0,898.0,10.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1682,Gangbang,1042.0,1052.0,10.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1668,BlowJob,1102.0,1128.0,26.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1683,Grabbing Boobs,1174.0,1202.0,28.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1684,Anal,1308.0,1318.0,10.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1670,BlowJob,1428.0,1462.0,34.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1685,Anal,1474.0,1488.0,14.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1686,Anal,1578.0,1608.0,30.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1687,Anal,1678.0,1694.0,16.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1671,BlowJob,1694.0,1706.0,12.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1688,Anal,1792.0,1802.0,10.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1689,Anal,1852.0,1866.0,14.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1692,DP,1904.0,1922.0,18.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1673,BlowJob,2238.0,2256.0,18.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1675,BlowJob,2756.0,2788.0,32.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1691,Anal,2924.0,2936.0,12.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1676,BlowJob,2932.0,2956.0,24.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1693,Cumshot,2976.0,2984.0,8.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1694,Cumshot,3016.0,3074.0,58.0
374,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc12,1695,Cumshot,3166.0,3222.0,56.0
376,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc8,1696,Grabbing Boobs,204.0,232.0,28.0
377,Veronica Avluv in Lewood Gangbang Battle of the MILFs Sc9,1697,BlowJob,218.0,234.0,16.0
378,Veronica Leal in Domination Gangbang,1698,BlowJob,76.0,94.0,18.0
378,Veronica Leal in Domination Gangbang,1699,BlowJob,148.0,184.0,36.0
378,Veronica Leal in Domination Gangbang,1709,Gangbang,236.0,252.0,16.0
378,Veronica Leal in Domination Gangbang,1710,Gangbang,292.0,324.0,32.0
378,Veronica Leal in Domination Gangbang,1711,Gangbang,570.0,580.0,10.0
378,Veronica Leal in Domination Gangbang,1712,Gangbang,640.0,652.0,12.0
378,Veronica Leal in Domination Gangbang,1724,Pissing,1232.0,1250.0,18.0
378,Veronica Leal in Domination Gangbang,1701,BlowJob,1280.0,1308.0,28.0
378,Veronica Leal in Domination Gangbang,1713,Gangbang,1330.0,1364.0,34.0
378,Veronica Leal in Domination Gangbang,1702,BlowJob,1342.0,1368.0,26.0
378,Veronica Leal in Domination Gangbang,1714,Gangbang,1430.0,1452.0,22.0
378,Veronica Leal in Domination Gangbang,1715,Gangbang,1498.0,1520.0,22.0
378,Veronica Leal in Domination Gangbang,1716,Gangbang,1964.0,1996.0,32.0
378,Veronica Leal in Domination Gangbang,1717,Gangbang,2178.0,2196.0,18.0
378,Veronica Leal in Domination Gangbang,1718,Gangbang,2254.0,2270.0,16.0
378,Veronica Leal in Domination Gangbang,1703,BlowJob,2398.0,2432.0,34.0
378,Veronica Leal in Domination Gangbang,1725,Pissing,2484.0,2512.0,28.0
378,Veronica Leal in Domination Gangbang,1719,Gangbang,2548.0,2578.0,30.0
378,Veronica Leal in Domination Gangbang,1704,BlowJob,2550.0,2562.0,12.0
378,Veronica Leal in Domination Gangbang,1705,BlowJob,2700.0,2726.0,26.0
378,Veronica Leal in Domination Gangbang,1706,BlowJob,2786.0,2812.0,26.0
378,Veronica Leal in Domination Gangbang,1720,Gangbang,2832.0,2856.0,24.0
378,Veronica Leal in Domination Gangbang,1707,BlowJob,2844.0,2858.0,14.0
378,Veronica Leal in Domination Gangbang,1721,Gangbang,2954.0,2970.0,16.0
378,Veronica Leal in Domination Gangbang,1723,Grabbing Boobs,3206.0,3238.0,32.0
379,Vittoria Devine in DP Pee 5on1,1726,BlowJob,1042.0,1078.0,36.0
379,Vittoria Devine in DP Pee 5on1,1727,BlowJob,1132.0,1190.0,58.0
379,Vittoria Devine in DP Pee 5on1,1741,Gangbang,1380.0,1426.0,46.0
379,Vittoria Devine in DP Pee 5on1,1750,Pissing,1606.0,1648.0,42.0
379,Vittoria Devine in DP Pee 5on1,1743,Gangbang,2144.0,2172.0,28.0
379,Vittoria Devine in DP Pee 5on1,1744,Gangbang,2224.0,2254.0,30.0
379,Vittoria Devine in DP Pee 5on1,1730,BlowJob,2418.0,2450.0,32.0
379,Vittoria Devine in DP Pee 5on1,1745,Gangbang,2622.0,2642.0,20.0
379,Vittoria Devine in DP Pee 5on1,1746,Gangbang,2782.0,2804.0,22.0
379,Vittoria Devine in DP Pee 5on1,1732,BlowJob,2942.0,2968.0,26.0
379,Vittoria Devine in DP Pee 5on1,1734,BlowJob,3510.0,3562.0,52.0
379,Vittoria Devine in DP Pee 5on1,1747,Gangbang,3592.0,3614.0,22.0
379,Vittoria Devine in DP Pee 5on1,1735,BlowJob,3650.0,3680.0,30.0
379,Vittoria Devine in DP Pee 5on1,1748,Gangbang,3760.0,3788.0,28.0
379,Vittoria Devine in DP Pee 5on1,1749,Gangbang,3862.0,3894.0,32.0
379,Vittoria Devine in DP Pee 5on1,1736,BlowJob,3868.0,3910.0,42.0
379,Vittoria Devine in DP Pee 5on1,1752,Cumshot,4692.0,4696.0,4.0
379,Vittoria Devine in DP Pee 5on1,1753,Cumshot,4734.0,4742.0,8.0
380,Vittoria Devine in Domination Gangbang,1757,BlowJob,296.0,346.0,50.0
380,Vittoria Devine in Domination Gangbang,1758,BlowJob,356.0,360.0,4.0
380,Vittoria Devine in Domination Gangbang,1759,BlowJob,394.0,432.0,38.0
380,Vittoria Devine in Domination Gangbang,1760,BlowJob,472.0,482.0,10.0
380,Vittoria Devine in Domination Gangbang,1754,Anal,698.0,716.0,18.0
380,Vittoria Devine in Domination Gangbang,1762,Gangbang,1048.0,1068.0,20.0
380,Vittoria Devine in Domination Gangbang,1764,Cumshot,1190.0,1194.0,4.0
380,Vittoria Devine in Domination Gangbang,1763,Gangbang,1204.0,1230.0,26.0
380,Vittoria Devine in Domination Gangbang,1755,Anal,1286.0,1328.0,42.0
380,Vittoria Devine in Domination Gangbang,1767,Pissing,1496.0,1502.0,6.0
380,Vittoria Devine in Domination Gangbang,1766,Cumshot,2060.0,2066.0,6.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1773,BlowJob,374.0,412.0,38.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1768,Anal,582.0,610.0,28.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1769,Anal,756.0,796.0,40.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1776,BlowJob,1612.0,1654.0,42.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1772,Anal,2292.0,2304.0,12.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1779,Cumshot,2590.0,2612.0,22.0
381,Vit√≥ria Beatriz in 2 Cocks Christmas Gift,1777,BlowJob,2594.0,2610.0,16.0
382,Vit√≥ria Beatriz in Edjunior VideoGuru,1784,Anal,380.0,396.0,16.0
382,Vit√≥ria Beatriz in Edjunior VideoGuru,1782,BlowJob,412.0,460.0,48.0
382,Vit√≥ria Beatriz in Edjunior VideoGuru,1785,Anal,610.0,620.0,10.0
383,Willow Ryder in I Love Anal 3 Sc3,1798,Cumshot,516.0,544.0,28.0
383,Willow Ryder in I Love Anal 3 Sc3,1795,BlowJob,948.0,952.0,4.0
383,Willow Ryder in I Love Anal 3 Sc3,1787,BlowJob,1052.0,1074.0,22.0
383,Willow Ryder in I Love Anal 3 Sc3,1788,BlowJob,1310.0,1346.0,36.0
383,Willow Ryder in I Love Anal 3 Sc3,1789,BlowJob,1430.0,1440.0,10.0
383,Willow Ryder in I Love Anal 3 Sc3,1790,BlowJob,1526.0,1542.0,16.0
383,Willow Ryder in I Love Anal 3 Sc3,1791,BlowJob,1658.0,1692.0,34.0
383,Willow Ryder in I Love Anal 3 Sc3,1792,BlowJob,1774.0,1788.0,14.0
383,Willow Ryder in I Love Anal 3 Sc3,1797,BlowJob,2142.0,2154.0,12.0
384,Yasmina Khan in Birthday Gangbang,1804,Gangbang,58.0,98.0,40.0
384,Yasmina Khan in Birthday Gangbang,1807,BlowJob,224.0,268.0,44.0
384,Yasmina Khan in Birthday Gangbang,1805,Gangbang,596.0,610.0,14.0
384,Yasmina Khan in Birthday Gangbang,1808,BlowJob,668.0,694.0,26.0
384,Yasmina Khan in Birthday Gangbang,1809,BlowJob,822.0,878.0,56.0
384,Yasmina Khan in Birthday Gangbang,1812,Anal,904.0,926.0,22.0
384,Yasmina Khan in Birthday Gangbang,1813,Anal,960.0,972.0,12.0
384,Yasmina Khan in Birthday Gangbang,1814,Anal,1072.0,1100.0,28.0
384,Yasmina Khan in Birthday Gangbang,1815,Cumshot,1240.0,1254.0,14.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1817,BlowJob,466.0,508.0,42.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1818,BlowJob,586.0,596.0,10.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1829,Anal,648.0,662.0,14.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1835,Gangbang,692.0,706.0,14.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1819,BlowJob,744.0,756.0,12.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1820,BlowJob,806.0,860.0,54.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1821,BlowJob,1070.0,1086.0,16.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1831,Anal,1594.0,1616.0,22.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1822,BlowJob,1708.0,1724.0,16.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1832,Anal,1742.0,1774.0,32.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1823,BlowJob,1814.0,1856.0,42.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1824,BlowJob,1966.0,2004.0,38.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1825,BlowJob,2092.0,2120.0,28.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1834,Anal,2234.0,2248.0,14.0
385,Yasmina Khan in Play with 4 Cocks at Once!,1826,BlowJob,2284.0,2306.0,22.0
386,AJ Applegate in Gangbang Me Sc1,2402,BlowJob,368.0,420.0,52.0
386,AJ Applegate in Gangbang Me Sc1,2406,Anal,984.0,996.0,12.0
386,AJ Applegate in Gangbang Me Sc1,2403,BlowJob,1166.0,1212.0,46.0
386,AJ Applegate in Gangbang Me Sc1,2416,Grabbing Boobs,1624.0,1672.0,48.0
386,AJ Applegate in Gangbang Me Sc1,2408,Anal,1774.0,1810.0,36.0
386,AJ Applegate in Gangbang Me Sc1,2409,Anal,1898.0,1950.0,52.0
386,AJ Applegate in Gangbang Me Sc1,2410,Anal,2090.0,2124.0,34.0
386,AJ Applegate in Gangbang Me Sc1,2411,Anal,2296.0,2338.0,42.0
386,AJ Applegate in Gangbang Me Sc1,2412,Anal,2384.0,2410.0,26.0
386,AJ Applegate in Gangbang Me Sc1,2414,Anal,2690.0,2714.0,24.0
386,AJ Applegate in Gangbang Me Sc1,2417,Cumshot,3090.0,3118.0,28.0
386,AJ Applegate in Gangbang Me Sc1,2419,Cumshot,3140.0,3144.0,4.0
386,AJ Applegate in Gangbang Me Sc1,2418,Cumshot,3178.0,3182.0,4.0
387,"Abigail Mac, Alina Lopez in Blindsided Sc3",3452,Grabbing Boobs,430.0,454.0,24.0
388,Adira Allure in Interracial Blowbang 24 Sc1,1977,Grabbing Boobs,826.0,856.0,30.0
388,Adira Allure in Interracial Blowbang 24 Sc1,1971,Gangbang,1510.0,1530.0,20.0
388,Adira Allure in Interracial Blowbang 24 Sc1,1972,Gangbang,1572.0,1588.0,16.0
388,Adira Allure in Interracial Blowbang 24 Sc1,1973,Gangbang,1784.0,1822.0,38.0
388,Adira Allure in Interracial Blowbang 24 Sc1,1979,Cumshot,1998.0,2018.0,20.0
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",2344,BlowJob,328.0,384.0,56.0
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",2345,BlowJob,680.0,736.0,56.0
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",2346,BlowJob,838.0,884.0,46.0
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",2348,Gangbang,916.0,926.0,10.0
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",2347,BlowJob,984.0,990.0,6.0
389,"Adira Allure, Andi Avalon, Annie King, Cami Strella, Charli Phoenix in Graduation Overload Sc2",2349,Gangbang,1126.0,1180.0,54.0
392,"Adriana Chechik, Gaia in Grease XXX A Parody Sc5",2390,BlowJob,1104.0,1138.0,34.0
392,"Adriana Chechik, Gaia in Grease XXX A Parody Sc5",2392,Cumshot,1232.0,1246.0,14.0
394,Adrianna Luna in Praise The Load 7 Sc1,2396,Gangbang,560.0,580.0,20.0
394,Adrianna Luna in Praise The Load 7 Sc1,2397,Gangbang,820.0,848.0,28.0
394,Adrianna Luna in Praise The Load 7 Sc1,2398,Grabbing Boobs,872.0,894.0,22.0
394,Adrianna Luna in Praise The Load 7 Sc1,2400,Titjob,976.0,998.0,22.0
394,Adrianna Luna in Praise The Load 7 Sc1,2399,Grabbing Boobs,1034.0,1056.0,22.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2128,BlowJob,682.0,724.0,42.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2129,BlowJob,1012.0,1036.0,24.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2136,BlowJob,1058.0,1062.0,4.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2130,BlowJob,1276.0,1296.0,20.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2131,BlowJob,1352.0,1368.0,16.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2132,BlowJob,1652.0,1694.0,42.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2137,Grabbing Boobs,1748.0,1788.0,40.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2133,BlowJob,1778.0,1838.0,60.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2134,BlowJob,1870.0,1884.0,14.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2135,BlowJob,1926.0,1946.0,20.0
395,Adrianna Luna in Slut Puppies 5 Sc5,2138,Cumshot,1930.0,1954.0,24.0
396,Aidra Fox in Gangbanged 7 Sc1,1987,BlowJob,1854.0,1898.0,44.0
396,Aidra Fox in Gangbanged 7 Sc1,2000,Gangbang,2018.0,2028.0,10.0
396,Aidra Fox in Gangbanged 7 Sc1,1996,Anal,2776.0,2800.0,24.0
396,Aidra Fox in Gangbanged 7 Sc1,1997,Anal,2842.0,2896.0,54.0
396,Aidra Fox in Gangbanged 7 Sc1,1989,BlowJob,2906.0,2918.0,12.0
396,Aidra Fox in Gangbanged 7 Sc1,1998,Anal,2990.0,3000.0,10.0
396,Aidra Fox in Gangbanged 7 Sc1,1990,BlowJob,3034.0,3046.0,12.0
396,Aidra Fox in Gangbanged 7 Sc1,2002,Cumshot,3420.0,3434.0,14.0
396,Aidra Fox in Gangbanged 7 Sc1,2004,Cumshot,3464.0,3474.0,10.0
397,Alena Croft in Blacks on Cougars 17 Sc1,2423,Anal,1128.0,1162.0,34.0
397,Alena Croft in Blacks on Cougars 17 Sc1,2421,BlowJob,1162.0,1182.0,20.0
397,Alena Croft in Blacks on Cougars 17 Sc1,2425,Cumshot,1702.0,1736.0,34.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2081,Gangbang,326.0,360.0,34.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2082,Gangbang,402.0,416.0,14.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2083,Gangbang,494.0,524.0,30.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2079,BlowJob,946.0,956.0,10.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2080,BlowJob,990.0,1002.0,12.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2084,Gangbang,1580.0,1610.0,30.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2087,Cumshot,2024.0,2028.0,4.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2085,Gangbang,2044.0,2054.0,10.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2076,Grabbing Boobs,2058.0,2076.0,18.0
399,Alena Croft in Gangbang her Little White Thang! 20 Sc1,2088,Cumshot,2068.0,2074.0,6.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2090,Grabbing Boobs,86.0,106.0,20.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2092,Grabbing Boobs,644.0,666.0,22.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2096,BlowJob,696.0,750.0,54.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2100,Anal,818.0,852.0,34.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2093,Grabbing Boobs,1058.0,1106.0,48.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2107,Gangbang,1362.0,1374.0,12.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2097,BlowJob,1810.0,1868.0,58.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2101,Anal,2110.0,2140.0,30.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2102,Anal,2600.0,2614.0,14.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2104,Anal,3068.0,3078.0,10.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2108,Gangbang,3228.0,3240.0,12.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2098,BlowJob,3240.0,3292.0,52.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2105,Anal,3500.0,3522.0,22.0
400,"Alena Croft, Cherie DeVille, Jasmine Jae, Nina Elle, Ryan Conner in Manuel Ferrara's Reverse Gangbang 3 Sc2",2106,Anal,3556.0,3588.0,32.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2118,Cumshot,728.0,760.0,32.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2111,Cumshot,812.0,856.0,44.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2120,Cumshot,912.0,934.0,22.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2112,Cumshot,1096.0,1100.0,4.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2113,Cumshot,1418.0,1440.0,22.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2114,Cumshot,1492.0,1502.0,10.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2115,Cumshot,1900.0,1912.0,12.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2116,Cumshot,2006.0,2030.0,24.0
401,Alexa Aimes in Darkko's Throat Fucks 5 Sc9,2121,Cumshot,2064.0,2106.0,42.0
402,Alexis Ford in Gang Bang Addiction Sc3,2459,Grabbing Boobs,322.0,352.0,30.0
402,Alexis Ford in Gang Bang Addiction Sc3,2462,BlowJob,360.0,416.0,56.0
402,Alexis Ford in Gang Bang Addiction Sc3,2475,BlowJob,462.0,484.0,22.0
402,Alexis Ford in Gang Bang Addiction Sc3,2482,Anal,1032.0,1050.0,18.0
402,Alexis Ford in Gang Bang Addiction Sc3,2481,Gangbang,1034.0,1094.0,60.0
402,Alexis Ford in Gang Bang Addiction Sc3,2460,Grabbing Boobs,1358.0,1394.0,36.0
402,Alexis Ford in Gang Bang Addiction Sc3,2483,Anal,1398.0,1408.0,10.0
402,Alexis Ford in Gang Bang Addiction Sc3,2466,BlowJob,1534.0,1570.0,36.0
402,Alexis Ford in Gang Bang Addiction Sc3,2467,BlowJob,1618.0,1670.0,52.0
402,Alexis Ford in Gang Bang Addiction Sc3,2484,Anal,1700.0,1726.0,26.0
402,Alexis Ford in Gang Bang Addiction Sc3,2468,BlowJob,1840.0,1860.0,20.0
402,Alexis Ford in Gang Bang Addiction Sc3,2485,Anal,1890.0,1914.0,24.0
402,Alexis Ford in Gang Bang Addiction Sc3,2469,BlowJob,1918.0,1950.0,32.0
402,Alexis Ford in Gang Bang Addiction Sc3,2470,BlowJob,2116.0,2136.0,20.0
402,Alexis Ford in Gang Bang Addiction Sc3,2471,BlowJob,2296.0,2312.0,16.0
402,Alexis Ford in Gang Bang Addiction Sc3,2472,BlowJob,2396.0,2408.0,12.0
402,Alexis Ford in Gang Bang Addiction Sc3,2473,BlowJob,2442.0,2468.0,26.0
402,Alexis Ford in Gang Bang Addiction Sc3,2461,Grabbing Boobs,2842.0,2864.0,22.0
402,Alexis Ford in Gang Bang Addiction Sc3,2479,BlowJob,2960.0,2972.0,12.0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2494,Grabbing Boobs,4.0,62.0,58.0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2500,Cumshot,320.0,326.0,6.0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2501,Cumshot,1094.0,1114.0,20.0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2502,Cumshot,1310.0,1342.0,32.0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2503,Cumshot,1428.0,1458.0,30.0
403,Alexis Malone in Darkko's Throat Fucks 8 Sc6,2495,Grabbing Boobs,2058.0,2082.0,24.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2509,Cumshot,130.0,150.0,20.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2520,Grabbing Boobs,1846.0,1874.0,28.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2512,BlowJob,1882.0,1892.0,10.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2521,Grabbing Boobs,2008.0,2044.0,36.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2513,BlowJob,2044.0,2078.0,34.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2514,BlowJob,2370.0,2382.0,12.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2518,Anal,2746.0,2774.0,28.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2522,Grabbing Boobs,2874.0,2892.0,18.0
404,Alexis Monroe in Gang Bang Addiction Sc5,2510,Cumshot,3138.0,3164.0,26.0
405,Alexis Texas in Gang Bang Addiction Sc1,2542,Gangbang,174.0,214.0,40.0
405,Alexis Texas in Gang Bang Addiction Sc1,2543,BlowJob,270.0,282.0,12.0
405,Alexis Texas in Gang Bang Addiction Sc1,2545,BlowJob,594.0,604.0,10.0
405,Alexis Texas in Gang Bang Addiction Sc1,2547,BlowJob,1164.0,1210.0,46.0
405,Alexis Texas in Gang Bang Addiction Sc1,2549,BlowJob,1392.0,1404.0,12.0
405,Alexis Texas in Gang Bang Addiction Sc1,2550,BlowJob,1448.0,1490.0,42.0
405,Alexis Texas in Gang Bang Addiction Sc1,2551,BlowJob,1542.0,1574.0,32.0
405,Alexis Texas in Gang Bang Addiction Sc1,2554,BlowJob,1738.0,1776.0,38.0
405,Alexis Texas in Gang Bang Addiction Sc1,2555,BlowJob,1826.0,1836.0,10.0
405,Alexis Texas in Gang Bang Addiction Sc1,2556,Cumshot,1848.0,1866.0,18.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2557,BlowJob,186.0,198.0,12.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2567,Cumshot,430.0,452.0,22.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2573,Anal,496.0,522.0,26.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2559,BlowJob,554.0,582.0,28.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2560,BlowJob,618.0,648.0,30.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2561,BlowJob,734.0,756.0,22.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2568,Cumshot,870.0,878.0,8.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2574,Anal,1010.0,1022.0,12.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2571,Cumshot,1220.0,1236.0,16.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2569,Cumshot,1712.0,1716.0,4.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2566,BlowJob,1730.0,1740.0,10.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2572,Cumshot,1734.0,1750.0,16.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2576,Anal,1794.0,1828.0,34.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2564,BlowJob,1962.0,1976.0,14.0
406,Alexxa Vice in Alexxa Sperma & Anal Total Sc1,2570,Cumshot,1996.0,2014.0,18.0
407,Alina in Annegret Zugekleistert Sc4,2605,Grabbing Boobs,28.0,88.0,60.0
407,Alina in Annegret Zugekleistert Sc4,2610,Anal,474.0,496.0,22.0
407,Alina in Annegret Zugekleistert Sc4,2611,Anal,578.0,590.0,12.0
407,Alina in Annegret Zugekleistert Sc4,2612,Anal,628.0,644.0,16.0
407,Alina in Annegret Zugekleistert Sc4,2606,Grabbing Boobs,820.0,862.0,42.0
407,Alina in Annegret Zugekleistert Sc4,2614,Anal,862.0,874.0,12.0
407,Alina in Annegret Zugekleistert Sc4,2608,BlowJob,898.0,930.0,32.0
409,Alina Lopez in No Going Back Sc1,2623,BlowJob,262.0,276.0,14.0
410,Alina Lopez in Perfectly Natural 19 Sc4,2625,BlowJob,572.0,582.0,10.0
410,Alina Lopez in Perfectly Natural 19 Sc4,2627,Cumshot,1294.0,1302.0,8.0
410,Alina Lopez in Perfectly Natural 19 Sc4,2626,BlowJob,1304.0,1320.0,16.0
411,Alina Lopez in Pussy is The Best Medicine 9 Sc5,2628,Grabbing Boobs,144.0,184.0,40.0
412,Alina Lopez in Sneaky Sex 30 Sc1,2629,BlowJob,1352.0,1366.0,14.0
412,Alina Lopez in Sneaky Sex 30 Sc1,2630,Cumshot,1390.0,1408.0,18.0
413,Alina Lopez in Wet Food 9 Sc1,2631,BlowJob,348.0,392.0,44.0
413,Alina Lopez in Wet Food 9 Sc1,2633,BlowJob,618.0,652.0,34.0
413,Alina Lopez in Wet Food 9 Sc1,2649,Gangbang,706.0,742.0,36.0
413,Alina Lopez in Wet Food 9 Sc1,2653,Cumshot,838.0,858.0,20.0
413,Alina Lopez in Wet Food 9 Sc1,2650,Gangbang,890.0,912.0,22.0
413,Alina Lopez in Wet Food 9 Sc1,2654,Cumshot,892.0,924.0,32.0
413,Alina Lopez in Wet Food 9 Sc1,2635,BlowJob,956.0,990.0,34.0
413,Alina Lopez in Wet Food 9 Sc1,2636,BlowJob,1040.0,1058.0,18.0
413,Alina Lopez in Wet Food 9 Sc1,2655,Cumshot,1092.0,1114.0,22.0
413,Alina Lopez in Wet Food 9 Sc1,2637,BlowJob,1106.0,1126.0,20.0
413,Alina Lopez in Wet Food 9 Sc1,2638,BlowJob,1254.0,1286.0,32.0
413,Alina Lopez in Wet Food 9 Sc1,2656,Cumshot,1286.0,1324.0,38.0
413,Alina Lopez in Wet Food 9 Sc1,2657,Cumshot,1366.0,1386.0,20.0
413,Alina Lopez in Wet Food 9 Sc1,2639,BlowJob,1372.0,1392.0,20.0
413,Alina Lopez in Wet Food 9 Sc1,2644,BlowJob,1444.0,1468.0,24.0
413,Alina Lopez in Wet Food 9 Sc1,2658,Cumshot,1454.0,1498.0,44.0
413,Alina Lopez in Wet Food 9 Sc1,2659,Cumshot,1654.0,1668.0,14.0
413,Alina Lopez in Wet Food 9 Sc1,2660,Cumshot,1710.0,1760.0,50.0
413,Alina Lopez in Wet Food 9 Sc1,2640,BlowJob,1738.0,1774.0,36.0
413,Alina Lopez in Wet Food 9 Sc1,2661,Cumshot,1798.0,1818.0,20.0
413,Alina Lopez in Wet Food 9 Sc1,2662,Cumshot,1882.0,1938.0,56.0
413,Alina Lopez in Wet Food 9 Sc1,2641,BlowJob,1886.0,1920.0,34.0
413,Alina Lopez in Wet Food 9 Sc1,2652,Gangbang,2168.0,2186.0,18.0
413,Alina Lopez in Wet Food 9 Sc1,2664,Cumshot,3008.0,3016.0,8.0
414,"Alina Lopez, Vera King in Mommy's Dream Sc4",2670,Grabbing Boobs,274.0,298.0,24.0
415,"Alyssa Divine, Kiki Minaj, Victoria Pure, Victoria Summers in Backstage Bangers Sc1",2747,BlowJob,324.0,358.0,34.0
415,"Alyssa Divine, Kiki Minaj, Victoria Pure, Victoria Summers in Backstage Bangers Sc1",2748,BlowJob,406.0,416.0,10.0
415,"Alyssa Divine, Kiki Minaj, Victoria Pure, Victoria Summers in Backstage Bangers Sc1",2750,Anal,582.0,594.0,12.0
415,"Alyssa Divine, Kiki Minaj, Victoria Pure, Victoria Summers in Backstage Bangers Sc1",2746,Grabbing Boobs,1082.0,1130.0,48.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2751,Gangbang,414.0,446.0,32.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2754,BlowJob,654.0,678.0,24.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2765,Anal,668.0,702.0,34.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2755,BlowJob,1210.0,1244.0,34.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2756,BlowJob,1466.0,1478.0,12.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2752,Gangbang,1774.0,1808.0,34.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2757,BlowJob,1790.0,1818.0,28.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2758,BlowJob,1944.0,1964.0,20.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2759,BlowJob,2010.0,2054.0,44.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2760,BlowJob,2582.0,2594.0,12.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2766,Anal,2636.0,2692.0,56.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2769,Grabbing Boobs,2852.0,2870.0,18.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2767,Anal,3218.0,3270.0,52.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2762,BlowJob,3302.0,3316.0,14.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2768,Anal,3306.0,3326.0,20.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2770,Cumshot,3344.0,3398.0,54.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2773,Cumshot,3424.0,3468.0,44.0
416,Amara Romani in Gangbang Auditions 31 Sc3,2774,Cumshot,3496.0,3534.0,38.0
417,"Amari Anne, Ana Foxxx, Jenna Foxx, Kira Noir, Maya Farrell in Kira vs Kira Sc2",2784,Anal,1232.0,1244.0,12.0
417,"Amari Anne, Ana Foxxx, Jenna Foxx, Kira Noir, Maya Farrell in Kira vs Kira Sc2",2785,Anal,1378.0,1398.0,20.0
418,Amber Deen in Dogfart Invades Europe! 4 Sc1,2787,BlowJob,1252.0,1272.0,20.0
418,Amber Deen in Dogfart Invades Europe! 4 Sc1,2789,BlowJob,1810.0,1818.0,8.0
418,Amber Deen in Dogfart Invades Europe! 4 Sc1,2790,Cumshot,1884.0,1890.0,6.0
418,Amber Deen in Dogfart Invades Europe! 4 Sc1,2791,Cumshot,1930.0,1934.0,4.0
418,Amber Deen in Dogfart Invades Europe! 4 Sc1,2792,Cumshot,1956.0,1964.0,8.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2793,Grabbing Boobs,544.0,564.0,20.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2799,BlowJob,760.0,770.0,10.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2800,BlowJob,1234.0,1256.0,22.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2808,Anal,1284.0,1296.0,12.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2809,Cumshot,1670.0,1684.0,14.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2810,Cumshot,1920.0,1924.0,4.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2803,BlowJob,2104.0,2124.0,20.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2804,BlowJob,2168.0,2216.0,48.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2806,BlowJob,2692.0,2702.0,10.0
419,"Ameena Green, Willow Ryder in Willow's Room Sc3",2797,BlowJob,2738.0,2766.0,28.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2844,Cumshot,534.0,558.0,24.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2846,Cumshot,630.0,634.0,4.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2847,Cumshot,800.0,806.0,6.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2848,Cumshot,976.0,1034.0,58.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2855,Cumshot,1214.0,1218.0,4.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2856,Cumshot,1258.0,1266.0,8.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2859,Anal,1384.0,1418.0,34.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2857,Cumshot,1510.0,1524.0,14.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2850,Cumshot,1570.0,1606.0,36.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2852,Cumshot,2158.0,2194.0,36.0
421,"Ana Foxxx, Cali Carter, Vicki Chase in Swallowed 3 Sc1",2853,Cumshot,2212.0,2216.0,4.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2932,Grabbing Boobs,16.0,42.0,26.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2933,Grabbing Boobs,314.0,340.0,26.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2934,Anal,428.0,454.0,26.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2950,BlowJob,702.0,750.0,48.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2935,Anal,788.0,802.0,14.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2936,Anal,878.0,902.0,24.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2951,BlowJob,906.0,956.0,50.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2937,Anal,1018.0,1050.0,32.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2939,Anal,1206.0,1226.0,20.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2940,Anal,1334.0,1364.0,30.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2941,Anal,1418.0,1468.0,50.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2954,BlowJob,1704.0,1716.0,12.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2955,BlowJob,1770.0,1810.0,40.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2956,BlowJob,1854.0,1906.0,52.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2957,BlowJob,1992.0,2032.0,40.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2958,BlowJob,2114.0,2148.0,34.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2959,BlowJob,2302.0,2328.0,26.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2943,Anal,2352.0,2386.0,34.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2944,Anal,2456.0,2470.0,14.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2945,Anal,2514.0,2532.0,18.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2961,BlowJob,2534.0,2570.0,36.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2962,BlowJob,2604.0,2620.0,16.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2964,BlowJob,2870.0,2886.0,16.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2969,Gangbang,3046.0,3102.0,56.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2966,BlowJob,3246.0,3290.0,44.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2946,Anal,3294.0,3344.0,50.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2967,BlowJob,3348.0,3362.0,14.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2947,Anal,3538.0,3552.0,14.0
422,Andi Anderson in Young Harlots Gang Bang Sc2,2948,Anal,3748.0,3800.0,52.0
423,"Angel Eyes, Jada Fire in Freak Nasty Sc1",2973,Grabbing Boobs,0.0,26.0,26.0
423,"Angel Eyes, Jada Fire in Freak Nasty Sc1",2974,Grabbing Boobs,258.0,316.0,58.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3074,Grabbing Boobs,242.0,272.0,30.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3082,Gangbang,580.0,594.0,14.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3075,Grabbing Boobs,596.0,640.0,44.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3079,BlowJob,610.0,642.0,32.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3083,Gangbang,632.0,668.0,36.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3081,BlowJob,996.0,1010.0,14.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3076,Grabbing Boobs,2380.0,2404.0,24.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3085,Cumshot,2458.0,2474.0,16.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3086,Cumshot,2624.0,2644.0,20.0
424,Angela White in Going All Out with a Gangbang 2 Sc4,3077,Grabbing Boobs,2774.0,2808.0,34.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3087,BlowJob,324.0,342.0,18.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3088,BlowJob,408.0,430.0,22.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3089,BlowJob,580.0,598.0,18.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3094,BlowJob,670.0,684.0,14.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3107,Titjob,900.0,952.0,52.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3091,BlowJob,1334.0,1366.0,32.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3103,Grabbing Boobs,1916.0,1960.0,44.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3104,Grabbing Boobs,2330.0,2380.0,50.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3097,BlowJob,2616.0,2628.0,12.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3105,Grabbing Boobs,2828.0,2862.0,34.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3093,BlowJob,3044.0,3054.0,10.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3098,BlowJob,3122.0,3126.0,4.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3108,Cumshot,3130.0,3148.0,18.0
425,"Angela White, Ava Addams, Bridgette B. in Pornstar Therapy Sc1",3109,Cumshot,3174.0,3182.0,8.0
426,"Angela White, Jada Stevens in Jada Loves Gonzo Sc1",3110,BlowJob,526.0,544.0,18.0
426,"Angela White, Jada Stevens in Jada Loves Gonzo Sc1",3111,Grabbing Boobs,570.0,602.0,32.0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",2365,BlowJob,34.0,44.0,10.0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",2366,BlowJob,188.0,248.0,60.0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",2369,Cumshot,200.0,228.0,28.0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",2367,BlowJob,432.0,456.0,24.0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",2371,Cumshot,818.0,838.0,20.0
427,"Adreena Winters, Rebecca Jane Smyth, Talula Thomas in Swinger Beach 100 Guys 3 CumSluts",2372,Cumshot,988.0,998.0,10.0
428,Alejandra Rico in Intense Latin Gangbang,1958,BlowJob,148.0,158.0,10.0
428,Alejandra Rico in Intense Latin Gangbang,1961,Anal,714.0,730.0,16.0
428,Alejandra Rico in Intense Latin Gangbang,1960,BlowJob,904.0,926.0,22.0
428,Alejandra Rico in Intense Latin Gangbang,1962,Cumshot,962.0,1000.0,38.0
428,Alejandra Rico in Intense Latin Gangbang,1965,Cumshot,1106.0,1136.0,30.0
429,Alejandra Rico in Tons of Cum,2013,Cumshot,1084.0,1090.0,6.0
429,Alejandra Rico in Tons of Cum,2008,BlowJob,1122.0,1130.0,8.0
429,Alejandra Rico in Tons of Cum,2009,BlowJob,1244.0,1272.0,28.0
429,Alejandra Rico in Tons of Cum,2010,BlowJob,1348.0,1352.0,4.0
429,Alejandra Rico in Tons of Cum,2015,Cumshot,1358.0,1366.0,8.0
429,Alejandra Rico in Tons of Cum,2011,BlowJob,1436.0,1442.0,6.0
429,Alejandra Rico in Tons of Cum,2012,BlowJob,1540.0,1546.0,6.0
430,Alex Grey in A Dirty Submissive Slut For Cock,1982,BlowJob,736.0,746.0,10.0
430,Alex Grey in A Dirty Submissive Slut For Cock,1985,Cumshot,1342.0,1348.0,6.0
431,Alexa Nova in GangBang Creampie 246,2437,BlowJob,76.0,98.0,22.0
431,Alexa Nova in GangBang Creampie 246,2438,BlowJob,376.0,414.0,38.0
431,Alexa Nova in GangBang Creampie 246,2444,Anal,972.0,982.0,10.0
431,Alexa Nova in GangBang Creampie 246,2440,BlowJob,1128.0,1150.0,22.0
431,Alexa Nova in GangBang Creampie 246,2441,BlowJob,1390.0,1400.0,10.0
431,Alexa Nova in GangBang Creampie 246,2443,BlowJob,1842.0,1876.0,34.0
432,Alexa Payne in Stepmom Seduction on the Kitchen Counter,2445,BlowJob,478.0,536.0,58.0
432,Alexa Payne in Stepmom Seduction on the Kitchen Counter,2446,BlowJob,618.0,646.0,28.0
432,Alexa Payne in Stepmom Seduction on the Kitchen Counter,2448,Grabbing Boobs,1344.0,1382.0,38.0
432,Alexa Payne in Stepmom Seduction on the Kitchen Counter,2449,Grabbing Boobs,1830.0,1882.0,52.0
433,"Alexia Vendome, Sothy Hiko, Yasmine Lafitte in No Limits Anal and Oral Orgy",2452,BlowJob,260.0,276.0,16.0
433,"Alexia Vendome, Sothy Hiko, Yasmine Lafitte in No Limits Anal and Oral Orgy",2453,BlowJob,316.0,354.0,38.0
433,"Alexia Vendome, Sothy Hiko, Yasmine Lafitte in No Limits Anal and Oral Orgy",2456,Anal,1018.0,1062.0,44.0
433,"Alexia Vendome, Sothy Hiko, Yasmine Lafitte in No Limits Anal and Oral Orgy",2455,BlowJob,1098.0,1108.0,10.0
433,"Alexia Vendome, Sothy Hiko, Yasmine Lafitte in No Limits Anal and Oral Orgy",2457,Anal,1258.0,1282.0,24.0
434,Alexis Kay in GangBang Creampie 417,2487,Gangbang,88.0,100.0,12.0
434,Alexis Kay in GangBang Creampie 417,2489,Grabbing Boobs,124.0,150.0,26.0
434,Alexis Kay in GangBang Creampie 417,2490,BlowJob,202.0,230.0,28.0
434,Alexis Kay in GangBang Creampie 417,2491,BlowJob,448.0,482.0,34.0
434,Alexis Kay in GangBang Creampie 417,2492,BlowJob,608.0,656.0,48.0
434,Alexis Kay in GangBang Creampie 417,2488,Gangbang,636.0,654.0,18.0
435,Alicia Ribeiro in Anal and Oral with 3 Big Black Cocks,2581,Gangbang,214.0,236.0,22.0
435,Alicia Ribeiro in Anal and Oral with 3 Big Black Cocks,2578,BlowJob,400.0,414.0,14.0
435,Alicia Ribeiro in Anal and Oral with 3 Big Black Cocks,2579,BlowJob,606.0,646.0,40.0
435,Alicia Ribeiro in Anal and Oral with 3 Big Black Cocks,2583,Cumshot,2666.0,2692.0,26.0
436,Alicia Trece in Rough Gangbang and Pee Play,2584,BlowJob,124.0,140.0,16.0
436,Alicia Trece in Rough Gangbang and Pee Play,2585,BlowJob,188.0,198.0,10.0
436,Alicia Trece in Rough Gangbang and Pee Play,2586,BlowJob,244.0,258.0,14.0
436,Alicia Trece in Rough Gangbang and Pee Play,2590,Anal,286.0,308.0,22.0
436,Alicia Trece in Rough Gangbang and Pee Play,2587,BlowJob,384.0,400.0,16.0
436,Alicia Trece in Rough Gangbang and Pee Play,2588,BlowJob,434.0,460.0,26.0
436,Alicia Trece in Rough Gangbang and Pee Play,2591,Anal,564.0,584.0,20.0
436,Alicia Trece in Rough Gangbang and Pee Play,2592,Anal,746.0,798.0,52.0
436,Alicia Trece in Rough Gangbang and Pee Play,2593,Anal,886.0,922.0,36.0
436,Alicia Trece in Rough Gangbang and Pee Play,2603,Grabbing Boobs,1968.0,1988.0,20.0
436,Alicia Trece in Rough Gangbang and Pee Play,2596,Anal,2004.0,2028.0,24.0
436,Alicia Trece in Rough Gangbang and Pee Play,2601,Gangbang,2030.0,2044.0,14.0
436,Alicia Trece in Rough Gangbang and Pee Play,2602,Gangbang,2180.0,2198.0,18.0
436,Alicia Trece in Rough Gangbang and Pee Play,2597,Anal,2206.0,2216.0,10.0
436,Alicia Trece in Rough Gangbang and Pee Play,2604,Cumshot,2816.0,2822.0,6.0
437,Aliyah Taylor in Gang Bang All Her Holes,2679,Gangbang,366.0,384.0,18.0
437,Aliyah Taylor in Gang Bang All Her Holes,2672,BlowJob,544.0,600.0,56.0
437,Aliyah Taylor in Gang Bang All Her Holes,2680,Anal,1182.0,1218.0,36.0
437,Aliyah Taylor in Gang Bang All Her Holes,2674,BlowJob,1364.0,1376.0,12.0
437,Aliyah Taylor in Gang Bang All Her Holes,2675,BlowJob,1724.0,1734.0,10.0
437,Aliyah Taylor in Gang Bang All Her Holes,2677,BlowJob,1750.0,1784.0,34.0
437,Aliyah Taylor in Gang Bang All Her Holes,2676,BlowJob,1794.0,1804.0,10.0
438,Allatra Hot in MILF Craving Hardcore Attention,2684,BlowJob,404.0,432.0,28.0
438,Allatra Hot in MILF Craving Hardcore Attention,2688,BlowJob,464.0,508.0,44.0
438,Allatra Hot in MILF Craving Hardcore Attention,2686,BlowJob,630.0,688.0,58.0
438,Allatra Hot in MILF Craving Hardcore Attention,2690,Anal,854.0,894.0,40.0
438,Allatra Hot in MILF Craving Hardcore Attention,2691,Anal,940.0,966.0,26.0
438,Allatra Hot in MILF Craving Hardcore Attention,2687,BlowJob,1094.0,1106.0,12.0
438,Allatra Hot in MILF Craving Hardcore Attention,2693,Cumshot,1316.0,1330.0,14.0
439,Alura Jenson in GangBang Creampie 240,2695,BlowJob,164.0,182.0,18.0
439,Alura Jenson in GangBang Creampie 240,2702,Gangbang,188.0,212.0,24.0
439,Alura Jenson in GangBang Creampie 240,2708,Grabbing Boobs,232.0,270.0,38.0
439,Alura Jenson in GangBang Creampie 240,2696,BlowJob,244.0,272.0,28.0
439,Alura Jenson in GangBang Creampie 240,2703,Gangbang,448.0,470.0,22.0
439,Alura Jenson in GangBang Creampie 240,2710,Anal,452.0,482.0,30.0
439,Alura Jenson in GangBang Creampie 240,2704,Gangbang,514.0,526.0,12.0
439,Alura Jenson in GangBang Creampie 240,2709,Grabbing Boobs,868.0,904.0,36.0
439,Alura Jenson in GangBang Creampie 240,2706,Gangbang,1166.0,1184.0,18.0
439,Alura Jenson in GangBang Creampie 240,2700,BlowJob,2186.0,2204.0,18.0
439,Alura Jenson in GangBang Creampie 240,2707,Gangbang,2188.0,2220.0,32.0
439,Alura Jenson in GangBang Creampie 240,2701,BlowJob,2344.0,2394.0,50.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2711,Gangbang,878.0,892.0,14.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2723,Anal,1268.0,1308.0,40.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2727,Grabbing Boobs,1434.0,1460.0,26.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2728,Titjob,1446.0,1462.0,16.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2715,BlowJob,1468.0,1502.0,34.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2722,BlowJob,1606.0,1610.0,4.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2724,Anal,1854.0,1872.0,18.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2725,Anal,1916.0,1966.0,50.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2716,BlowJob,1930.0,1948.0,18.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2717,BlowJob,2092.0,2104.0,12.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2718,BlowJob,2888.0,2914.0,26.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2719,BlowJob,3134.0,3146.0,12.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2712,Gangbang,3276.0,3294.0,18.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2713,Gangbang,3348.0,3362.0,14.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2720,BlowJob,3440.0,3460.0,20.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2721,BlowJob,3640.0,3654.0,14.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2729,Cumshot,3644.0,3650.0,6.0
440,"Alura Jenson, Jenevieve Hexxx, Maci May, Miss Raquel, Savana Styles, Ziggy Star in Workout Orgy!",2726,Anal,3712.0,3730.0,18.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2731,Gangbang,534.0,554.0,20.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2732,Anal,692.0,708.0,16.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2740,BlowJob,1122.0,1134.0,12.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2734,Anal,1502.0,1514.0,12.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2736,Anal,1828.0,1872.0,44.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2742,Cumshot,2656.0,2676.0,20.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2730,Grabbing Boobs,2722.0,2748.0,26.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2743,Cumshot,2724.0,2766.0,42.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2741,BlowJob,2836.0,2848.0,12.0
441,"Alyssa Bounty, Polly Pons, Shalina Devine in Wild 1920s Party",2744,Cumshot,2840.0,2874.0,34.0
442,Amari Anne in You Wanna Cheat Again,2778,BlowJob,644.0,654.0,10.0
442,Amari Anne in You Wanna Cheat Again,2779,BlowJob,694.0,716.0,22.0
442,Amari Anne in You Wanna Cheat Again,2780,BlowJob,760.0,784.0,24.0
442,Amari Anne in You Wanna Cheat Again,2781,BlowJob,846.0,884.0,38.0
442,Amari Anne in You Wanna Cheat Again,2782,Anal,1662.0,1680.0,18.0
442,Amari Anne in You Wanna Cheat Again,2777,Grabbing Boobs,2286.0,2326.0,40.0
442,Amari Anne in You Wanna Cheat Again,2783,Cumshot,2380.0,2406.0,26.0
443,Amirah Adara in Rough Gangbang Session,2828,BlowJob,1188.0,1222.0,34.0
443,Amirah Adara in Rough Gangbang Session,2830,Anal,1314.0,1324.0,10.0
443,Amirah Adara in Rough Gangbang Session,2829,BlowJob,2712.0,2732.0,20.0
444,Amy Reid in AllOut Blowbang Session,2834,Gangbang,408.0,442.0,34.0
444,Amy Reid in AllOut Blowbang Session,2835,Cumshot,980.0,1024.0,44.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2860,BlowJob,394.0,410.0,16.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2866,Gangbang,562.0,574.0,12.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2862,BlowJob,1116.0,1130.0,14.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2867,Gangbang,1740.0,1768.0,28.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2869,Anal,2026.0,2042.0,16.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2868,Gangbang,2462.0,2498.0,36.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2863,BlowJob,2468.0,2494.0,26.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2864,BlowJob,2720.0,2734.0,14.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2872,Anal,2870.0,2890.0,20.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2865,BlowJob,3160.0,3178.0,18.0
445,Ana J√∫lia in A Cavala Ana Fodendo no Pelo com 4 Dotados,2873,Cumshot,3164.0,3216.0,52.0
446,Ana J√∫lia in DP com a Mulata Cavala,2876,Grabbing Boobs,238.0,264.0,26.0
446,Ana J√∫lia in DP com a Mulata Cavala,2879,BlowJob,516.0,534.0,18.0
446,Ana J√∫lia in DP com a Mulata Cavala,2880,BlowJob,1096.0,1136.0,40.0
446,Ana J√∫lia in DP com a Mulata Cavala,2881,BlowJob,1192.0,1214.0,22.0
446,Ana J√∫lia in DP com a Mulata Cavala,2882,Anal,1674.0,1702.0,28.0
446,Ana J√∫lia in DP com a Mulata Cavala,2884,Cumshot,2372.0,2408.0,36.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2885,BlowJob,38.0,76.0,38.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2887,Anal,168.0,188.0,20.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2888,Anal,286.0,314.0,28.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2889,Anal,346.0,366.0,20.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2886,BlowJob,502.0,528.0,26.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2890,69,662.0,676.0,14.0
447,"Ana J√∫lia, Shayenne Samara in M√©nage a 3 com as Estrelas do Porn√¥",2891,Cumshot,1492.0,1504.0,12.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2920,Anal,342.0,360.0,18.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2921,Anal,2394.0,2404.0,10.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2928,Grabbing Boobs,2410.0,2468.0,58.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2922,Anal,2438.0,2458.0,20.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2923,Anal,2550.0,2562.0,12.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2929,BlowJob,2622.0,2632.0,10.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2925,Anal,2928.0,2944.0,16.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2926,Anal,2980.0,2998.0,18.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2930,BlowJob,4210.0,4242.0,32.0
448,"Anais Hayek, Hannah Hayek, Natasha Teen in Hayek Twins Anal Bash",2931,BlowJob,4882.0,4906.0,24.0
449,Angel Lima in Big Butt Airtight Show,2977,Grabbing Boobs,486.0,516.0,30.0
449,Angel Lima in Big Butt Airtight Show,2978,Grabbing Boobs,588.0,628.0,40.0
449,Angel Lima in Big Butt Airtight Show,2981,Anal,1022.0,1034.0,12.0
449,Angel Lima in Big Butt Airtight Show,2979,Grabbing Boobs,1326.0,1350.0,24.0
449,Angel Lima in Big Butt Airtight Show,2982,Anal,1868.0,1914.0,46.0
449,Angel Lima in Big Butt Airtight Show,2983,Anal,2192.0,2230.0,38.0
449,Angel Lima in Big Butt Airtight Show,2984,Anal,2328.0,2362.0,34.0
449,Angel Lima in Big Butt Airtight Show,2985,Anal,2420.0,2434.0,14.0
449,Angel Lima in Big Butt Airtight Show,2980,Grabbing Boobs,2714.0,2734.0,20.0
450,Angel Lima in Hardcore Brazilian Double Anal,2988,Grabbing Boobs,40.0,100.0,60.0
450,Angel Lima in Hardcore Brazilian Double Anal,2989,Grabbing Boobs,154.0,172.0,18.0
450,Angel Lima in Hardcore Brazilian Double Anal,2995,BlowJob,732.0,766.0,34.0
450,Angel Lima in Hardcore Brazilian Double Anal,2998,Titjob,756.0,808.0,52.0
450,Angel Lima in Hardcore Brazilian Double Anal,2991,Grabbing Boobs,876.0,894.0,18.0
450,Angel Lima in Hardcore Brazilian Double Anal,2992,Grabbing Boobs,972.0,998.0,26.0
450,Angel Lima in Hardcore Brazilian Double Anal,2996,BlowJob,984.0,1000.0,16.0
450,Angel Lima in Hardcore Brazilian Double Anal,2997,BlowJob,1046.0,1062.0,16.0
450,Angel Lima in Hardcore Brazilian Double Anal,2993,Grabbing Boobs,1820.0,1850.0,30.0
450,Angel Lima in Hardcore Brazilian Double Anal,3001,Anal,1964.0,1980.0,16.0
450,Angel Lima in Hardcore Brazilian Double Anal,3004,Anal,2530.0,2542.0,12.0
450,Angel Lima in Hardcore Brazilian Double Anal,3005,Anal,2608.0,2628.0,20.0
450,Angel Lima in Hardcore Brazilian Double Anal,2994,Grabbing Boobs,3346.0,3364.0,18.0
450,Angel Lima in Hardcore Brazilian Double Anal,3008,Cumshot,3392.0,3408.0,16.0
450,Angel Lima in Hardcore Brazilian Double Anal,3009,Cumshot,3442.0,3472.0,30.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3010,Grabbing Boobs,18.0,44.0,26.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3011,Grabbing Boobs,710.0,738.0,28.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3012,Grabbing Boobs,828.0,878.0,50.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3017,BlowJob,1986.0,2002.0,16.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3013,Grabbing Boobs,2194.0,2244.0,50.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3014,Grabbing Boobs,2412.0,2430.0,18.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3021,Anal,2574.0,2630.0,56.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3018,BlowJob,2768.0,2786.0,18.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3019,BlowJob,2926.0,2960.0,34.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3022,Cumshot,3022.0,3078.0,56.0
451,"Angel Lima, Elisa Sanches in Angel y Elisa Pela 1¬∞ vez Juntas",3015,Grabbing Boobs,3064.0,3092.0,28.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3023,Grabbing Boobs,198.0,220.0,22.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3024,Grabbing Boobs,254.0,286.0,32.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3025,Grabbing Boobs,330.0,352.0,22.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3027,BlowJob,414.0,446.0,32.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3029,BlowJob,684.0,744.0,60.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3031,Anal,904.0,924.0,20.0
452,"Angel Lima, Elisa Sanches in Family Heat Fiesta",3026,Grabbing Boobs,950.0,970.0,20.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3032,Grabbing Boobs,18.0,44.0,26.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3033,Grabbing Boobs,176.0,206.0,30.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3034,Grabbing Boobs,710.0,738.0,28.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3043,BlowJob,1280.0,1294.0,14.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3040,BlowJob,1312.0,1316.0,4.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3047,Anal,1356.0,1368.0,12.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3035,Grabbing Boobs,1530.0,1564.0,34.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3036,Grabbing Boobs,2220.0,2244.0,24.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3048,Anal,2348.0,2372.0,24.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3045,BlowJob,2734.0,2776.0,42.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3041,BlowJob,2954.0,2960.0,6.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3038,Grabbing Boobs,3072.0,3100.0,28.0
453,"Angel Lima, Elisa Sanches in Tropical Lesbians & Orgy",3050,Cumshot,3076.0,3080.0,4.0
455,"Angel Smalls, Anna De Ville, Barbie Sins, Jureka Del Mar, May Thai, Nathaly Cherie, Selvaggia in Messy Facial Compilation",3057,Grabbing Boobs,442.0,468.0,26.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3112,Grabbing Boobs,64.0,86.0,22.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3117,BlowJob,730.0,758.0,28.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3118,BlowJob,1152.0,1196.0,44.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3132,Gangbang,1176.0,1188.0,12.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3135,Pissing,1256.0,1262.0,6.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3119,BlowJob,1268.0,1316.0,48.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3113,Grabbing Boobs,1300.0,1340.0,40.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3120,BlowJob,1392.0,1434.0,42.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3121,BlowJob,1486.0,1502.0,16.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3134,Gangbang,2370.0,2420.0,50.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3123,BlowJob,2406.0,2424.0,18.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3137,Cumshot,2468.0,2490.0,22.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3124,BlowJob,2474.0,2486.0,12.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3125,BlowJob,2524.0,2580.0,56.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3138,Cumshot,2584.0,2590.0,6.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3126,BlowJob,3060.0,3076.0,16.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3130,Anal,3820.0,3830.0,10.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3131,Anal,3928.0,3954.0,26.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3115,BlowJob,4124.0,4134.0,10.0
456,"Angie Bloom in Rough Anal, Pee Cocktail & Squirting Shower",3136,Pissing,4630.0,4634.0,4.0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,3158,BlowJob,1248.0,1282.0,34.0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,3160,BlowJob,2064.0,2078.0,14.0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,3161,BlowJob,2360.0,2384.0,24.0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,3162,BlowJob,2616.0,2646.0,30.0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,3163,BlowJob,2696.0,2726.0,30.0
458,Ania Kinski in Intense Gangbang with Deep DP and Swallow,3164,BlowJob,2766.0,2790.0,24.0
459,Ania Kinski in Kinky DP Session At The Clinic,3169,Anal,1408.0,1446.0,38.0
459,Ania Kinski in Kinky DP Session At The Clinic,3170,Anal,1522.0,1536.0,14.0
459,Ania Kinski in Kinky DP Session At The Clinic,3172,Cumshot,2032.0,2060.0,28.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3173,Grabbing Boobs,710.0,732.0,22.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3175,Cumshot,712.0,726.0,14.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3178,BlowJob,1182.0,1196.0,14.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3179,BlowJob,1258.0,1310.0,52.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3180,BlowJob,1400.0,1420.0,20.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3186,Anal,1422.0,1440.0,18.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3181,BlowJob,1482.0,1500.0,18.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3182,BlowJob,1554.0,1572.0,18.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3187,Anal,1574.0,1612.0,38.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3188,Anal,1708.0,1742.0,34.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3185,BlowJob,2074.0,2094.0,20.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3176,Cumshot,2078.0,2086.0,8.0
460,Ania Kinski in Rich Busty Milf DP'ed by the Pool Boys,3174,Grabbing Boobs,2154.0,2182.0,28.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3203,Grabbing Boobs,1352.0,1372.0,20.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3204,Grabbing Boobs,2140.0,2168.0,28.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3210,BlowJob,2266.0,2276.0,10.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3218,Anal,2294.0,2314.0,20.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3211,BlowJob,2382.0,2398.0,16.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3205,Grabbing Boobs,2434.0,2478.0,44.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3212,BlowJob,2534.0,2584.0,50.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3206,Grabbing Boobs,2634.0,2682.0,48.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3219,Anal,2728.0,2782.0,54.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3220,Anal,2828.0,2870.0,42.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3221,Anal,3466.0,3488.0,22.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3222,Anal,3532.0,3560.0,28.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3213,BlowJob,3728.0,3746.0,18.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3216,BlowJob,4028.0,4052.0,24.0
462,"Ania Kinski, Anissa Kate in French Duo's Interracial Bowling Encounter",3207,Grabbing Boobs,4126.0,4160.0,34.0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",3231,Anal,340.0,350.0,10.0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",3234,DP,954.0,960.0,6.0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",3228,BlowJob,1074.0,1108.0,34.0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",3233,Grabbing Boobs,1128.0,1146.0,18.0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",3229,BlowJob,1168.0,1178.0,10.0
463,"Ania Kinski, Anissa Kate in Nympho Teachers Seduce Colleagues",3235,Cumshot,1924.0,1976.0,52.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3246,BlowJob,678.0,716.0,38.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3250,Anal,822.0,832.0,10.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3253,Anal,1390.0,1412.0,22.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3244,BlowJob,1518.0,1528.0,10.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3249,BlowJob,1606.0,1640.0,34.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3254,Anal,1696.0,1716.0,20.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3255,Anal,1820.0,1830.0,10.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3256,Anal,1938.0,1954.0,16.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3257,Anal,2002.0,2028.0,26.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3258,Anal,2102.0,2134.0,32.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3247,BlowJob,2306.0,2318.0,12.0
465,"Ania Kinski, Jasmine Jae in MILFs' Real Estate Threesome",3242,Grabbing Boobs,2356.0,2404.0,48.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3259,BlowJob,896.0,924.0,28.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3260,BlowJob,1216.0,1238.0,22.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3272,Grabbing Boobs,1746.0,1770.0,24.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3266,Anal,1748.0,1774.0,26.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3267,Anal,1978.0,1998.0,20.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3268,Anal,2038.0,2054.0,16.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3262,BlowJob,2082.0,2094.0,12.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3269,Anal,2122.0,2180.0,58.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3263,BlowJob,2562.0,2598.0,36.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3273,69,2614.0,2636.0,22.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3271,Anal,2898.0,2908.0,10.0
466,"Ania Kinski, Paola Guerra in Backdoor Threesome Delight",3264,BlowJob,3312.0,3344.0,32.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3274,Grabbing Boobs,82.0,102.0,20.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3279,BlowJob,352.0,362.0,10.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3280,BlowJob,408.0,420.0,12.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3275,Grabbing Boobs,666.0,708.0,42.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3276,Grabbing Boobs,1070.0,1102.0,32.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3277,Grabbing Boobs,1316.0,1352.0,36.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3278,Grabbing Boobs,1430.0,1480.0,50.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3281,BlowJob,1506.0,1536.0,30.0
467,"Ania Kinski, Valentina Ricci in Curvy Housewives Share Lutro",3282,BlowJob,1576.0,1620.0,44.0
468,Anissa Kate in A Hot Surfer Threesome,3290,BlowJob,170.0,184.0,14.0
468,Anissa Kate in A Hot Surfer Threesome,3296,BlowJob,298.0,312.0,14.0
468,Anissa Kate in A Hot Surfer Threesome,3292,BlowJob,370.0,388.0,18.0
468,Anissa Kate in A Hot Surfer Threesome,3293,BlowJob,468.0,484.0,16.0
468,Anissa Kate in A Hot Surfer Threesome,3283,Anal,740.0,796.0,56.0
468,Anissa Kate in A Hot Surfer Threesome,3285,Anal,1042.0,1064.0,22.0
468,Anissa Kate in A Hot Surfer Threesome,3294,BlowJob,1138.0,1178.0,40.0
468,Anissa Kate in A Hot Surfer Threesome,3286,Anal,1414.0,1426.0,12.0
468,Anissa Kate in A Hot Surfer Threesome,3287,Anal,1466.0,1476.0,10.0
468,Anissa Kate in A Hot Surfer Threesome,3288,Anal,1520.0,1578.0,58.0
468,Anissa Kate in A Hot Surfer Threesome,3289,Anal,1662.0,1688.0,26.0
468,Anissa Kate in A Hot Surfer Threesome,3298,Grabbing Boobs,1730.0,1754.0,24.0
468,Anissa Kate in A Hot Surfer Threesome,3300,Cumshot,1750.0,1766.0,16.0
468,Anissa Kate in A Hot Surfer Threesome,3297,BlowJob,1766.0,1790.0,24.0
468,Anissa Kate in A Hot Surfer Threesome,3301,Cumshot,1790.0,1794.0,4.0
468,Anissa Kate in A Hot Surfer Threesome,3299,Grabbing Boobs,1906.0,1924.0,18.0
469,Anissa Kate in Hardcore Business Meeting,3310,Titjob,662.0,674.0,12.0
469,Anissa Kate in Hardcore Business Meeting,3306,BlowJob,754.0,764.0,10.0
469,Anissa Kate in Hardcore Business Meeting,3307,BlowJob,1356.0,1376.0,20.0
469,Anissa Kate in Hardcore Business Meeting,3308,BlowJob,1488.0,1514.0,26.0
469,Anissa Kate in Hardcore Business Meeting,3312,Cumshot,2226.0,2282.0,56.0
469,Anissa Kate in Hardcore Business Meeting,3313,Cumshot,2332.0,2364.0,32.0
469,Anissa Kate in Hardcore Business Meeting,3304,Grabbing Boobs,2340.0,2358.0,18.0
470,Anissa Kate in Poolside DP with BBC and Cumshot Finish,3326,BlowJob,1768.0,1826.0,58.0
470,Anissa Kate in Poolside DP with BBC and Cumshot Finish,3327,BlowJob,2204.0,2248.0,44.0
470,Anissa Kate in Poolside DP with BBC and Cumshot Finish,3330,Anal,2542.0,2554.0,12.0
470,Anissa Kate in Poolside DP with BBC and Cumshot Finish,3331,Anal,2672.0,2698.0,26.0
470,Anissa Kate in Poolside DP with BBC and Cumshot Finish,3328,BlowJob,2916.0,2922.0,6.0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3356,Grabbing Boobs,790.0,814.0,24.0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3357,Grabbing Boobs,864.0,888.0,24.0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3349,BlowJob,1520.0,1576.0,56.0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3351,BlowJob,2034.0,2056.0,22.0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3352,BlowJob,2478.0,2488.0,10.0
471,"Anissa Kate, Brittany Bardot, Kitana Lure in New Year's Game Grand Finale",3355,BlowJob,2960.0,2966.0,6.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3362,Grabbing Boobs,1214.0,1246.0,32.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3363,BlowJob,1500.0,1528.0,28.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3367,BlowJob,1564.0,1606.0,42.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3368,BlowJob,1710.0,1732.0,22.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3369,BlowJob,1906.0,1940.0,34.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3371,Anal,2126.0,2140.0,14.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3372,Anal,2236.0,2254.0,18.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3373,Anal,2404.0,2440.0,36.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3370,BlowJob,2422.0,2436.0,14.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3374,Anal,2502.0,2516.0,14.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3375,Anal,2550.0,2562.0,12.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3376,Anal,2664.0,2696.0,32.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3377,Anal,2786.0,2800.0,14.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3365,BlowJob,2936.0,2984.0,48.0
472,"Anissa Kate, Lia Lin in Intense DP Experience",3378,Cumshot,2964.0,2994.0,30.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3401,BlowJob,644.0,654.0,10.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3396,BlowJob,836.0,848.0,12.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3402,BlowJob,866.0,870.0,4.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3397,BlowJob,1440.0,1452.0,12.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3398,BlowJob,1654.0,1692.0,38.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3404,BlowJob,1904.0,1926.0,22.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3399,BlowJob,2028.0,2040.0,12.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3405,Titjob,2048.0,2064.0,16.0
474,"Anissa Kate, Nicole Doshi in Anissa and Nicole with a BBC",3394,Grabbing Boobs,2152.0,2170.0,18.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3407,Grabbing Boobs,100.0,160.0,60.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3416,Anal,450.0,472.0,22.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3409,Grabbing Boobs,986.0,1018.0,32.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3410,Grabbing Boobs,1100.0,1144.0,44.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3412,BlowJob,1884.0,1930.0,46.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3418,Anal,2346.0,2380.0,34.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3419,Anal,2440.0,2464.0,24.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3420,Anal,2542.0,2570.0,28.0
475,"Anissa Kate, Shalina Devine in Rumbling in the Ring",3421,Anal,2612.0,2658.0,46.0
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",3423,BlowJob,102.0,150.0,48.0
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",3427,Cumshot,184.0,220.0,36.0
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",3428,Cumshot,304.0,340.0,36.0
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",3424,BlowJob,372.0,416.0,44.0
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",3425,BlowJob,456.0,490.0,34.0
476,"Anna Gold, Sai Tai Tiger in Sperma Studio 1",3426,BlowJob,524.0,558.0,34.0
477,Anni Star in Lingerie Pleasure Premi√®re,3445,BlowJob,66.0,118.0,52.0
477,Anni Star in Lingerie Pleasure Premi√®re,3446,BlowJob,156.0,166.0,10.0
477,Anni Star in Lingerie Pleasure Premi√®re,3447,Grabbing Boobs,188.0,228.0,40.0
477,Anni Star in Lingerie Pleasure Premi√®re,3448,Titjob,204.0,234.0,30.0
477,Anni Star in Lingerie Pleasure Premi√®re,3449,Anal,350.0,374.0,24.0
477,Anni Star in Lingerie Pleasure Premi√®re,3451,Anal,732.0,756.0,24.0
479,April Snow in GangBang Creampie 232,5205,Gangbang,10.0,40.0,30.0
479,April Snow in GangBang Creampie 232,5206,BlowJob,82.0,104.0,22.0
479,April Snow in GangBang Creampie 232,5216,Grabbing Boobs,268.0,288.0,20.0
479,April Snow in GangBang Creampie 232,5208,BlowJob,866.0,910.0,44.0
479,April Snow in GangBang Creampie 232,5218,Anal,984.0,1030.0,46.0
479,April Snow in GangBang Creampie 232,5209,BlowJob,1160.0,1174.0,14.0
479,April Snow in GangBang Creampie 232,5210,BlowJob,1232.0,1280.0,48.0
479,April Snow in GangBang Creampie 232,5217,Grabbing Boobs,1302.0,1322.0,20.0
479,April Snow in GangBang Creampie 232,5211,BlowJob,1336.0,1358.0,22.0
479,April Snow in GangBang Creampie 232,5213,BlowJob,1750.0,1768.0,18.0
479,April Snow in GangBang Creampie 232,5214,BlowJob,1802.0,1822.0,20.0
479,April Snow in GangBang Creampie 232,5215,BlowJob,1924.0,1952.0,28.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5221,BlowJob,302.0,310.0,8.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5223,BlowJob,406.0,442.0,36.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5227,Anal,690.0,718.0,28.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5228,Anal,1100.0,1114.0,14.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5229,Anal,1168.0,1200.0,32.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5230,Anal,1258.0,1292.0,34.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5224,BlowJob,1616.0,1660.0,44.0
481,Ariella Ferraz in Monster Cock Brazilian Anal & Sloppy BJ,5225,BlowJob,1702.0,1718.0,16.0
482,"Ashby Winter in Vogue 2, Part 5",5232,BlowJob,1246.0,1266.0,20.0
482,"Ashby Winter in Vogue 2, Part 5",5237,DP,2152.0,2180.0,28.0
482,"Ashby Winter in Vogue 2, Part 5",5234,BlowJob,2490.0,2508.0,18.0
482,"Ashby Winter in Vogue 2, Part 5",5235,BlowJob,2548.0,2584.0,36.0
482,"Ashby Winter in Vogue 2, Part 5",5238,Cumshot,2636.0,2694.0,58.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5239,BlowJob,8.0,18.0,10.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5245,BlowJob,36.0,86.0,50.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5241,BlowJob,146.0,168.0,22.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5242,BlowJob,218.0,242.0,24.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5243,BlowJob,340.0,372.0,32.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5244,BlowJob,786.0,836.0,50.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5247,Cumshot,794.0,810.0,16.0
483,Ashley Cumstar in Curvy MILF enjoys Threesome,5249,Cumshot,926.0,946.0,20.0
484,Ashley Cumstar in Gangbang Party,5250,Grabbing Boobs,14.0,32.0,18.0
484,Ashley Cumstar in Gangbang Party,5252,BlowJob,230.0,250.0,20.0
484,Ashley Cumstar in Gangbang Party,5256,Cumshot,302.0,314.0,12.0
484,Ashley Cumstar in Gangbang Party,5253,BlowJob,346.0,398.0,52.0
484,Ashley Cumstar in Gangbang Party,5255,BlowJob,432.0,442.0,10.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5257,Anal,160.0,206.0,46.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5272,BlowJob,298.0,358.0,60.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5259,Anal,448.0,494.0,46.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5274,Gangbang,604.0,616.0,12.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5260,Anal,654.0,712.0,58.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5261,Anal,856.0,892.0,36.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5276,Gangbang,1068.0,1080.0,12.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5277,Gangbang,1162.0,1172.0,10.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5278,Gangbang,1332.0,1348.0,16.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5263,Anal,1380.0,1406.0,26.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5264,Anal,1468.0,1504.0,36.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5279,Gangbang,1470.0,1484.0,14.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5280,Gangbang,1646.0,1678.0,32.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5281,Gangbang,1740.0,1752.0,12.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5266,Anal,1796.0,1856.0,60.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5267,Anal,1950.0,1974.0,24.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5282,Gangbang,2024.0,2052.0,28.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5283,Gangbang,2108.0,2120.0,12.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5284,Gangbang,2258.0,2312.0,54.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5268,Anal,2282.0,2302.0,20.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5286,Pissing,2354.0,2358.0,4.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5285,Gangbang,2540.0,2594.0,54.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5270,Anal,2864.0,2908.0,44.0
485,Athenea Rose in 5on1 Hardcore Gangbang,5273,BlowJob,2910.0,2926.0,16.0
486,Athenea Rose in 7on1 DAP Gangbang,5287,Gangbang,28.0,50.0,22.0
486,Athenea Rose in 7on1 DAP Gangbang,5294,Grabbing Boobs,30.0,54.0,24.0
486,Athenea Rose in 7on1 DAP Gangbang,5297,Anal,68.0,94.0,26.0
486,Athenea Rose in 7on1 DAP Gangbang,5298,Anal,170.0,198.0,28.0
486,Athenea Rose in 7on1 DAP Gangbang,5299,Anal,372.0,410.0,38.0
486,Athenea Rose in 7on1 DAP Gangbang,5304,BlowJob,562.0,592.0,30.0
486,Athenea Rose in 7on1 DAP Gangbang,5300,Anal,572.0,628.0,56.0
486,Athenea Rose in 7on1 DAP Gangbang,5288,Gangbang,586.0,618.0,32.0
486,Athenea Rose in 7on1 DAP Gangbang,5301,Anal,704.0,714.0,10.0
486,Athenea Rose in 7on1 DAP Gangbang,5295,Grabbing Boobs,844.0,870.0,26.0
486,Athenea Rose in 7on1 DAP Gangbang,5306,BlowJob,976.0,992.0,16.0
486,Athenea Rose in 7on1 DAP Gangbang,5289,Gangbang,1024.0,1062.0,38.0
486,Athenea Rose in 7on1 DAP Gangbang,5307,BlowJob,1026.0,1066.0,40.0
486,Athenea Rose in 7on1 DAP Gangbang,5290,Gangbang,1210.0,1226.0,16.0
486,Athenea Rose in 7on1 DAP Gangbang,5291,Gangbang,1426.0,1466.0,40.0
486,Athenea Rose in 7on1 DAP Gangbang,5292,Gangbang,1662.0,1694.0,32.0
486,Athenea Rose in 7on1 DAP Gangbang,5310,Pissing,3466.0,3478.0,12.0
486,Athenea Rose in 7on1 DAP Gangbang,5296,Grabbing Boobs,3646.0,3694.0,48.0
486,Athenea Rose in 7on1 DAP Gangbang,5309,BlowJob,3702.0,3708.0,6.0
487,Athenea Rose in Airtight 6on1 Destruction,5317,Gangbang,356.0,378.0,22.0
487,Athenea Rose in Airtight 6on1 Destruction,5318,Gangbang,424.0,440.0,16.0
487,Athenea Rose in Airtight 6on1 Destruction,5319,Gangbang,514.0,528.0,14.0
487,Athenea Rose in Airtight 6on1 Destruction,5331,BlowJob,704.0,716.0,12.0
487,Athenea Rose in Airtight 6on1 Destruction,5320,Gangbang,716.0,752.0,36.0
487,Athenea Rose in Airtight 6on1 Destruction,5332,BlowJob,904.0,918.0,14.0
487,Athenea Rose in Airtight 6on1 Destruction,5322,Gangbang,1144.0,1160.0,16.0
487,Athenea Rose in Airtight 6on1 Destruction,5334,BlowJob,1534.0,1548.0,14.0
487,Athenea Rose in Airtight 6on1 Destruction,5323,Gangbang,1570.0,1586.0,16.0
487,Athenea Rose in Airtight 6on1 Destruction,5336,BlowJob,1824.0,1870.0,46.0
487,Athenea Rose in Airtight 6on1 Destruction,5343,Anal,1952.0,1964.0,12.0
487,Athenea Rose in Airtight 6on1 Destruction,5337,BlowJob,1974.0,1986.0,12.0
487,Athenea Rose in Airtight 6on1 Destruction,5344,Anal,2108.0,2126.0,18.0
487,Athenea Rose in Airtight 6on1 Destruction,5345,Anal,2176.0,2208.0,32.0
487,Athenea Rose in Airtight 6on1 Destruction,5314,Grabbing Boobs,2276.0,2320.0,44.0
487,Athenea Rose in Airtight 6on1 Destruction,5346,Anal,2300.0,2312.0,12.0
487,Athenea Rose in Airtight 6on1 Destruction,5315,Grabbing Boobs,2436.0,2474.0,38.0
487,Athenea Rose in Airtight 6on1 Destruction,5338,BlowJob,2450.0,2462.0,12.0
487,Athenea Rose in Airtight 6on1 Destruction,5326,Gangbang,2470.0,2484.0,14.0
487,Athenea Rose in Airtight 6on1 Destruction,5347,Anal,2532.0,2548.0,16.0
487,Athenea Rose in Airtight 6on1 Destruction,5316,Grabbing Boobs,2534.0,2580.0,46.0
487,Athenea Rose in Airtight 6on1 Destruction,5327,Gangbang,2560.0,2608.0,48.0
487,Athenea Rose in Airtight 6on1 Destruction,5339,BlowJob,2638.0,2662.0,24.0
487,Athenea Rose in Airtight 6on1 Destruction,5348,Anal,2656.0,2702.0,46.0
487,Athenea Rose in Airtight 6on1 Destruction,5328,Gangbang,2680.0,2708.0,28.0
487,Athenea Rose in Airtight 6on1 Destruction,5349,Anal,2758.0,2804.0,46.0
487,Athenea Rose in Airtight 6on1 Destruction,5350,Anal,2836.0,2848.0,12.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5445,Grabbing Boobs,142.0,170.0,28.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5447,Anal,258.0,276.0,18.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5451,BlowJob,338.0,398.0,60.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5452,BlowJob,462.0,504.0,42.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5446,Grabbing Boobs,1796.0,1822.0,26.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5455,BlowJob,2302.0,2320.0,18.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5456,BlowJob,2490.0,2512.0,22.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5450,Gangbang,2620.0,2630.0,10.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5458,BlowJob,2922.0,2932.0,10.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5459,BlowJob,2964.0,3002.0,38.0
488,Athenea Rose in Brutal 5on1 DAP and Piss Challenge,5460,BlowJob,3472.0,3484.0,12.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5483,BlowJob,272.0,298.0,26.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5484,BlowJob,338.0,366.0,28.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5472,Gangbang,436.0,482.0,46.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5485,BlowJob,456.0,504.0,48.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5493,Anal,538.0,552.0,14.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5486,BlowJob,808.0,832.0,24.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5473,Gangbang,832.0,842.0,10.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5474,Gangbang,890.0,914.0,24.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5475,Gangbang,952.0,978.0,26.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5509,Pissing,1168.0,1180.0,12.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5511,Cumshot,1180.0,1212.0,32.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5487,BlowJob,1290.0,1300.0,10.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5488,BlowJob,1380.0,1390.0,10.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5496,Anal,1688.0,1726.0,38.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5477,Gangbang,1692.0,1720.0,28.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5478,Gangbang,1758.0,1814.0,56.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5497,Anal,1780.0,1798.0,18.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5479,Gangbang,1854.0,1874.0,20.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5480,Gangbang,1930.0,1952.0,22.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5498,Anal,2186.0,2216.0,30.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5489,BlowJob,2236.0,2248.0,12.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5499,Anal,2274.0,2286.0,12.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5500,Anal,2318.0,2340.0,22.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5501,Anal,2418.0,2458.0,40.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5490,BlowJob,2606.0,2620.0,14.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5491,BlowJob,2918.0,2936.0,18.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5492,BlowJob,3024.0,3042.0,18.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5502,Anal,3128.0,3144.0,16.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5504,Anal,3534.0,3568.0,34.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5505,Anal,3610.0,3646.0,36.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5471,Grabbing Boobs,3736.0,3768.0,32.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5506,Anal,3908.0,3968.0,60.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5507,Anal,4066.0,4082.0,16.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5508,Anal,4394.0,4414.0,20.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5510,Pissing,4450.0,4460.0,10.0
489,Athenea Rose in Hardcore 7on1 with Swallow Finish,5482,Gangbang,4522.0,4532.0,10.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5520,BlowJob,360.0,388.0,28.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5521,BlowJob,600.0,610.0,10.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5522,BlowJob,1032.0,1048.0,16.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5523,BlowJob,1294.0,1342.0,48.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5531,Pissing,1442.0,1450.0,8.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5514,Gangbang,1878.0,1932.0,54.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5515,Gangbang,2034.0,2058.0,24.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5533,Cumshot,2278.0,2312.0,34.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5532,Pissing,2296.0,2310.0,14.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5526,Anal,2648.0,2664.0,16.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5516,Gangbang,2742.0,2754.0,12.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5524,BlowJob,2882.0,2920.0,38.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5517,Gangbang,3088.0,3118.0,30.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5534,Cumshot,3824.0,3830.0,6.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5519,Grabbing Boobs,3838.0,3868.0,30.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5535,Cumshot,3900.0,3946.0,46.0
490,Athenea Rose in Hardcore BBC Gangbang and Pee Action,5530,Anal,4144.0,4154.0,10.0
491,Athenea Rose in Hardcore Interracial DAP,5441,BlowJob,790.0,806.0,16.0
491,Athenea Rose in Hardcore Interracial DAP,5442,BlowJob,2560.0,2600.0,40.0
491,Athenea Rose in Hardcore Interracial DAP,5443,Cumshot,3038.0,3054.0,16.0
492,Athenea Rose in Hecho en Medelln,5358,Anal,1758.0,1772.0,14.0
492,Athenea Rose in Hecho en Medelln,5361,Anal,2516.0,2542.0,26.0
492,Athenea Rose in Hecho en Medelln,5362,Anal,2578.0,2624.0,46.0
492,Athenea Rose in Hecho en Medelln,5354,BlowJob,2790.0,2812.0,22.0
492,Athenea Rose in Hecho en Medelln,5364,Anal,2842.0,2860.0,18.0
492,Athenea Rose in Hecho en Medelln,5355,BlowJob,3158.0,3194.0,36.0
492,Athenea Rose in Hecho en Medelln,5366,Anal,3274.0,3290.0,16.0
492,Athenea Rose in Hecho en Medelln,5367,Anal,3342.0,3364.0,22.0
493,Athenea Rose in Intense Anal Destruction,5461,BlowJob,152.0,164.0,12.0
493,Athenea Rose in Intense Anal Destruction,5463,Anal,422.0,462.0,40.0
493,Athenea Rose in Intense Anal Destruction,5464,Anal,498.0,532.0,34.0
493,Athenea Rose in Intense Anal Destruction,5465,Anal,660.0,688.0,28.0
493,Athenea Rose in Intense Anal Destruction,5468,Anal,1412.0,1446.0,34.0
493,Athenea Rose in Intense Anal Destruction,5469,Anal,1534.0,1586.0,52.0
493,Athenea Rose in Intense Anal Destruction,5462,BlowJob,1760.0,1792.0,32.0
494,Athenea Rose in Loves Public Anal,5536,BlowJob,628.0,642.0,14.0
494,Athenea Rose in Loves Public Anal,5538,Anal,1008.0,1030.0,22.0
494,Athenea Rose in Loves Public Anal,5539,Anal,1276.0,1304.0,28.0
494,Athenea Rose in Loves Public Anal,5537,BlowJob,1734.0,1764.0,30.0
494,Athenea Rose in Loves Public Anal,5540,Anal,1916.0,1964.0,48.0
494,Athenea Rose in Loves Public Anal,5541,Anal,1998.0,2024.0,26.0
494,Athenea Rose in Loves Public Anal,5542,Anal,2152.0,2168.0,16.0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,5547,Anal,672.0,714.0,42.0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,5548,Anal,858.0,912.0,54.0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,5550,Anal,1146.0,1174.0,28.0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,5551,Anal,1320.0,1334.0,14.0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,5552,Anal,1376.0,1404.0,28.0
495,Athenea Rose in Mike Zander‚Äôs Brutal Casting,5553,Anal,1688.0,1704.0,16.0
496,Athenea Rose in Playing with 3 BBC,5560,Anal,660.0,700.0,40.0
496,Athenea Rose in Playing with 3 BBC,5561,Anal,746.0,790.0,44.0
496,Athenea Rose in Playing with 3 BBC,5555,BlowJob,844.0,896.0,52.0
496,Athenea Rose in Playing with 3 BBC,5556,BlowJob,932.0,960.0,28.0
496,Athenea Rose in Playing with 3 BBC,5563,Anal,1578.0,1626.0,48.0
496,Athenea Rose in Playing with 3 BBC,5557,BlowJob,1666.0,1702.0,36.0
496,Athenea Rose in Playing with 3 BBC,5558,BlowJob,1750.0,1796.0,46.0
497,Athenea Rose in PremiumBukkake #1,5570,BlowJob,582.0,604.0,22.0
497,Athenea Rose in PremiumBukkake #1,5573,Cumshot,592.0,602.0,10.0
497,Athenea Rose in PremiumBukkake #1,5574,Cumshot,646.0,674.0,28.0
497,Athenea Rose in PremiumBukkake #1,5580,Cumshot,712.0,756.0,44.0
497,Athenea Rose in PremiumBukkake #1,5575,Cumshot,766.0,820.0,54.0
497,Athenea Rose in PremiumBukkake #1,5576,Cumshot,846.0,896.0,50.0
497,Athenea Rose in PremiumBukkake #1,5571,BlowJob,884.0,888.0,4.0
497,Athenea Rose in PremiumBukkake #1,5583,Cumshot,932.0,964.0,32.0
497,Athenea Rose in PremiumBukkake #1,5584,Cumshot,1080.0,1132.0,52.0
497,Athenea Rose in PremiumBukkake #1,5585,Cumshot,1220.0,1250.0,30.0
499,Athenea Rose in PremiumBukkake #3,5594,BlowJob,12.0,16.0,4.0
499,Athenea Rose in PremiumBukkake #3,5598,BlowJob,552.0,580.0,28.0
500,Athenea Rose in Sex Crazed Slut 4on1,5613,Grabbing Boobs,60.0,104.0,44.0
500,Athenea Rose in Sex Crazed Slut 4on1,5614,Grabbing Boobs,152.0,172.0,20.0
500,Athenea Rose in Sex Crazed Slut 4on1,5621,Cumshot,678.0,686.0,8.0
500,Athenea Rose in Sex Crazed Slut 4on1,5615,Grabbing Boobs,922.0,944.0,22.0
500,Athenea Rose in Sex Crazed Slut 4on1,5618,BlowJob,1528.0,1550.0,22.0
500,Athenea Rose in Sex Crazed Slut 4on1,5622,Pissing,1870.0,1896.0,26.0
500,Athenea Rose in Sex Crazed Slut 4on1,5619,BlowJob,2648.0,2662.0,14.0
500,Athenea Rose in Sex Crazed Slut 4on1,5620,BlowJob,2692.0,2696.0,4.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5623,Grabbing Boobs,156.0,188.0,32.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5627,Gangbang,294.0,342.0,48.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5635,BlowJob,356.0,396.0,40.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5639,Anal,560.0,596.0,36.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5628,Gangbang,642.0,670.0,28.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5640,Anal,670.0,680.0,10.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5641,Anal,788.0,848.0,60.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5642,Anal,956.0,978.0,22.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5636,BlowJob,970.0,996.0,26.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5624,Grabbing Boobs,1340.0,1364.0,24.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5629,Gangbang,1356.0,1384.0,28.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5630,Gangbang,1524.0,1534.0,10.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5631,Gangbang,1672.0,1696.0,24.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5632,Gangbang,1774.0,1790.0,16.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5633,Gangbang,1836.0,1868.0,32.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5644,Anal,1850.0,1872.0,22.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5638,BlowJob,1980.0,2014.0,34.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5645,Anal,1984.0,2020.0,36.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5650,DP,2038.0,2042.0,4.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5646,Anal,2078.0,2138.0,60.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5651,DP,2102.0,2106.0,4.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5647,Anal,2180.0,2208.0,28.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5634,Gangbang,2368.0,2394.0,26.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5652,Pissing,2406.0,2414.0,8.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5653,Cumshot,2448.0,2490.0,42.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5655,Cumshot,3302.0,3314.0,12.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5625,Grabbing Boobs,3700.0,3730.0,30.0
501,Athenea Rose in Waka Waka Blacks Are Coming Goes Wet,5626,Grabbing Boobs,3774.0,3792.0,18.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5658,Anal,378.0,394.0,16.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5659,Anal,1350.0,1384.0,34.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5669,BlowJob,1376.0,1400.0,24.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5670,BlowJob,1462.0,1472.0,10.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5660,Anal,1516.0,1562.0,46.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5661,Anal,1612.0,1636.0,24.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5662,Anal,1732.0,1786.0,54.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5663,Anal,1980.0,1992.0,12.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5664,Anal,2058.0,2070.0,12.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5666,Anal,2464.0,2488.0,24.0
502,"Athenea Rose, Natasha Teen in Hot Sexy Colombiana in Fishnet BBC Fantasies Outdoors",5671,BlowJob,2908.0,2946.0,38.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5672,Grabbing Boobs,212.0,230.0,18.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5673,BlowJob,340.0,394.0,54.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5675,BlowJob,782.0,798.0,16.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5690,Gangbang,798.0,808.0,10.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5676,BlowJob,842.0,870.0,28.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5691,Gangbang,876.0,922.0,46.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5692,Gangbang,962.0,994.0,32.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5677,BlowJob,966.0,996.0,30.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5693,Gangbang,1134.0,1162.0,28.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5694,Gangbang,1840.0,1850.0,10.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5683,Anal,2876.0,2900.0,24.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5684,Anal,3208.0,3238.0,30.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5678,BlowJob,3234.0,3260.0,26.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5685,Anal,3306.0,3350.0,44.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5679,BlowJob,3430.0,3470.0,40.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5695,Gangbang,3432.0,3444.0,12.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5686,Anal,3438.0,3458.0,20.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5680,BlowJob,3544.0,3576.0,32.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5687,Anal,3606.0,3624.0,18.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5688,Anal,3730.0,3740.0,10.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5689,Anal,3990.0,4006.0,16.0
503,"Athenea Rose, Vivian Lola in Swallowed and Stuffed",5696,Gangbang,4118.0,4146.0,28.0
504,Aubrey Black in GangBang Creampie 225,5700,BlowJob,324.0,374.0,50.0
504,Aubrey Black in GangBang Creampie 225,5698,Grabbing Boobs,446.0,486.0,40.0
504,Aubrey Black in GangBang Creampie 225,5699,Grabbing Boobs,672.0,692.0,20.0
504,Aubrey Black in GangBang Creampie 225,5701,BlowJob,982.0,1036.0,54.0
504,Aubrey Black in GangBang Creampie 225,5703,Cumshot,1474.0,1498.0,24.0
504,Aubrey Black in GangBang Creampie 225,5704,Cumshot,1560.0,1564.0,4.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5707,BlowJob,194.0,212.0,18.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5708,BlowJob,282.0,334.0,52.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5717,Cumshot,1094.0,1136.0,42.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5710,BlowJob,1098.0,1110.0,12.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5720,Gangbang,1130.0,1152.0,22.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5721,Gangbang,1184.0,1218.0,34.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5711,BlowJob,1268.0,1280.0,12.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5722,Gangbang,1366.0,1398.0,32.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5712,BlowJob,2386.0,2430.0,44.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5724,Gangbang,2628.0,2648.0,20.0
505,"Avery Jane in 7on1, Pee Drink, Cum Mouth, Swallow",5706,Grabbing Boobs,4086.0,4108.0,22.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5726,Gangbang,432.0,480.0,48.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5725,Grabbing Boobs,434.0,472.0,38.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5733,BlowJob,532.0,548.0,16.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5734,BlowJob,670.0,712.0,42.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5739,Anal,718.0,740.0,22.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5735,BlowJob,1090.0,1118.0,28.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5741,Cumshot,1092.0,1098.0,6.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5728,Gangbang,1610.0,1662.0,52.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5743,DP,1886.0,1916.0,30.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5729,Gangbang,1972.0,2024.0,52.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5736,BlowJob,2278.0,2290.0,12.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5737,BlowJob,3060.0,3082.0,22.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5742,Cumshot,3082.0,3092.0,10.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5731,Gangbang,3408.0,3418.0,10.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5738,BlowJob,3428.0,3440.0,12.0
506,Avery Jane in Brutal 7on1 DAP Birthday,5732,Gangbang,3462.0,3476.0,14.0
507,Avery Jane in Milking Mike Adriano,5744,Grabbing Boobs,58.0,92.0,34.0
507,Avery Jane in Milking Mike Adriano,5747,Titjob,356.0,370.0,14.0
507,Avery Jane in Milking Mike Adriano,5748,Cumshot,674.0,686.0,12.0
507,Avery Jane in Milking Mike Adriano,5749,Cumshot,778.0,814.0,36.0
507,Avery Jane in Milking Mike Adriano,5752,Anal,1014.0,1064.0,50.0
507,Avery Jane in Milking Mike Adriano,5754,Anal,1236.0,1262.0,26.0
507,Avery Jane in Milking Mike Adriano,5755,Anal,1302.0,1322.0,20.0
507,Avery Jane in Milking Mike Adriano,5757,Anal,1904.0,1952.0,48.0
507,Avery Jane in Milking Mike Adriano,5758,Anal,2004.0,2042.0,38.0
507,Avery Jane in Milking Mike Adriano,5759,Anal,2158.0,2172.0,14.0
507,Avery Jane in Milking Mike Adriano,5760,Anal,2222.0,2254.0,32.0
508,Avery Jane in Piss Soaked Backdoor Debut,5761,Grabbing Boobs,84.0,110.0,26.0
508,Avery Jane in Piss Soaked Backdoor Debut,5762,Grabbing Boobs,182.0,212.0,30.0
508,Avery Jane in Piss Soaked Backdoor Debut,5776,Gangbang,1268.0,1314.0,46.0
508,Avery Jane in Piss Soaked Backdoor Debut,5791,Pissing,1414.0,1464.0,50.0
508,Avery Jane in Piss Soaked Backdoor Debut,5782,BlowJob,1504.0,1514.0,10.0
508,Avery Jane in Piss Soaked Backdoor Debut,5783,BlowJob,1582.0,1642.0,60.0
508,Avery Jane in Piss Soaked Backdoor Debut,5792,Cumshot,1586.0,1612.0,26.0
508,Avery Jane in Piss Soaked Backdoor Debut,5784,BlowJob,1680.0,1702.0,22.0
508,Avery Jane in Piss Soaked Backdoor Debut,5797,DP,1744.0,1748.0,4.0
508,Avery Jane in Piss Soaked Backdoor Debut,5765,Anal,1764.0,1784.0,20.0
508,Avery Jane in Piss Soaked Backdoor Debut,5777,Gangbang,1768.0,1790.0,22.0
508,Avery Jane in Piss Soaked Backdoor Debut,5766,Anal,1840.0,1850.0,10.0
508,Avery Jane in Piss Soaked Backdoor Debut,5767,Anal,1890.0,1914.0,24.0
508,Avery Jane in Piss Soaked Backdoor Debut,5768,Anal,1990.0,2000.0,10.0
508,Avery Jane in Piss Soaked Backdoor Debut,5769,Anal,2046.0,2092.0,46.0
508,Avery Jane in Piss Soaked Backdoor Debut,5770,Anal,2440.0,2470.0,30.0
508,Avery Jane in Piss Soaked Backdoor Debut,5786,BlowJob,2514.0,2528.0,14.0
508,Avery Jane in Piss Soaked Backdoor Debut,5771,Anal,2516.0,2546.0,30.0
508,Avery Jane in Piss Soaked Backdoor Debut,5778,Gangbang,2520.0,2556.0,36.0
508,Avery Jane in Piss Soaked Backdoor Debut,5772,Anal,2596.0,2646.0,50.0
508,Avery Jane in Piss Soaked Backdoor Debut,5787,BlowJob,2792.0,2824.0,32.0
508,Avery Jane in Piss Soaked Backdoor Debut,5779,Gangbang,2902.0,2920.0,18.0
508,Avery Jane in Piss Soaked Backdoor Debut,5788,BlowJob,2906.0,2926.0,20.0
508,Avery Jane in Piss Soaked Backdoor Debut,5789,BlowJob,3004.0,3040.0,36.0
508,Avery Jane in Piss Soaked Backdoor Debut,5793,Cumshot,3030.0,3062.0,32.0
508,Avery Jane in Piss Soaked Backdoor Debut,5780,Gangbang,3034.0,3052.0,18.0
508,Avery Jane in Piss Soaked Backdoor Debut,5774,Anal,3388.0,3414.0,26.0
508,Avery Jane in Piss Soaked Backdoor Debut,5775,Anal,3496.0,3520.0,24.0
508,Avery Jane in Piss Soaked Backdoor Debut,5794,Cumshot,3722.0,3730.0,8.0
508,Avery Jane in Piss Soaked Backdoor Debut,5795,Cumshot,3802.0,3856.0,54.0
509,Avi Love in GangBang Creampie 216,5798,BlowJob,138.0,154.0,16.0
509,Avi Love in GangBang Creampie 216,5799,BlowJob,258.0,270.0,12.0
509,Avi Love in GangBang Creampie 216,5800,BlowJob,418.0,468.0,50.0
509,Avi Love in GangBang Creampie 216,5801,BlowJob,640.0,652.0,12.0
509,Avi Love in GangBang Creampie 216,5802,BlowJob,844.0,894.0,50.0
509,Avi Love in GangBang Creampie 216,5807,Gangbang,920.0,930.0,10.0
509,Avi Love in GangBang Creampie 216,5803,BlowJob,944.0,978.0,34.0
509,Avi Love in GangBang Creampie 216,5804,BlowJob,1016.0,1044.0,28.0
509,Avi Love in GangBang Creampie 216,5808,Gangbang,1036.0,1062.0,26.0
509,Avi Love in GangBang Creampie 216,5809,Grabbing Boobs,1662.0,1686.0,24.0
509,Avi Love in GangBang Creampie 216,5810,Grabbing Boobs,2128.0,2164.0,36.0
509,Avi Love in GangBang Creampie 216,5805,BlowJob,2232.0,2246.0,14.0
509,Avi Love in GangBang Creampie 216,5812,Cumshot,2342.0,2384.0,42.0
509,Avi Love in GangBang Creampie 216,5811,Grabbing Boobs,2620.0,2640.0,20.0
511,Baby Gemini in All About The Booty,5813,BlowJob,322.0,382.0,60.0
511,Baby Gemini in All About The Booty,5814,Cumshot,352.0,384.0,32.0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,5815,Grabbing Boobs,306.0,334.0,28.0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,5818,Cumshot,1254.0,1260.0,6.0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,5820,Cumshot,1708.0,1726.0,18.0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,5821,Cumshot,1836.0,1862.0,26.0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,5828,Cumshot,2202.0,2260.0,58.0
512,Baby Gemini in Sexy Ebony Gonna Make You Hard,5829,Cumshot,2312.0,2342.0,30.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5831,Grabbing Boobs,6.0,48.0,42.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5832,Grabbing Boobs,180.0,234.0,54.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5836,Anal,502.0,522.0,20.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5833,Grabbing Boobs,860.0,884.0,24.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5834,Grabbing Boobs,994.0,1020.0,26.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5840,BlowJob,1578.0,1610.0,32.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5835,Grabbing Boobs,3052.0,3072.0,20.0
513,"Bambi Bella, Blanche Bradburry in Blonde Sluts Anal Fucking",5838,Anal,3122.0,3166.0,44.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5841,BlowJob,310.0,330.0,20.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5850,Gangbang,484.0,516.0,32.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5842,BlowJob,490.0,530.0,40.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5843,BlowJob,582.0,634.0,52.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5844,BlowJob,732.0,746.0,14.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5851,Gangbang,1118.0,1158.0,40.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5845,BlowJob,1658.0,1678.0,20.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5852,Gangbang,2044.0,2076.0,32.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5846,BlowJob,2180.0,2224.0,44.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5848,BlowJob,2392.0,2404.0,12.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5847,BlowJob,2642.0,2674.0,32.0
514,Barbie Esm in Rough 4on1 DAP Fantasy,5855,Cumshot,2896.0,2904.0,8.0
515,Barbie Sins in Barbie Gets wet with 2 BBC,5857,BlowJob,232.0,260.0,28.0
515,Barbie Sins in Barbie Gets wet with 2 BBC,5858,BlowJob,292.0,308.0,16.0
515,Barbie Sins in Barbie Gets wet with 2 BBC,5860,BlowJob,696.0,736.0,40.0
515,Barbie Sins in Barbie Gets wet with 2 BBC,5861,BlowJob,1662.0,1692.0,30.0
516,Barbie Sins in Creampie and Swallow Showdown,5145,BlowJob,248.0,266.0,18.0
516,Barbie Sins in Creampie and Swallow Showdown,5146,BlowJob,298.0,356.0,58.0
516,Barbie Sins in Creampie and Swallow Showdown,5147,BlowJob,664.0,716.0,52.0
516,Barbie Sins in Creampie and Swallow Showdown,5148,BlowJob,1046.0,1070.0,24.0
516,Barbie Sins in Creampie and Swallow Showdown,5149,BlowJob,1534.0,1580.0,46.0
517,"Barbie Sins in DAP, Piss and Power Play",5864,BlowJob,716.0,774.0,58.0
517,"Barbie Sins in DAP, Piss and Power Play",5868,Gangbang,790.0,836.0,46.0
517,"Barbie Sins in DAP, Piss and Power Play",5869,Cumshot,992.0,1002.0,10.0
517,"Barbie Sins in DAP, Piss and Power Play",5865,BlowJob,1234.0,1262.0,28.0
517,"Barbie Sins in DAP, Piss and Power Play",5867,BlowJob,1732.0,1744.0,12.0
517,"Barbie Sins in DAP, Piss and Power Play",5866,BlowJob,2490.0,2538.0,48.0
517,"Barbie Sins in DAP, Piss and Power Play",5871,Cumshot,3022.0,3034.0,12.0
517,"Barbie Sins in DAP, Piss and Power Play",5872,Cumshot,3112.0,3116.0,4.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5873,Grabbing Boobs,36.0,70.0,34.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5874,BlowJob,626.0,648.0,22.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5888,Gangbang,746.0,764.0,18.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5877,BlowJob,908.0,928.0,20.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5878,BlowJob,960.0,1000.0,40.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5879,BlowJob,1040.0,1052.0,12.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5880,BlowJob,1172.0,1202.0,30.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5881,BlowJob,1292.0,1302.0,10.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5882,BlowJob,1374.0,1394.0,20.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5883,BlowJob,1478.0,1502.0,24.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5885,BlowJob,2392.0,2406.0,14.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5889,Gangbang,2464.0,2510.0,46.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5886,BlowJob,2778.0,2822.0,44.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5887,BlowJob,3030.0,3060.0,30.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5891,Cumshot,3080.0,3084.0,4.0
518,Barbie Sins in No Holes Barred Gonzo Assault,5892,Cumshot,3102.0,3108.0,6.0
519,Barbie Sins in Rough DAP & Swallow Madness,5894,BlowJob,594.0,622.0,28.0
519,Barbie Sins in Rough DAP & Swallow Madness,5895,BlowJob,904.0,938.0,34.0
519,Barbie Sins in Rough DAP & Swallow Madness,5903,Gangbang,994.0,1026.0,32.0
519,Barbie Sins in Rough DAP & Swallow Madness,5905,Gangbang,1290.0,1322.0,32.0
519,Barbie Sins in Rough DAP & Swallow Madness,5897,BlowJob,1424.0,1450.0,26.0
519,Barbie Sins in Rough DAP & Swallow Madness,5906,DP,1454.0,1468.0,14.0
519,Barbie Sins in Rough DAP & Swallow Madness,5898,BlowJob,2168.0,2186.0,18.0
519,Barbie Sins in Rough DAP & Swallow Madness,5899,BlowJob,2250.0,2264.0,14.0
519,Barbie Sins in Rough DAP & Swallow Madness,5900,BlowJob,2430.0,2440.0,10.0
519,Barbie Sins in Rough DAP & Swallow Madness,5907,Cumshot,2688.0,2696.0,8.0
519,Barbie Sins in Rough DAP & Swallow Madness,5908,Cumshot,2754.0,2806.0,52.0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",5909,Anal,186.0,196.0,10.0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",5916,BlowJob,336.0,350.0,14.0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",5917,BlowJob,696.0,720.0,24.0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",5918,BlowJob,916.0,950.0,34.0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",5913,Anal,1194.0,1210.0,16.0
520,"Barbie Sins, Jasmine Jae in Destroyed on Valentines",5914,Anal,1304.0,1314.0,10.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5369,Grabbing Boobs,538.0,582.0,44.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5370,Grabbing Boobs,658.0,698.0,40.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5371,Grabbing Boobs,790.0,822.0,32.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5386,BlowJob,1882.0,1892.0,10.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5387,BlowJob,1944.0,1958.0,14.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5373,Grabbing Boobs,2074.0,2098.0,24.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5388,BlowJob,2110.0,2124.0,14.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5377,BlowJob,2404.0,2428.0,24.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5393,BlowJob,2448.0,2458.0,10.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5378,BlowJob,2796.0,2854.0,58.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5389,BlowJob,2812.0,2830.0,18.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5390,BlowJob,3140.0,3170.0,30.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5380,BlowJob,3212.0,3228.0,16.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5391,BlowJob,3216.0,3222.0,6.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5381,BlowJob,3328.0,3358.0,30.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5374,Grabbing Boobs,3778.0,3814.0,36.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5383,BlowJob,3846.0,3900.0,54.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5392,BlowJob,3864.0,3870.0,6.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5384,BlowJob,4162.0,4192.0,30.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5375,Grabbing Boobs,4192.0,4246.0,54.0
538,"Blondie Fesser, Briana Banderas in Bosses Fuck Bisexual Secretaries",5394,Cumshot,4202.0,4230.0,28.0
557,Candela X in There is a Star Around the Gape!,5075,BlowJob,504.0,546.0,42.0
557,Candela X in There is a Star Around the Gape!,5079,Gangbang,520.0,574.0,54.0
557,Candela X in There is a Star Around the Gape!,5076,BlowJob,712.0,754.0,42.0
557,Candela X in There is a Star Around the Gape!,5077,BlowJob,814.0,840.0,26.0
557,Candela X in There is a Star Around the Gape!,5078,BlowJob,972.0,984.0,12.0
557,Candela X in There is a Star Around the Gape!,5081,Gangbang,976.0,1018.0,42.0
557,Candela X in There is a Star Around the Gape!,5082,Gangbang,1508.0,1536.0,28.0
557,Candela X in There is a Star Around the Gape!,5083,Gangbang,1702.0,1712.0,10.0
557,Candela X in There is a Star Around the Gape!,5084,Gangbang,1754.0,1784.0,30.0
557,Candela X in There is a Star Around the Gape!,5086,Cumshot,2642.0,2654.0,12.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4811,BlowJob,562.0,588.0,26.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4813,BlowJob,702.0,728.0,26.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4815,BlowJob,1836.0,1850.0,14.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4817,BlowJob,2468.0,2480.0,12.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4830,69,2600.0,2612.0,12.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4826,Anal,2660.0,2672.0,12.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4827,Anal,2728.0,2772.0,44.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4818,BlowJob,2830.0,2870.0,40.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4828,Anal,2922.0,2940.0,18.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4819,BlowJob,2946.0,2958.0,12.0
574,"Cherry Kiss, Lana Roy, Polly Pons in DP Poolside Gangbang",4829,Anal,2978.0,3006.0,28.0
575,Chessie Kay in Double Team During Match,4866,BlowJob,88.0,134.0,46.0
575,Chessie Kay in Double Team During Match,4867,BlowJob,242.0,250.0,8.0
575,Chessie Kay in Double Team During Match,4863,BlowJob,384.0,394.0,10.0
575,Chessie Kay in Double Team During Match,4864,BlowJob,526.0,572.0,46.0
575,Chessie Kay in Double Team During Match,4868,BlowJob,556.0,570.0,14.0
575,Chessie Kay in Double Team During Match,4870,Grabbing Boobs,680.0,706.0,26.0
575,Chessie Kay in Double Team During Match,4872,Anal,740.0,794.0,54.0
575,Chessie Kay in Double Team During Match,4869,BlowJob,1008.0,1026.0,18.0
575,Chessie Kay in Double Team During Match,4871,Grabbing Boobs,1500.0,1528.0,28.0
575,Chessie Kay in Double Team During Match,4873,Cumshot,1762.0,1780.0,18.0
575,Chessie Kay in Double Team During Match,4874,Cumshot,1852.0,1880.0,28.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5087,Grabbing Boobs,200.0,244.0,44.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5089,BlowJob,302.0,308.0,6.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5090,BlowJob,354.0,370.0,16.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5091,BlowJob,428.0,470.0,42.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5092,BlowJob,522.0,556.0,34.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5093,Titjob,622.0,644.0,22.0
608,Elena Guzman in She Sucked Like a Vacuum Cleaner,5094,Cumshot,1126.0,1136.0,10.0
726,Jolee Love in Rough Squirting and Pee Play,5098,BlowJob,230.0,246.0,16.0
726,Jolee Love in Rough Squirting and Pee Play,5099,BlowJob,282.0,332.0,50.0
726,Jolee Love in Rough Squirting and Pee Play,5106,Pissing,368.0,392.0,24.0
726,Jolee Love in Rough Squirting and Pee Play,5108,Cumshot,508.0,536.0,28.0
726,Jolee Love in Rough Squirting and Pee Play,5100,BlowJob,522.0,550.0,28.0
726,Jolee Love in Rough Squirting and Pee Play,5101,BlowJob,1286.0,1304.0,18.0
726,Jolee Love in Rough Squirting and Pee Play,5102,BlowJob,1408.0,1436.0,28.0
726,Jolee Love in Rough Squirting and Pee Play,5095,Gangbang,1754.0,1794.0,40.0
726,Jolee Love in Rough Squirting and Pee Play,5103,BlowJob,1830.0,1870.0,40.0
726,Jolee Love in Rough Squirting and Pee Play,5109,Cumshot,2014.0,2022.0,8.0
726,Jolee Love in Rough Squirting and Pee Play,5096,Gangbang,2200.0,2234.0,34.0
726,Jolee Love in Rough Squirting and Pee Play,5097,Gangbang,2634.0,2654.0,20.0
726,Jolee Love in Rough Squirting and Pee Play,5116,Grabbing Boobs,2740.0,2770.0,30.0
726,Jolee Love in Rough Squirting and Pee Play,5104,BlowJob,3020.0,3032.0,12.0
726,Jolee Love in Rough Squirting and Pee Play,5105,BlowJob,3134.0,3164.0,30.0
726,Jolee Love in Rough Squirting and Pee Play,5115,Anal,3318.0,3344.0,26.0
726,Jolee Love in Rough Squirting and Pee Play,5110,Cumshot,3414.0,3448.0,34.0
726,Jolee Love in Rough Squirting and Pee Play,5107,Pissing,3516.0,3534.0,18.0
726,Jolee Love in Rough Squirting and Pee Play,5111,Cumshot,3518.0,3542.0,24.0
726,Jolee Love in Rough Squirting and Pee Play,5117,Grabbing Boobs,3550.0,3570.0,20.0
726,Jolee Love in Rough Squirting and Pee Play,5112,Cumshot,3640.0,3644.0,4.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4832,Grabbing Boobs,210.0,268.0,58.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4833,Grabbing Boobs,334.0,374.0,40.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4835,Gangbang,378.0,404.0,26.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4836,Gangbang,498.0,520.0,22.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4840,BlowJob,652.0,682.0,30.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4841,BlowJob,748.0,758.0,10.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4842,BlowJob,826.0,860.0,34.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4834,Grabbing Boobs,1042.0,1098.0,56.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4843,BlowJob,1046.0,1058.0,12.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4837,Gangbang,1158.0,1170.0,12.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4851,Anal,1344.0,1386.0,42.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4844,BlowJob,1540.0,1574.0,34.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4853,Anal,1674.0,1726.0,52.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4854,Anal,1782.0,1808.0,26.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4845,BlowJob,1818.0,1866.0,48.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4838,Gangbang,2062.0,2078.0,16.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4856,Anal,2116.0,2128.0,12.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4857,Anal,2166.0,2184.0,18.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4848,BlowJob,2692.0,2708.0,16.0
729,"Jolee Love in Waka Waka, Blacks are Coming!",4849,BlowJob,3108.0,3126.0,18.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5029,BlowJob,250.0,262.0,12.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5031,BlowJob,720.0,772.0,52.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5032,BlowJob,872.0,912.0,40.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5033,BlowJob,946.0,958.0,12.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5034,BlowJob,1060.0,1076.0,16.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5036,Anal,1084.0,1108.0,24.0
732,"Jolee Love, Nicole Black in Atm #2 Orgy",5039,Cumshot,1398.0,1426.0,28.0
815,May Thai in Asian Beauty's Intense DP Encounter,4996,Grabbing Boobs,526.0,546.0,20.0
815,May Thai in Asian Beauty's Intense DP Encounter,4997,Grabbing Boobs,582.0,630.0,48.0
815,May Thai in Asian Beauty's Intense DP Encounter,4999,Grabbing Boobs,908.0,960.0,52.0
815,May Thai in Asian Beauty's Intense DP Encounter,5001,BlowJob,1246.0,1260.0,14.0
815,May Thai in Asian Beauty's Intense DP Encounter,5002,BlowJob,1398.0,1408.0,10.0
815,May Thai in Asian Beauty's Intense DP Encounter,5003,BlowJob,1458.0,1500.0,42.0
815,May Thai in Asian Beauty's Intense DP Encounter,5005,Anal,1530.0,1544.0,14.0
815,May Thai in Asian Beauty's Intense DP Encounter,5004,BlowJob,1600.0,1616.0,16.0
815,May Thai in Asian Beauty's Intense DP Encounter,5000,Grabbing Boobs,1848.0,1872.0,24.0
815,May Thai in Asian Beauty's Intense DP Encounter,5006,Anal,1988.0,2020.0,32.0
815,May Thai in Asian Beauty's Intense DP Encounter,5007,Anal,2164.0,2188.0,24.0
815,May Thai in Asian Beauty's Intense DP Encounter,5009,Cumshot,2536.0,2576.0,40.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5151,BlowJob,86.0,116.0,30.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5158,Gangbang,140.0,152.0,12.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5152,BlowJob,158.0,180.0,22.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5166,Anal,274.0,332.0,58.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5159,Gangbang,278.0,336.0,58.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5167,Anal,424.0,438.0,14.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5160,Gangbang,532.0,558.0,26.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5162,Gangbang,926.0,948.0,22.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5170,Anal,942.0,990.0,48.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5153,BlowJob,1004.0,1032.0,28.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5171,Anal,1172.0,1204.0,32.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5154,BlowJob,1184.0,1220.0,36.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5155,BlowJob,1284.0,1298.0,14.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5172,Anal,1286.0,1326.0,40.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5173,Anal,1414.0,1434.0,20.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5174,Anal,1466.0,1484.0,18.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5163,Gangbang,1758.0,1802.0,44.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5164,Gangbang,1844.0,1880.0,36.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5177,Anal,1974.0,2028.0,54.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5178,Anal,2100.0,2116.0,16.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5179,Anal,2190.0,2248.0,58.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5156,BlowJob,2914.0,2934.0,20.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5181,Cumshot,2958.0,2996.0,38.0
857,Nathaly Cherie in 7on1 DAP Gangbang,5157,BlowJob,2966.0,2992.0,26.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4773,Cumshot,272.0,288.0,16.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4774,Cumshot,336.0,342.0,6.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4762,BlowJob,1010.0,1034.0,24.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4763,BlowJob,1104.0,1114.0,10.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4768,BlowJob,1328.0,1342.0,14.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4775,Cumshot,1374.0,1398.0,24.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4777,Cumshot,1440.0,1482.0,42.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4769,BlowJob,1982.0,1988.0,6.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4770,BlowJob,2194.0,2208.0,14.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4771,BlowJob,2282.0,2320.0,38.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4765,BlowJob,2524.0,2538.0,14.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4766,BlowJob,2596.0,2608.0,12.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4772,BlowJob,2622.0,2656.0,34.0
873,"Nicky Sweetheart, Tekohas in Tekohas and Nicki Gangbanged",4779,Cumshot,2626.0,2644.0,18.0
931,"Sarai Minx, Savanah Storm in Messy Manicure Threesome",4785,BlowJob,318.0,338.0,20.0
931,"Sarai Minx, Savanah Storm in Messy Manicure Threesome",4786,BlowJob,424.0,434.0,10.0
931,"Sarai Minx, Savanah Storm in Messy Manicure Threesome",4787,BlowJob,576.0,602.0,26.0
931,"Sarai Minx, Savanah Storm in Messy Manicure Threesome",4789,Grabbing Boobs,1328.0,1350.0,22.0
946,Shalina Devine in Housewife's Dirty Double Plan,4965,Grabbing Boobs,408.0,430.0,22.0
946,Shalina Devine in Housewife's Dirty Double Plan,4966,Grabbing Boobs,464.0,494.0,30.0
946,Shalina Devine in Housewife's Dirty Double Plan,4968,BlowJob,1608.0,1628.0,20.0
946,Shalina Devine in Housewife's Dirty Double Plan,4969,BlowJob,1746.0,1798.0,52.0
946,Shalina Devine in Housewife's Dirty Double Plan,4970,BlowJob,2076.0,2086.0,10.0
946,Shalina Devine in Housewife's Dirty Double Plan,4973,BlowJob,2258.0,2290.0,32.0
946,Shalina Devine in Housewife's Dirty Double Plan,4967,Grabbing Boobs,2528.0,2556.0,28.0
946,Shalina Devine in Housewife's Dirty Double Plan,4972,BlowJob,2650.0,2694.0,44.0
946,Shalina Devine in Housewife's Dirty Double Plan,4974,BlowJob,2870.0,2876.0,6.0
946,Shalina Devine in Housewife's Dirty Double Plan,4976,Cumshot,2886.0,2892.0,6.0
946,Shalina Devine in Housewife's Dirty Double Plan,4977,Cumshot,2926.0,2946.0,20.0
1008,"Sai Tai Tiger, Salma De Nora in Die Haremsw√§chterin des √ñl Scheichs Sc4",96,BlowJob,240.0,252.0,12.0
1020,,4792,Anal,536.0,546.0,10.0
1020,,4802,BlowJob,856.0,870.0,14.0
1020,,4803,BlowJob,910.0,924.0,14.0
1020,,4804,BlowJob,1002.0,1036.0,34.0
1020,,4793,Anal,1006.0,1044.0,38.0
1020,,4805,BlowJob,1222.0,1236.0,14.0
1020,,4795,Anal,1410.0,1434.0,24.0
1020,,4806,BlowJob,1438.0,1458.0,20.0
1020,,4808,BlowJob,2086.0,2096.0,10.0
1020,,4798,Anal,2128.0,2140.0,12.0
1020,,4799,Anal,2224.0,2236.0,12.0
1020,,4809,BlowJob,2610.0,2622.0,12.0
1023,Sai Tai Tiger in Die Haremsw√§chterin die Verbannung Sc3,67,BlowJob,432.0,450.0,18.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3190,Grabbing Boobs,6.0,54.0,48.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3191,BlowJob,142.0,170.0,28.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3192,BlowJob,406.0,424.0,18.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3193,BlowJob,864.0,874.0,10.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3194,BlowJob,1106.0,1120.0,14.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3195,BlowJob,1468.0,1504.0,36.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3199,Anal,1540.0,1574.0,34.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3200,Anal,1612.0,1660.0,48.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",4919,Gangbang,1620.0,1642.0,22.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3196,BlowJob,1632.0,1666.0,34.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3197,BlowJob,1678.0,1692.0,14.0
1025,"Ania Kinski, Anissa Kate in DP Workout Madness",3201,Cumshot,1788.0,1826.0,38.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,4952,Grabbing Boobs,108.0,160.0,52.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,732,BlowJob,178.0,202.0,24.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,740,Gangbang,232.0,260.0,28.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,4960,Gangbang,238.0,260.0,22.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,734,BlowJob,426.0,450.0,24.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,741,Gangbang,472.0,486.0,14.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,735,BlowJob,496.0,530.0,34.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,742,Gangbang,668.0,694.0,26.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,730,Grabbing Boobs,814.0,866.0,52.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,4961,Gangbang,1000.0,1022.0,22.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,737,BlowJob,1130.0,1142.0,12.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,738,BlowJob,1220.0,1254.0,34.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,739,BlowJob,1222.0,1252.0,30.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,731,Grabbing Boobs,1386.0,1408.0,22.0
1027,Kelly Oliveira in 5on1 DP for Brazilian Slut,4959,BlowJob,1402.0,1406.0,4.0
1030,Sai Tai Tiger in Die Haremsw√§chterin die Verbannung Sc2,87,BlowJob,290.0,306.0,16.0
1030,Sai Tai Tiger in Die Haremsw√§chterin die Verbannung Sc2,89,BlowJob,692.0,710.0,18.0
1030,Sai Tai Tiger in Die Haremsw√§chterin die Verbannung Sc2,90,BlowJob,858.0,918.0,60.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4979,BlowJob,620.0,654.0,34.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4987,Anal,642.0,690.0,48.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4995,Anal,1014.0,1032.0,18.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4988,Anal,1082.0,1122.0,40.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4981,BlowJob,1134.0,1146.0,12.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4989,Anal,1162.0,1196.0,34.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4990,Anal,1232.0,1270.0,38.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4982,BlowJob,1456.0,1510.0,54.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4983,BlowJob,1644.0,1704.0,60.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4992,Anal,1752.0,1768.0,16.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4993,Anal,1826.0,1844.0,18.0
1032,"Elisa Sanches, Monica Santhiago in Outdoor Threesome",4994,Anal,1976.0,1996.0,20.0
1036,Dana Dearmond in MILF Cumsluts Sc2,197,Grabbing Boobs,284.0,302.0,18.0
1036,Dana Dearmond in MILF Cumsluts Sc2,5043,BlowJob,368.0,378.0,10.0
1036,Dana Dearmond in MILF Cumsluts Sc2,210,BlowJob,654.0,670.0,16.0
1036,Dana Dearmond in MILF Cumsluts Sc2,199,Anal,698.0,710.0,12.0
1036,Dana Dearmond in MILF Cumsluts Sc2,200,Anal,750.0,810.0,60.0
1036,Dana Dearmond in MILF Cumsluts Sc2,201,Anal,842.0,854.0,12.0
1036,Dana Dearmond in MILF Cumsluts Sc2,203,Anal,1114.0,1132.0,18.0
1036,Dana Dearmond in MILF Cumsluts Sc2,204,Anal,1178.0,1198.0,20.0
1036,Dana Dearmond in MILF Cumsluts Sc2,5042,Grabbing Boobs,1192.0,1216.0,24.0
1036,Dana Dearmond in MILF Cumsluts Sc2,211,BlowJob,1422.0,1452.0,30.0
1036,Dana Dearmond in MILF Cumsluts Sc2,207,Anal,1788.0,1842.0,54.0
1036,Dana Dearmond in MILF Cumsluts Sc2,212,DP,1930.0,1954.0,24.0
1036,Dana Dearmond in MILF Cumsluts Sc2,208,Anal,2002.0,2052.0,50.0
1038,Alina Li in Asian Fuck Faces 3 Sc6,5073,Cumshot,1158.0,1204.0,46.0
1043,"Anni Star, CJ Miles in Glamorous Double Penetration",5136,Grabbing Boobs,254.0,276.0,22.0
1043,"Anni Star, CJ Miles in Glamorous Double Penetration",5201,BlowJob,1248.0,1270.0,22.0
1044,"Ania Kinski, Anissa Kate in Real Estate Gets Real Dirty",3237,BlowJob,836.0,854.0,18.0
1044,"Ania Kinski, Anissa Kate in Real Estate Gets Real Dirty",3238,BlowJob,1284.0,1308.0,24.0
1044,"Ania Kinski, Anissa Kate in Real Estate Gets Real Dirty",3239,BlowJob,1600.0,1606.0,6.0
1047,Ania Kinski in Home Alone Double Penetration,3143,Grabbing Boobs,420.0,440.0,20.0
1047,Ania Kinski in Home Alone Double Penetration,3145,BlowJob,972.0,994.0,22.0
1047,Ania Kinski in Home Alone Double Penetration,3153,Anal,1158.0,1188.0,30.0
1047,Ania Kinski in Home Alone Double Penetration,3146,BlowJob,1202.0,1238.0,36.0
1047,Ania Kinski in Home Alone Double Penetration,3147,BlowJob,1298.0,1314.0,16.0
1047,Ania Kinski in Home Alone Double Penetration,5188,BlowJob,1500.0,1516.0,16.0
1047,Ania Kinski in Home Alone Double Penetration,3154,Anal,1534.0,1548.0,14.0
1047,Ania Kinski in Home Alone Double Penetration,3148,BlowJob,1552.0,1568.0,16.0
1047,Ania Kinski in Home Alone Double Penetration,3149,BlowJob,1604.0,1624.0,20.0
1047,Ania Kinski in Home Alone Double Penetration,3150,BlowJob,1662.0,1710.0,48.0
1047,Ania Kinski in Home Alone Double Penetration,3151,BlowJob,1764.0,1794.0,30.0
1047,Ania Kinski in Home Alone Double Penetration,5196,Anal,1828.0,1844.0,16.0
1047,Ania Kinski in Home Alone Double Penetration,3156,DP,2168.0,2176.0,8.0
1047,Ania Kinski in Home Alone Double Penetration,3144,Grabbing Boobs,2520.0,2572.0,52.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3379,Grabbing Boobs,210.0,264.0,54.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3380,BlowJob,532.0,572.0,40.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3383,BlowJob,930.0,966.0,36.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3384,BlowJob,1062.0,1100.0,38.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3386,Anal,1266.0,1286.0,20.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3387,Anal,1434.0,1448.0,14.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3385,BlowJob,1452.0,1466.0,14.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3388,Anal,1642.0,1670.0,28.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3389,Anal,1814.0,1824.0,10.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3390,Anal,1872.0,1912.0,40.0
1053,"Anissa Kate, Lia Lin in Kinky Babetreats her Girlfriend to a DP Gangbang",3391,Cumshot,1986.0,2024.0,38.0
1060,Nia Nacci in MVP,108,Grabbing Boobs,26.0,62.0,36.0
1060,Nia Nacci in MVP,109,Grabbing Boobs,152.0,210.0,58.0
1062,Six Bodies In Motion,5395,BlowJob,196.0,230.0,34.0
1062,Six Bodies In Motion,5396,BlowJob,296.0,334.0,38.0
1062,Six Bodies In Motion,5397,BlowJob,408.0,424.0,16.0
1062,Six Bodies In Motion,5398,BlowJob,480.0,496.0,16.0
1062,Six Bodies In Motion,5399,BlowJob,538.0,554.0,16.0
1062,Six Bodies In Motion,5400,BlowJob,1100.0,1128.0,28.0
1062,Six Bodies In Motion,5403,Gangbang,2396.0,2420.0,24.0
1062,Six Bodies In Motion,5402,BlowJob,2810.0,2830.0,20.0
1063,,5406,BlowJob,238.0,278.0,40.0
1063,,5407,BlowJob,332.0,380.0,48.0
1063,,5410,Anal,532.0,586.0,54.0
1063,,5411,Anal,678.0,736.0,58.0
1063,,5405,Grabbing Boobs,798.0,854.0,56.0
1063,,5408,BlowJob,1042.0,1072.0,30.0
1063,,5412,Anal,1150.0,1170.0,20.0
1063,,5413,Anal,1462.0,1484.0,22.0
1065,,5414,Grabbing Boobs,38.0,58.0,20.0
1065,,5415,Grabbing Boobs,274.0,316.0,42.0
1065,,5422,Cumshot,1008.0,1018.0,10.0
1065,,5419,BlowJob,1242.0,1276.0,34.0
1065,,5426,DP,1816.0,1834.0,18.0
1065,,5420,BlowJob,2034.0,2044.0,10.0
1065,,5421,BlowJob,2094.0,2112.0,18.0
1065,,5417,Anal,2430.0,2476.0,46.0
1068,,5427,BlowJob,122.0,158.0,36.0
1068,,5428,BlowJob,332.0,382.0,50.0
1068,,5430,Anal,346.0,364.0,18.0
1068,,5431,Gangbang,388.0,406.0,18.0
1068,,5432,Cumshot,670.0,692.0,22.0
1068,,5437,Cumshot,806.0,856.0,50.0
1068,,5429,BlowJob,856.0,872.0,16.0
1068,,5438,Cumshot,868.0,900.0,32.0
1068,,5439,Cumshot,950.0,1000.0,50.0


============================================================
[83/83] utils\url_cleaner.py
------------------------------------------------------------
import re
import requests
from urllib.parse import urlparse

def normalize_url(url):
    url = url.strip()
    url = re.sub(r'^https?://(www\.)?', '', url.lower())
    return url

def is_valid_url(url):
    if not url.startswith('http'):
        return False
    if 'wikipedia.org' in url and not url.startswith('https://en.wikipedia.org'):
        return False
    blacklist = ['google.com', 'bing.com']
    for b in blacklist:
        if b in url:
            return False
    return True

def check_link_health(url, timeout=2):
    try:
        resp = requests.head(url, timeout=timeout, allow_redirects=True, headers={"User-Agent": "Mozilla/5.0"})
        return resp.status_code < 400
    except Exception:
        return False

def categorize_links(urls, skip_health_check_urls=None):
    if skip_health_check_urls is None:
        skip_health_check_urls = set()
    seen = set()
    valid_final = []
    rejected = []
    for url in urls:
        if not url.strip():
            continue
        norm = normalize_url(url)
        if norm in seen:
            continue
        seen.add(norm)
        if not is_valid_url(url):
            rejected.append(url)
            continue
        if url in skip_health_check_urls:
            valid_final.append(url)
            continue
        if not check_link_health(url):
            rejected.append(url)
            continue
        valid_final.append(url)
    # Tri par priorit√©
    site_domains = ['iafd.com', 'babepedia.com', 'wikidata.org', 'boobpedia.com',
                   'imdb.com', 'themoviedb.org', 'freeones.com', 'thenude.com']
    social_domains = ['twitter.com', 'x.com', 'instagram.com', 'onlyfans.com',
                     'facebook.com', 'tiktok.com', 'twitch.tv', 'youtube.com']
    def domain_priority(url):
        d = urlparse(url).netloc
        for i, dom in enumerate(site_domains):
            if dom in d:
                return i
        if any(s in d for s in social_domains):
            return 100 + sorted(social_domains).index([s for s in social_domains if s in d][0])
        return 99
    valid_final = sorted(valid_final, key=domain_priority)
    return valid_final, rejected


============================================================
[101/124] read_docx.py
------------------------------------------------------------
from docx import Document
path = r'F:\Nouveau dossier\StashMaster_Corrections_Prompts.docx'
try:
    doc = Document(path)
    for para in doc.paragraphs:
        print(para.text)
except Exception as e:
    print('ERROR', e)


============================================================
[102/124] README.md
------------------------------------------------------------
# StashMaster V2 - Interface Unifi√©e

Application Python/Tkinter pour la gestion et le scraping de m√©tadonn√©es de performers, avec g√©n√©ration automatique de biographies.

## üéØ Caract√©ristiques Principales

### Interface Unifi√©e
- **Fusion Phase 1 & Phase 2** : Une seule GUI pour toutes les op√©rations
- **Organisation par onglets** : M√©tadonn√©es, Champs Avanc√©s, Bio
- **Workflow intuitif** : Scraping ‚Üí Validation ‚Üí G√©n√©ration Bio

### Syst√®me de Tags Intelligent
- ‚úÖ **G√©n√©ration automatique** bas√©e sur des r√®gles m√©tadonn√©es
- ‚úÖ **PAS de scraping de tags** depuis les sources
- ‚úÖ **R√®gles intelligentes** : ethnicit√©, couleur de cheveux, mesures, piercings, tattoos, √¢ge

### Champs Optimis√©s
- **Champs simple ligne** : Nom, Aliases, Dates, Pays, etc.
- **Champs multilignes** :
  - üìù Piercings
  - üìù Tattoos
  - üîó URLs (une par ligne)

### Trivia & Awards
- **Fen√™tre d√©di√©e** avec requ√™te et r√©sultats s√©par√©s
- **Scraping cibl√©** depuis IAFD et autres sources
- **Nettoyage automatique** : 1 award par ligne
- **Format structur√©** : Ann√©e ‚Üí C√©r√©monie ‚Üí Awards

### G√©n√©ration de Bio Automatique
- **Bio Google** : 3000 caract√®res, format professionnel (bas√© sur mod√®le)
- **Bio Ollama** : G√©n√©ration IA locale optionnelle
- **Prompt personnalis√©** : Directives pr√©cises pour l'IA
- **Choix flexible** : Cases √† cocher pour type de bio

## üìã Installation

### Pr√©requis
```bash
# Python 3.8 ou sup√©rieur
python --version

# Tkinter (normalement inclus avec Python)
# Sur Ubuntu/Debian si besoin :
sudo apt-get install python3-tk
```

### Installation des d√©pendances
```bash
# Installer les packages Python requis
pip install -r requirements.txt
```

### Installation d'Ollama (optionnel)
Si vous voulez utiliser la g√©n√©ration de bio avec IA locale :

```bash
# T√©l√©charger et installer Ollama depuis https://ollama.ai
# Puis t√©l√©charger un mod√®le
ollama pull llama2
```

## üöÄ Utilisation

### Lancement
```bash
python stashmaster_unified.py
```

### Workflow Complet

#### 1. Saisie des URLs
- Ouvrir l'onglet **"Champs Avanc√©s"**
- Coller les URLs des sources (une par ligne) :
  ```
  https://www.iafd.com/person.rme/perfid=...
  https://www.freeones.xxx/...
  https://www.babepedia.com/...
  ```

#### 2. Scraping
- Menu **"Actions" ‚Üí "Scraper & Lancer le flux Bio IA"**
- L'application scrape automatiquement toutes les URLs
- Affiche les r√©sultats avec :
  - ‚úÖ Donn√©es confirm√©es (m√™me valeur de plusieurs sources)
  - üÜï Nouvelles donn√©es (une seule source)
  - ‚ö†Ô∏è Conflits (valeurs diff√©rentes entre sources)

#### 3. Validation des M√©tadonn√©es
- V√©rifier et compl√©ter les champs dans l'onglet **"M√©tadonn√©es"**
- Les valeurs confirm√©es sont pr√©-remplies
- R√©soudre les conflits manuellement si n√©cessaire

#### 4. G√©n√©ration des Tags
- Onglet **"Champs Avanc√©s"**
- Cliquer sur **"üîÑ G√©n√©rer Tags"**
- Les tags sont cr√©√©s automatiquement selon les r√®gles :
  - Ethnicit√© ‚Üí Caucasian, Latina, Asian, Ebony
  - Cheveux ‚Üí Blonde, Brunette, Redhead, Black Hair
  - Mesures ‚Üí Big Boobs, Small Boobs
  - Piercings ‚Üí Pierced
  - Tattoos ‚Üí Tattooed
  - Carri√®re ‚Üí MILF (si > 10 ans)

#### 5. Trivia & Awards
- Menu **"Actions" ‚Üí "Trivia & Awards..."**
- Fen√™tre d√©di√©e s'ouvre avec deux sections :
  
  **Trivia**
  - Cliquer **"Scraper Trivia"**
  - Les anecdotes sont r√©cup√©r√©es et affich√©es
  
  **Awards**
  - Cliquer **"Scraper Awards"**
  - Tous les prix/nominations sont list√©s
  - Cliquer **"Nettoyer Awards"** pour formater (1 par ligne)
  
- **"Appliquer et continuer"** pour sauvegarder

#### 6. G√©n√©ration de Bio
- Menu **"Actions" ‚Üí "G√©n√©rer Bio..."** ou onglet **"Bio"**
- Fen√™tre de g√©n√©ration s'ouvre avec 3 options :

  **Option 1 : Bio Google (recommand√©)**
  - ‚úÖ G√©n√©ration automatique instantan√©e
  - ‚úÖ Format professionnel de 3000 caract√®res
  - ‚úÖ Structure avec sections : Introduction, Origines, Carri√®re, Vie Personnelle, Apparence, Prix
  - ‚úÖ Bas√© sur le mod√®le BioGooglemodele.txt
  
  **Option 2 : Bio Ollama**
  - G√©n√©ration avec IA locale (Ollama doit √™tre install√©)
  - Prompt par d√©faut optimis√©
  
  **Option 3 : Bio Ollama avec prompt personnalis√©**
  - √âcrire vos directives pr√©cises dans le champ
  - Contr√¥le total sur le style et le contenu
  
- Cliquer **"G√©n√©rer la Bio"**
- V√©rifier le compteur de caract√®res
- **"Appliquer"** pour ins√©rer dans l'onglet Bio

#### 7. Sauvegarde
- Bouton **"üíæ Sauvegarder"** en bas √† droite
- Toutes les donn√©es sont sauvegard√©es

## üìä Architecture

### Structure des Fichiers
```
stashmaster_unified/
‚îÇ
‚îú‚îÄ‚îÄ stashmaster_unified.py    # Application principale
‚îú‚îÄ‚îÄ scrapers.py                # Modules de scraping
‚îú‚îÄ‚îÄ requirements.txt           # D√©pendances Python
‚îú‚îÄ‚îÄ README.md                  # Ce fichier
‚îÇ
‚îî‚îÄ‚îÄ data/                      # Donn√©es sauvegard√©es (√† cr√©er)
    ‚îú‚îÄ‚îÄ performers/            # JSON des performers
    ‚îî‚îÄ‚îÄ database.sqlite        # Base de donn√©es (futur)
```

### Composants Principaux

#### `MainWindow`
Interface principale unifi√©e avec 3 onglets :
- üìã M√©tadonn√©es : Champs de base
- ‚öôÔ∏è Champs Avanc√©s : Tags, Piercings, Tattoos, URLs
  - nouvelles actions disponibles :
    - üßπ **Nettoyer URLs** (enleve vides/duplications)
    - üîó **Valider URLs** (verifie les liens et colore le texte)
    - l'analyse/validation est √©galement lanc√©e automatiquement lors du
      chargement d'un performer ou d√®s qu'on modifie les URLs
- üìù Bio : Biographie finale

#### `TriviaAwardsWindow`
Fen√™tre d√©di√©e pour :
- Scraping et affichage des trivia
- Scraping et nettoyage des awards
- Format structur√© : 1 award par ligne

#### `BioGenerationWindow`
Fen√™tre de g√©n√©ration avec :
- Choix du type de bio (Google/Ollama)
- Champ pour prompt personnalis√©
- Pr√©visualisation et compteur de caract√®res

#### `TagRulesEngine`
Moteur de r√®gles pour g√©n√©rer les tags automatiquement selon :
- Les m√©tadonn√©es collect√©es (ethnicit√©, cheveux, mesures)
- Les attributs physiques (piercings, tattoos)
- L'√¢ge de carri√®re

#### `AwardsCleaner`
Nettoyeur d'awards pour :
- Formater les awards (1 par ligne)
- Organiser par ann√©e et c√©r√©monie
- Distinguer Winner vs Nominee

#### `BioGenerator`
G√©n√©rateur de biographies avec 2 modes :
- **Google Bio** : Template de 3000 caract√®res
- **Ollama Bio** : IA locale avec prompt personnalis√©

#### `ScraperOrchestrator`
Orchestre le scraping de plusieurs sources :
- IAFD
- Freeones
- Babepedia
- TheNude

#### `DataMerger`
Fusionne intelligemment les donn√©es de plusieurs sources :
- D√©tecte les valeurs confirm√©es (consensus)
- Identifie les nouvelles donn√©es (source unique)
- Signale les conflits (valeurs diff√©rentes)

## üé® R√®gles de Tags

Les tags sont g√©n√©r√©s automatiquement selon ces r√®gles :

### Ethnicit√©
| M√©tadonn√©e | Tag G√©n√©r√© |
|------------|------------|
| Caucasian  | Caucasian  |
| Cuban, Latin, Latina | Latina |
| Asian | Asian |
| Ebony, African | Ebony |

### Couleur de Cheveux
| M√©tadonn√©e | Tag G√©n√©r√© |
|------------|------------|
| Blonde, Blond | Blonde |
| Brown, Brunette | Brunette |
| Red, Auburn | Redhead |
| Black | Black Hair |

### Mesures
| Condition | Tag G√©n√©r√© |
|-----------|------------|
| Tour de poitrine ‚â• 36" | Big Boobs |
| Tour de poitrine ‚â§ 32" | Small Boobs |

### Attributs
| M√©tadonn√©e | Tag G√©n√©r√© |
|------------|------------|
| Piercings (non vide) | Pierced |
| Tattoos (non vide) | Tattooed |
| Carri√®re > 10 ans | MILF |

## üìù Format de Bio Google

La bio g√©n√©r√©e suit ce template de 3000 caract√®res :

```markdown
### [Nom] : L'√©toile charismatique au parcours diversifi√©

**Introduction**
Contexte, d√©but de carri√®re, pseudonymes...

**üìÖ Origines et Premiers Pas**
Lieu de naissance, origines, d√©but de carri√®re...

**üèÜ Carri√®re et Filmographie**
√âvolution, studios, performances, apog√©e...

**üí° Faits Int√©ressants & Vie Personnelle**
Personnalit√©, trivia, vie priv√©e...

**üëó Apparence et Style**
Description physique, mesures, tatouages, piercings...

**üèÜ Prix et Distinctions**
Awards, nominations, reconnaissance...

**Conclusion rapide**
R√©sum√©, impact, h√©ritage...
```

## üîß Configuration Avanc√©e

### Personnaliser les R√®gles de Tags
Modifier la classe `TagRulesEngine` dans `stashmaster_unified.py` :

```python
@staticmethod
def generate_tags(metadata: Dict) -> List[str]:
    tags = []
    
    # Ajouter vos r√®gles personnalis√©es ici
    if condition:
        tags.append('YourTag')
    
    return list(set(tags))
```

### Ajouter un Nouveau Scraper
Cr√©er une nouvelle classe dans `scrapers.py` :

```python
class NewSourceScraper(ScraperBase):
    def scrape_performer(self, url: str) -> Dict:
        # Votre code de scraping
        return data
```

Puis l'enregistrer dans `ScraperOrchestrator` :

```python
self.scrapers['newsource'] = NewSourceScraper()
```

### Personnaliser le Template de Bio
Modifier la m√©thode `generate_google_bio` dans la classe `BioGenerator`.

## ‚ùì FAQ

### Les tags ne se g√©n√®rent pas automatiquement ?
‚Üí V√©rifiez que vous avez bien rempli les champs de base (ethnicit√©, cheveux, mesures) et cliquez sur "üîÑ G√©n√©rer Tags"

### Ollama ne fonctionne pas ?
‚Üí V√©rifiez qu'Ollama est install√© et en cours d'ex√©cution :
```bash
ollama serve
```

### Les awards ne sont pas nettoy√©s correctement ?
‚Üí Utilisez le bouton "Nettoyer Awards" apr√®s le scraping pour formater automatiquement

### Comment r√©soudre les conflits de donn√©es ?
‚Üí Les conflits sont affich√©s lors du scraping. Choisissez manuellement la valeur correcte ou conservez celle de la source la plus fiable (g√©n√©ralement IAFD)

### La bio est trop longue/courte ?
‚Üí Bio Google : ~3000 caract√®res (fixe)
‚Üí Bio Ollama : Ajustez dans le prompt personnalis√© : "√âcris une bio de [X] caract√®res..."

## üìÑ Licence

Ce projet est fourni tel quel pour usage personnel.

## ü§ù Contribution

Pour toute am√©lioration ou correction :
1. Cr√©er une branche pour votre fonctionnalit√©
2. Commiter vos changements
3. Cr√©er une Pull Request

## üìÆ Support

Pour toute question ou probl√®me, cr√©er une issue sur le repository.

---

**Version** : 2.0  
**Date** : F√©vrier 2026  
**Statut** : Production


============================================================
[103/124] requirements.txt
------------------------------------------------------------
requests==2.31.0
beautifulsoup4==4.12.3
lxml==5.1.0


============================================================
[104/124] services\__init__.py
------------------------------------------------------------
# StashMaster V2 Services Package


============================================================
[105/124] services\bio_generator.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BioGenerator - Service de g√©n√©ration de biographies
"""

import os
import re
import json
import urllib.request
import requests
from typing import Dict, List, Optional

GEMINI_MODEL   = "gemini-2.0-flash"
GEMINI_API_URL="***MASKED***"
                  "/{model}:generateContent?key={key}")

SYSTEM_PROMPT_BIO = """Tu es un r√©dacteur expert pour une base de donn√©es de films pour adultes.
Ton objectif est de r√©diger une biographie structur√©e et professionnelle en FRAN√áAIS (Qu√©bec) pour l'artiste, bas√©e sur les faits fournis ET sur tes connaissances personnelles sur cet artiste.

STRUCTURE OBLIGATOIRE (7 sections, dans cet ordre) :

### [Nom] : [Sous-titre accrocheur]

**Introduction** ‚Äî 2-3 phrases : identit√© compl√®te, date et lieu de naissance, ann√©e d√©but de carri√®re, pseudonymes principaux.

**üìÖ Origines et Premiers Pas** ‚Äî 3-4 phrases : origines culturelles, vie avant l'industrie, entr√©e dans l'industrie, ambition.

**üèÜ Carri√®re et Filmographie** ‚Äî 4-5 phrases : studios partenaires, diversit√© des r√¥les, √©volution, apog√©e, constance qualitative. Enrichis avec des faits r√©els si tu les connais.

**üí° Faits Marquants & Personnalit√©** ‚Äî 3-4 phrases : personnalit√©, vie priv√©e, loisirs, anecdotes notables. Utilise les trivia fournis.

**üëó Apparence et Style** ‚Äî 3-4 phrases : description physique compl√®te en prose (cheveux, mensurations, origines, tatouages/piercings), style sc√©nique.

**üèÜ Prix et Distinctions** ‚Äî 3-4 phrases : c√©r√©monies et victoires sp√©cifiques int√©gr√©es en prose, jamais en liste.

**Conclusion rapide** ‚Äî 2 phrases : bilan, h√©ritage, avenir.

R√àGLES ABSOLUES :
- Z√âRO liste √† puces ‚Äî uniquement paragraphes en prose fluide
- Mesures/taille/poids int√©gr√©s naturellement dans la prose d'Apparence
- Prix int√©gr√©s en phrase, JAMAIS sous forme ann√©e-cat√©gorie
- Fran√ßais professionnel et soutenu, avec une touche qu√©b√©coise
- Utiliser ABSOLUTEMENT toutes les donn√©es fournies
- Tu peux enrichir avec tes propres connaissances sur l'artiste (studios r√©els, prix connus, etc.)
- Ne pas mentionner que tu es une IA
- Longueur cible : 2800 √† 3500 caract√®res
"""


class BioGenerator:
    """G√©n√©rateur de biographies avec Gemini (recherche web) et Ollama (local)"""

    def __init__(self, ollama_url: str = "http://localhost:11434/api/generate"):
        self.ollama_url = ollama_url
        self.gemini_key = self._load_gemini_key()
        if self.gemini_key:
            print("[BioGenerator] Cl√© Gemini charg√©e ‚Äî g√©n√©ration Google avec IA activ√©e.")
        else:
            print("[BioGenerator] Pas de cl√© Gemini ‚Äî g√©n√©ration Google en mode template.")

    def _load_gemini_key(self) -> Optional[str]:
        """Cherche .gemini_key √† la racine du projet."""
        root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
        for path in [os.path.join(root, ".gemini_key"), r"F:\Nouveau dossier\.gemini_key"]:
            if os.path.exists(path):
                try:
key="***MASKED***"
                    if key:
                        return key
                except Exception:
                    pass
        return None

    def _call_gemini(self, user_prompt: str, use_search: bool = True) -> Optional[str]:
        """Appelle Gemini 2.0 Flash, avec grounding Google Search si use_search=True."""
        if not self.gemini_key:
            return None
        url = GEMINI_API_URL.format(model=GEMINI_MODEL, key=self.gemini_key)
        payload: Dict = {
            "system_instruction": {"parts": [{"text": SYSTEM_PROMPT_BIO}]},
            "contents": [{"parts": [{"text": user_prompt}]}],
            "generationConfig": {
                "temperature": 0.75,
                "maxOutputTokens": 1500,
            },
        }
        if use_search:
            # Grounding Google Search : Gemini va chercher sur le web pour enrichir la bio
            payload["tools"] = [{"google_search": {}}]

        try:
            data = json.dumps(payload).encode("utf-8")
            req = urllib.request.Request(
                url, data=data,
                headers={"Content-Type": "application/json"}
            )
            with urllib.request.urlopen(req, timeout=30) as resp:
                result = json.loads(resp.read())
            text = result["candidates"][0]["content"]["parts"][0]["text"]
            return text.strip()
        except Exception as e:
            print(f"[GEMINI] Erreur : {e}")
            return None


    def _summarize_awards(self, awards_raw: str) -> str:
        """Convertit une liste brute de prix en une phrase de prose."""
        if not awards_raw or not awards_raw.strip():
            return ""
        lines = [l.strip() for l in awards_raw.splitlines() if l.strip()]
        ceremonies = []
        wins = []
        nom_count = 0
        for line in lines:
            # D√©tecter le nom de c√©r√©monie (ligne sans tiret ni ann√©e en t√™te)
            if re.match(r'^[A-Za-z]', line) and not re.match(r'^\d{4}', line):
                name = line.split('\n')[0].strip()
                if name and name not in ceremonies:
                    ceremonies.append(name)
            m = re.match(r'^(\d{4})\s*[-‚Äì]\s*(Winner|Nominee)\s*:\s*(.+)$', line, re.I)
            if m:
                year, status, category = m.group(1), m.group(2), m.group(3)
                # Supprimer le titre du film entre parenth√®ses dans la cat√©gorie
                category = re.sub(r'\s*\(.*?\)', '', category).strip()
                if 'winner' in status.lower():
                    wins.append(f"{category} ({year})")
                else:
                    nom_count += 1
        if not ceremonies and not wins and nom_count == 0:
            return ""
        cer_str = " et ".join(ceremonies) if ceremonies else "plusieurs c√©r√©monies de l'industrie"
        parts = []
        if wins:
            win_str = ", ".join(wins[:3])
            if len(wins) > 3:
                win_str += f" et {len(wins)-3} autre(s) troph√©e(s)"
            parts.append(f"remportant notamment {win_str}")
        if nom_count:
            parts.append(f"cumulant plus de {nom_count} nomination(s)")
        detail = ", ".join(parts)
        if detail:
            return f"Son talent a √©t√© salu√© aux {cer_str}, {detail}."
        return f"Son talent a √©t√© reconnu par de multiples distinctions aux {cer_str}."

    def _prose_appearance(self, measurements: str, height: str, weight: str,
                          hair_color: str, ethnicity: str,
                          tattoos: str, piercings: str) -> str:
        """R√©dige la section apparence sous forme de prose."""
        parts = []
        if hair_color and hair_color not in ('[couleur]', 'Non disponible'):
            parts.append(f"Sa chevelure {hair_color.lower()} encadre un visage expressif")
        if measurements and measurements not in ('[mesures]', 'Non disponible'):
            parts.append(f"sa silhouette est mise en valeur par des mensurations de {measurements}")
        if height and height not in ('[taille]', 'Non disponible'):
            h = str(height).replace('cm', '').strip()
            parts.append(f"une stature de {h} cm")
        if weight and weight not in ('[poids]', 'Non disponible'):
            w = str(weight).replace('kg', '').strip()
            parts.append(f"un poids de {w} kg")
        prose = ""
        if parts:
            prose = ". ".join(p.capitalize() for p in parts) + "."

        body_art = []
        tat = str(tattoos).strip()
        if tat and tat.lower() not in ('none', 'information non disponible', '[mesures]', ''):
            # Condenser une liste multi-lignes en une courte mention
            tat_lines = [l.strip() for l in tat.splitlines() if l.strip()]
            if len(tat_lines) > 2:
                body_art.append(f"plusieurs tatouages ornent son corps")
            elif len(tat_lines) > 0:
                body_art.append(f"elle arbore {tat_lines[0].lower()}")
        pier = str(piercings).strip()
        if pier and pier.lower() not in ('none', 'information non disponible', ''):
            body_art.append(f"des piercings {pier.lower()}")
        if body_art:
            prose += " " + " et ".join(body_art).capitalize() + "."
        return prose.strip()

    def _prose_trivia(self, trivia: str) -> str:
        """Condense une liste de faits trivia en prose fluide."""
        if not trivia or not trivia.strip():
            return ""
        lines = [l.strip().rstrip('.') for l in trivia.splitlines() if l.strip()]
        if len(lines) == 1:
            return lines[0] + "."
        selected = lines[:3]
        if len(selected) == 1:
            return selected[0] + "."
        return ". ".join(selected) + "."

    def _prose_bio_raw(self, bio_raw: str, performer_name: str) -> str:
        """Extrait 2-3 phrases pertinentes du bio_raw scrapp√© pour enrichir la section carri√®re."""
        if not bio_raw or not bio_raw.strip():
            return ""
        # Garder les phrases qui contiennent des infos de carri√®re (studios, ann√©es, prix...)
        sentences = re.split(r'(?<=[.!?])\s+', bio_raw.strip())
keywords="***MASKED***"
            r'\b(studio|brazzers|evil angel|digital|mofos|naughty|reality|\d{4}|'
            r'award|avn|xbiz|carri√®re|career|film|sc√®ne|scene|travaill|work|'
            r'collaborate|nomm|nomin|won|remport|gagn)\b', re.I)
        relevant = [s.strip() for s in sentences if keywords.search(s) and len(s) > 40]
        if not relevant:
            # fallback : prendre les 2 premi√®res phrases non vides
            relevant = [s.strip() for s in sentences if len(s.strip()) > 40][:2]
        return ' '.join(relevant[:3])
        if len(lines) == 1:
            return lines[0] + "."
        # Garder les 3 premiers faits max, les joindre en prose
        selected = lines[:3]
        if len(selected) == 1:
            return selected[0] + "."
        return ". ".join(selected) + "."

    def generate_google_bio(self, performer_name: str, metadata: Dict) -> str:
        """G√©n√®re une bio via Gemini 2.0 Flash (avec recherche web) si cl√© dispo, sinon template local."""
        # Donn√©es brutes
        birthdate    = metadata.get('birthdate')    or ''
        birthplace   = metadata.get('birthplace')   or metadata.get('country') or ''
        career_start = metadata.get('career_start') or ''
        if not career_start and metadata.get('career_length'):
            career_start = str(metadata['career_length']).split('-')[0].strip()
        aliases      = metadata.get('aliases') or []
        ethnicity    = metadata.get('ethnicity')    or ''
        height       = metadata.get('height')       or ''
        weight       = metadata.get('weight')       or ''
        measurements = metadata.get('measurements') or ''
        hair_color   = metadata.get('hair_color')   or ''
        tattoos      = metadata.get('tattoos')      or ''
        piercings    = metadata.get('piercings')    or ''
        trivia       = metadata.get('trivia')       or ''
        awards_raw   = metadata.get('awards') or metadata.get('awards_summary') or ''
        bio_raw      = metadata.get('bio_raw') or metadata.get('details', '')
        stash_bio    = metadata.get('stash_bio', '')  # Bio d√©j√† pr√©sente dans Stash
        if isinstance(aliases, str):
            aliases = [a.strip() for a in re.split(r'[,\n]', aliases) if a.strip()]
        alias_str = ', '.join(aliases) if aliases else performer_name

        # Essai Gemini (avec Google Search grounding)
        if self.gemini_key:
            lines = [
                f"R√©dige une biographie compl√®te en fran√ßais (Qu√©bec) pour : {performer_name}",
                "",
                "DONN√âES FACTUELLES DISPONIBLES :",
            ]
            if birthdate:    lines.append(f"- Date de naissance : {birthdate}")
            if birthplace:   lines.append(f"- Lieu de naissance : {birthplace}")
            if career_start: lines.append(f"- D√©but de carri√®re : {career_start}")
            cl = metadata.get('career_length')
            if cl:           lines.append(f"- Ann√©es d'activit√© : {cl}")
            if alias_str != performer_name: lines.append(f"- Pseudonymes : {alias_str}")
            if ethnicity:    lines.append(f"- Ethnicit√© : {ethnicity}")
            if hair_color:   lines.append(f"- Cheveux : {hair_color}")
            if measurements: lines.append(f"- Mensurations : {measurements}")
            if height:       lines.append(f"- Taille : {height} cm")
            if weight:       lines.append(f"- Poids : {weight} kg")
            if tattoos:      lines.append(f"- Tatouages : {tattoos}")
            if piercings:    lines.append(f"- Piercings : {piercings}")
            if trivia:       lines.append(f"\nTrivia :\n{trivia[:800]}")
            if awards_raw:   lines.append(f"\nAwards :\n{awards_raw[:1200]}")
            if bio_raw:      lines.append(f"\nBio scrap√©e :\n{bio_raw[:1000]}")
            if stash_bio and stash_bio != bio_raw:
                lines.append(f"\nBio actuelle dans Stash (√† am√©liorer/enrichir) :\n{stash_bio[:1200]}")
            lines += [
                "",
                "Tu peux enrichir avec tes connaissances r√©elles (studios, prix v√©rifiables, faits publics).",
                "Respecte la structure en 7 sections d√©finie dans le syst√®me.",
            ]
            print(f"[GEMINI] G√©n√©ration bio pour {performer_name} (avec recherche web)...")
            result = self._call_gemini("\n".join(lines), use_search=True)
            if result:
                print(f"[GEMINI] Bio g√©n√©r√©e ({len(result)} caract√®res)")
                return result
            print("[GEMINI] √âchec ‚Äî repli sur template local.")

        # Repli template local
        print(f"[BIO] G√©n√©ration template local pour {performer_name}")
        awards_prose     = self._summarize_awards(awards_raw)
        trivia_prose     = self._prose_trivia(trivia)
        appearance_prose = self._prose_appearance(
            measurements, height, weight, hair_color, ethnicity, tattoos, piercings)
        career_enrich    = self._prose_bio_raw(bio_raw or stash_bio, performer_name)
        bd  = birthdate    or '[date de naissance]'
        bp  = birthplace   or '[lieu]'
        cs  = career_start or '[ann√©e de d√©but]'
        eth = ethnicity    or '[origine]'

        intro = (
            f"N√©e le {bd} √† {bp}, {performer_name} est une personnalit√© respect√©e du monde du "
            f"divertissement adulte. D√®s son entr√©e remarqu√©e en {cs}, elle a su s'imposer par "
            f"son charisme et son √©nergie. Connue sous les noms de {alias_str}, elle a navigu√© "
            f"avec succ√®s dans une industrie comp√©titive."
        )
        origines = (
            f"Issue d'une culture {eth}, {performer_name} a pass√© ses premi√®res ann√©es dans la "
            f"r√©gion de {bp}. Son engagement d√®s {cs} t√©moigne d'une volont√© farouche de r√©ussir."
        )
        carriere = (
            "Sa carri√®re est jalonn√©e de succ√®s et de collaborations avec les leaders de l'industrie."
            + (" " + career_enrich if career_enrich else "")
        )
        faits = (
            f"En dehors des plateaux, {performer_name} cultive un univers personnel riche."
            + (" " + trivia_prose if trivia_prose else "")
        )
        apparence = (
            f"Sa beaut√© distinctive, reflet de ses origines {eth}, est l'un de ses traits les plus remarquables. "
            + appearance_prose
        )
        prix = awards_prose or "Ses efforts ont √©t√© couronn√©s par de nombreuses nominations et r√©compenses."
        conclusion = (
            f"En r√©sum√©, {performer_name} est une v√©ritable ic√¥ne de son temps. "
            "Son influence perdurera, laissant une trace ind√©l√©bile dans l'histoire du divertissement moderne."
        )

        tmpl = "\n\n".join([
            f"### {performer_name} : Une Carri√®re d'Excellence et un Parcours Inspirant",
            f"**Introduction**\n{intro}",
            f"**üìÖ Origines et Premiers Pas**\n{origines}",
            f"**üèÜ Carri√®re et Filmographie**\n{carriere}",
            f"**üí° Faits Marquants & Personnalit√©**\n{faits}",
            f"**üëó Apparence et Style**\n{apparence}",
            f"**üèÜ Prix et Distinctions**\n{prix}",
            f"**Conclusion**\n{conclusion}",
        ])
        if len(tmpl) > 3500:
            tmpl = tmpl[:3497] + "..."
        return tmpl

    def generate_ollama_bio(self, performer_name: str, metadata: Dict, custom_prompt: str = "", model: str = "dolphin-mistral:7b") -> Optional[str]:
        """G√©n√®re une bio avec Ollama en int√©grant des directives personnalis√©es"""
        try:
            # Variables pour les f-strings des prompts
            ethnicity   = metadata.get('ethnicity', 'Non disponible')
            hair_color  = metadata.get('hair_color', 'Non disponible')
            measurements= metadata.get('measurements', 'Non disponible')
            height      = metadata.get('height', 'Non disponible')
            weight      = metadata.get('weight', 'Non disponible')
            career_start= metadata.get('career_start', 'Non disponible')

            # Construction des infos de base
            aliases_str = (', '.join(metadata.get('aliases', []))
                           if isinstance(metadata.get('aliases'), list)
                           else metadata.get('aliases', ''))
            info_str = f"""
            - Nom : {performer_name}
            - Aliases / Pseudonymes : {aliases_str}
            - Date de naissance : {metadata.get('birthdate', 'Non disponible')}
            - Lieu de naissance : {metadata.get('birthplace', 'Non disponible')}
            - Ethnicit√© : {ethnicity}
            - D√©but de carri√®re : {career_start}
            - Carri√®re (ann√©es) : {metadata.get('career_length', 'Non disponible')}
            - Mensurations : {measurements}
            - Taille : {height} cm
            - Poids : {weight} kg
            - Couleur de cheveux : {hair_color}
            - Tatouages : {metadata.get('tattoos', 'Non disponible')}
            - Piercings : {metadata.get('piercings', 'Non disponible')}
            """
            
            # Ajout de contexte riche si pr√©sent
            extra_context = ""
            if metadata.get('trivia'):
                extra_context += f"\nFaits marquants (Trivia) :\n{metadata['trivia']}"
            bio_source = metadata.get('bio_raw') or metadata.get('details', '')
            if bio_source:
                extra_context += f"\nBio source scrapp√©e :\n{bio_source}"
            if metadata.get('awards'):
                extra_context += f"\nR√©compenses (brut) :\n{metadata['awards']}"

            if custom_prompt:
                prompt = f"""Tu es un r√©dacteur expert en biographies pour l'industrie du divertissement adulte.

OBJECTIF : R√©diger une biographie de 2800 √† 3200 caract√®res pour {performer_name}.
Directives personnalis√©es : {custom_prompt}

STRUCTURE OBLIGATOIRE (7 sections, dans cet ordre) :

### {performer_name} : [sous-titre accrocheur bas√© sur les donn√©es]

**Introduction** ‚Äî 2-3 phrases : identit√© compl√®te, date et lieu de naissance, ann√©e d√©but de carri√®re, pseudonymes principaux.

**üìÖ Origines et Premiers Pas** ‚Äî 3-4 phrases : origines culturelles ({ethnicity}), vie priv√©e discrois, √¢ge/contexte au d√©but de carri√®re ({career_start}), ambition.

**üèÜ Carri√®re et Filmographie** ‚Äî 4-5 phrases : studios partenaires, diversit√© des r√¥les, √©volution, apog√©e, constance qualitative.

**üí° Faits Marquants & Personnalit√©** ‚Äî 3-4 phrases : personnalit√©, approche professionnelle, myst√®re/discroistion sur la vie priv√©e, anecdotes des trivia si piscine.

**üëó Apparence et Style** ‚Äî 3-4 phrases : description physique compl√®te en prose (cheveux {hair_color}, origines {ethnicity}, {measurements}, {height}cm, {weight}kg, tatouages, piercings), style sc√©nique.

**üèÜ Prix et Distinctions** ‚Äî 3-4 phrases : c√©r√©monies et victoires sp√©cifiques int√©gr√©es en prose, jamais en liste.

**Conclusion rapide** ‚Äî 2 phrases : bilan, h√©ritage, avenir.

R√àGLES ABSOLUES :
- Z√âRO liste √† puces ‚Äî uniquement paragraphes en prose fluide
- Mesures/taille/poids int√©gr√©s naturellement dans la prose d'Apparence
- Prix int√©gr√©s en phrase, JAMAIS sous forme ann√©e-cat√©gorie
- Fran√ßais professionnel et soutenu, avec une touche qu√©b√©coise si pertinent
- Utiliser ABSOLUMENT toutes les donn√©es fournies ci-dessous
- Ne pas mentionner que tu es une IA

DONN√âES FACTUELLES :
{info_str}
{extra_context}

R√©ponds UNIQUEMENT avec le texte de la biographie, sans pr√©ambule ni commentaire."""
            else:
                prompt = f"""Tu es un r√©dacteur expert en biographies pour l'industrie du divertissement adulte.

OBJECTIF : R√©diger une biographie compl√®te de 2800 √† 3200 caract√®res pour {performer_name}.

MOD√àLE DE STRUCTURE √† suivre (7 sections) :

### {performer_name} : [sous-titre accrocheur]

**Introduction** ‚Äî N√©e le [date] √† [lieu], [nom] a marqu√© l'industrie d√®s [ann√©e]. Reconnue pour [traits], elle a rapidement acquis une notori√©t√© significative. Ses alias [liste] ont contribu√© √† forger une image polyvalente.

**üìÖ Origines et Premiers Pas** ‚Äî Origines [ethnie], vie priv√©e discroistion, entr√©e dans l'industrie en [ann√©e] √† [age] ans.

**üèÜ Carri√®re et Filmographie** ‚Äî Studios, collaborations, diversit√© des r√¥les, apog√©e, longuit√©vit√©.

**üí° Faits Marquants & Personnalit√©** ‚Äî Personnalit√© authentique, vie priv√©e, loisirs si connus, anecdotes.

**üëó Apparence et Style** ‚Äî Description physique int√©gr√©e en prose (cheveux, mensurations, style).

**üèÜ Prix et Distinctions** ‚Äî Nominations/victoires cit√©es en phrases, jamais en liste.

**Conclusion rapide** ‚Äî Bilan et h√©ritage.

R√àGLES ABSOLUES :
- Z√âRO liste √† puces ‚Äî prose fluide uniquement
- Tous les chiffres (mesures, ann√©es, taille) int√©gr√©s naturellement dans les phrases
- Fran√ßais professionnel, touche qu√©b√©coise bienveille
- Utiliser TOUTES les donn√©es fournies ci-dessous
- Ne pas mentionner l'IA

DONN√âES FACTUELLES COMPL√àTES :
{info_str}
{extra_context}

R√©ponds UNIQUEMENT avec le texte de la biographie, sans pr√©ambule."""
            
            response = requests.post(
                self.ollama_url,
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=360
            )
            
            if response.status_code == 200:
                return response.json().get('response', '')
            return None
        except requests.exceptions.ReadTimeout:
            print("[OLLAMA] Timeout d√©pass√© (360s) ‚Äî essayez un mod√®le plus l√©ger.")
            return None
        except Exception as e:
            print(f"Erreur Ollama (generation): {e}")
            return None

    def refine_bio(self, current_bio: str, custom_prompt: str, model: str = "dolphin-mistral:7b") -> Optional[str]:
        """Raffine ou fusionne une bio existante selon des directives IA"""
        try:
            prompt = f"""Tu es un √©diteur expert en biographies pour l'industrie du divertissement adulte.
Modifie le texte suivant en appliquant STRICTEMENT ces directives : {custom_prompt}

R√àGLES :
- CONSERVER la structure 7 sections
- Z√âRO liste √† puces, prose fluide uniquement
- Fran√ßais professionnel
- Ne pas mentionner l'IA

Texte actuel :
---
{current_bio}
---

Renvoie UNIQUEMENT la biographie modifi√©e, sans commentaires."""
            
            response = requests.post(
                self.ollama_url,
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=360
            )
            
            if response.status_code == 200:
                return response.json().get('response', '')
            return None
        except requests.exceptions.ReadTimeout:
            print("[OLLAMA] Timeout d√©pass√© (360s) lors du raffinement.")
            return None
        except Exception as e:
            print(f"Erreur Ollama (refinement): {e}")
            return None

    def translate_qc(self, text: str, field_name: str = "", model: str = "dolphin-mistral:7b") -> str:
        """Traduit un texte sp√©cifique en Fran√ßais/QC avec Ollama."""
        if not text or text.lower() == 'none' or len(text.strip()) < 2:
            return text
            
        try:
            prompt = f"""Traduis le texte suivant (champ '{field_name}') en Fran√ßais (style Qu√©b√©cois/QC) de mani√®re naturelle. 
            Si c'est d√©j√† en fran√ßais, am√©liore le style.
            Texte √† traduire : {text}
            Renvoie UNIQUEMENT la traduction, sans commentaires."""
            
            response = requests.post(
                self.ollama_url,
                json={"model": model, "prompt": prompt, "stream": False},
                timeout=60
            )
            if response.status_code == 200:
                result = response.json().get('response', '').strip()
                return result if result else text
        except Exception as e:
            print(f"[OLLAMA] Erreur traduction {field_name}: {e}")
        return text

    def translate_google(self, text: str, target_lang: str = "fr") -> str:
        """Traduit un texte via l'API Google Translate gratuite (gtx)."""
        if not text or text.lower() == 'none' or len(text.strip()) < 2:
            return text
            
        try:
            url = f"https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl={target_lang}&dt=t&q={text}"
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                # La structure est [[["trad", "orig", ...], ...]]
                translated = "".join([part[0] for part in data[0] if part[0]])
                return translated
        except Exception as e:
            print(f"[GOOGLE] Erreur traduction : {e}")
        return text

    def translate_hybrid(self, text: str, field_name: str = "") -> str:
        """Tente Google Translate, bascule sur Ollama si √©chec ou contenu vide."""
        # On tente Google d'abord (recommandation utilisateur pour contenu peu explicite)
        res = self.translate_google(text)
        
        # Si Google √©choue ou si le r√©sultat est suspect (trop court par rapport √† l'original)
        # ou si on veut forcer le style QC via Ollama
        if not res or res == text or len(res) < len(text) * 0.3:
            return self.translate_qc(text, field_name)
            
        return res


============================================================
[106/124] services\config_manager.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ConfigManager - Gestion de la configuration globale
"""

import json
import os
from typing import Dict, Any

class ConfigManager:
    """G√®re la lecture et l'√©criture de la configuration (config.json)"""
    
    def __init__(self, config_path: str = "config.json"):
        self.config_path = config_path
        self.config = self._load_defaults()
        self.load()

    def _load_defaults(self) -> Dict[str, Any]:
        return {
            "stash_url": "http://localhost:9999",
            "stash_api_key": "***MASKED***",
            "database_path": "H:/Stash/stash-go.sqlite",
            "ollama_url": "http://localhost:11434/api/generate",
            "performer_bio_length": 3000,
            "theme": "dark"
        }

    def load(self):
        """Charge la configuration depuis le fichier JSON"""
        if os.path.exists(self.config_path):
            try:
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    self.config.update(user_config)
            except Exception as e:
                print(f"Erreur lors du chargement de la config: {e}")

    def save(self):
        """Sauvegarde la configuration actuelle"""
        try:
            with open(self.config_path, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=4, ensure_ascii=False)
        except Exception as e:
            print(f"Erreur lors de la sauvegarde de la config: {e}")

    def get(self, key: str, default: Any = None) -> Any:
        return self.config.get(key, default)

    def set(self, key: str, value: Any):
        self.config[key] = value
        self.save()


============================================================
[107/124] services\database.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Database - Service d'interaction avec la base de donn√©es Stash (SQLite)
"""

import os
import sqlite3
import re
import threading
from typing import Dict, List, Optional, Any

class StashDatabase:
    """G√®re les requ√™tes vers stash-go.sqlite"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        # r√©initialiser toute connexion existante pour ce thread (nouvelle base)
        try:
            if hasattr(self._thread_conn, 'conn'):
                self._thread_conn.conn.close()
                self._thread_conn.conn = None
        except Exception:
            pass

    # utiliser connexion thread-local pour √©viter les erreurs sqlite cross-thread
    _thread_conn = threading.local()

    def _get_connection(self):
        """Retourne une connexion SQLite sp√©cifique au thread courant."""
        conn = getattr(self._thread_conn, 'conn', None)
        if conn is None:
            conn = sqlite3.connect(self.db_path, check_same_thread=False)
            conn.row_factory = sqlite3.Row
            self._thread_conn.conn = conn
        else:
            # V√©rifie que la connexion n'est pas ferm√©e
            try:
                conn.cursor()
            except Exception:
                conn = sqlite3.connect(self.db_path, check_same_thread=False)
                conn.row_factory = sqlite3.Row
                self._thread_conn.conn = conn
        return conn

    def _close_conn(self):
        """Ferme proprement la connexion thread-local et la r√©initialise."""
        conn = getattr(self._thread_conn, 'conn', None)
        if conn is not None:
            try:
                conn.close()
            except Exception:
                pass
            self._thread_conn.conn = None

    def get_performer_metadata(self, performer_id: str) -> Optional[Dict]:
        """R√©cup√®re les m√©tadonn√©es actuelles d'un performer"""
        if not os.path.exists(self.db_path):
            print(f"Erreur: Base de donn√©es non trouv√©e √† {self.db_path}")
            return None
            
        try:
            conn = self._get_connection()
            cur = conn.cursor()
            
            # Performer de base
            cur.execute("SELECT * FROM performers WHERE id=?", (performer_id,))
            row = cur.fetchone()
            if not row:
                return None
                
            data = dict(row)
            
            # Aliases
            cur.execute("SELECT alias FROM performer_aliases WHERE performer_id=?", (performer_id,))
            data['aliases'] = [r['alias'] for r in cur.fetchall()]
            
            # URLs
            cur.execute("SELECT url FROM performer_urls WHERE performer_id=?", (performer_id,))
            data['urls'] = [r['url'] for r in cur.fetchall()]
            
            # Tags
            query = """
                SELECT t.name 
                FROM tags t
                JOIN performers_tags pt ON pt.tag_id = t.id
                WHERE pt.performer_id = ?
            """
            cur.execute(query, (performer_id,))
            data['tags'] = [r['name'] for r in cur.fetchall()]
            
            # Trivia/Awards (si stock√©s dans performers ou tables d√©di√©es)
            # Dans la BDD Stash standard, trivia et awards ne sont pas des colonnes natives de 'performers'
            # mais souvent stock√©es dans 'details' ou via des plugins.
            # Mapping des champs sp√©cifiques UI
            data['career_start'] = data.get('career_length', '')
            data['details'] = data.get('details', '')

            # Custom Fields
            cur.execute("SELECT field, value FROM performer_custom_fields WHERE performer_id=?", (performer_id,))
            for cfield in cur.fetchall():
                fname = cfield['field'].lower()
                fval = cfield['value']
                # On mappe 'birthplace' et 'dob' si pr√©sents dans les custom fields
                if fname == 'birthplace':
                    data['birthplace'] = fval
                elif fname == 'dob' or fname == 'date of birth':
                    data['birthdate'] = fval
                elif fname == 'awards':
                    data['awards'] = fval

            # Fallback: certains setups utilisent la colonne 'disambiguation' comme lieu de naissance
            if not data.get('birthplace') and data.get('disambiguation'):
                data['birthplace'] = data.get('disambiguation')
            
            return data
        except Exception as e:
            print(f"Erreur lors de la lecture DB: {e}")
            return None

    def get_all_performers(self) -> List[Dict]:
        """R√©cup√®re tous les performers pour une liste de s√©lection"""
        try:
            conn = self._get_connection()
            cur = conn.cursor()
            cur.execute("SELECT id, name FROM performers ORDER BY name")
            return [dict(r) for r in cur.fetchall()]
        except Exception as e:
            print(f"Erreur get_all_performers: {e}")
            return []

    def get_group_metadata(self, group_id: str) -> Optional[Dict]:
        """R√©cup√®re les m√©tadonn√©es d'un groupe (DVD)"""
        if not os.path.exists(self.db_path): return None
        try:
            conn = self._get_connection()
            cur = conn.cursor()
            cur.execute("SELECT * FROM groups WHERE id=?", (group_id,))
            row = cur.fetchone()
            if not row:
                return None
            data = dict(row)
            
            # Studio (dans Stash 'groups' a souvent une colonne studio_id)
            if data.get('studio_id'):
                cur.execute("SELECT name FROM studios WHERE id=?", (data['studio_id'],))
                s_row = cur.fetchone()
                if s_row: data['studio_name'] = s_row['name']
            
            return data
        except Exception as e:
            print(f"Erreur get_group_metadata: {e}")
            return None

    def get_all_groups(self) -> List[Dict]:
        """R√©cup√®re tous les groupes pour une liste de s√©lection"""
        try:
            conn = self._get_connection()
            cur = conn.cursor()
            cur.execute("SELECT id, name FROM groups ORDER BY name")
            return [dict(r) for r in cur.fetchall()]
        except Exception as e:
            print(f"Erreur get_all_groups: {e}")
            return []

    def save_performer_metadata(self, performer_id: str, updates: Dict):
        """Met √† jour le performer dans Stash"""
        if not os.path.exists(self.db_path):
            return
            
        try:
            conn = self._get_connection()
            cur = conn.cursor()
            
            # 1. Mise √† jour de la table performers
            # On ne met √† jour que ce qui est fourni
            fields_to_update = []
            values = []
            
            # Mapping UI keys -> DB columns
            mapping = {
                'name': 'name',
                'birthdate': 'birthdate',
                'birthplace': 'disambiguation', # Faute de mieux si birthplace absent
                'ethnicity': 'ethnicity',
                'country': 'country',
                'eye_color': 'eye_color',
                'hair_color': 'hair_color',
                'height': 'height',
                'weight': 'weight',
                'measurements': 'measurements',
                'fake_tits': 'fake_tits',
                'details': 'details',
                'deathdate': 'death_date',
                'tattoos': 'tattoos',
                'piercings': 'piercings',
                'career_start': 'career_length'
            }
            
            for ui_key, db_col in mapping.items():
                if ui_key in updates:
                    fields_to_update.append(f"{db_col}=?")
                    values.append(updates[ui_key])
            
            if fields_to_update:
                query = f"UPDATE performers SET {', '.join(fields_to_update)} WHERE id=?"
                values.append(performer_id)
                cur.execute(query, tuple(values))
            
            # 2. Mise √† jour des aliases
            if 'aliases' in updates:
                aliases_in = updates['aliases']
                if isinstance(aliases_in, str):
                    new_aliases = [a.strip() for a in re.split(r'[,\n\r]+', aliases_in) if a.strip()]
                elif isinstance(aliases_in, list):
                    new_aliases = [str(a).strip() for a in aliases_in if str(a).strip()]
                else:
                    new_aliases = [str(aliases_in).strip()] if str(aliases_in).strip() else []

                # Fusion automatique: conserve les aliases existants + ajoute les nouveaux (sans doublons)
                existing_aliases: List[str] = []
                try:
                    cur.execute("SELECT alias FROM performer_aliases WHERE performer_id=?", (performer_id,))
                    existing_aliases = [r['alias'] for r in cur.fetchall() if r.get('alias')]
                except Exception:
                    existing_aliases = []

                def dedupe_keep_order(items: List[str]) -> List[str]:
                    out: List[str] = []
                    seen = set()
                    for it in items:
                        it = str(it).strip()
                        if not it:
                            continue
                        k = it.casefold()
                        if k in seen:
                            continue
                        seen.add(k)
                        out.append(it)
                    return out

                merged_aliases = dedupe_keep_order(existing_aliases + new_aliases)

                cur.execute("DELETE FROM performer_aliases WHERE performer_id=?", (performer_id,))
                for alias in merged_aliases:
                    cur.execute(
                        "INSERT INTO performer_aliases (performer_id, alias) VALUES (?, ?)",
                        (performer_id, alias),
                    )

            # 2bis. Mise √† jour des tags
            if 'tags' in updates:
                # 1. Supprimer les anciens liens
                cur.execute("DELETE FROM performers_tags WHERE performer_id=?", (performer_id,))
                
                tags_raw = updates['tags']
                if isinstance(tags_raw, str):
                    tag_list = [t.strip() for t in re.split(r'[,\n\r]+', tags_raw) if t.strip()]
                else:
                    tag_list = tags_raw
                
                for tag_name in tag_list:
                    # 2. Trouver ou cr√©er le tag
                    cur.execute("SELECT id FROM tags WHERE name=?", (tag_name,))
                    tag_row = cur.fetchone()
                    if tag_row:
                        tag_id = tag_row['id']
                    else:
                        cur.execute("INSERT INTO tags (name) VALUES (?)", (tag_name,))
                        tag_id = cur.lastrowid
                    
                    # 3. Lier le performer au tag
                    cur.execute("INSERT INTO performers_tags (performer_id, tag_id) VALUES (?, ?)", (performer_id, tag_id))
            
            # 3. Mise √† jour des champs personnalis√©s (Custom Fields)
            # On identifie les champs qui doivent aller dans performer_custom_fields
            custom_map = {
                'birthplace': 'Birthplace',
                'awards': 'Awards',
                'trivia': 'Trivia',
                'trivia_fr': 'Trivia FR',
                'tattoos_fr': 'Tattoos FR',
                'piercings_fr': 'Piercings FR',
                'website': 'Official Website',
                'instagram': 'Instagram',
                'onlyfans': 'OnlyFans',
                'tiktok': 'TikTok',
                'youtube': 'YouTube',
                'twitch': 'Twitch',
                'imdb': 'IMDb',
                'twitter': 'Twitter',
                'facebook': 'Facebook'
            }
            # Note: Si l'utilisateur veut DOB en custom field, on peut l'ajouter ici
            
            for ui_key, custom_name in custom_map.items():
                if ui_key in updates:
                    val = str(updates[ui_key]).strip()
                    # Delete existing and re-insert
                    cur.execute("DELETE FROM performer_custom_fields WHERE performer_id=? AND field=?", (performer_id, custom_name))
                    if val:
                        cur.execute("INSERT INTO performer_custom_fields (performer_id, field, value) VALUES (?, ?, ?)", 
                                   (performer_id, custom_name, val))

            # 4. Mise √† jour des URLs (Discovery)
            if 'discovered_urls' in updates:
                cur.execute("DELETE FROM performer_urls WHERE performer_id=?", (performer_id,))
                urls = updates['discovered_urls']
                if isinstance(urls, str):
                    urls = [u.strip() for u in re.split(r'[,\n\r\s]+', urls) if u.strip()]
                # D√©dupe en conservant l'ordre, et attribue une position (colonne NOT NULL dans Stash)
                cleaned_urls = []
                seen = set()
                for u in urls or []:
                    u = str(u).strip()
                    if not u or u in seen:
                        continue
                    seen.add(u)
                    cleaned_urls.append(u)

                for pos, url in enumerate(cleaned_urls):
                    cur.execute(
                        "INSERT INTO performer_urls (performer_id, position, url) VALUES (?, ?, ?)",
                        (performer_id, pos, url),
                    )

            # 5. Propagation des Tags vers les Sc√®nes (Optionnel mais demand√©)
            if 'tags' in updates:
                self._propagate_tags_to_scenes(cur, performer_id, tag_list if 'tag_list' in locals() else [])

            conn.commit()
            return True
        except Exception as e:
            print(f"Erreur lors de la sauvegarde DB: {e}")
            try:
                self._get_connection().rollback()
            except Exception:
                pass
            self._close_conn()
            return False

    def _propagate_tags_to_scenes(self, cur, performer_id: str, tag_names: List[str]):
        """Propage les tags d'un performer vers toutes ses sc√®nes"""
        if not tag_names:
            return

        def table_exists(table_name: str) -> bool:
            try:
                row = cur.execute(
                    "SELECT 1 FROM sqlite_master WHERE type='table' AND name=? LIMIT 1",
                    (table_name,),
                ).fetchone()
                return bool(row)
            except Exception:
                return False

        # Certaines variantes de sch√©ma Stash n'ont pas ces tables.
        if not table_exists("scenes_tags"):
            return

        link_table = None
        if table_exists("scenes_performers"):
            link_table = "scenes_performers"
        elif table_exists("performers_scenes"):
            link_table = "performers_scenes"
        else:
            return
            
        # 1. R√©cup√©rer les IDs des tags
        tag_ids = []
        for name in tag_names:
            cur.execute("SELECT id FROM tags WHERE name=?", (name,))
            row = cur.fetchone()
            if row:
                tag_ids.append(row['id'])
        
        if not tag_ids:
            return

        # 2. Trouver toutes les sc√®nes du performer
        try:
            cur.execute(f"SELECT scene_id FROM {link_table} WHERE performer_id=?", (performer_id,))
            scene_ids = [r['scene_id'] for r in cur.fetchall()]
        except sqlite3.OperationalError:
            return
        except Exception:
            return
        
        for scene_id in scene_ids:
            for tag_id in tag_ids:
                # 3. V√©rifier si le lien existe d√©j√† pour √©viter les doublons
                try:
                    cur.execute("SELECT 1 FROM scenes_tags WHERE scene_id=? AND tag_id=?", (scene_id, tag_id))
                    if not cur.fetchone():
                        cur.execute("INSERT INTO scenes_tags (scene_id, tag_id) VALUES (?, ?)", (scene_id, tag_id))
                except sqlite3.OperationalError:
                    return
                except Exception:
                    return
    def save_group_metadata(self, group_id: str, updates: Dict):
        """Met √† jour un groupe dans Stash"""
        if not os.path.exists(self.db_path): return False
        try:
            conn = self._get_connection()
            cur = conn.cursor()
            
            fields = []
            values = []
            mapping = {
                'name': 'name',
                'date': 'date',
                'details': 'details',
                'director': 'director',
                'duration': 'duration', # Stash utilise souvent des secondes ou string
                'rating': 'rating'
            }
            
            for k, v in mapping.items():
                if k in updates:
                    fields.append(f"{v}=?")
                    values.append(updates[k])
            
            if fields:
                query = f"UPDATE groups SET {', '.join(fields)} WHERE id=?"
                values.append(group_id)
                cur.execute(query, tuple(values))
            
            conn.commit()
            return True
        except Exception as e:
            print(f"Erreur save_group_metadata: {e}")
            self._close_conn()
            return False

    def get_scene_metadata(self, scene_id: str) -> Optional[Dict]:
        """R√©cup√®re les m√©tadonn√©es d'une sc√®ne"""
        if not os.path.exists(self.db_path): return None
        try:
            conn = self._get_connection()
            cur = conn.cursor()
            cur.execute("SELECT * FROM scenes WHERE id=?", (scene_id,))
            row = cur.fetchone()
            if not row:
                return None
            
            data = dict(row)
            
            # Tags
            cur.execute("""
                SELECT t.name FROM tags t
                JOIN scenes_tags st ON st.tag_id = t.id
                WHERE st.scene_id = ?
            """, (scene_id,))
            data['tags'] = [r['name'] for r in cur.fetchall()]
            
            # Performers
            link_table = None
            try:
                row = cur.execute(
                    "SELECT 1 FROM sqlite_master WHERE type='table' AND name='scenes_performers' LIMIT 1"
                ).fetchone()
                if row:
                    link_table = "scenes_performers"
                else:
                    row2 = cur.execute(
                        "SELECT 1 FROM sqlite_master WHERE type='table' AND name='performers_scenes' LIMIT 1"
                    ).fetchone()
                    if row2:
                        link_table = "performers_scenes"
            except Exception:
                link_table = None

            if link_table:
                cur.execute(
                    f"""
                    SELECT p.name FROM performers p
                    JOIN {link_table} sp ON sp.performer_id = p.id
                    WHERE sp.scene_id = ?
                    """,
                    (scene_id,),
                )
                data['performers'] = [r['name'] for r in cur.fetchall()]
            else:
                data['performers'] = []
            
            # Studio
            if data.get('studio_id'):
                cur.execute("SELECT name FROM studios WHERE id=?", (data['studio_id'],))
                s_row = cur.fetchone()
                if s_row: data['studio'] = s_row['name']
            
            return data
        except Exception as e:
            print(f"Erreur get_scene_metadata: {e}")
            return None

    def save_scene_metadata(self, scene_id: str, updates: Dict) -> bool:
        """Met √† jour une sc√®ne dans Stash"""
        if not os.path.exists(self.db_path): return False
        try:
            conn = self._get_connection()
            cur = conn.cursor()
            
            fields = []
            values = []
            mapping = {
                'title': 'title',
                'details': 'details',
                'date': 'date',
                'code': 'code',
                'rating': 'rating'
            }
            
            for k, v in mapping.items():
                if k in updates:
                    fields.append(f"{v}=?")
                    values.append(updates[k])
            
            if fields:
                query = f"UPDATE scenes SET {', '.join(fields)} WHERE id=?"
                values.append(scene_id)
                cur.execute(query, tuple(values))
            
            # Tags & Performers (sync complexe si besoin, mais on reste simple ici pour le moment)
            
            conn.commit()
            return True
        except Exception as e:
            print(f"Erreur save_scene_metadata: {e}")
            self._close_conn()
            return False

    def get_scenes_for_group(self, group_id: str) -> List[Dict]:
        """R√©cup√®re les sc√®nes rattach√©es √† un groupe"""
        try:
            conn = self._get_connection()
            cur = conn.cursor()
            query = """
                SELECT s.id, s.title
                FROM scenes s
                JOIN groups_scenes gs ON gs.scene_id = s.id
                WHERE gs.group_id = ?
            """
            cur.execute(query, (group_id,))
            return [dict(r) for r in cur.fetchall()]
        except Exception as e:
            print(f"Erreur get_scenes_for_group: {e}")
            return []

    def add_scene_url(self, scene_id: str, url: str) -> bool:
        """Ajoute une URL √† une sc√®ne si elle n'existe pas d√©j√†"""
        try:
            conn = self._get_connection()
            cur = conn.cursor()
            
            # V√©rifier si l'URL existe d√©j√†
            cur.execute("SELECT 1 FROM scene_urls WHERE scene_id=? AND url=?", (scene_id, url))
            if cur.fetchone():
                return True
                
            cur.execute("INSERT INTO scene_urls (scene_id, url) VALUES (?, ?)", (scene_id, url))
            conn.commit()
            return True
        except Exception as e:
            print(f"Erreur add_scene_url: {e}")
            return False


============================================================
[108/124] services\scrapers.py
------------------------------------------------------------
"""
scrapers.py - Modules de scraping pour StashMaster V2
Sources support√©es : IAFD, FreeOnes, TheNude, Babepedia

Chaque scraper retourne un dict avec les cl√©s normalis√©es :
    name, aliases, birthdate, birthplace, country, ethnicity,
    hair, eyes, height_cm, weight_kg, measurements, fake_boobs,
    career_start, career_end, tattoos, piercings, awards
"""

import re
import requests
from bs4 import BeautifulSoup
from typing import Dict, List, Optional, Any


# ---------------------------------------------------------------------------
# Constantes
# ---------------------------------------------------------------------------

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "en-US,en;q=0.9",
}

TIMEOUT = 15  # secondes


# ---------------------------------------------------------------------------
# Utilitaires
# ---------------------------------------------------------------------------

def _clean(text: str) -> str:
    """Nettoie une cha√Æne : supprime les espaces superflus."""
    return re.sub(r'\s+', ' ', text).strip()


def _clean_career(val: str) -> str:
    """Supprime tout texte entre parenth√®ses dans une p√©riode de carri√®re.
    Exemple¬†: "2007-2025 (Started around 2007)" ‚Üí "2007-2025".
    """
    if not val:
        return val
    return re.sub(r"\s*\(.*?\)", "", val).strip()


def _fetch_with_curl(url: str) -> Optional[str]:
    """Fallback utilisant curl pour contourner les blocages 403."""
    import subprocess
    try:
        # On utilise curl avec des headers standards et un timeout
        cmd = [
            "curl", "-s", "-L",
            "-A", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "-H", "Accept-Language: en-US,en;q=0.9",
            "--connect-timeout", str(TIMEOUT),
            url
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore')
        if result.returncode == 0 and result.stdout:
            return result.stdout
    except Exception as e:
        print(f"[SCRAPER] √âchec fallback curl : {e}")
    return None


def _fetch(url: str) -> Optional[BeautifulSoup]:
    """T√©l√©charge une page et retourne un objet BeautifulSoup, ou None."""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
        resp.raise_for_status()
        return BeautifulSoup(resp.text, "html.parser")
    except Exception as e:
        # Fallback pour les erreurs 403 (Forbidden) fr√©quentes sur IAFD/Babepedia
        resp_obj = getattr(e, 'response', None)
        status_code = getattr(resp_obj, 'status_code', 0)
        is_forbidden = status_code == 403
        
        if is_forbidden or "403" in str(e):
            print(f"[SCRAPER] 403 d√©tect√© sur {url}, tentative fallback curl...")
            html = _fetch_with_curl(url)
            if html:
                print(f"[SCRAPER] Succ√®s fallback curl pour {url}")
                return BeautifulSoup(html, "html.parser")
        
        print(f"[SCRAPER] Erreur fetch {url} : {e}")
        return None


def _parse_html(html_content: str) -> BeautifulSoup:
    """Parse du HTML brut (pour les tests avec fichiers locaux)."""
    return BeautifulSoup(html_content, "html.parser")


def _extract_cm(text: str) -> str:
    """Extrait les cm d'une cha√Æne comme '5 feet, 8 inches (173 cm)', '172 cm' ou '1.63m'."""
    # Cas '1.63m' ou '1.63 m'
    m_meter = re.search(r'(\d\.\d{2})\s*m', text, re.IGNORECASE)
    if m_meter:
        return str(int(float(m_meter.group(1)) * 100))

    m = re.search(r'(\d{2,3})\s*cm', text, re.IGNORECASE)
    return m.group(1) if m else ""


def _extract_kg(text: str) -> str:
    """Extrait les kg d'une cha√Æne comme '129 lbs (59 kg)' ou '59 kg'."""
    m = re.search(r'(\d{2,3})\s*kg', text, re.IGNORECASE)
    return m.group(1) if m else ""


def _extract_year(text: str) -> str:
    """Extrait la premi√®re ann√©e sur 4 chiffres trouv√©e."""
    m = re.search(r'(\d{4})', text)
    return m.group(1) if m else ""


# ---------------------------------------------------------------------------
# Scraper de base
# ---------------------------------------------------------------------------

class ScraperBase:
    """Classe de base pour tous les scrapers."""

    SOURCE_NAME = "unknown"

    def scrape(self, url: str) -> Dict[str, Any]:
        """Scrape une URL et retourne un dict normalis√©."""
        soup = _fetch(url)
        if soup is None:
            return {}
        data = self._parse(soup, url)
        data['url'] = url
        data['source'] = self.SOURCE_NAME
        return data

    def scrape_from_html(self, html_content: str, url: str = "") -> Dict[str, Any]:
        """Scrape depuis du HTML brut (pour les tests locaux)."""
        soup = _parse_html(html_content)
        return self._parse(soup, url)

    def _parse(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        raise NotImplementedError

    @staticmethod
    def _detect_url(url: str) -> bool:
        """Retourne True si l'URL correspond √† cette source."""
        raise NotImplementedError


# ===========================================================================
# SCRAPER IAFD
# ===========================================================================

class IAFDScraper(ScraperBase):
    """
    Scrape iafd.com
    URL type : https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm

    Structure HTML :
    - <p class="bioheading"> contient le nom du champ
    - <p class="biodata"> ou <div class="biodata"> contient la valeur
    - Onglet awards : <div id="awards"> avec p.bioheading (c√©r√©monie),
      div.showyear (ann√©e), div.biodata (mention + cat√©gorie)
    """

    SOURCE_NAME = "IAFD"

    @staticmethod
    def _detect_url(url: str) -> bool:
        return "iafd.com" in url

    def _parse(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        data: Dict[str, Any] = {"source": self.SOURCE_NAME, "url": url}

        # --- Nom principal ---
        h1 = soup.find("h1")
        if h1:
            data["name"] = _clean(h1.get_text())

        # --- Champs bioheading / biodata ---
        headings = soup.find_all("p", class_="bioheading")
        for heading in headings:
key="***MASKED***"
            # La valeur est dans le prochain √©l√©ment fr√®re (p ou div .biodata)
            sib = heading.find_next_sibling()
            if sib is None:
                continue
            val = _clean(sib.get_text(separator=" "))

            if key in ("Performer AKA", "AKA"):
                # IAFD : alias s√©par√©s par des <br/> dans un div.biodata
                # On remplace les <br> par des s√©parateurs avant de parser
                for br in sib.find_all("br"):
                    br.replace_with("|")
                aliases = [_clean(a) for a in sib.get_text().split("|") if _clean(a)]
                data["aliases"] = aliases

            elif key == "Birthday":
                # "October 15, 1983 (42 years old)"
                m = re.match(r"(.+?)\s*\(\d+", val)
                data["birthdate"] = _clean(m.group(1)) if m else val

            elif key == "Birthplace":
                data["birthplace"] = val

            elif key == "Ethnicity":
                data["ethnicity"] = val

            elif key == "Hair Colors":
                data["hair_color"] = val

            elif key == "Eye Color":
                data["eye_color"] = val

            elif key == "Height":
                data["height"] = _extract_cm(val)

            elif key == "Weight":
                data["weight"] = _extract_kg(val)

            elif key == "Measurements":
                data["measurements"] = val

            elif key == "Years Active":
                # nettoyer les parenth√®ses √©ventuelles
                data["career_length"] = _clean_career(val)

            elif key == "Tattoos":
                data["tattoos"] = val

            elif key == "Nationality":
                data["country"] = val

            elif key in ("Breast implants", "Implant", "Boobs"):
                # IAFD affiche souvent "Yes" ou "No"
                data["fake_tits"] = val

        # --- Awards (onglet d√©di√©) ---
        awards_div = soup.find("div", id="awards")
        if awards_div:
            # Essayer de r√©cup√©rer tous les awards depuis la page d√©di√©e
            m = re.search(r'id=([a-f0-9-]+)', url)
            if m:
                awards_url = f"https://www.iafd.com/awards.asp?id={m.group(1)}"
                awards_soup = _fetch(awards_url)
                if awards_soup:
                    full_awards_div = awards_soup.find("div", id="awards")
                    if full_awards_div:
                        data["awards"] = self._parse_awards(full_awards_div)
                    else:
                        data["awards"] = self._parse_awards(awards_div)
                else:
                    data["awards"] = self._parse_awards(awards_div)
            else:
                data["awards"] = self._parse_awards(awards_div)

        # --- Liens Sociaux / Externes ---
        ext_links = []
        
        # 1. Liens dans bioheading "External Sites"
        ext_heading = soup.find("p", class_="bioheading", string=re.compile(r"External Sites", re.I))
        if ext_heading:
            sib = ext_heading.find_next_sibling()
            if sib:
                ext_links.extend([a["href"] for a in sib.find_all("a", href=True)])
        
        # 2. Liens dans bioheading "Social Network" (nouveau format IAFD)
        social_heading = soup.find("p", class_="bioheading", string=re.compile(r"Social Network", re.I))
        if social_heading:
            sib = social_heading.find_next_sibling()
            if sib:
                ext_links.extend([a["href"] for a in sib.find_all("a", href=True)])
        
        data["discovered_urls"] = list(set(ext_links))

        return data

    def _parse_awards(self, awards_div) -> str:
        """
        Parse le bloc awards IAFD et retourne une cha√Æne format√©e :
            CEREMONIE
            ANNEE - Winner/Nominee: Cat√©gorie
            ...
        """
        lines = []
        current_ceremony = ""
        current_year = ""

        for child in awards_div.children:
            if not hasattr(child, "name") or child.name is None:
                continue

            tag_name = child.name
            tag_class = child.get("class", [])
            text = _clean(child.get_text(separator=" "))

            if tag_name == "p" and "bioheading" in tag_class:
                # Nouvelle c√©r√©monie
                current_ceremony = text
                current_year = ""
                lines.append(f"\n{current_ceremony}")

            elif tag_name == "div" and "showyear" in tag_class:
                current_year = text

            elif tag_name == "div" and "biodata" in tag_class:
                if text:
                    lines.append(f"{current_year} - {text}")

        return "\n".join(lines).strip()


# ===========================================================================
# SCRAPER FREEONES
# ===========================================================================

class FreeOnesScraper(ScraperBase):
    """
    Scrape freeones.xxx
    URL type : https://www.freeones.xxx/bridgette-b/bio

    Structure HTML :
    - <li class="hide-on-edit"> contenant :
        <span> (cl√©) + <span class="font-size-xs"> (valeur)
      Parfois la valeur est dans des liens <a>
    """

    SOURCE_NAME = "FreeOnes"

    @staticmethod
    def _detect_url(url: str) -> bool:
        return "freeones" in url

    def _parse(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        data: Dict[str, Any] = {"source": self.SOURCE_NAME, "url": url}

        for li in soup.find_all("li", class_="hide-on-edit"):
            spans = li.find_all("span", recursive=False)
            if not spans:
                # Essayer tous les spans
                spans = li.find_all("span")
            if len(spans) < 1:
                continue
key="***MASKED***"

            # Valeur : liens <a> ou dernier span
            links = [_clean(a.get_text()) for a in li.find_all("a") if _clean(a.get_text())]
            if links:
                val = links[0]
                all_links = links
            elif len(spans) > 1:
                val = _clean(spans[-1].get_text())
                all_links = [val]
            else:
                continue

            if not val or val.lower() in ("unknown", "n/a", "no"):
                # On garde quand m√™me certains champs n√©gatifs
                if key not in ("Piercings", "Tattoos", "Feature dancer"):
                    continue

            # Mapping des cl√©s FreeOnes ‚Üí cl√©s normalis√©es
            if key == "Name":
                data["name"] = val

            elif key == "Aliases":
                data["aliases"] = all_links

            elif key == "Date of birth":
                data["birthdate"] = val

            elif key == "Place of birth":
                # "Barcelona Spain" ou links = ['Barcelona', 'Spain']
                data["birthplace"] = ", ".join(all_links) if len(all_links) > 1 else val

            elif key == "Nationality":
                data["country"] = val

            elif key == "Ethnicity":
                data["ethnicity"] = val

            elif key == "Hair Color":
                data["hair_color"] = val

            elif key == "Eye Color":
                data["eye_color"] = val

            elif key == "Height":
                data["height"] = _extract_cm(val)

            elif key == "Weight":
                data["weight"] = _extract_kg(val)

            elif key in ("Measurements", "Bust", "Waist", "Hip"):
                # On stocke les mesures brutes disponibles
                if key == "Measurements":
                    data["measurements"] = val
                else:
                    data[f"freeones_{key.lower()}"] = val

            elif key == "Boobs":
                data["fake_tits"] = val  # "Fake" ou "Natural"

            elif key == "Tattoos":
                # val = "Yes" ou description
                tattoo_loc = li.find("span", class_="font-size-xs")
                tattoo_val = _clean(tattoo_loc.get_text()) if tattoo_loc else val
                data["tattoos"] = tattoo_val if tattoo_val.lower() not in ("no",) else ""

            elif key == "Tattoo locations":
                data["tattoos"] = val  # Description compl√®te

            elif key == "Piercings":
                piercing_val = val
                data["piercings"] = piercing_val if piercing_val.lower() not in ("no",) else ""

            elif key == "Piercing locations":
                if val and val.lower() not in ("no",):
                    data["piercings"] = val

            elif key == "Career start":
                val2 = _clean_career(val)
                data["career_length"] = val2 if "career_length" not in data else f"{val2} - {data['career_length']}"

            elif key == "Career end":
                val2 = _clean_career(val)
                data["career_length"] = f"{data.get('career_length', '???')} - {val2}"

        # --- Liens Sociaux / Externes ---
        ext_links = []
        social_box = soup.find("div", class_="social-links") or soup.find("ul", class_="social-links")
        if social_box:
            ext_links.extend([a["href"] for a in social_box.find_all("a", href=True)])
        
        # On peut aussi chercher dans les liens de la bio qui ne sont pas internes
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if "freeones.xxx" not in href and href.startswith("http"):
                # √âviter les liens de navigation communs
                if not any(x in href for x in ["twitter.com/intent", "facebook.com/sharer"]):
                    ext_links.append(href)

        data["discovered_urls"] = list(set(ext_links))

        return data


# ===========================================================================
# SCRAPER THENUDE
# ===========================================================================

class TheNudeScraper(ScraperBase):
    """
    Scrape thenude.com
    URL type : https://www.thenude.com/bridgette-b-51339.htm

    Structure HTML :
    - <li> contenant <span class="list-quest">Cl√©:</span> suivi du texte de valeur
    """

    SOURCE_NAME = "TheNude"

    @staticmethod
    def _detect_url(url: str) -> bool:
        return "thenude.com" in url

    def _parse(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        data: Dict[str, Any] = {"source": self.SOURCE_NAME, "url": url}

        for li in soup.find_all("li"):
            quest = li.find("span", class_="list-quest")
            if not quest:
                continue
key="***MASKED***"
            # Valeur = texte du li sans le span
            full_text = _clean(li.get_text(separator=" "))
key_text="***MASKED***"
            val = _clean(full_text.replace(key_text, "", 1))

            if not val:
                continue

            if key == "AKA":
                aliases = [a.strip() for a in val.split(",") if a.strip()]
                data["aliases"] = aliases

            elif key == "Born":
                data["birthdate"] = val  # "October 1983" (pas de jour)

            elif key == "Birthplace":
                data["birthplace"] = val

            elif key == "Measurements":
                # Format: "38F-27-36 / ~97-69-92"
                parts = val.split("/")
                data["measurements"] = _clean(parts[0])  # format US
                data["measurements_metric"] = _clean(parts[1].lstrip("~")) if len(parts) > 1 else ""

            elif key == "Height":
                data["height_cm"] = _extract_cm(val)

            elif key == "Weight":
                data["weight"] = _extract_kg(val)

            elif key in ("Hair Colour", "Hair Color"):
                data["hair_color"] = val

            elif key == "Ethnicity":
                data["ethnicity"] = val

            elif key == "Breasts":
                # "Large (Fake)" ‚Üí fake_tits
                data["breast_size"] = val
                data["fake_tits"] = "Fake" if "fake" in val.lower() else "Natural"

            elif key == "Piercings":
                data["piercings"] = val if val.lower() not in ("none", "no") else ""

            elif key == "Tattoos":
                data["tattoos"] = val

            elif key == "Activities":
                data["activities"] = val

            elif key in ("First Seen", "Last Seen"):
                year = _extract_year(val)
                if "career_length" not in data:
                    data["career_length"] = year
                else:
                    if key == "First Seen":
                        data["career_length"] = f"{year} - {data['career_length']}"
                    else:
                        data["career_length"] = f"{data['career_length']} - {year}"

            elif key == "Tags":
                data["thenude_tags"] = [t.strip() for t in val.split(",")]

            elif key == "Agencies":
                data["agency"] = val

        # --- Liens Externes ---
        ext_links = []
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if "thenude.com" not in href and href.startswith("http"):
                ext_links.append(href)
        
        data["discovered_urls"] = list(set(ext_links))

        return data


# ===========================================================================
# SCRAPER BABEPEDIA
# ===========================================================================

class BabepediaScraper(ScraperBase):
    """
    Scrape babepedia.com
    URL type : https://www.babepedia.com/babe/Bridgette_B

    Structure HTML :
    - <div class="info-item"> contenant :
        <span class="label">Cl√©:</span> + texte/liens pour la valeur
    - Aliases dans <div class="aliases"> ou section "Also known as:"
    """

    SOURCE_NAME = "Babepedia"

    @staticmethod
    def _detect_url(url: str) -> bool:
        return "babepedia.com" in url

    def _parse(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        data: Dict[str, Any] = {"source": self.SOURCE_NAME, "url": url}

        # --- Nom principal ---
        h1 = soup.find("h1")
        if h1:
            data["name"] = _clean(h1.get_text())

        # --- Aliases ---
        aliases = []
        # Chercher "Also known as:" ou section alias
        for tag in soup.find_all(string=re.compile(r"Also known as", re.I)):
            parent = tag.parent
            # Chercher les liens dans le parent ou ses fr√®res
            container = parent.parent if parent else None
            if container:
                alias_links = [_clean(a.get_text()) for a in container.find_all("a") if _clean(a.get_text())]
                aliases.extend(alias_links)
        if aliases:
            data["aliases"] = list(set(aliases))

        # --- Champs info-item ---
        for div in soup.find_all("div", class_="info-item"):
            label = div.find("span", class_="label")
            if not label:
                continue
key="***MASKED***"

            # Valeur : tout le texte du div moins le label
            # On utilise \n pour pr√©server la structure (ex: Tattoos 1 par ligne)
            full_text = _clean(div.get_text(separator="\n"))
            label_text = _clean(label.get_text())
            val = _clean(full_text.replace(label_text, "", 1))

            if not val:
                continue

            # Mapping Babepedia ‚Üí normalis√©
            if key == "Age":
                pass  # On ignore, on pr√©f√®re la date de naissance

            elif key == "Born":
                # "Saturday 15th of October 1983"
                m = re.search(r'(\d+)(?:st|nd|rd|th)?\s+of\s+(\w+)\s+(\d{4})', val, re.I)
                if m:
                    data["birthdate"] = f"{m.group(3)}-{_month_to_num(m.group(2)):02d}-{int(m.group(1)):02d}"
                else:
                    data["birthdate"] = val

            elif key == "Years active":
                data["career_length"] = val

            elif key == "Birthplace":
                data["birthplace"] = val

            elif key == "Nationality":
                # Enlever les parenth√®ses √©ventuelles
                data["country"] = val.strip("()")

            elif key == "Ethnicity":
                data["ethnicity"] = val

            elif key == "Hair color":
                data["hair_color"] = val

            elif key == "Eye color":
                data["eye_color"] = val

            elif key == "Height":
                data["height"] = _extract_cm(val)

            elif key == "Weight":
                data["weight"] = _extract_kg(val)

            elif key == "Measurements":
                data["measurements"] = val

            elif key in ("Boobs", "Breast"):
                data["fake_tits"] = val  # "Fake/Enhanced" ou "Natural"

            elif key == "Instagram follower count":
                data["instagram_followers"] = val

            elif key == "Tattoos":
                # Babepedia : parfois une liste multi‚Äëligne ou s√©par√©e par des virgules
                # extraire chaque √©l√©ment et conserver localisation si fournie
                items = [l.strip() for l in re.split(r"[\n,]", val) if l.strip()]
                formatted = []
                for line in items:
                    if line.lower() in ("none", "n/a"):
                        continue
                    m = re.search(r"\((.+?)\)", line)
                    pos = m.group(1) if m else None
                    desc = re.sub(r"\(.+?\)", "", line).strip()
                    if pos:
                        formatted.append(f"{pos} ‚Äî {desc}")
                    else:
                        formatted.append(desc)
                data["tattoos"] = "\n".join(formatted)

        # --- Section TRIVIA ---
        # (d√©j√† extrait plus haut) ; la traduction se fait en aval du scraping
        # Souvent une liste <ul> apr√®s un <h2>Trivia</h2>
        trivia_list = []
        trivia_h2 = soup.find("h2", string=re.compile(r"Trivia", re.I))
        if trivia_h2:
            container = trivia_h2.find_next_sibling(["ul", "ol"])
            if container:
                trivia_list = [_clean(li.get_text()) for li in container.find_all("li") if _clean(li.get_text())]
        if trivia_list:
            data["trivia"] = "\n".join(trivia_list)

        # --- Section ABOUT / BIO RAW ---
        about_div = soup.find("div", id="about") or soup.find("div", class_="about")
        if not about_div:
            # Essayer de trouver le texte apr√®s "About [Name]"
            about_h2 = soup.find("h2", string=re.compile(r"About", re.I))
            if about_h2:
                about_div = about_h2.find_next_sibling("p")
        if about_div:
            data["bio_raw"] = _clean(about_div.get_text(separator="\n"))  # mini-bio Babepedia/IAFD

        # --- Liens Sociaux / Externes ---
        ext_links = []
        socials = {}
        
        # Mapping Socials
        social_map = {
            "instagram.com": "instagram",
            "twitter.com": "x",
            "x.com": "x",
            "onlyfans.com": "onlyfans",
            "tiktok.com": "tiktok",
            "youtube.com": "youtube",
            "twitch.tv": "twitch",
            "imdb.com": "imdb",
            "facebook.com": "facebook"
        }

        # Babepedia a souvent une section "Links"
        link_container = soup.find("div", class_="links") or soup.find("ul", class_="links")
        # official website
        if not data.get('official_website'):
            off_tag = soup.find('a', class_=re.compile(r'official', re.I))
            if not off_tag:
                off_tag = soup.find('a', string=re.compile(r'Official', re.I))
            if off_tag and off_tag.get('href'):
                data['official_website'] = off_tag['href']
        if not link_container:
            # Chercher dans la barre de r√©seaux sociaux sous l'image
            link_container = soup.find("div", class_="social-icons")
            
        if link_container:
            for a in link_container.find_all("a", href=True):
                href = a["href"]
                if href.startswith("//"): href = "https:" + href
                if not href.startswith("http"): continue
                if url in href: continue # Ignorer le lien vers la page elle-m√™me
                
                ext_links.append(href)
                for domain, key in social_map.items():
                    if domain in href.lower():
                        socials[key] = href
                        break
        
        # Fallback : tous les liens sortants
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if href.startswith("//"): href = "https:" + href
            if not href.startswith("http"): continue
            
            if "babepedia.com" not in href:
                if url in href: continue
                ext_links.append(href)
                for domain, key in social_map.items():
                    if domain in href.lower() and key not in socials:
                        socials[key] = href

        # --- Raffinement final des URLs ---
        final_urls = []
        seen_domains = set()
        
        # On trie pour donner la priorit√© aux sociaux d√©j√† identifi√©s
        # Mais on va surtout assurer l'unicit√© par domaine
        for link in ext_links:
            if link == url: continue
            
            # Extraire le domaine de base
            domain_match = re.search(r'https?://(?:www\.)?([^/]+)', link.lower())
            if domain_match:
                domain = domain_match.group(1)
                if domain not in seen_domains:
                    final_urls.append(link)
                    seen_domains.add(domain)
            else:
                # Cas rare o√π le regex √©choue
                if link not in final_urls:
                    final_urls.append(link)

        data["discovered_urls"] = final_urls
        data["socials"] = socials

        return data


def _month_to_num(month_name: str) -> int:
    """Convertit un nom de mois anglais en num√©ro."""
    months = {
        "january": 1, "february": 2, "march": 3, "april": 4,
        "may": 5, "june": 6, "july": 7, "august": 8,
        "september": 9, "october": 10, "november": 11, "december": 12
    }
    return months.get(month_name.lower(), 1)


# ===========================================================================
# SCRAPER BOOBPEDIA
# ===========================================================================

class BoobpediaScraper(ScraperBase):
    """
    Scrape boobpedia.com
    URL type : https://www.boobpedia.com/boobs/Abella_Anderson
    """

    SOURCE_NAME = "Boobpedia"

    @staticmethod
    def _detect_url(url: str) -> bool:
        return "boobpedia.com" in url

    def _parse(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        data: Dict[str, Any] = {"source": self.SOURCE_NAME, "url": url}
        socials: Dict[str, str] = {}
        ext_links: List[str] = []

        # --- Nom ---
        h1 = soup.find("h1", id="firstHeading") or soup.find("h1")
        if h1:
            data["name"] = _clean(h1.get_text())

        # --- Infobox (table de donn√©es biographiques) ---
        infobox = soup.find("table", class_=re.compile(r"infobox|wikitable", re.I))
        if infobox:
            for row in infobox.find_all("tr"):
                cells = row.find_all(["th", "td"])
                if len(cells) < 2:
                    continue
key="***MASKED***"
                val_cell = cells[1]
                # Extraire les liens
                links_in_cell = [a["href"] for a in val_cell.find_all("a", href=True) if a["href"].startswith("http")]
                val = _clean(val_cell.get_text(separator=" "))

                if not val:
                    continue

                if key in ("Born", "Date of Birth", "Birthday"):
                    m = re.search(r'(\d{4})-(\d{2})-(\d{2})', val)
                    if m:
                        data["birthdate"] = f"{m.group(1)}-{m.group(2)}-{m.group(3)}"
                    else:
                        data["birthdate"] = val
                elif key == "Birthplace":
                    data["birthplace"] = val
                elif key in ("Height",):
                    data["height"] = _extract_cm(val)
                elif key in ("Weight",):
                    data["weight"] = _extract_kg(val)
                elif key in ("Measurements", "Bust", "Bra size"):
                    data["measurements"] = val
                elif key in ("Hair color", "Hair"):
                    data["hair_color"] = val
                elif key in ("Eye color", "Eyes"):
                    data["eye_color"] = val
                elif key in ("Boobs", "Breast", "Breast type"):
                    data["fake_tits"] = val
                elif key in ("Ethnicity",):
                    data["ethnicity"] = val
                elif key in ("Nationality", "Country"):
                    data["country"] = val
                elif key in ("Years active", "Career"):
                    data["career_length"] = val
                elif key in ("Tattoos",):
                    data["tattoos"] = val
                elif key in ("Also known as", "AKA", "Aliases"):
                    aliases = [a.strip() for a in re.split(r"[,\n|]", val) if a.strip()]
                    data["aliases"] = aliases

                # R√©seaux sociaux dans les liens de l'infobox
                for lnk in links_in_cell:
                    lnk_l = lnk.lower()
                    if "instagram.com" in lnk_l:
                        socials["instagram"] = lnk
                    elif "twitter.com" in lnk_l or "x.com" in lnk_l:
                        socials["twitter"] = lnk
                    elif "onlyfans.com" in lnk_l:
                        socials["onlyfans"] = lnk
                    elif "facebook.com" in lnk_l:
                        socials["facebook"] = lnk
                    ext_links.append(lnk)

        # --- Bio / intro paragraphe ---
        content_div = soup.find("div", id="mw-content-text") or soup.find("div", class_="mw-parser-output")
        if content_div:
            paragraphs = []
            for p in content_div.find_all("p", recursive=False):
                text = _clean(p.get_text())
                if text and len(text) > 40:
                    paragraphs.append(text)
                if len(paragraphs) >= 4:
                    break
            if paragraphs:
                data["bio_raw"] = "\n\n".join(paragraphs)

            # Awards dans les listes
            award_lines = []
            for ul in content_div.find_all(["ul", "ol"]):
                header = ul.find_previous_sibling(re.compile(r"h[2-4]"))
                if header and re.search(r"award|nominat|accolad", header.get_text(), re.I):
                    for li in ul.find_all("li"):
                        award_lines.append("- " + _clean(li.get_text()))
            if award_lines:
                data["awards"] = "\n".join(award_lines)

            # Liens externes
            ext_section = content_div.find("span", id=re.compile(r"External|Links", re.I))
            if ext_section:
                parent = ext_section.find_parent()
                if parent:
                    sib = parent.find_next_sibling()
                    if sib:
                        for a in sib.find_all("a", href=True):
                            href = a["href"]
                            if href.startswith("http"):
                                lnk_l = href.lower()
                                if "instagram.com" in lnk_l:
                                    socials["instagram"] = href
                                elif "twitter.com" in lnk_l or "x.com" in lnk_l:
                                    socials["twitter"] = href
                                elif "onlyfans.com" in lnk_l:
                                    socials["onlyfans"] = href
                                ext_links.append(href)

        data["discovered_urls"] = list(dict.fromkeys(ext_links))
        data["socials"] = socials
        return data


# ===========================================================================
# SCRAPER XXXBIOS
# ===========================================================================

class XXXBiosScraper(ScraperBase):
    """
    Scrape xxxbios.com
    URL type : https://xxxbios.com/abella-anderson-biography/

    Construction automatique de l'URL √† partir du nom :
        "Abella Anderson" ‚Üí "abella-anderson-biography"
    """

    SOURCE_NAME = "XXXBios"

    @staticmethod
    def _detect_url(url: str) -> bool:
        return "xxxbios.com" in url

    @staticmethod
    def build_url(performer_name: str) -> str:
        slug = re.sub(r"[^a-zA-Z0-9\s]", "", performer_name.lower().strip())
        slug = re.sub(r"\s+", "-", slug)
        return f"https://xxxbios.com/{slug}-biography/"

    def search(self, performer_name: str) -> Optional[str]:
        """Recherche l'URL de la fiche sur XXXBios via le moteur de recherche."""
        if not performer_name:
            return None

        # Nettoyage nom
        query = performer_name.replace(" ", "+")
        search_url = f"https://xxxbios.com/?s={query}"
        
        # On utilise _fetch pour r√©cup√©rer la page de r√©sultats
        soup = _fetch(search_url)
        if not soup:
            return None

        # Les r√©sultats sont g√©n√©ralement dans des titres h2, h3 ou h4 avec class="entry-title"
        # Exemple : <h2 class="entry-title"><a href="...">Performer Name Biography</a></h2>
        candidates = []
        for tag in soup.find_all(["h1", "h2", "h3", "h4"], class_=re.compile(r"entry-title|post-title", re.I)):
            link = tag.find("a", href=True)
            if link:
                candidates.append((link.get_text().strip(), link["href"]))

        # Fallback : liens g√©n√©riques dans le contenu principal si pas de balises titres standards
        if not candidates:
            main_content = soup.find("main") or soup.find("div", id="content") or soup.find("div", class_="site-content")
            if main_content:
                for link in main_content.find_all("a", href=True):
                    candidates.append((link.get_text().strip(), link["href"]))

        performer_lower = performer_name.lower()
        
        # Strat√©gie de s√©lection
        for text, href in candidates:
            t_lower = text.lower()
            # 1. Correspondance forte : contient le nom ET "biography"
            if performer_lower in t_lower and "biography" in t_lower:
                return href
            # 2. Correspondance exacte du nom (ex: "Riley Reid")
            if t_lower == performer_lower:
                return href

        # 3. Essai de correspondance partielle si lien contient "biography"
        for text, href in candidates:
            if performer_lower in text.lower() and "biography" in href:
                return href

        return None

    def _parse(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        data: Dict[str, Any] = {"source": self.SOURCE_NAME, "url": url}
        socials: Dict[str, str] = {}
        ext_links: List[str] = []

        # --- Nom ---
        h1 = soup.find("h1")
        if h1:
            raw_name = _clean(h1.get_text())
            # Enlever le suffixe " Biography" s'il est pr√©sent
            data["name"] = re.sub(r" Biography$", "", raw_name, flags=re.IGNORECASE).strip()

        # --- Infos personnelles (tableau ou liste .personal-info) ---
        info_section = (
            soup.find("div", class_=re.compile(r"personal.?info|bio.?info|profile.?info", re.I))
            or soup.find("table", class_=re.compile(r"personal|bio|profile", re.I))
        )

        def _process_kv(k, v):
            if not v: return
            if k in ("Born", "Date of Birth", "Birthday"):
                data["birthdate"] = v
            elif k in ("Birthplace", "Place of Birth"):
                data["birthplace"] = v
            elif k in ("Height",):
                data["height"] = _extract_cm(v)
            elif k in ("Weight",):
                data["weight"] = _extract_kg(v)
            elif k in ("Measurements",):
                data["measurements"] = v
            elif k in ("Hair Color", "Hair"):
                data["hair_color"] = v
            elif k in ("Eye Color", "Eyes"):
                data["eye_color"] = v
            elif k in ("Boobs", "Breast Size", "Breasts"):
                data["fake_tits"] = v
            elif k in ("Ethnicity",):
                data["ethnicity"] = v
            elif k in ("Nationality", "Country"):
                data["country"] = v
            elif k in ("Years Active", "Career"):
                data["career_length"] = v
            elif k in ("Also Known As", "AKA", "Aliases"):
                aliases = [a.strip() for a in re.split(r"[,\n|]", v) if a.strip()]
                data["aliases"] = aliases

        if info_section:
            rows = info_section.find_all("tr") or info_section.find_all("li")
            for row in rows:
                cells = row.find_all(["th", "td", "span"])
                if len(cells) >= 2:
key="***MASKED***"
                    val = _clean(cells[1].get_text())
                elif row.find("strong") or row.find("b"):
                    lbl = row.find("strong") or row.find("b")
key="***MASKED***"
                    val = _clean(row.get_text().replace(lbl.get_text(), "", 1))
                else:
                    continue
                _process_kv(key, val)
        else:
            # Fallback : Recherche d'un titre "Personal Info" et parsing des paragraphes suivants
            header = soup.find(["h2", "h3", "h4"], string=re.compile(r"Personal Info", re.I))
            if header:
                for sibling in header.find_next_siblings():
                    if sibling.name in ("h2", "h3", "h4", "div", "section"):
                        break
                    text = _clean(sibling.get_text())
                    if ":" in text:
                        parts = text.split(":", 1)
key="***MASKED***"
                        val = _clean(parts[1])
                        _process_kv(key, val)

        # --- Biographie (corps texte) ---
        # Chercher les grandes sections de contenu
        main_content = (
            soup.find("div", class_=re.compile(r"entry-content|post-content|article-content", re.I))
            or soup.find("article")
            or soup.find("div", id=re.compile(r"content", re.I))
        )
        if main_content:
            bio_paragraphs = []
            trivia_lines = []
            award_lines = []
            current_section = "bio"

            for tag in main_content.find_all(["h2", "h3", "h4", "p", "ul", "li"], recursive=True):
                text = _clean(tag.get_text())
                if not text:
                    continue

                if tag.name in ("h2", "h3", "h4"):
                    tl = text.lower()
                    if any(w in tl for w in ("award", "nominat", "accolad", "recognition")):
                        current_section = "awards"
                    elif any(w in tl for w in ("trivia", "fact", "did you know", "interesting")):
                        current_section = "trivia"
                    elif any(w in tl for w in ("career", "early life", "personal", "biography", "about", "social")):
                        current_section = "bio"
                    continue

                if tag.name in ("p",) and len(text) > 50:
                    if current_section == "bio":
                        bio_paragraphs.append(text)
                    elif current_section == "trivia":
                        trivia_lines.append("- " + text)
                    elif current_section == "awards":
                        award_lines.append(text)

                if tag.name == "li":
                    if current_section == "trivia":
                        trivia_lines.append("- " + text)
                    elif current_section == "awards":
                        award_lines.append("- " + text)

            if bio_paragraphs:
                data["bio_raw"] = "\n\n".join(bio_paragraphs[:8])
            if trivia_lines:
                data["trivia"] = "\n".join(trivia_lines[:20])
            if award_lines:
                data["awards"] = "\n".join(award_lines[:50])

            # R√©seaux sociaux (liens dans le contenu)
            for a in main_content.find_all("a", href=True):
                href = a["href"]
                if not href.startswith("http"):
                    continue
                lnk_l = href.lower()
                if "instagram.com" in lnk_l:
                    socials["instagram"] = href
                elif "twitter.com" in lnk_l or "x.com" in lnk_l:
                    socials["twitter"] = href
                elif "onlyfans.com" in lnk_l:
                    socials["onlyfans"] = href
                elif "facebook.com" in lnk_l:
                    socials["facebook"] = href
                elif "brazzers.com" in lnk_l or "naughtyamerica.com" in lnk_l or "digitalplayground.com" in lnk_l:
                    ext_links.append(href)
                elif href.startswith("http") and "xxxbios.com" not in lnk_l:
                    ext_links.append(href)

        # Awards depuis section d√©di√©e (tables)
        if not data.get("awards"):
            award_tables = soup.find_all("table")
            award_rows = []
            for tbl in award_tables:
                caption = tbl.find("caption")
                if caption and re.search(r"award|nominat", caption.get_text(), re.I):
                    for tr in tbl.find_all("tr")[1:]:
                        cells = [_clean(td.get_text()) for td in tr.find_all("td")]
                        if cells:
                            award_rows.append(" - ".join(c for c in cells if c))
            if award_rows:
                data["awards"] = "\n".join(award_rows)

        data["discovered_urls"] = list(dict.fromkeys(ext_links))
        data["socials"] = socials
        return data


# ===========================================================================
# ORCHESTRATEUR
# ===========================================================================

class ScraperOrchestrator:
    """
    Orchestre le scraping depuis plusieurs URLs.
    D√©tecte automatiquement la source selon l'URL.
    """

    def __init__(self):
        self.scrapers = {
            "iafd": IAFDScraper(),
            "freeones": FreeOnesScraper(),
            "thenude": TheNudeScraper(),
            "babepedia": BabepediaScraper(),
            "boobpedia": BoobpediaScraper(),
            "xxxbios": XXXBiosScraper(),
        }

    def detect_source(self, url: str) -> Optional[ScraperBase]:
        """Retourne le scraper appropri√© pour une URL."""
        for scraper in self.scrapers.values():
            if scraper._detect_url(url):
                return scraper
        return None

    def scrape_all(self, urls: List[str], progress_callback=None, performer_name: str = "") -> List[Dict[str, Any]]:
        """
        Scrape toutes les URLs fournies.
        Si performer_name est fourni, auto-construit les URLs Boobpedia et XXXBios
        si elles ne sont pas d√©j√† dans la liste.
        Retourne une liste de dicts (un par source).
        """
        # Auto-d√©couverte Boobpedia / XXXBios si performer_name fourni
        urls_lower = [u.lower() for u in urls]
        extra_urls = []

        if performer_name:
            # Boobpedia : https://www.boobpedia.com/boobs/Firstname_Lastname
            if not any("boobpedia.com" in u for u in urls_lower):
                slug = re.sub(r"\s+", "_", performer_name.strip().title())
                extra_urls.append(f"https://www.boobpedia.com/boobs/{slug}")
            # XXXBios : recherche via le moteur du site
            if not any("xxxbios.com" in u for u in urls_lower):
                found_url = self.scrapers["xxxbios"].search(performer_name)
                if found_url:
                    extra_urls.append(found_url)
                else:
                    # Fallback sur la m√©thode g√©n√©rative si la recherche √©choue
                    # (Utile si le site de recherche est down ou change)
                    extra_urls.append(XXXBiosScraper.build_url(performer_name))

        all_urls = list(urls) + extra_urls

        results = []
        total = len(all_urls)
        for i, url in enumerate(all_urls):
            url = url.strip()
            if not url:
                continue
            scraper = self.detect_source(url)
            if scraper is None:
                continue
            
            if progress_callback:
                progress_callback(i, total, scraper.SOURCE_NAME)
                
            print(f"[ORCHESTRATOR] Scraping {scraper.SOURCE_NAME} : {url}")
            result = scraper.scrape(url)
            if result:
                results.append(result)
        
        if progress_callback:
            progress_callback(total, total, "Termin√©")
            
        return results


# ===========================================================================
# DATA MERGER
# ===========================================================================

class DataMerger:
    """
    Fusionne intelligemment les donn√©es provenant de plusieurs sources.

    Cat√©gories de r√©sultats :
    - confirmed  : m√™me valeur dans ‚â•2 sources
    - new        : valeur pr√©sente dans une seule source
    - conflict   : valeurs diff√©rentes entre sources
    """

    # Champs pour lesquels on accepte des listes (fusion au lieu de conflit)
    LIST_FIELDS = {"aliases", "thenude_tags", "activities", "trivia"}

    # Priorit√© des sources (index bas = priorit√© haute)
    SOURCE_PRIORITY = ["IAFD", "FreeOnes", "Babepedia", "TheNude"]

    # Pour certains champs, on pr√©f√®re d'autres sources qu'IAFD
    FIELD_PRIORITY_OVERRIDE = {
        "ethnicity": ["FreeOnes", "Babepedia", "TheNude", "IAFD"],
        "hair":      ["FreeOnes", "Babepedia", "IAFD", "TheNude"],
        "tattoos":   ["Babepedia", "FreeOnes", "IAFD", "TheNude"],
        "trivia":    ["Babepedia", "TheNude", "IAFD", "FreeOnes"],
        "bio_raw":   ["Babepedia", "IAFD", "FreeOnes", "TheNude"]
    }

    def merge(self, sources: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Fusionne les sources et retourne un dict avec :
        {
            "merged": {...},          # Valeurs fusionn√©es (meilleure source)
            "confirmed": {...},       # Champs confirm√©s par ‚â•2 sources
            "conflicts": {...},       # Champs en conflit {champ: {source: val}}
            "new_fields": {...},      # Champs apparus dans 1 seule source
            "awards": "...",          # Awards bruts depuis IAFD
        }
        """
        if not sources:
            return {}

        # Collecter toutes les valeurs par champ
        field_values: Dict[str, Dict[str, Any]] = {}  # {field: {source_name: value}}

        for src in sources:
            src_name = src.get("source", "unknown")
            for key, val in src.items():
                if key in ("source", "url"):
                    continue
                if key not in field_values:
                    field_values[key] = {}
                field_values[key][src_name] = val

        merged = {}
        confirmed = {}
        conflicts = {}
        new_fields = {}
        awards_text = ""

        for field, source_vals in field_values.items():
            # Awards : champ sp√©cial, on garde la valeur IAFD brute
            if field == "awards":
                awards_text = source_vals.get("IAFD", "")
                continue

            # Champs list : union
            if field in self.LIST_FIELDS:
                all_vals = []
                for v in source_vals.values():
                    if isinstance(v, list):
                        all_vals.extend(v)
                    else:
                        all_vals.append(v)
                # d√©duplication normale mais on veut normaliser les aliases par casse
                if field == 'aliases':
                    normalized = {}
                    for a in all_vals:
                        if not a:
                            continue
key="***MASKED***"
                        if key not in normalized:
                            normalized[key] = a.strip()
                    merged[field] = list(normalized.values())
                else:
                    merged[field] = list(dict.fromkeys(all_vals))  # d√©dupliqu√©, ordre pr√©serv√©
                confirmed[field] = merged[field]
                continue

            unique_vals = list(set(str(v).strip() for v in source_vals.values() if v))
            n_sources = len(source_vals)

            if n_sources == 1:
                # Valeur unique ‚Üí nouvelle donn√©e
                val = list(source_vals.values())[0]
                merged[field] = val
                new_fields[field] = {list(source_vals.keys())[0]: val}

            elif len(unique_vals) == 1:
                # Valeur identique dans plusieurs sources ‚Üí confirm√©e
                merged[field] = unique_vals[0]
                confirmed[field] = unique_vals[0]

            else:
                chosen_val = self._pick_by_priority(source_vals, field=field)
                merged[field] = chosen_val
                conflicts[field] = source_vals

        return {
            "merged": merged,
            "confirmed": confirmed,
            "conflicts": conflicts,
            "new_fields": new_fields,
            "awards": awards_text,
            "socials": self._merge_socials(sources),
            "discovered_urls": self._merge_discovered_urls(sources)
        }

    def _merge_socials(self, sources: List[Dict]) -> Dict[str, str]:
        """Fusionne les r√©seaux sociaux trouv√©s."""
        merged_socials = {}
        for src in sources:
            socials = src.get("socials", {})
            if isinstance(socials, dict):
                for k, v in socials.items():
                    if k not in merged_socials or not merged_socials[k]:
                        merged_socials[k] = v
        return merged_socials

    def _merge_discovered_urls(self, sources: List[Dict]) -> List[str]:
        """Agr√®ge et d√©duplique toutes les URLs d√©couvertes."""
        all_urls = []
        for src in sources:
            urls = src.get("discovered_urls", [])
            if isinstance(urls, list):
                all_urls.extend(urls)
        # D√©dupliquer tout en gardant l'ordre
        return list(dict.fromkeys(all_urls))

    def _pick_by_priority(self, source_vals: Dict[str, Any], field: str = "") -> Any:
        """Choisit la valeur selon la priorit√© des sources (avec override par champ)."""
        priority = self.FIELD_PRIORITY_OVERRIDE.get(field, self.SOURCE_PRIORITY)
        for preferred_source in priority:
            if preferred_source in source_vals and source_vals[preferred_source]:
                return source_vals[preferred_source]
        # Fallback : premi√®re valeur non vide
        for val in source_vals.values():
            if val:
                return val
        return ""

    def format_report(self, merge_result: Dict[str, Any]) -> str:
        """
        G√©n√®re un rapport lisible du r√©sultat de la fusion.
        """
        lines = []
        merged = merge_result.get("merged", {})
        confirmed = merge_result.get("confirmed", {})
        conflicts = merge_result.get("conflicts", {})
        new_fields = merge_result.get("new_fields", {})

        lines.append("=" * 60)
        lines.append("R√âSULTAT DE LA FUSION")
        lines.append("=" * 60)

        lines.append("\n‚úÖ DONN√âES CONFIRM√âES (‚â•2 sources concordantes) :")
        for field, val in confirmed.items():
            lines.append(f"  {field}: {val}")

        if conflicts:
            lines.append("\n‚ö†Ô∏è  CONFLITS (valeurs diff√©rentes entre sources) :")
            for field, src_vals in conflicts.items():
                lines.append(f"  {field}:")
                for src, val in src_vals.items():
                    lines.append(f"    [{src}] {val}")
                lines.append(f"    ‚Üí Retenu: {merged.get(field)}")

        if new_fields:
            lines.append("\nüÜï NOUVELLES DONN√âES (source unique) :")
            for field, src_val in new_fields.items():
                src, val = list(src_val.items())[0]
                lines.append(f"  {field}: {val}  (source: {src})")

        if merge_result.get("awards"):
            lines.append("\nüèÜ AWARDS (depuis IAFD) :")
            lines.append(merge_result["awards"])

        return "\n".join(lines)


# ===========================================================================
# AWARDS CLEANER
# ===========================================================================

class AwardsCleaner:
    """
    Nettoie et formate les awards bruts en format standardis√© :
        CEREMONIE
        ANNEE - Winner/Nominee: Cat√©gorie
    """

    def clean(self, raw_awards: str) -> str:
        """Nettoie le texte brut des awards."""
        if not raw_awards:
            return ""
        lines = [_clean(l) for l in raw_awards.split("\n") if _clean(l)]
        return "\n".join(lines)

    def to_structured_list(self, raw_awards: str) -> List[Dict]:
        """
        Convertit les awards en liste de dicts :
        [{"ceremony": "AVN", "year": "2012", "type": "Winner", "category": "..."}]
        """
        result = []
        current_ceremony = ""
        current_year = ""

        for line in raw_awards.split("\n"):
            line = _clean(line)
            if not line:
                continue
            # C√©r√©monie (ligne sans tiret ni ann√©e isol√©e)
            if re.match(r'^[A-Z][^-\n]+$', line) and not re.match(r'^\d{4}', line) and "-" not in line:
                current_ceremony = line
            # Ligne avec ann√©e et mention
            elif re.match(r'^\d{4}\s*-\s*(Winner|Nominee):', line):
                m = re.match(r'^(\d{4})\s*-\s*(Winner|Nominee):\s*(.+)', line)
                if m:
                    result.append({
                        "ceremony": current_ceremony,
                        "year": m.group(1),
                        "type": m.group(2),
                        "category": m.group(3),
                    })

        return result


# ===========================================================================
# TEST / D√âMO (ex√©cution directe)
# ===========================================================================

if __name__ == "__main__":
    import sys
    import os

    print("=== TEST SCRAPERS DEPUIS FICHIERS LOCAUX ===\n")

    # Chemins vers les fichiers HTML de test
    test_files = {
        "IAFD":      "/tmp/urlscraping/bridgette b - iafd.com.html",
        "FreeOnes":  "/tmp/urlscraping/Bridgette B bio _ Read about her profile at FreeOnes.html",
        "TheNude":   "/tmp/urlscraping/Bridgette B nude from Scoreland and Twistys at theNude.com.html",
        "Babepedia": "/mnt/user-data/uploads/Bridgette_B_-_Free_nude_pics__galleries___more_at_Babepedia.html",
    }

    scrapers_map = {
        "IAFD":      IAFDScraper(),
        "FreeOnes":  FreeOnesScraper(),
        "TheNude":   TheNudeScraper(),
        "Babepedia": BabepediaScraper(),
    }

    results = []
    for source_name, filepath in test_files.items():
        if not os.path.exists(filepath):
            print(f"[SKIP] Fichier manquant : {filepath}")
            continue
        scraper = scrapers_map[source_name]
        with open(filepath, "r", encoding="utf-8", errors="ignore") as f:
            html = f.read()
        result = scraper.scrape_from_html(html, url=f"file://{filepath}")
        results.append(result)
        print(f"\n{'='*50}")
        print(f"SOURCE : {source_name}")
        print(f"{'='*50}")
        for k, v in result.items():
            if k in ("source", "url"):
                continue
            if k == "awards":
                print(f"  awards: [voir section d√©di√©e]")
            elif isinstance(v, list):
                print(f"  {k}: {', '.join(str(x) for x in v)}")
            else:
                print(f"  {k}: {v}")

    # Fusion
    if results:
        print("\n\n" + "="*60)
        print("FUSION DES SOURCES")
        print("="*60)
        merger = DataMerger()
        merged = merger.merge(results)
        print(merger.format_report(merged))

        # Nettoyage awards
        if merged.get("awards"):
            print("\n\n" + "="*60)
            print("AWARDS STRUCTUR√âS")
            print("="*60)
            cleaner = AwardsCleaner()
            structured = cleaner.to_structured_list(merged["awards"])
            for award in structured[:10]:
                status = "üèÜ" if award["type"] == "Winner" else "  "
                print(f"  {status} [{award['ceremony']}] {award['year']} - {award['type']}: {award['category']}")
            if len(structured) > 10:
                print(f"  ... et {len(structured)-10} awards suppl√©mentaires")


============================================================
[109/124] services\source_finder.py
------------------------------------------------------------
"""
source_finder.py - Recherche des URLs manquantes pour les 4 sources
====================================================================

Pour un performer donn√©, d√©tecte quelles sources manquent parmi :
    IAFD, FreeOnes, TheNude, Babepedia

Puis effectue une recherche sur chaque site manquant et propose
les meilleurs candidats √† valider.

Usage standalone :
    python source_finder.py --name "Bridgette B" [--aliases "Spanish Doll"]

Usage depuis StashMaster (import) :
    from source_finder import SourceFinder
    finder = SourceFinder()
    missing  = finder.detect_missing(existing_urls)
    results  = finder.find_missing(name="Bridgette B", missing_sources=missing)
    best     = finder.best_candidates(results)
"""

import re
import time
import unicodedata
import requests
from urllib.parse import quote_plus, urlparse
from bs4 import BeautifulSoup
from typing import Dict, List, Optional
from dataclasses import dataclass, field


# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,*/*;q=0.9",
    "Accept-Language": "en-US,en;q=0.9",
}

TIMEOUT         = 12
DELAY           = 1.0
SCORE_THRESHOLD = 40
ALL_SOURCES     = ["IAFD", "FreeOnes", "TheNude", "Babepedia"]

SOURCE_DOMAINS = {
    "IAFD":      ["iafd.com"],
    "FreeOnes":  ["freeones.com", "freeones.xxx"],
    "TheNude":   ["thenude.com"],
    "Babepedia": ["babepedia.com"],
}

SEARCH_URLS = {
    "IAFD":      "https://www.iafd.com/results.asp?pagetype=person&searchtype=name&sex=f&q={query}",
    "FreeOnes":  "https://www.freeones.com/search?q={query}&t=person",
    "TheNude":   "https://www.thenude.com/srch.php?q={query}&type=model",
    "Babepedia": "https://www.babepedia.com/search/{query}",
}

SEARCH_URLS_ALT = {
    "IAFD":      "https://www.iafd.com/results.asp?pagetype=person&searchtype=name&q={query}",
    "FreeOnes":  "https://www.freeones.com/search?q={query}",
    "TheNude":   "https://www.thenude.com/srch.php?q={query}",
    "Babepedia": "https://www.babepedia.com/search.php?q={query}",
}


# ---------------------------------------------------------------------------
# Mod√®le de donn√©es
# ---------------------------------------------------------------------------

@dataclass
class SearchCandidate:
    source:     str
    url:        str
    found_name: str
    score:      int = 0
    extra:      Dict = field(default_factory=dict)

    @property
    def is_good_match(self):
        return self.score >= SCORE_THRESHOLD

    def __str__(self):
        stars = "‚òÖ" * (self.score // 20)
        return f"[{self.source}] {stars} ({self.score}/100) {self.found_name}\n  ‚Üí {self.url}"


@dataclass
class FinderResult:
    source:     str
    searched:   bool = False
    candidates: List = field(default_factory=list)
    error:      str = ""

    @property
    def best(self):
        valid = [c for c in self.candidates if c.is_good_match]
        return max(valid, key=lambda c: c.score) if valid else None

    @property
    def has_match(self):
        return self.best is not None


# ---------------------------------------------------------------------------
# Utilitaires de scoring
# ---------------------------------------------------------------------------

def _normalize(text):
    text = text.lower().strip()
    text = unicodedata.normalize("NFD", text)
    text = "".join(c for c in text if unicodedata.category(c) != "Mn")
    text = re.sub(r"[^\w\s]", " ", text)
    return re.sub(r"\s+", " ", text).strip()


def _score_name_match(target, found, aliases=None):
    aliases = aliases or []
    t_norm  = _normalize(target)
    f_norm  = _normalize(found)

    if t_norm == f_norm:
        return 100
    for alias in aliases:
        if _normalize(alias) == f_norm:
            return 95
    if t_norm in f_norm or f_norm in t_norm:
        return 80
    for alias in aliases:
        a_norm = _normalize(alias)
        if a_norm in f_norm or f_norm in a_norm:
            return 70
    t_words = set(t_norm.split())
    f_words = set(f_norm.split())
    if t_words and t_words.issubset(f_words):
        return 60
    common = t_words & f_words
    if common:
        return int(30 + (len(common) / max(len(t_words), 1)) * 20)
    return 0


def _extract_domain(url):
    try:
        return urlparse(url).netloc.replace("www.", "").lower()
    except Exception:
        return ""


def _fetch(url, timeout=TIMEOUT):
    try:
        resp = requests.get(url, headers=HEADERS, timeout=timeout, allow_redirects=True)
        if resp.status_code == 200:
            return BeautifulSoup(resp.text, "html.parser")
        return None
    except Exception:
        return None


def _build_search_url(template, query):
    return template.format(query=quote_plus(query))


def _deduplicate(candidates):
    seen = {}
    for c in candidates:
        url = c.url.rstrip("/")
        if url not in seen or c.score > seen[url].score:
            seen[url] = c
    return sorted(seen.values(), key=lambda c: c.score, reverse=True)


# ---------------------------------------------------------------------------
# Parseurs de r√©sultats de recherche
# ---------------------------------------------------------------------------

def _parse_iafd_results(soup, name, aliases):
    candidates = []
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if "person.rme" not in href:
            continue
        if href.startswith("/"):
            href = "https://www.iafd.com" + href
        href = href.rstrip("#")
        found_name = a.get_text(strip=True)
        if not found_name:
            p = a.find_parent()
            found_name = p.get_text(strip=True) if p else ""
        extra = {}
        row = a.find_parent("tr") or a.find_parent("div")
        if row:
            m = re.search(r"\b(\w+ \d+, \d{4}|\d{4}-\d{2}-\d{2})\b",
                          row.get_text(separator="|", strip=True))
            if m:
                extra["birthdate"] = m.group(1)
        score = _score_name_match(name, found_name, aliases)
        if score > 0:
            candidates.append(SearchCandidate(
                source="IAFD", url=href, found_name=found_name,
                score=score, extra=extra))
    return _deduplicate(candidates)


def _parse_freeones_results(soup, name, aliases):
    candidates = []
    excluded = {"performers", "search", "login", "register", "videos",
                "photos", "clips", "blog", "contact", "terms", "privacy"}
    for a in soup.find_all("a", href=True):
        href = a["href"]
        # Pattern : /slug ou /slug/bio (freeones.com)
        if not re.search(r"freeones\.(?:com|xxx)/[a-z0-9-]+(?:/bio)?$", href):
            if not re.match(r"^/[a-z0-9][a-z0-9-]+(?:/bio)?$", href):
                continue
        if href.startswith("/"):
            href = "https://www.freeones.com" + href
        if not href.endswith("/bio"):
            href = href.rstrip("/") + "/bio"
        slug = href.replace("/bio", "").rstrip("/").split("/")[-1]
        if slug in excluded or len(slug) < 3:
            continue
        found_name = a.get_text(strip=True)
        if not found_name:
            img = a.find("img")
            found_name = img.get("alt", "") if img else ""
        if not found_name:
            found_name = slug.replace("-", " ").title()
        score = _score_name_match(name, found_name, aliases)
        if score > 0:
            candidates.append(SearchCandidate(
                source="FreeOnes", url=href, found_name=found_name, score=score))
    return _deduplicate(candidates)


def _parse_thenude_results(soup, name, aliases):
    candidates = []
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if not re.search(r"thenude\.com/[^/]+_\d+\.htm", href):
            if not re.match(r"^/[^/]+_\d+\.htm$", href):
                continue
        # Ignorer liens de pagination (contiennent des query params)
        if "?" in href or "page_nr" in href or "filter_" in href:
            continue
        if href.startswith("/"):
            href = "https://www.thenude.com" + href
        found_name = a.get_text(strip=True)
        if not found_name:
            m = re.search(r"/([^/]+)_\d+\.htm$", href)
            if m:
                found_name = m.group(1).replace("%20", " ").replace("_", " ").strip()
        # Ignorer noms trop courts (ic√¥nes, chiffres de pagination)
        if len(found_name.strip()) < 3:
            continue
        score = _score_name_match(name, found_name, aliases)
        if score > 0:
            candidates.append(SearchCandidate(
                source="TheNude", url=href, found_name=found_name, score=score))
    return _deduplicate(candidates)


def _parse_babepedia_results(soup, name, aliases):
    candidates = []
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if not re.search(r"babepedia\.com/babe/[^/\s#]+$", href):
            if not re.match(r"^/babe/[^/\s#]+$", href):
                continue
        # Ignorer ancres (#menu, #info, etc.)
        if "#" in href:
            continue
        if href.startswith("/"):
            href = "https://www.babepedia.com" + href
        found_name = a.get_text(strip=True)
        if not found_name:
            m = re.search(r"/babe/([^/\s]+)$", href)
            if m:
                found_name = m.group(1).replace("_", " ").strip()
        extra = {}
        card = a.find_parent(["div", "li", "tr"])
        if card:
            m = re.search(r"\b(\d{4})\b", card.get_text(separator=" ", strip=True))
            if m:
                extra["year"] = m.group(1)
        # Ignorer noms trop courts (ic√¥nes)
        if len(found_name.strip()) < 3:
            continue
        score = _score_name_match(name, found_name, aliases)
        if score > 0:
            candidates.append(SearchCandidate(
                source="Babepedia", url=href, found_name=found_name,
                score=score, extra=extra))
    return _deduplicate(candidates)


PARSERS = {
    "IAFD":      _parse_iafd_results,
    "FreeOnes":  _parse_freeones_results,
    "TheNude":   _parse_thenude_results,
    "Babepedia": _parse_babepedia_results,
}


# ---------------------------------------------------------------------------
# Construction d'URLs directes + v√©rification
# ---------------------------------------------------------------------------

def _build_direct_urls(name, aliases):
    all_names = [name] + (aliases or [])
    result = {s: [] for s in ALL_SOURCES}
    for n in all_names:
        n = n.strip()
        perfid = re.sub(r"[^a-z0-9]", "", n.lower())
        slug   = re.sub(r"[^a-z0-9]+", "-", n.lower()).strip("-")
        result["IAFD"].append(
            f"https://www.iafd.com/person.rme/perfid={perfid}/gender=f/{slug}.htm")
        result["FreeOnes"].append(
            f"https://www.freeones.com/{slug}/bio")
        result["Babepedia"].append(
            f"https://www.babepedia.com/babe/{n.replace(' ', '_').replace('.', '')}")
        # TheNude n√©cessite un ID num√©rique ‚Üí pas de construction directe fiable
    return result


def _verify_direct_url(url, source, name, aliases, timeout=TIMEOUT):
    try:
        resp = requests.get(url, headers=HEADERS, timeout=timeout, allow_redirects=True)
        if resp.status_code != 200:
            return None
        soup = BeautifulSoup(resp.text, "html.parser")
        h1 = soup.find("h1")
        found_name = h1.get_text(strip=True) if h1 else ""
        if not found_name:
            title = soup.find("title")
            if title:
                found_name = title.get_text(strip=True).split("-")[0].split("|")[0].strip()
        score = _score_name_match(name, found_name, aliases)
        if score >= SCORE_THRESHOLD:
            return SearchCandidate(
                source=source, url=str(resp.url), found_name=found_name,
                score=score, extra={"method": "direct"})
        return None
    except Exception:
        return None


# ---------------------------------------------------------------------------
# Classe principale
# ---------------------------------------------------------------------------

class SourceFinder:

    def __init__(self, timeout=TIMEOUT, delay=DELAY):
        self.timeout = timeout
        self.delay   = delay

    # ‚îÄ‚îÄ D√©tection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def detect_missing(self, existing_urls):
        """Retourne la liste des sources absentes dans les URLs existantes."""
        present = set()
        for url in existing_urls:
            domain = _extract_domain(url)
            for source, domains in SOURCE_DOMAINS.items():
                if any(d in domain for d in domains):
                    present.add(source)
        return [s for s in ALL_SOURCES if s not in present]

    def sources_status(self, existing_urls):
        """Retourne { source: texte_statut } pour affichage."""
        present_map = {}
        for url in existing_urls:
            domain = _extract_domain(url)
            for source, domains in SOURCE_DOMAINS.items():
                if any(d in domain for d in domains):
                    present_map[source] = url
        status = {}
        for source in ALL_SOURCES:
            if source in present_map:
                status[source] = f"‚úÖ  {present_map[source]}"
            else:
                status[source] = "‚ùå  manquante"
        return status

    # ‚îÄ‚îÄ Recherche pour une source ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def find_for_source(self, source, name, aliases=None):
        aliases = aliases or []
        result  = FinderResult(source=source, searched=True)

        # √âtape 1 : URL directe (rapide, sans recherche)
        for url in _build_direct_urls(name, aliases).get(source, []):
            candidate = _verify_direct_url(url, source, name, aliases, self.timeout)
            if candidate:
                result.candidates.append(candidate)
        if result.candidates:
            return result

        time.sleep(self.delay)

        # √âtape 2 : Page de recherche du site
        candidates = self._search_on_site(source, name, aliases)
        # Essayer avec un alias si peu de r√©sultats
        if len(candidates) < 2 and aliases:
            for alias in aliases[:2]:
                candidates += self._search_on_site(source, alias, aliases)
        result.candidates = _deduplicate(candidates)
        return result

    def _search_on_site(self, source, query, aliases):
        parser = PARSERS.get(source)
        if not parser:
            return []
        soup = _fetch(_build_search_url(SEARCH_URLS[source], query), self.timeout)
        if soup is None and source in SEARCH_URLS_ALT:
            soup = _fetch(_build_search_url(SEARCH_URLS_ALT[source], query), self.timeout)
        if soup is None:
            return []
        return parser(soup, query, aliases)

    # ‚îÄ‚îÄ Recherche toutes sources manquantes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def find_missing(self, name, existing_urls=None, aliases=None,
                     missing_sources=None, progress_callback=None):
        existing_urls   = existing_urls or []
        aliases         = aliases or []
        missing_sources = missing_sources or self.detect_missing(existing_urls)
        results         = {}

        for i, source in enumerate(missing_sources):
            if progress_callback:
                progress_callback(source, None)
            result = self.find_for_source(source, name, aliases)
            results[source] = result
            if progress_callback:
                progress_callback(source, result)
            if i < len(missing_sources) - 1:
                time.sleep(self.delay)

        return results

    # ‚îÄ‚îÄ S√©lection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def best_candidates(self, results):
        return {source: result.best for source, result in results.items()}

    def auto_select_urls(self, results, min_score=80):
        """URLs s√©lectionn√©es automatiquement si score >= min_score."""
        return {
            source: result.best.url
            for source, result in results.items()
            if result.best and result.best.score >= min_score
        }

    # ‚îÄ‚îÄ Rapport ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def build_report(self, name, existing_urls, results):
        lines = [
            "=" * 70,
            f"RECHERCHE DE SOURCES MANQUANTES ‚Äî {name}",
            "=" * 70,
            "\nüìã STATUT DES SOURCES :",
        ]
        for source, st in self.sources_status(existing_urls).items():
            lines.append(f"  {source:12} {st}")

        lines.append("\nüîç R√âSULTATS DE RECHERCHE :")
        for source, result in results.items():
            lines.append(f"\n  ‚îÄ‚îÄ {source} ‚îÄ‚îÄ")
            if result.error:
                lines.append(f"  üí• Erreur : {result.error}")
            elif not result.candidates:
                lines.append("  ‚ùå Aucun candidat trouv√©")
            else:
                for c in result.candidates[:5]:
                    flag = "‚≠ê" if c.score >= 80 else ("üëç" if c.score >= 50 else "‚ùì")
                    lines.append(f"  {flag} [{c.score:3d}/100] {c.found_name}")
                    lines.append(f"       {c.url}")
                    if c.extra.get("birthdate"):
                        lines.append(f"       Naissance : {c.extra['birthdate']}")
                    if c.extra.get("method") == "direct":
                        lines.append(f"       (trouv√© par URL directe)")

        auto = self.auto_select_urls(results)
        if auto:
            lines.append("\n‚úÖ S√âLECTION AUTOMATIQUE (score ‚â• 80) :")
            for source, url in auto.items():
                lines.append(f"  {source:12} {url}")

        no_match = [s for s, r in results.items() if not r.has_match]
        if no_match:
            lines.append(f"\n‚ö†Ô∏è  Non trouv√©es : {', '.join(no_match)}")
            lines.append("   ‚Üí V√©rification manuelle recommand√©e")

        lines.append("\n" + "=" * 70)
        return "\n".join(lines)


# ===========================================================================
# Int√©gration ScraperOrchestrator
# ===========================================================================

def patch_orchestrator_with_finder(orchestrator_class):
    """
    Ajoute find_and_scrape_missing √† ScraperOrchestrator.
    Usage :
        from source_finder import patch_orchestrator_with_finder
        from scrapers import ScraperOrchestrator
        patch_orchestrator_with_finder(ScraperOrchestrator)
        orchestrator = ScraperOrchestrator()
        result = orchestrator.find_and_scrape_missing("Bridgette B", existing_urls)
    """
    def find_and_scrape_missing(self, name, existing_urls,
                                 aliases=None, auto_threshold=80):
        finder  = SourceFinder()
        missing = finder.detect_missing(existing_urls)
        if not missing:
            print("[FINDER] Toutes les sources sont d√©j√† pr√©sentes.")
            return {"missing": [], "found": {}, "new_data": []}

        print(f"[FINDER] Sources manquantes : {', '.join(missing)}")
        search_results = finder.find_missing(
            name=name, existing_urls=existing_urls, aliases=aliases or [])
        auto_urls = finder.auto_select_urls(search_results, min_score=auto_threshold)

        new_data = []
        for source, url in auto_urls.items():
            scraper = self.detect_source(url)
            if scraper:
                print(f"[FINDER] Scraping {source} : {url}")
                data = scraper.scrape(url)
                if data:
                    new_data.append(data)

        return {
            "missing":        missing,
            "found":          auto_urls,
            "new_data":       new_data,
            "search_results": search_results,
        }

    orchestrator_class.find_and_scrape_missing = find_and_scrape_missing
    return orchestrator_class


# ===========================================================================
# Widget Tkinter
# ===========================================================================

class SourceFinderWidget:
    """
    Fen√™tre Tkinter secondaire pour rechercher et s√©lectionner
    les URLs manquantes, int√©grable dans StashMaster.

    Usage :
        widget = SourceFinderWidget(
            parent, name="Bridgette B",
            existing_urls=[...], aliases=[...],
            on_urls_selected=lambda selected_dict: ...
        )
        widget.show()
    """

    def __init__(self, parent, name, existing_urls=None,
                 aliases=None, on_urls_selected=None):
        import tkinter as tk
        from tkinter import ttk, messagebox
        self.tk  = tk
        self.ttk = ttk
        self.messagebox = messagebox

        self.parent           = parent
        self.name             = name
        self.existing_urls    = existing_urls or []
        self.aliases          = aliases or []
        self.on_urls_selected = on_urls_selected
        self.finder           = SourceFinder()
        self.results          = {}
        self._selected_vars   = {}
        self._missing         = []

    def show(self):
        tk  = self.tk
        ttk = self.ttk

        self.win = tk.Toplevel(self.parent)
        self.win.title(f"üîç Sources manquantes ‚Äî {self.name}")
        self.win.geometry("880x600")
        self.win.grab_set()

        # ‚îÄ‚îÄ Header ‚îÄ‚îÄ
        hdr = tk.Frame(self.win, bg="#0d3349", pady=8)
        hdr.pack(fill="x")
        tk.Label(hdr, text="üîç Recherche de sources manquantes",
                 font=("Arial", 12, "bold"), bg="#0d3349", fg="white").pack()
        tk.Label(hdr, text=f"Performer : {self.name}  |  Aliases : {', '.join(self.aliases) or '‚Äî'}",
                 font=("Arial", 9), bg="#0d3349", fg="#aad4f5").pack()

        # ‚îÄ‚îÄ Statut des 4 sources ‚îÄ‚îÄ
        stat_frame = tk.LabelFrame(self.win, text="Statut des sources", padx=8, pady=4)
        stat_frame.pack(fill="x", padx=10, pady=5)

        status = self.finder.sources_status(self.existing_urls)
        self._missing = self.finder.detect_missing(self.existing_urls)
        self._status_labels = {}
        for col, source in enumerate(ALL_SOURCES):
            frm = tk.Frame(stat_frame)
            frm.grid(row=0, column=col, padx=16, pady=2, sticky="w")
            tk.Label(frm, text=source, font=("Arial", 9, "bold")).pack(anchor="w")
            is_present = "‚úÖ" in status[source]
            short = status[source] if is_present else "‚ùå  manquante"
            lbl = tk.Label(frm, text=short[:42], font=("Arial", 8),
                           fg="#2e7d32" if is_present else "#c62828")
            lbl.pack(anchor="w")
            self._status_labels[source] = lbl

        # ‚îÄ‚îÄ Progression ‚îÄ‚îÄ
        self.prog_label = tk.Label(self.win, text="En attente‚Ä¶",
                                   anchor="w", font=("Arial", 9))
        self.prog_label.pack(fill="x", padx=10, pady=(4, 0))
        self.prog_bar = ttk.Progressbar(self.win, mode="determinate")
        self.prog_bar.pack(fill="x", padx=10, pady=(0, 4))

        # ‚îÄ‚îÄ Notebook (onglet par source manquante) ‚îÄ‚îÄ
        nb_frame = tk.LabelFrame(self.win, text="Candidats", padx=6, pady=4)
        nb_frame.pack(fill="both", expand=True, padx=10, pady=2)

        if not self._missing:
            tk.Label(nb_frame,
                     text="‚úÖ Toutes les sources sont d√©j√† pr√©sentes !",
                     font=("Arial", 11), fg="#2e7d32").pack(pady=30)
        else:
            self.notebook = ttk.Notebook(nb_frame)
            self.notebook.pack(fill="both", expand=True)
            self._tabs = {}
            for source in self._missing:
                tab = tk.Frame(self.notebook)
                self.notebook.add(tab, text=f"  {source}  ")
                self._tabs[source] = tab
                tk.Label(tab, text="‚è≥ En attente‚Ä¶",
                         font=("Arial", 9), fg="gray").pack(pady=20)

        # ‚îÄ‚îÄ Boutons ‚îÄ‚îÄ
        btn_frame = tk.Frame(self.win, pady=6)
        btn_frame.pack(fill="x", padx=10)

        self.btn_search = ttk.Button(
            btn_frame, text="üîç Lancer la recherche",
            command=self._start_search,
            state="normal" if self._missing else "disabled")
        self.btn_search.pack(side="left", padx=4)

        self.btn_apply = ttk.Button(
            btn_frame, text="‚úÖ Ajouter les URLs s√©lectionn√©es",
            command=self._apply, state="disabled")
        self.btn_apply.pack(side="left", padx=4)

        self.result_count_lbl = tk.Label(btn_frame, text="", fg="#1565c0",
                                          font=("Arial", 9, "bold"))
        self.result_count_lbl.pack(side="left", padx=10)

        ttk.Button(btn_frame, text="Fermer",
                   command=self.win.destroy).pack(side="right", padx=4)

    # ‚îÄ‚îÄ Logique de recherche ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _start_search(self):
        import threading
        self.btn_search.config(state="disabled")
        self.btn_apply.config(state="disabled")
        self.prog_bar.config(maximum=max(len(self._missing), 1), value=0)
        self._done = 0
        self.results = {}

        def run():
            for source in self._missing:
                self.win.after(0, self.prog_label.config,
                               {"text": f"üîÑ Recherche {source}‚Ä¶"})
                result = self.finder.find_for_source(
                    source, self.name, self.aliases)
                self.results[source] = result
                self._done += 1
                self.win.after(0, self._on_source_done, source, result, self._done)
            self.win.after(0, self._search_done)

        threading.Thread(target=run, daemon=True).start()

    def _on_source_done(self, source, result, done):
        self.prog_bar["value"] = done
        self._render_tab(source, result)

    def _render_tab(self, source, result):
        tk  = self.tk
        ttk = self.ttk
        tab = self._tabs.get(source)
        if not tab:
            return
        for w in tab.winfo_children():
            w.destroy()

        if not result.candidates:
            tk.Label(tab, text="‚ùå Aucun r√©sultat ‚Äî recherche manuelle requise",
                     font=("Arial", 9), fg="#c62828").pack(pady=15)
            return

        # Variable radio pour s√©lection
        var = tk.StringVar(value=result.best.url if result.best else "")
        self._selected_vars[source] = var

        # En-t√™te colonnes
        hdr = tk.Frame(tab, bg="#e3f2fd")
        hdr.pack(fill="x", padx=4, pady=(4, 0))
        for txt, w in [("‚úî", 4), ("Score", 7), ("Nom trouv√©", 22), ("URL", 0)]:
            tk.Label(hdr, text=txt, width=w, bg="#e3f2fd",
                     font=("Arial", 8, "bold"), anchor="w").pack(side="left")

        # Scroll area
        canvas = tk.Canvas(tab, height=210, highlightthickness=0)
        sb = ttk.Scrollbar(tab, orient="vertical", command=canvas.yview)
        inner = tk.Frame(canvas)
        inner.bind("<Configure>",
                   lambda e: canvas.configure(scrollregion=canvas.bbox("all")))
        canvas.create_window((0, 0), window=inner, anchor="nw")
        canvas.configure(yscrollcommand=sb.set)
        sb.pack(side="right", fill="y")
        canvas.pack(side="left", fill="both", expand=True, padx=4)

        for c in result.candidates[:12]:
            color = "#1b5e20" if c.score >= 80 else (
                    "#e65100" if c.score >= 50 else "#757575")
            method_tag = " üìédirect" if c.extra.get("method") == "direct" else ""
            row = tk.Frame(inner, bd=0)
            row.pack(fill="x", pady=1)
            ttk.Radiobutton(row, variable=var, value=c.url).pack(side="left")
            tk.Label(row, text=f"{c.score}/100", width=7,
                     fg=color, font=("Arial", 8, "bold")).pack(side="left")
            tk.Label(row, text=(c.found_name[:22] + method_tag)[:26], width=26,
                     anchor="w", font=("Arial", 8)).pack(side="left")
            tk.Label(row, text=c.url[:68], anchor="w",
                     font=("Arial", 8), fg="#0d47a1",
                     cursor="hand2").pack(side="left", fill="x")

        # Note si aucun bon score
        if not result.has_match:
            tk.Label(tab,
                     text="‚ö†Ô∏è  Aucun r√©sultat fiable ‚Äî s√©lectionner manuellement ou ignorer",
                     font=("Arial", 8), fg="#e65100").pack(pady=3)

    def _search_done(self):
        found = sum(1 for r in self.results.values() if r.has_match)
        self.prog_label.config(
            text=f"‚úÖ Termin√© ‚Äî {found}/{len(self._missing)} source(s) trouv√©e(s)")
        self.btn_search.config(state="normal")
        self.result_count_lbl.config(
            text=f"{found} nouvelle(s) source(s) trouv√©e(s)" if found else "")
        if found:
            self.btn_apply.config(state="normal")

    def _apply(self):
        selected = {s: v.get() for s, v in self._selected_vars.items() if v.get()}
        if not selected:
            self.messagebox.showwarning("Aucune s√©lection",
                                        "S√©lectionnez au moins une URL.")
            return
        msg = "Ajouter ces URLs ?\n\n" + "\n".join(
            f"  [{s}]  {u}" for s, u in selected.items())
        if not self.messagebox.askyesno("Confirmer", msg):
            return
        if self.on_urls_selected:
            self.on_urls_selected(selected)
        self.messagebox.showinfo("Succ√®s",
                                  f"{len(selected)} URL(s) ajout√©e(s).")
        self.win.destroy()


# ===========================================================================
# CLI
# ===========================================================================

def _cli():
    import argparse, sys

    parser = argparse.ArgumentParser(
        description="Recherche les URLs manquantes pour un performer Stash")
    parser.add_argument("--name",          required=True)
    parser.add_argument("--aliases",       nargs="*", default=[])
    parser.add_argument("--existing-urls", nargs="*", default=[])
    parser.add_argument("--sources",       nargs="*", choices=ALL_SOURCES)
    parser.add_argument("--timeout",       type=int, default=TIMEOUT)
    parser.add_argument("--min-score",     type=int, default=SCORE_THRESHOLD)
    args = parser.parse_args()

    finder = SourceFinder(timeout=args.timeout)

    print(f"\nPerformer : {args.name}")
    if args.aliases:
        print(f"Aliases   : {', '.join(args.aliases)}")
    print()
    for s, st in finder.sources_status(args.existing_urls).items():
        print(f"  {s:12} {st}")

    missing = args.sources or finder.detect_missing(args.existing_urls)
    if not missing:
        print("\n‚úÖ Toutes les sources sont d√©j√† pr√©sentes !")
        sys.exit(0)

    print(f"\nRecherche pour : {', '.join(missing)}")
    print("‚îÄ" * 60)

    def progress(source, result):
        if result is None:
            print(f"  üîÑ {source}‚Ä¶", flush=True)
        elif result.has_match:
            print(f"  ‚úÖ {source} : {len(result.candidates)} candidat(s)")
        else:
            print(f"  ‚ùå {source} : aucun r√©sultat")

    results = finder.find_missing(
        name=args.name, existing_urls=args.existing_urls,
        aliases=args.aliases, missing_sources=missing,
        progress_callback=progress)

    print()
    print(finder.build_report(args.name, args.existing_urls, results))


if __name__ == "__main__":
    _cli()


============================================================
[110/124] services\url_manager.py
------------------------------------------------------------
import re
import requests
import time
from typing import List, Dict, Optional, Tuple
from urllib.parse import urlparse

# Import des scrapers
from services.scrapers import (
    IAFDScraper, FreeOnesScraper, TheNudeScraper, 
    BabepediaScraper, BoobpediaScraper, XXXBiosScraper,
    _fetch, HEADERS, TIMEOUT
)

class URLManager:
    """
    G√®re la v√©rification, le tri et l'acquisition des URLs prioritaires.
    
    Ordre de priorit√© :
    1. IAFD
    2. FreeOnes
    3. TheNude
    4. Babepedia
    5. Boobpedia
    6. XXXBios
    """
    
    PRIORITY_ORDER = [
        ("iafd.com", IAFDScraper),
        ("freeones.xxx", FreeOnesScraper),
        ("thenude.com", TheNudeScraper),
        ("babepedia.com", BabepediaScraper),
        ("boobpedia.com", BoobpediaScraper),
        ("xxxbios.com", XXXBiosScraper)
    ]

    def __init__(self):
        # Initialisation des scrapers
        self.scrapers = {
            "iafd.com": IAFDScraper(),
            "freeones.xxx": FreeOnesScraper(),
            "thenude.com": TheNudeScraper(),
            "babepedia.com": BabepediaScraper(),
            "boobpedia.com": BoobpediaScraper(),
            "xxxbios.com": XXXBiosScraper()
        }

    def process_performer_urls(self, existing_urls: List[str], performer_name: str, progress_callback=None) -> List[str]:
        """
        Processus complet :
        1. Identification des URLs prioritaires pr√©sentes
        2. Validation de ces URLs (ping)
        3. Recherche/Acquisition des URLs manquantes
        4. Nettoyage et validation des autres URLs
        5. Retourne la liste finale tri√©e (Max 50)
        """
        if progress_callback:
            progress_callback(0, "Analyse des URLs existantes...")

        # 1. Structure pour stocker les URLs prioritaires (index 0 √† 5)
        priority_slots: List[Optional[str]] = [None] * 6
        other_urls: List[str] = []
        
        # Mapping domain -> index
        domain_to_index = {
            "iafd.com": 0,
            "freeones.xxx": 1, 
            "thenude.com": 2,
            "babepedia.com": 3,
            "boobpedia.com": 4,
            "xxxbios.com": 5
        }

        # 2. Tri des URLs existantes
        for url in existing_urls:
            url = url.strip()
            if not url:
                continue
                
            domain = self.get_domain_key(url)
            
            # V√©rifier si c'est un domaine prioritaire
            index = domain_to_index.get(domain)
            
            if index is not None:
                # Si le slot est vide, on prend. Si d√©j√† pris, on garde le "meilleur" (√† impl√©menter si besoin)
                # Ici on prend le premier trouv√© qui semble √™tre un profil
                if priority_slots[index] is None:
                     if self.is_profile_url(url, domain):
                         priority_slots[index] = url
                     else:
                         other_urls.append(url)
                else:
                    # Doublon pour ce domaine, on l'ajoute aux autres pour l'instant
                    other_urls.append(url)
            else:
                other_urls.append(url)

        # 3. Validation et Acquisition des manquants
        for i, (domain, scraper_class) in enumerate(self.PRIORITY_ORDER):
            if progress_callback:
                progress_callback(int((i / 6) * 50), f"V√©rification {domain}...")

            current_url = priority_slots[i]
            
            # A. Validation si pr√©sent
            if current_url:
                if not self.is_url_reachable(current_url):
                    print(f"URL morte d√©tect√©e : {current_url}")
                    priority_slots[i] = None # On le supprime pour forcer la recherche
                    # On ne l'ajoute pas √† other_urls car morte
            
            # B. Acquisition si manquant (ou devenu manquant apr√®s validation)
            if priority_slots[i] is None:
                if progress_callback:
                    progress_callback(int((i / 6) * 50) + 5, f"Recherche {domain}...")
                
                found_url = self.search_url_for_domain(domain, performer_name)
                if found_url:
                    priority_slots[i] = found_url

        # 4. Nettoyage et validation des autres URLs
        if progress_callback:
            progress_callback(60, "Validation des autres URLs...")
            
        validated_others = []
        unique_seen = set()
        
        # Ajouter les prioritaires au set pour √©viter doublons
        for purl in priority_slots:
            if purl:
                unique_seen.add(purl)

        for url in other_urls:
            if url in unique_seen:
                continue
            
            if self.is_url_reachable(url):
                validated_others.append(url)
                unique_seen.add(url)
        
        # 5. Construction liste finale
        final_list = []
        
        # Ajouter les prioritaires dans l'ordre (1 √† 6)
        for url in priority_slots:
            if url:
                final_list.append(url)
                
        # Ajouter le reste
        final_list.extend(validated_others)
        
        # Limiter √† 50
        return final_list[:50]

    def get_domain_key(self, url: str) -> str:
        """Normalise le domaine pour correspondre aux cl√©s."""
        try:
            parsed = urlparse(url)
            netloc = parsed.netloc.lower()
            if "iafd.com" in netloc: return "iafd.com"
            if "freeones" in netloc: return "freeones.xxx" # G√®re freeones.com, freeones.xxx
            if "thenude.com" in netloc: return "thenude.com"
            if "babepedia.com" in netloc: return "babepedia.com"
            if "boobpedia.com" in netloc: return "boobpedia.com"
            if "xxxbios.com" in netloc: return "xxxbios.com"
            return netloc
        except:
            return ""

    def is_profile_url(self, url: str, domain: str) -> bool:
        """V√©rifie sommairement si l'URL ressemble √† une page profil."""
        # Logique simplifi√©e, peut √™tre affin√©e par scraper
        if domain == "iafd.com" and ("person.rme" in url or "person.rvm" in url):
            return True
        if domain == "freeones.xxx" and "/feed/" not in url: return True
        if domain == "thenude.com" and "_" in url: return True
        if domain == "babepedia.com" and "/babe/" in url: return True
        if domain == "boobpedia.com" and "/boobs/" in url: return True
        # XXXBios : doit avoir le pattern exact slug-biography (pas juste -biography)
        if domain == "xxxbios.com":
            # Pattern plus strict : xxxbios.com/[nom-artiste]-biography/
            # Rejette les pages comme xxxbios.com/category/biography
            import re
            # Accepte les slugs avec lettres, chiffres et tirets, se terminant par -biography
            if re.search(r'/[a-z0-9][a-z0-9-]*-biography/?$', url, re.I):
                return True
            return False
        return True 

    def is_url_reachable(self, url: str) -> bool:
        """V√©rifie si l'URL r√©pond (code 200)."""
        try:
            # Headers enrichis pour √©viter les blocages
            headers = {
                **HEADERS,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Encoding": "gzip, deflate",
                "Connection": "keep-alive",
            }
domain_key="***MASKED***"
            
            # IAFD et Babepedia bloquent les bots (Cloudflare) ‚Üí on accepte 403 si l'URL correspond au pattern
            if domain_key in ("iafd.com", "babepedia.com"):
                if self.is_profile_url(url, domain_key):
                    # URL valide par structure ‚Üí on l'accepte m√™me si 403
                    print(f"[URLManager] {domain_key} accept√© par pattern : {url}")
                    return True
                # Si ce n'est pas un profil valide, on teste quand m√™me
                resp = requests.get(url, headers=headers, timeout=10, stream=True, allow_redirects=True)
                resp.close()
                print(f"[URLManager] {domain_key} GET ‚Üí {resp.status_code}")
                # Accepter 200 ou 403 (403 = Cloudflare mais page existe)
                return resp.status_code in (200, 403)
            
            # Pour les autres : HEAD puis GET en fallback
            resp = requests.head(url, headers=headers, timeout=10, allow_redirects=True)
            
            if resp.status_code == 200:
                return True
                
            # Si HEAD √©choue (405, 403, 404), fallback sur GET
            if resp.status_code in (405, 403, 404):
                resp = requests.get(url, headers=headers, timeout=10, stream=True, allow_redirects=True)
                resp.close()
                return resp.status_code == 200
                
            return False
        except Exception as e:
            print(f"[URLManager] Erreur v√©rification {url}: {type(e).__name__}: {e}")
            return False

    def search_url_for_domain(self, domain: str, name: str) -> Optional[str]:
        """Tente de trouver l'URL manquante via le scraper associ√©."""
        scraper = self.scrapers.get(domain)
        if not scraper:
            return None
            
        try:
            # 1. Si le scraper a une m√©thode 'search' (impl√©ment√©e pour XXXBios)
            if hasattr(scraper, "search"):
                return scraper.search(name)
            
            # 2. Construction d'URL (Boobpedia, XXXBios fallback)
            # XXXBiosScraper a build_url, BoobpediaScraper a build_url aussi ?
            # V√©rifions les scrapers un par un ou utilisons une logique g√©n√©rique
            
            if domain == "boobpedia.com":
                 slug = re.sub(r"\s+", "_", name.strip().title())
                 url = f"https://www.boobpedia.com/boobs/{slug}"
                 if self.is_url_reachable(url):
                     return url

            if domain == "xxxbios.com":
                 # Fallback si search n'a pas march√© (mais search est appel√© ci-dessus si dispo)
                 if hasattr(scraper, "build_url"):
                     url = scraper.build_url(name)
                     if self.is_url_reachable(url):
                         return url

            # 3. Pour IAFD, FreeOnes, TheNude, Babepedia
            # Id√©alement on utiliserait une recherche Google ou interne
            # Pour l'instant on retourne None si pas de m√©thode de recherche explicite
            # TODO: Impl√©menter recherche IAFD/FreeOnes
            
            return None
            
        except Exception as e:
            print(f"Erreur recherche {domain} pour {name}: {e}")
            return None



============================================================
[111/124] services\url_validator.py
------------------------------------------------------------
"""
url_validator.py - Validation et nettoyage des URLs de performers Stash
=======================================================================

Fonctionnement :
  1. Lit les URLs depuis la table `performer_urls` de la base SQLite Stash
  2. Teste chaque URL (HEAD puis GET fallback) avec timeout court
  3. Classifie : ‚úÖ Active / ‚ùå Morte / ‚ö†Ô∏è Redirig√©e / ‚è≠Ô∏è Ignor√©e
  4. Propose la suppression des URLs mortes (avec confirmation ou mode auto)
  5. √âcrit les suppressions dans la BDD ET met √† jour Stash via GraphQL

Usage standalone :
    python url_validator.py --db "H:/Stash/stash-go.sqlite" [--auto-delete] [--dry-run]

Usage depuis StashMaster (import) :
    from url_validator import URLValidator
    validator = URLValidator(db_path="...", stash_url="http://localhost:9999")
    results = validator.validate_all(performer_id=42)
    validator.delete_dead_urls(results, confirm=True)
"""

import sqlite3
import requests
import threading
import time
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple
from enum import Enum


# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

DEFAULT_DB_PATH     = "H:/Stash/stash-go.sqlite"
DEFAULT_STASH_URL   = "http://localhost:9999"
DEFAULT_TIMEOUT     = 10       # secondes par requ√™te
DEFAULT_MAX_WORKERS = 8        # requ√™tes parall√®les
DEFAULT_RETRY       = 1        # nb de retry en cas d'√©chec r√©seau

# Domaines √† ne jamais supprimer m√™me si inaccessibles (VPN, abonnement, etc.)
WHITELIST_DOMAINS = {
    "onlyfans.com",
    "fansly.com",
    "manyvids.com",
    "loyalfans.com",
    "fancentro.com",
    "4based.com",
    "unlockd.com",
}

# Codes HTTP consid√©r√©s comme "vivants" (l'URL existe)
ALIVE_CODES = {200, 201, 202, 203, 204, 206, 301, 302, 303, 307, 308}
# Codes consid√©r√©s comme "mortes" (contenu supprim√©/inexistant)
DEAD_CODES  = {404, 410, 451}
# Codes ambigus (acc√®s refus√© mais la page peut exister)
AMBIGUOUS_CODES = {401, 403, 429, 503}

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,*/*;q=0.9",
    "Accept-Language": "en-US,en;q=0.9",
}


# ---------------------------------------------------------------------------
# Mod√®le de donn√©es
# ---------------------------------------------------------------------------

class URLStatus(Enum):
    ACTIVE    = "active"      # ‚úÖ URL vivante
    DEAD      = "dead"        # ‚ùå URL morte (404/410 confirm√©)
    REDIRECT  = "redirect"    # ‚ö†Ô∏è Redirig√©e (301/302)
    AMBIGUOUS = "ambiguous"   # ‚ùì Acc√®s refus√© / timeout (on garde)
    WHITELISTED = "whitelisted"  # ‚è≠Ô∏è Domaine whitelist (pas de test)
    ERROR     = "error"       # üí• Erreur r√©seau inattendue


@dataclass
class URLCheckResult:
    performer_id: int
    performer_name: str
    position: int
    url: str
    status: URLStatus
    http_code: Optional[int] = None
    redirect_to: Optional[str] = None
    error_msg: Optional[str] = None
    check_duration_ms: int = 0
    domain: str = field(init=False)

    def __post_init__(self):
        self.domain = _extract_domain(self.url)

    @property
    def should_delete(self) -> bool:
        return self.status == URLStatus.DEAD

    @property
    def icon(self) -> str:
        icons = {
            URLStatus.ACTIVE:      "‚úÖ",
            URLStatus.DEAD:        "‚ùå",
            URLStatus.REDIRECT:    "‚Ü™Ô∏è ",
            URLStatus.AMBIGUOUS:   "‚ùì",
            URLStatus.WHITELISTED: "‚è≠Ô∏è ",
            URLStatus.ERROR:       "üí•",
        }
        return icons.get(self.status, "?")

    def __str__(self) -> str:
        base = f"{self.icon} [{self.http_code or '---'}] {self.url}"
        if self.redirect_to:
            base += f"\n     ‚Üí {self.redirect_to}"
        if self.error_msg:
            base += f"\n     ! {self.error_msg}"
        return base


# ---------------------------------------------------------------------------
# Utilitaires
# ---------------------------------------------------------------------------

def _extract_domain(url: str) -> str:
    """Extrait le domaine d'une URL."""
    m = re.search(r'https?://(?:www\.)?([^/]+)', url)
    return m.group(1).lower() if m else ""


def _is_whitelisted(url: str) -> bool:
    """Retourne True si le domaine est dans la whitelist."""
    domain = _extract_domain(url)
    return any(w in domain for w in WHITELIST_DOMAINS)


def _check_single_url(url: str, timeout: int = DEFAULT_TIMEOUT,
                       retry: int = DEFAULT_RETRY) -> Tuple[Optional[int], Optional[str], str]:
    """
    V√©rifie une URL. Retourne (http_code, redirect_url, error_msg).
    Essaie d'abord HEAD, puis GET si HEAD √©choue ou donne un r√©sultat suspect.
    """
    last_error = ""
    for attempt in range(retry + 1):
        try:
            # 1. Tentative HEAD (plus rapide, pas de body)
            resp = requests.head(
                url, headers=HEADERS, timeout=timeout,
                allow_redirects=True
            )
            code = resp.status_code
            redirect = str(resp.url) if resp.url and str(resp.url) != url else None

            # Certains serveurs renvoient 405 sur HEAD ‚Üí fallback GET
            if code in (405, 501):
                raise requests.exceptions.InvalidSchema("HEAD not allowed")

            return code, redirect, ""

        except requests.exceptions.InvalidSchema:
            # Fallback GET
            try:
                resp = requests.get(
                    url, headers=HEADERS, timeout=timeout,
                    allow_redirects=True, stream=True
                )
                resp.close()
                code = resp.status_code
                redirect = str(resp.url) if resp.url and str(resp.url) != url else None
                return code, redirect, ""
            except Exception as e2:
                last_error = str(e2)

        except requests.exceptions.ConnectionError as e:
            last_error = f"ConnectionError: {e}"
        except requests.exceptions.Timeout:
            last_error = f"Timeout ({timeout}s)"
        except requests.exceptions.TooManyRedirects:
            return None, None, "TooManyRedirects"
        except Exception as e:
            last_error = str(e)

        if attempt < retry:
            time.sleep(1)

    return None, None, last_error


# ---------------------------------------------------------------------------
# Classe principale
# ---------------------------------------------------------------------------

class URLValidator:
    """
    Valide les URLs des performers stock√©es dans la BDD Stash.

    Param√®tres
    ----------
    db_path   : chemin vers stash-go.sqlite
    stash_url : URL de l'API Stash (pour les suppressions GraphQL)
    api_key   : cl√© API Stash si authentification requise
    timeout   : timeout HTTP en secondes
    max_workers : nombre de threads parall√®les
    """

    def __init__(
        self,
        db_path: str = DEFAULT_DB_PATH,
        stash_url: str = DEFAULT_STASH_URL,
        api_key: str = "",
        timeout: int = DEFAULT_TIMEOUT,
        max_workers: int = DEFAULT_MAX_WORKERS,
    ):
        self.db_path = db_path
        self.stash_url = stash_url.rstrip("/")
        self.api_key = api_key
        self.timeout = timeout
        self.max_workers = max_workers
        self._lock = threading.Lock()

    # ------------------------------------------------------------------
    # Lecture BDD
    # ------------------------------------------------------------------

    def _get_connection(self) -> sqlite3.Connection:
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def get_all_performer_urls(
        self, performer_id: Optional[int] = None
    ) -> List[Dict]:
        """
        Retourne toutes les URLs de performers depuis performer_urls.
        Si performer_id est fourni, filtre sur ce performer.
        """
        conn = self._get_connection()
        try:
            if performer_id:
                rows = conn.execute(
                    """
                    SELECT pu.performer_id, p.name, pu.position, pu.url
                    FROM performer_urls pu
                    JOIN performers p ON p.id = pu.performer_id
                    WHERE pu.performer_id = ?
                    ORDER BY pu.performer_id, pu.position
                    """,
                    (performer_id,)
                ).fetchall()
            else:
                rows = conn.execute(
                    """
                    SELECT pu.performer_id, p.name, pu.position, pu.url
                    FROM performer_urls pu
                    JOIN performers p ON p.id = pu.performer_id
                    ORDER BY pu.performer_id, pu.position
                    """
                ).fetchall()
            return [dict(r) for r in rows]
        finally:
            conn.close()

    def get_performer_count(self) -> Tuple[int, int]:
        """Retourne (nb performers avec URLs, nb total d'URLs)."""
        conn = self._get_connection()
        try:
            nb_urls = conn.execute("SELECT COUNT(*) FROM performer_urls").fetchone()[0]
            nb_perf = conn.execute(
                "SELECT COUNT(DISTINCT performer_id) FROM performer_urls"
            ).fetchone()[0]
            return nb_perf, nb_urls
        finally:
            conn.close()

    # ------------------------------------------------------------------
    # V√©rification HTTP
    # ------------------------------------------------------------------

    def _check_url_entry(self, entry: Dict) -> URLCheckResult:
        """V√©rifie une entr√©e URL et retourne un URLCheckResult."""
        url = entry["url"]
        result_base = dict(
            performer_id=entry["performer_id"],
            performer_name=entry["name"],
            position=entry["position"],
            url=url,
        )

        # Whitelist
        if _is_whitelisted(url):
            return URLCheckResult(
                **result_base,
                status=URLStatus.WHITELISTED,
            )

        # V√©rification HTTP
        t0 = time.monotonic()
        code, redirect, error = _check_single_url(url, self.timeout)
        elapsed_ms = int((time.monotonic() - t0) * 1000)

        if error and code is None:
            # Timeout ou erreur r√©seau ‚Üí ambigu√Øt√©, on ne supprime pas
            return URLCheckResult(
                **result_base,
                status=URLStatus.ERROR,
                error_msg=error,
                check_duration_ms=elapsed_ms,
            )

        if code in DEAD_CODES:
            status = URLStatus.DEAD
        elif code in AMBIGUOUS_CODES:
            status = URLStatus.AMBIGUOUS
        elif code in (301, 302, 303, 307, 308) and redirect and redirect != url:
            status = URLStatus.REDIRECT
        elif code and code < 400:
            status = URLStatus.ACTIVE
        else:
            status = URLStatus.AMBIGUOUS  # code inconnu ‚Üí on garde

        return URLCheckResult(
            **result_base,
            status=status,
            http_code=code,
            redirect_to=redirect if redirect != url else None,
            check_duration_ms=elapsed_ms,
        )

    def validate_urls(
        self,
        urls: List[Dict],
        progress_callback=None,
    ) -> List[URLCheckResult]:
        """
        Valide une liste d'entr√©es URL en parall√®le.
        progress_callback(current, total, result) est appel√© apr√®s chaque check.
        """
        results = []
        total = len(urls)

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(self._check_url_entry, entry): entry for entry in urls}
            done = 0
            for future in as_completed(futures):
                result = future.result()
                with self._lock:
                    results.append(result)
                    done += 1
                if progress_callback:
                    progress_callback(done, total, result)

        # Tri : performer_id puis position
        results.sort(key=lambda r: (r.performer_id, r.position))
        return results

    def validate_all(
        self,
        performer_id: Optional[int] = None,
        progress_callback=None,
    ) -> List[URLCheckResult]:
        """
        Valide toutes les URLs (ou celles d'un seul performer).
        """
        entries = self.get_all_performer_urls(performer_id)
        if not entries:
            return []
        return self.validate_urls(entries, progress_callback)

    # ------------------------------------------------------------------
    # Suppression des URLs mortes
    # ------------------------------------------------------------------

    def delete_dead_urls_from_db(self, results: List[URLCheckResult]) -> int:
        """
        Supprime les URLs mortes directement dans la BDD SQLite.
        Retourne le nombre de suppressions effectu√©es.
        """
        dead = [r for r in results if r.should_delete]
        if not dead:
            return 0

        conn = self._get_connection()
        count = 0
        try:
            for r in dead:
                conn.execute(
                    "DELETE FROM performer_urls WHERE performer_id = ? AND url = ?",
                    (r.performer_id, r.url)
                )
                # R√©indexer les positions restantes pour ce performer
                conn.execute(
                    """
                    UPDATE performer_urls
                    SET position = (
                        SELECT COUNT(*) FROM performer_urls pu2
                        WHERE pu2.performer_id = performer_urls.performer_id
                          AND pu2.position < performer_urls.position
                    )
                    WHERE performer_id = ?
                    """,
                    (r.performer_id,)
                )
                count += 1
            conn.commit()
        except Exception as e:
            conn.rollback()
            raise RuntimeError(f"Erreur suppression BDD : {e}") from e
        finally:
            conn.close()

        return count

    def delete_dead_urls_via_graphql(self, results: List[URLCheckResult]) -> Dict:
        """
        Supprime les URLs mortes via l'API GraphQL de Stash.
        Cette m√©thode est pr√©f√©r√©e si Stash tourne (mise √† jour propre).
        Retourne un dict {performer_id: success/error}.
        """
        # Grouper les URLs mortes par performer
        dead_by_performer: Dict[int, List[URLCheckResult]] = {}
        for r in results:
            if r.should_delete:
                dead_by_performer.setdefault(r.performer_id, []).append(r)

        if not dead_by_performer:
            return {}

        # Pour chaque performer, r√©cup√©rer ses URLs actuelles et retirer les mortes
        report = {}
        for performer_id, dead_list in dead_by_performer.items():
            dead_urls_set = {r.url for r in dead_list}
            try:
                # 1. R√©cup√©rer les URLs actuelles via GraphQL
                current_urls = self._gql_get_performer_urls(performer_id)
                # 2. Filtrer les mortes
                new_urls = [u for u in current_urls if u not in dead_urls_set]
                # 3. Mettre √† jour via mutation
                self._gql_update_performer_urls(performer_id, new_urls)
                report[performer_id] = {
                    "status": "ok",
                    "removed": list(dead_urls_set),
                    "remaining": new_urls,
                }
            except Exception as e:
                report[performer_id] = {"status": "error", "error": str(e)}

        return report

    def _gql_headers(self) -> Dict:
        h = {"Content-Type": "application/json"}
        if self.api_key:
            h["ApiKey"] = self.api_key
        return h

    def _gql_get_performer_urls(self, performer_id: int) -> List[str]:
        """R√©cup√®re la liste des URLs d'un performer via GraphQL."""
        query = """
        query FindPerformer($id: ID!) {
          findPerformer(id: $id) {
            urls
          }
        }
        """
        resp = requests.post(
            f"{self.stash_url}/graphql",
            json={"query": query, "variables": {"id": str(performer_id)}},
            headers=self._gql_headers(),
            timeout=10,
        )
        resp.raise_for_status()
        data = resp.json()
        return data.get("data", {}).get("findPerformer", {}).get("urls", [])

    def _gql_update_performer_urls(self, performer_id: int, urls: List[str]) -> bool:
        """Met √† jour les URLs d'un performer via GraphQL."""
        mutation = """
        mutation PerformerUpdate($input: PerformerUpdateInput!) {
          performerUpdate(input: $input) {
            id
            urls
          }
        }
        """
        resp = requests.post(
            f"{self.stash_url}/graphql",
            json={
                "query": mutation,
                "variables": {"input": {"id": str(performer_id), "urls": urls}},
            },
            headers=self._gql_headers(),
            timeout=10,
        )
        resp.raise_for_status()
        return True

    def delete_dead_urls(
        self,
        results: List[URLCheckResult],
        mode: str = "auto",  # "auto" | "db_only" | "graphql_only"
        dry_run: bool = False,
    ) -> Dict:
        """
        Supprime les URLs mortes.

        mode:
          "auto"         ‚Üí essaie GraphQL, fallback sur SQLite direct
          "db_only"      ‚Üí suppression directe SQLite
          "graphql_only" ‚Üí suppression via GraphQL uniquement

        dry_run: si True, simule sans modifier la BDD.
        """
        dead = [r for r in results if r.should_delete]
        if not dead:
            return {"deleted": 0, "mode": "none", "dry_run": dry_run}

        if dry_run:
            return {
                "deleted": len(dead),
                "mode": "dry_run",
                "dry_run": True,
                "urls": [r.url for r in dead],
            }

        if mode == "db_only":
            count = self.delete_dead_urls_from_db(results)
            return {"deleted": count, "mode": "db_only", "dry_run": False}

        if mode == "graphql_only":
            report = self.delete_dead_urls_via_graphql(results)
            deleted = sum(1 for v in report.values() if v.get("status") == "ok")
            return {"deleted": deleted, "mode": "graphql", "report": report}

        # Auto : tente GraphQL, fallback SQLite
        try:
            report = self.delete_dead_urls_via_graphql(results)
            deleted = sum(1 for v in report.values() if v.get("status") == "ok")
            return {"deleted": deleted, "mode": "graphql", "report": report}
        except Exception as e:
            print(f"[URLValidator] GraphQL indisponible ({e}), fallback SQLite...")
            count = self.delete_dead_urls_from_db(results)
            return {"deleted": count, "mode": "db_fallback", "dry_run": False}

    # ------------------------------------------------------------------
    # Rapport
    # ------------------------------------------------------------------

    def build_report(self, results: List[URLCheckResult]) -> str:
        """G√©n√®re un rapport textuel lisible des r√©sultats."""
        active      = [r for r in results if r.status == URLStatus.ACTIVE]
        dead        = [r for r in results if r.status == URLStatus.DEAD]
        redirect    = [r for r in results if r.status == URLStatus.REDIRECT]
        ambiguous   = [r for r in results if r.status == URLStatus.AMBIGUOUS]
        whitelisted = [r for r in results if r.status == URLStatus.WHITELISTED]
        error       = [r for r in results if r.status == URLStatus.ERROR]

        lines = [
            "=" * 70,
            "RAPPORT DE VALIDATION DES URLs",
            "=" * 70,
            f"  ‚úÖ Actives      : {len(active)}",
            f"  ‚ùå Mortes       : {len(dead)}   ‚Üí seront supprim√©es",
            f"  ‚Ü™Ô∏è  Redirig√©es   : {len(redirect)}",
            f"  ‚ùì Ambigu√´s     : {len(ambiguous)}   (403/429/timeout ‚Äî conserv√©es)",
            f"  ‚è≠Ô∏è  Whitelist√©es : {len(whitelisted)}  (non test√©es)",
            f"  üí• Erreurs      : {len(error)}   (r√©seau ‚Äî conserv√©es)",
            f"  TOTAL          : {len(results)}",
        ]

        if dead:
            lines += ["", "‚îÄ" * 70, "‚ùå URLs MORTES (√† supprimer) :"]
            for r in dead:
                lines.append(f"  [{r.performer_name}] {r.url}  [{r.http_code}]")

        if redirect:
            lines += ["", "‚îÄ" * 70, "‚Ü™Ô∏è  URLs REDIRIG√âES :"]
            for r in redirect:
                lines.append(f"  [{r.performer_name}] {r.url}")
                if r.redirect_to:
                    lines.append(f"    ‚Üí {r.redirect_to}")

        if ambiguous:
            lines += ["", "‚îÄ" * 70, "‚ùì URLs AMBIGU√ãS (conserv√©es) :"]
            for r in ambiguous:
                lines.append(f"  [{r.performer_name}] {r.url}  [{r.http_code}]")

        if error:
            lines += ["", "‚îÄ" * 70, "üí• ERREURS R√âSEAU (conserv√©es) :"]
            for r in error:
                lines.append(f"  [{r.performer_name}] {r.url}  ‚Äî {r.error_msg}")

        lines.append("=" * 70)
        return "\n".join(lines)

    def build_summary_by_performer(self, results: List[URLCheckResult]) -> str:
        """Rapport r√©sum√© group√© par performer."""
        by_performer: Dict[int, List[URLCheckResult]] = {}
        for r in results:
            by_performer.setdefault(r.performer_id, []).append(r)

        lines = ["=" * 70, "R√âSUM√â PAR PERFORMER", "=" * 70]
        for pid, pr in sorted(by_performer.items(), key=lambda x: x[1][0].performer_name):
            name = pr[0].performer_name
            dead_count = sum(1 for r in pr if r.should_delete)
            flag = f"  ‚Üê {dead_count} √† supprimer" if dead_count else ""
            lines.append(f"\n  [{pid}] {name}{flag}")
            for r in pr:
                lines.append(f"    {r}")
        lines.append("\n" + "=" * 70)
        return "\n".join(lines)


# ===========================================================================
# Widget Tkinter int√©grable dans StashMaster
# ===========================================================================

class URLValidatorWidget:
    """
    Widget Tkinter autonome pour valider les URLs depuis StashMaster.
    S'int√®gre comme fen√™tre secondaire (Toplevel).

    Usage :
        from url_validator import URLValidatorWidget
        widget = URLValidatorWidget(parent, db_path="...", stash_url="...")
        widget.show()
    """

    def __init__(self, parent, db_path: str, stash_url: str = DEFAULT_STASH_URL,
                 api_key: str = "", performer_id: Optional[int] = None):
        try:
            import tkinter as tk
            from tkinter import ttk, messagebox, scrolledtext
            self.tk = tk
            self.ttk = ttk
            self.messagebox = messagebox
            self.scrolledtext = scrolledtext
        except ImportError:
            raise ImportError("Tkinter requis pour URLValidatorWidget")

        self.parent = parent
        self.db_path = db_path
        self.stash_url = stash_url
        self.api_key = api_key
        self.performer_id = performer_id
        self.validator = URLValidator(db_path, stash_url, api_key)
        self.results: List[URLCheckResult] = []

    def show(self):
        """Ouvre la fen√™tre de validation."""
        tk = self.tk
        ttk = self.ttk

        self.win = tk.Toplevel(self.parent)
        self.win.title("üîó Validation des URLs de Performers")
        self.win.geometry("900x650")
        self.win.grab_set()

        # ‚îÄ‚îÄ Header ‚îÄ‚îÄ
        hdr = tk.Frame(self.win, bg="#1a1a2e", pady=8)
        hdr.pack(fill="x")
        tk.Label(hdr, text="üîó Validation des URLs", font=("Arial", 13, "bold"),
                 bg="#1a1a2e", fg="white").pack()

        # Infos BDD
        try:
            nb_perf, nb_urls = self.validator.get_performer_count()
            info_txt = f"{nb_perf} performers ¬∑ {nb_urls} URLs dans la BDD"
        except Exception as e:
            info_txt = f"Erreur lecture BDD : {e}"
        tk.Label(hdr, text=info_txt, font=("Arial", 9), bg="#1a1a2e", fg="#aaa").pack()

        # ‚îÄ‚îÄ Options ‚îÄ‚îÄ
        opt_frame = tk.LabelFrame(self.win, text="Options", padx=8, pady=6)
        opt_frame.pack(fill="x", padx=10, pady=5)

        # Timeout
        tk.Label(opt_frame, text="Timeout (s):").grid(row=0, column=0, sticky="w")
        self.timeout_var = tk.IntVar(value=DEFAULT_TIMEOUT)
        ttk.Spinbox(opt_frame, from_=3, to=30, textvariable=self.timeout_var, width=5
                    ).grid(row=0, column=1, padx=4)

        # Workers
        tk.Label(opt_frame, text="Threads:").grid(row=0, column=2, padx=(12,0), sticky="w")
        self.workers_var = tk.IntVar(value=DEFAULT_MAX_WORKERS)
        ttk.Spinbox(opt_frame, from_=1, to=20, textvariable=self.workers_var, width=5
                    ).grid(row=0, column=3, padx=4)

        # Dry run
        self.dry_run_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(opt_frame, text="Dry run (simuler sans supprimer)",
                        variable=self.dry_run_var).grid(row=0, column=4, padx=12)

        # ‚îÄ‚îÄ Barre de progression ‚îÄ‚îÄ
        prog_frame = tk.Frame(self.win)
        prog_frame.pack(fill="x", padx=10, pady=3)
        self.progress_label = tk.Label(prog_frame, text="En attente...", anchor="w")
        self.progress_label.pack(side="left", fill="x", expand=True)
        self.progress_bar = ttk.Progressbar(self.win, mode="determinate")
        self.progress_bar.pack(fill="x", padx=10, pady=2)

        # ‚îÄ‚îÄ Zone de r√©sultats (tableau) ‚îÄ‚îÄ
        cols_frame = tk.Frame(self.win)
        cols_frame.pack(fill="both", expand=True, padx=10, pady=4)

        columns = ("status", "performer", "url", "code", "ms")
        self.tree = ttk.Treeview(cols_frame, columns=columns, show="headings", height=18)
        self.tree.heading("status",    text="√âtat")
        self.tree.heading("performer", text="Performer")
        self.tree.heading("url",       text="URL")
        self.tree.heading("code",      text="HTTP")
        self.tree.heading("ms",        text="ms")
        self.tree.column("status",    width=90,  anchor="center")
        self.tree.column("performer", width=140, anchor="w")
        self.tree.column("url",       width=440, anchor="w")
        self.tree.column("code",      width=55,  anchor="center")
        self.tree.column("ms",        width=55,  anchor="center")

        # Tags de couleurs
        self.tree.tag_configure("active",      foreground="#2e7d32")
        self.tree.tag_configure("dead",        foreground="#c62828", background="#fff3f3")
        self.tree.tag_configure("redirect",    foreground="#e65100")
        self.tree.tag_configure("ambiguous",   foreground="#5d4037")
        self.tree.tag_configure("whitelisted", foreground="#1565c0")
        self.tree.tag_configure("error",       foreground="#6a1b9a")

        scroll_y = ttk.Scrollbar(cols_frame, orient="vertical", command=self.tree.yview)
        scroll_x = ttk.Scrollbar(self.win, orient="horizontal", command=self.tree.xview)
        self.tree.configure(yscrollcommand=scroll_y.set, xscrollcommand=scroll_x.set)
        scroll_y.pack(side="right", fill="y")
        self.tree.pack(side="left", fill="both", expand=True)
        scroll_x.pack(fill="x", padx=10)

        # ‚îÄ‚îÄ Boutons ‚îÄ‚îÄ
        btn_frame = tk.Frame(self.win, pady=6)
        btn_frame.pack(fill="x", padx=10)

        self.btn_validate = ttk.Button(btn_frame, text="‚ñ∂ Lancer la validation",
                                       command=self._start_validation)
        self.btn_validate.pack(side="left", padx=4)

        self.btn_delete = ttk.Button(btn_frame, text="üóë Supprimer les URLs mortes",
                                     command=self._delete_dead, state="disabled")
        self.btn_delete.pack(side="left", padx=4)

        self.btn_report = ttk.Button(btn_frame, text="üìã Rapport complet",
                                     command=self._show_report, state="disabled")
        self.btn_report.pack(side="left", padx=4)

        self.dead_label = tk.Label(btn_frame, text="", fg="#c62828", font=("Arial", 9, "bold"))
        self.dead_label.pack(side="right", padx=8)

    def _start_validation(self):
        """Lance la validation dans un thread s√©par√©."""
        import threading

        self.btn_validate.config(state="disabled")
        self.btn_delete.config(state="disabled")
        self.btn_report.config(state="disabled")
        self.results = []
        for item in self.tree.get_children():
            self.tree.delete(item)
        self.progress_bar["value"] = 0
        self.dead_label.config(text="")

        self.validator.timeout = self.timeout_var.get()
        self.validator.max_workers = self.workers_var.get()

        def run():
            try:
                entries = self.validator.get_all_performer_urls(self.performer_id)
                total = len(entries)
                self.progress_bar["maximum"] = max(total, 1)

                def on_progress(current, total, result):
                    self.results.append(result)
                    self.win.after(0, self._add_tree_row, result)
                    self.win.after(0, self._update_progress, current, total)

                self.validator.validate_urls(entries, progress_callback=on_progress)

            except Exception as e:
                self.win.after(0, lambda: self.messagebox.showerror(
                    "Erreur", f"Erreur de validation :\n{e}"))
            finally:
                self.win.after(0, self._validation_done)

        threading.Thread(target=run, daemon=True).start()

    def _add_tree_row(self, r: URLCheckResult):
        tag = r.status.value
        label = {
            URLStatus.ACTIVE:      "‚úÖ Active",
            URLStatus.DEAD:        "‚ùå Morte",
            URLStatus.REDIRECT:    "‚Ü™Ô∏è  Redirect",
            URLStatus.AMBIGUOUS:   "‚ùì Ambigu√´",
            URLStatus.WHITELISTED: "‚è≠Ô∏è  Whitelist",
            URLStatus.ERROR:       "üí• Erreur",
        }.get(r.status, "?")

        self.tree.insert("", "end",
                         values=(label, r.performer_name, r.url,
                                 r.http_code or "‚Äî", r.check_duration_ms),
                         tags=(tag,))

    def _update_progress(self, current, total):
        self.progress_bar["value"] = current
        self.progress_label.config(text=f"V√©rification {current}/{total}‚Ä¶")

    def _validation_done(self):
        dead_count = sum(1 for r in self.results if r.should_delete)
        self.progress_label.config(text=f"‚úÖ Termin√© ‚Äî {len(self.results)} URLs v√©rifi√©es")
        self.btn_validate.config(state="normal")
        self.btn_report.config(state="normal")
        if dead_count:
            self.btn_delete.config(state="normal")
            self.dead_label.config(text=f"{dead_count} URL(s) morte(s) √† supprimer")

    def _delete_dead(self):
        dead = [r for r in self.results if r.should_delete]
        if not dead:
            self.messagebox.showinfo("Info", "Aucune URL morte √† supprimer.")
            return

        dry = self.dry_run_var.get()
        msg = (
            f"{'[DRY RUN] ' if dry else ''}"
            f"Supprimer {len(dead)} URL(s) morte(s) ?\n\n"
            + "\n".join(f"  ‚Ä¢ [{r.performer_name}] {r.url}" for r in dead[:15])
            + (f"\n  ... et {len(dead)-15} autres" if len(dead) > 15 else "")
        )
        if not self.messagebox.askyesno("Confirmation", msg):
            return

        try:
            result = self.validator.delete_dead_urls(self.results, dry_run=dry)
            mode = "GraphQL" if "graphql" in result.get("mode","") else "SQLite"
            msg_ok = (
                f"{'[DRY RUN] ' if dry else ''}"
                f"{result['deleted']} URL(s) supprim√©e(s) via {mode}."
            )
            self.messagebox.showinfo("Succ√®s", msg_ok)
            self.btn_delete.config(state="disabled")
            self.dead_label.config(text="")
        except Exception as e:
            self.messagebox.showerror("Erreur", f"Erreur lors de la suppression :\n{e}")

    def _show_report(self):
        """Affiche le rapport complet dans une fen√™tre."""
        tk = self.tk
        report = self.validator.build_report(self.results)

        win = tk.Toplevel(self.win)
        win.title("üìã Rapport complet")
        win.geometry("800x550")

        txt = self.scrolledtext.ScrolledText(win, font=("Courier New", 9), wrap="none")
        txt.pack(fill="both", expand=True, padx=8, pady=8)
        txt.insert("1.0", report)
        txt.config(state="disabled")

        ttk = self.ttk
        ttk.Button(win, text="Fermer", command=win.destroy).pack(pady=4)


# ===========================================================================
# CLI (lancement direct)
# ===========================================================================

def _cli():
    import argparse
    import sys

    parser = argparse.ArgumentParser(
        description="Valide les URLs de performers dans la BDD Stash"
    )
    parser.add_argument("--db", default=DEFAULT_DB_PATH,
                        help="Chemin vers stash-go.sqlite")
    parser.add_argument("--stash-url", default=DEFAULT_STASH_URL,
                        help="URL de l'API Stash")
    parser.add_argument("--api-key", default="",
                        help="Cl√© API Stash")
    parser.add_argument("--performer-id", type=int, default=None,
                        help="Valider seulement ce performer (ID)")
    parser.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT,
                        help="Timeout HTTP en secondes")
    parser.add_argument("--workers", type=int, default=DEFAULT_MAX_WORKERS,
                        help="Nombre de threads parall√®les")
    parser.add_argument("--auto-delete", action="store_true",
                        help="Supprimer automatiquement sans confirmation")
    parser.add_argument("--dry-run", action="store_true",
                        help="Simuler sans modifier la BDD")
    parser.add_argument("--mode", choices=["auto","db_only","graphql_only"],
                        default="auto", help="Mode de suppression")
    args = parser.parse_args()

    validator = URLValidator(
        db_path=args.db,
        stash_url=args.stash_url,
api_key="***MASKED***"
        timeout=args.timeout,
        max_workers=args.workers,
    )

    # Infos
    try:
        nb_perf, nb_urls = validator.get_performer_count()
        print(f"BDD : {args.db}")
        print(f"Performers avec URLs : {nb_perf} | Total URLs : {nb_urls}")
    except Exception as e:
        print(f"‚ùå Impossible d'ouvrir la BDD : {e}")
        sys.exit(1)

    # Validation
    done_count = [0]
    def progress(current, total, result):
        done_count[0] = current
        print(f"  [{current:>4}/{total}] {result}", flush=True)

    print(f"\n{'‚îÄ'*70}")
    print(f"Validation en cours ({args.workers} threads, timeout={args.timeout}s)‚Ä¶")
    print(f"{'‚îÄ'*70}")

    results = validator.validate_all(
        performer_id=args.performer_id,
        progress_callback=progress,
    )

    # Rapport
    print("\n")
    print(validator.build_report(results))

    # Suppression
    dead = [r for r in results if r.should_delete]
    if not dead:
        print("\n‚úÖ Aucune URL morte ‚Äî base de donn√©es propre !")
        return

    if args.auto_delete or args.dry_run:
        result = validator.delete_dead_urls(results, mode=args.mode, dry_run=args.dry_run)
        if args.dry_run:
            print(f"\n[DRY RUN] {result['deleted']} URL(s) auraient √©t√© supprim√©es.")
        else:
            print(f"\n‚úÖ {result['deleted']} URL(s) supprim√©es (mode: {result['mode']}).")
    else:
        ans = input(f"\nSupprimer les {len(dead)} URLs mortes ? [o/N] ").strip().lower()
        if ans in ("o", "oui", "y", "yes"):
            result = validator.delete_dead_urls(results, mode=args.mode)
            print(f"‚úÖ {result['deleted']} URL(s) supprim√©es (mode: {result['mode']}).")
        else:
            print("Annul√© ‚Äî aucune modification.")


if __name__ == "__main__":
    _cli()


============================================================
[112/124] start.bat
------------------------------------------------------------
@echo off
REM Optimized launcher: checks venv, dependencies, installs if missing, generates a report, and then runs main.py

REM Set venv directory name
set VENV_DIR=.venv

REM Check if venv exists and is valid
if not exist %VENV_DIR%\Scripts\activate.bat (
    echo Creating virtual environment...
    python -m venv %VENV_DIR%
    if %errorlevel% neq 0 (
        echo ERROR: Failed to create virtual environment. Please ensure Python is installed and in your PATH.
        pause
        exit /b 1
    )
)

REM Activate venv
call %VENV_DIR%\Scripts\activate.bat

REM Check and install dependencies if needed
echo Upgrading pip and installing requirements from requirements.txt...
python -m pip install --upgrade pip >nul
pip install -r requirements.txt

REM --- System Hardware and AI Report ---

REM Verify for NVIDIA GPU and Ollama, and generate a report
echo Generating system report...
for /f %%i in ('powershell -Command "Get-Date -format 'yyyyMMdd-HHmmss'"') do set TIMESTAMP=%%i
set REPORT_FILE=rapport_Ollama_%TIMESTAMP%.txt

(
    echo Report generated on %DATE% at %TIME%
    echo.
) > %REPORT_FILE%

REM Check for NVIDIA GPU
nvidia-smi >nul 2>&1
if %errorlevel% neq 0 (
    echo WARNING: NVIDIA GPU not found or nvidia-smi is not in your PATH.
    (
        echo === NVIDIA GPU Status ===
        echo NVIDIA GPU not found or nvidia-smi command failed.
    ) >> %REPORT_FILE%
) else (
    echo NVIDIA GPU detected.
    (
        echo === NVIDIA GPU Status ===
        nvidia-smi
    ) >> %REPORT_FILE%
)

REM Check for Ollama
set PATH=E:\Ollama;%PATH%
where ollama >nul 2>&1
if %errorlevel% neq 0 (
    echo WARNING: Ollama not found. Please ensure it is installed and in your PATH.
    (
        echo.
        echo === Ollama Status ===
        echo Ollama not found.
    ) >> %REPORT_FILE%
) else (
    echo Ollama detected at E:\Ollama or in PATH.
    (
        echo.
        echo === Ollama Models List ===
        ollama list
    ) >> %REPORT_FILE%
)

echo Report saved to %REPORT_FILE%
echo.

REM --- Launching Application ---
echo Starting the main application...
python main.py

pause


============================================================
[113/124] stashmaster_unified.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
StashMaster V2 - Interface Unifi√©e
Fusion des Phase 1 et Phase 2 avec g√©n√©ration automatique de bio
"""

import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox, simpledialog
import requests
from bs4 import BeautifulSoup
import re
import json
from datetime import datetime
from typing import Dict, List, Optional
import threading
from services.bio_generator import BioGenerator


class TagRulesEngine:
    """Moteur de r√®gles pour g√©n√©rer des tags bas√©s sur les m√©tadonn√©es"""
    
    @staticmethod
    def generate_tags(metadata: Dict) -> List[str]:
        """G√©n√®re des tags bas√©s sur les m√©tadonn√©es collect√©es"""
        tags = []
        
        # Tags bas√©s sur l'ethnicit√© (mots entiers pour √©viter les faux positifs)
        ethnicity = metadata.get('ethnicity', '').lower()
        if ethnicity:
            if re.search(r'\bcaucasian\b', ethnicity):
                tags.append('Caucasian')
            elif re.search(r'\b(?:latin(?:a)?|cuban)\b', ethnicity):
                tags.append('Latina')
            elif re.search(r'\basian\b', ethnicity):
                tags.append('Asian')
            elif re.search(r'\b(?:ebony|african)\b', ethnicity):
                tags.append('Ebony')
        
        # Tags bas√©s sur la couleur de cheveux
        hair_color = metadata.get('hair_color', '').lower()
        if 'blonde' in hair_color or 'blond' in hair_color:
            tags.append('Blonde')
        elif 'brown' in hair_color or 'brunette' in hair_color:
            tags.append('Brunette')
        elif 'red' in hair_color:
            tags.append('Redhead')
        elif 'black' in hair_color:
            tags.append('Black Hair')
        
        # Tags bas√©s sur les mesures
        measurements = metadata.get('measurements', '')
        if measurements:
            # Extraire la taille des seins (premi√®re valeur)
            match = re.match(r'(\d+)', measurements)
            if match:
                size = int(match.group(1))
                if size >= 36:
                    tags.append('Big Boobs')
                elif size <= 32:
                    tags.append('Small Boobs')
        
        # Tags bas√©s sur les piercings
        piercings = metadata.get('piercings', '').lower()
        if piercings and piercings != 'none':
            tags.append('Pierced')
        
        # Tags bas√©s sur les tattoos
        tattoos = metadata.get('tattoos', '').lower()
        if tattoos and tattoos != 'none':
            tags.append('Tattooed')
        
        # Tags bas√©s sur le trivia (big butt, bimbo)
        trivia = metadata.get('trivia', '').lower()
        if 'big butt' in trivia or 'bubble butt' in trivia:
            tags.append('BigButt')
        if 'bimbo' in trivia:
            tags.append('Bimbo')
        
        # Tags bas√©s sur l'√¢ge de carri√®re
        career_start = metadata.get('career_start', '')
        if career_start:
            try:
                year = int(career_start.split('-')[0])
                current_year = datetime.now().year
                if current_year - year > 10:
                    tags.append('MILF')
            except:
                pass
        
        return list(set(tags))  # √âliminer les doublons


class AwardsCleaner:
    """Nettoie et formate les awards pour avoir 1 par ligne"""
    
    @staticmethod
    def clean_awards(raw_awards: str) -> str:
        """Nettoie le texte brut des awards pour avoir un format lisible"""
        if not raw_awards:
            return ""
        
        # S√©parer par les num√©ros d'ann√©e
        lines = []
        current_year = None
        
        # Pattern pour d√©tecter les ann√©es
        year_pattern = re.compile(r'\b(19\d{2}|20\d{2})\b')
        
        # Pattern pour d√©tecter les types d'awards
        award_types = ['AVN AWARDS', 'XBIZ AWARDS', 'NIGHTMOVES', 'XRCO AWARDS']
        
        text = raw_awards
        for award_type in award_types:
            text = text.replace(award_type, f'\n{award_type}\n')
        
        # Diviser en lignes et nettoyer
        for line in text.split('\n'):
            line = line.strip()
            if not line:
                continue
            
            # D√©tecter si c'est une ann√©e
            if re.match(r'^\d{4}$', line):
                current_year = line
                lines.append(f'\n{current_year}')
                continue
            
            # D√©tecter si c'est un type d'award
            if any(award_type in line.upper() for award_type in award_types):
                lines.append(f'\n{line}')
                continue
            
            # D√©tecter Winner ou Nominee
            if line.startswith('Winner:') or line.startswith('Nominee:'):
                lines.append(f'  {line}')
            elif current_year and not line.startswith(' '):
                lines.append(f'  {line}')
            else:
                lines.append(line)
        
        return '\n'.join(lines)


class BioGenerator:
    """G√©n√©rateur de biographies avec Google Search et Ollama"""
    
    def __init__(self):
        self.ollama_url = "http://localhost:11434/api/generate"
    
    def generate_google_bio(self, performer_name: str, metadata: Dict) -> str:
        """G√©n√®re une bio de 3000 caract√®res style Google"""
        
        # Template bas√© sur BioGooglemodele.txt
        template = f"""### {performer_name} : L'√©toile charismatique au parcours diversifi√©

**Introduction**
N√©e le {metadata.get('birthdate', '[date]')} √† {metadata.get('birthplace', '[lieu]')}, {performer_name} a marqu√© de son empreinte l'industrie du divertissement pour adultes d√®s son entr√©e en sc√®ne en {metadata.get('career_start', '[ann√©e]')}. Reconnue pour son charisme naturel et son √©nergie captivante, elle a rapidement acquis une notori√©t√© significative. Au fil de sa carri√®re, elle a adopt√© plusieurs pseudonymes, tels que {', '.join(metadata.get('aliases', []))}, qui ont tous contribu√© √† forger son image polyvalente et √† laisser un impact m√©morable dans le secteur.

**üìÖ Origines et Premiers Pas**
Issue d'une famille d'origine {metadata.get('ethnicity', '[origine]')} et ayant grandi dans le vibrant paysage de {metadata.get('birthplace', '[lieu]')}, la vie de {performer_name} avant son immersion dans l'industrie est envelopp√©e d'une certaine discr√©tion. Les informations d√©taill√©es concernant son enfance ou son parcours scolaire ne sont pas largement divulgu√©es publiquement, soulignant une volont√© de pr√©server sa sph√®re priv√©e. C'est √† l'√¢ge de {metadata.get('career_start_age', '[√¢ge]')} ans, en {metadata.get('career_start', '[ann√©e]')}, qu'elle a franchi le seuil du monde du divertissement pour adultes, un choix qui allait d√©finir une d√©cennie de sa vie professionnelle et la propulser sur le devant de la sc√®ne internationale.

**üèÜ Carri√®re et Filmographie**
La trajectoire professionnelle de {performer_name} a d√©but√© avec une force consid√©rable, la menant √† collaborer avec certains des plus grands noms de l'industrie. D√®s les premi√®res ann√©es de sa carri√®re, elle a √©t√© une pr√©sence r√©guli√®re sur des plateformes de renom. Ces partenariats pr√©coces lui ont permis d'acqu√©rir une visibilit√© rapide et de se b√¢tir une solide r√©putation en tant qu'interpr√®te polyvalente.

Son √©volution l'a ensuite amen√©e √† diversifier ses r√¥les et √† travailler avec d'autres studios influents. Elle a su s'adapter √† diff√©rents types de sc√®nes, d√©montrant une gamme de performances qui ont plu √† un large public. Bien que sa carri√®re en sc√®nes explicites ait connu son apog√©e autour de {metadata.get('peak_years', '[p√©riode]')}, sa vaste filmographie et la qualit√© constante de ses prestations lui ont assur√© une place de choix parmi les √©toiles de sa g√©n√©ration.

**üí° Faits Int√©ressants & Vie Personnelle**
Au-del√† de l'√©cran, {performer_name} est r√©put√©e pour sa personnalit√© authentique et son approche terre-√†-terre. La sph√®re de sa vie personnelle reste, comme il est courant dans cette industrie, relativement priv√©e. {metadata.get('trivia', '')}

**üëó Apparence et Style**
{performer_name} est souvent caract√©ris√©e par une beaut√© distinctive, ancr√©e dans ses origines {metadata.get('ethnicity', '[origine]')}. Elle arbore typiquement une chevelure {metadata.get('hair_color', '[couleur]')}, souvent longue et soyeuse, qui encadre un visage expressif et une silhouette g√©n√©ralement {metadata.get('body_type', 'fine et athl√©tique')}. Son style sur sc√®ne est marqu√© par une √©nergie palpable et une capacit√© √† incarner des personnages vari√©s avec cr√©dibilit√©.

Mesures physiques : {metadata.get('measurements', '[mesures]')} - Taille : {metadata.get('height', '[taille]')} - Poids : {metadata.get('weight', '[poids]')}
Tatouages : {metadata.get('tattoos', 'Information non disponible')}
Piercings : {metadata.get('piercings', 'Information non disponible')}

**üèÜ Prix et Distinctions**
La reconnaissance de l'industrie n'a pas tard√© √† se manifester pour {performer_name}, qui a √©t√© honor√©e de nombreuses nominations au cours de sa carri√®re. {metadata.get('awards_summary', '')}

**Conclusion rapide**
En somme, {performer_name} demeure une figure embl√©matique et respect√©e de l'industrie pour adultes. Son parcours, caract√©ris√© par une entr√©e remarqu√©e en {metadata.get('career_start', '[ann√©e]')} et une carri√®re diversifi√©e sous plusieurs alias, a laiss√© une impression durable. Son professionnalisme, son charme et sa capacit√© √† captiver le public continuent d'√™tre salu√©s par ses fans et les connaisseurs du milieu, confirmant son statut d'√©toile marquante de sa g√©n√©ration."""
        
        # Limiter √† environ 3000 caract√®res
        if len(template) > 3000:
            # Couper intelligemment
            template = template[:2950] + "..."
        
        return template
    
    def generate_ollama_bio(self, performer_name: str, metadata: Dict, custom_prompt: str = "") -> Optional[str]:
        """G√©n√®re une bio avec Ollama"""
        try:
            # Construire le prompt
            if custom_prompt:
                prompt = custom_prompt
            else:
                prompt = f"""√âcris une biographie professionnelle de 3000 caract√®res pour {performer_name}, 
                actrice de l'industrie du divertissement pour adultes.
                
                Informations disponibles :
                - Nom : {performer_name}
                - Aliases : {', '.join(metadata.get('aliases', []))}
                - Date de naissance : {metadata.get('birthdate', 'Non disponible')}
                - Lieu de naissance : {metadata.get('birthplace', 'Non disponible')}
                - Ethnicit√© : {metadata.get('ethnicity', 'Non disponible')}
                - D√©but de carri√®re : {metadata.get('career_start', 'Non disponible')}
                - Mesures : {metadata.get('measurements', 'Non disponible')}
                
                La biographie doit √™tre :
                - Professionnelle et respectueuse
                - Structur√©e avec des sections claires
                - D'environ 3000 caract√®res
                - En fran√ßais
                """
            
            # Appel √† Ollama
            response = requests.post(
                self.ollama_url,
                json={
                    "model": "llama2",
                    "prompt": prompt,
                    "stream": False
                },
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '')
            else:
                return None
        except Exception as e:
            print(f"Erreur Ollama: {e}")
            return None


class TriviaAwardsWindow(tk.Toplevel):
    """Fen√™tre s√©par√©e pour Trivia et Awards avec requ√™te et r√©sultats"""
    
    def __init__(self, parent, performer_name: str, urls: List[str]):
        super().__init__(parent)
        self.title(f"Trivia & Awards ‚Äî {performer_name}")
        self.geometry("1000x700")
        
        self.performer_name = performer_name
        self.urls = urls
        self.awards_cleaner = AwardsCleaner()
        
        self._create_widgets()
    
    def _create_widgets(self):
        # Frame principal
        main_frame = ttk.Frame(self, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        self.columnconfigure(0, weight=1)
        self.rowconfigure(0, weight=1)
        main_frame.columnconfigure(0, weight=1)
        main_frame.rowconfigure(2, weight=1)
        main_frame.rowconfigure(4, weight=1)
        
        # Section Trivia
        trivia_label = ttk.Label(main_frame, text="üìù Trivia", font=('Segoe UI', 12, 'bold'))
        trivia_label.grid(row=0, column=0, sticky=tk.W, pady=(0, 5))
        
        # Boutons de scraping pour Trivia
        trivia_btn_frame = ttk.Frame(main_frame)
        trivia_btn_frame.grid(row=1, column=0, sticky=tk.W, pady=(0, 5))
        
        ttk.Button(trivia_btn_frame, text="Scraper Trivia", 
                   command=self._scrape_trivia).pack(side=tk.LEFT, padx=(0, 5))
        
        # Champ Trivia (multiligne)
        self.trivia_text = scrolledtext.ScrolledText(main_frame, height=8, wrap=tk.WORD)
        self.trivia_text.grid(row=2, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        
        # Section Awards
        awards_label = ttk.Label(main_frame, text="üèÜ Awards & Nominations", 
                                 font=('Segoe UI', 12, 'bold'))
        awards_label.grid(row=3, column=0, sticky=tk.W, pady=(10, 5))
        
        # Boutons de scraping pour Awards
        awards_btn_frame = ttk.Frame(main_frame)
        awards_btn_frame.grid(row=4, column=0, sticky=tk.W, pady=(0, 5))
        
        ttk.Button(awards_btn_frame, text="Scraper Awards", 
                   command=self._scrape_awards).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(awards_btn_frame, text="Nettoyer Awards", 
                   command=self._clean_awards).pack(side=tk.LEFT, padx=(0, 5))
        
        # Champ Awards (multiligne)
        self.awards_text = scrolledtext.ScrolledText(main_frame, height=15, wrap=tk.WORD)
        self.awards_text.grid(row=5, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        
        # Boutons d'action
        btn_frame = ttk.Frame(main_frame)
        btn_frame.grid(row=6, column=0, sticky=tk.E, pady=(10, 0))
        
        ttk.Button(btn_frame, text="Annuler", command=self.destroy).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(btn_frame, text="Appliquer et continuer", 
                   command=self._apply_and_continue).pack(side=tk.LEFT)
    
    def _scrape_trivia(self):
        """Scrape les trivia depuis les URLs"""
        self.trivia_text.delete('1.0', tk.END)
        self.trivia_text.insert('1.0', "Scraping en cours...\n")
        
        def scrape():
            trivia_items = []
            for url in self.urls:
                if 'iafd.com' in url:
                    trivia = self._scrape_iafd_trivia(url)
                    if trivia:
                        trivia_items.extend(trivia)
            
            # Afficher les r√©sultats
            self.after(0, lambda: self._display_trivia(trivia_items))
        
        thread = threading.Thread(target=scrape)
        thread.daemon = True
        thread.start()
    
    def _scrape_iafd_trivia(self, url: str) -> List[str]:
        """Scrape les trivia depuis IAFD"""
        try:
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Chercher la section trivia (√† adapter selon la structure r√©elle)
            trivia = []
            # Code de scraping √† adapter selon la structure du site
            
            return trivia
        except Exception as e:
            print(f"Erreur scraping trivia: {e}")
            return []
    
    def _display_trivia(self, trivia_items: List[str]):
        """Affiche les trivia scrap√©s"""
        self.trivia_text.delete('1.0', tk.END)
        if trivia_items:
            for item in trivia_items:
                self.trivia_text.insert(tk.END, f"‚Ä¢ {item}\n")
        else:
            self.trivia_text.insert(tk.END, "Aucun trivia trouv√©.")
    
    def _scrape_awards(self):
        """Scrape les awards depuis les URLs"""
        self.awards_text.delete('1.0', tk.END)
        self.awards_text.insert('1.0', "Scraping en cours...\n")
        
        def scrape():
            awards_text = ""
            for url in self.urls:
                if 'iafd.com' in url:
                    awards = self._scrape_iafd_awards(url)
                    if awards:
                        awards_text += awards + "\n\n"
            
            # Afficher les r√©sultats
            self.after(0, lambda: self._display_awards(awards_text))
        
        thread = threading.Thread(target=scrape)
        thread.daemon = True
        thread.start()
    
    def _scrape_iafd_awards(self, url: str) -> str:
        """Scrape les awards depuis IAFD"""
        try:
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Chercher la section awards (√† adapter selon la structure r√©elle)
            awards = ""
            # Code de scraping √† adapter
            
            return awards
        except Exception as e:
            print(f"Erreur scraping awards: {e}")
            return ""
    
    def _display_awards(self, awards_text: str):
        """Affiche les awards scrap√©s"""
        self.awards_text.delete('1.0', tk.END)
        if awards_text:
            self.awards_text.insert(tk.END, awards_text)
        else:
            self.awards_text.insert(tk.END, "Aucun award trouv√©.")
    
    def _clean_awards(self):
        """Nettoie les awards pour avoir 1 par ligne"""
        raw_text = self.awards_text.get('1.0', tk.END)
        cleaned_text = self.awards_cleaner.clean_awards(raw_text)
        self.awards_text.delete('1.0', tk.END)
        self.awards_text.insert('1.0', cleaned_text)
    
    def _apply_and_continue(self):
        """Applique les modifications et ferme la fen√™tre"""
        # R√©cup√©rer les donn√©es
        self.trivia_data = self.trivia_text.get('1.0', tk.END).strip()
        self.awards_data = self.awards_text.get('1.0', tk.END).strip()
        self.destroy()
    
    def get_data(self) -> Dict:
        """Retourne les donn√©es collect√©es"""
        return {
            'trivia': getattr(self, 'trivia_data', ''),
            'awards': getattr(self, 'awards_data', '')
        }


class BioGenerationWindow(tk.Toplevel):
    """Fen√™tre pour la g√©n√©ration de bio"""
    
    def __init__(self, parent, performer_name: str, metadata: Dict):
        super().__init__(parent)
        self.title(f"G√©n√©ration de Bio ‚Äî {performer_name}")
        self.geometry("900x800")
        
        self.performer_name = performer_name
        self.metadata = metadata
        self.bio_generator = BioGenerator()
        
        self._create_widgets()
    
    def _create_widgets(self):
        # Frame principal
        main_frame = ttk.Frame(self, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        self.columnconfigure(0, weight=1)
        self.rowconfigure(0, weight=1)
        main_frame.columnconfigure(0, weight=1)
        main_frame.rowconfigure(3, weight=1)
        
        # Options de g√©n√©ration
        options_frame = ttk.LabelFrame(main_frame, text="Options de g√©n√©ration", padding="10")
        options_frame.grid(row=0, column=0, sticky=(tk.W, tk.E), pady=(0, 10))
        options_frame.columnconfigure(1, weight=1)
        
        # Choix du type de bio
        self.bio_type_var = tk.StringVar(value="google")
        
        ttk.Radiobutton(options_frame, text="Bio Google (3000 car. automatique)", 
                        variable=self.bio_type_var, value="google").grid(row=0, column=0, sticky=tk.W)
        
        ttk.Radiobutton(options_frame, text="Bio Ollama (avec IA locale)", 
                        variable=self.bio_type_var, value="ollama").grid(row=1, column=0, sticky=tk.W)
        
        ttk.Radiobutton(options_frame, text="Bio Ollama avec prompt personnalis√©", 
                        variable=self.bio_type_var, value="ollama_custom").grid(row=2, column=0, sticky=tk.W)
        
        # Section prompt personnalis√©
        prompt_frame = ttk.LabelFrame(main_frame, text="Prompt personnalis√© (optionnel)", padding="10")
        prompt_frame.grid(row=1, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        prompt_frame.columnconfigure(0, weight=1)
        prompt_frame.rowconfigure(1, weight=1)
        
        ttk.Label(prompt_frame, text="Entrez vos directives pr√©cises pour la g√©n√©ration de la bio :").grid(
            row=0, column=0, sticky=tk.W, pady=(0, 5))
        
        self.prompt_text = scrolledtext.ScrolledText(prompt_frame, height=6, wrap=tk.WORD)
        self.prompt_text.grid(row=1, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # Bouton de g√©n√©ration
        ttk.Button(main_frame, text="G√©n√©rer la Bio", 
                   command=self._generate_bio).grid(row=2, column=0, pady=(0, 10))
        
        # R√©sultat
        result_frame = ttk.LabelFrame(main_frame, text="Bio g√©n√©r√©e", padding="10")
        result_frame.grid(row=3, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        result_frame.columnconfigure(0, weight=1)
        result_frame.rowconfigure(0, weight=1)
        
        self.bio_text = scrolledtext.ScrolledText(result_frame, wrap=tk.WORD)
        self.bio_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # Label pour le compteur de caract√®res
        self.char_count_label = ttk.Label(result_frame, text="Caract√®res : 0")
        self.char_count_label.grid(row=1, column=0, sticky=tk.E, pady=(5, 0))
        
        # Boutons d'action
        btn_frame = ttk.Frame(main_frame)
        btn_frame.grid(row=4, column=0, sticky=tk.E, pady=(10, 0))
        
        ttk.Button(btn_frame, text="Annuler", command=self.destroy).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(btn_frame, text="Appliquer", 
                   command=self._apply_bio).pack(side=tk.LEFT)
    
    def _generate_bio(self):
        """G√©n√®re la bio selon le type choisi"""
        bio_type = self.bio_type_var.get()
        
        self.bio_text.delete('1.0', tk.END)
        self.bio_text.insert('1.0', "G√©n√©ration en cours...\n")
        
        def generate():
            bio = ""
            if bio_type == "google":
                bio = self.bio_generator.generate_google_bio(self.performer_name, self.metadata)
            elif bio_type in ["ollama", "ollama_custom"]:
                custom_prompt = ""
                if bio_type == "ollama_custom":
                    custom_prompt = self.prompt_text.get('1.0', tk.END).strip()
                bio = self.bio_generator.generate_ollama_bio(
                    self.performer_name, self.metadata, custom_prompt)
                if bio is None:
                    bio = "Erreur: Ollama n'est pas disponible ou n'a pas r√©pondu."
            
            # Afficher le r√©sultat
            self.after(0, lambda: self._display_bio(bio))
        
        thread = threading.Thread(target=generate)
        thread.daemon = True
        thread.start()
    
    def _display_bio(self, bio: str):
        """Affiche la bio g√©n√©r√©e"""
        self.bio_text.delete('1.0', tk.END)
        self.bio_text.insert('1.0', bio)
        
        # Mettre √† jour le compteur de caract√®res
        char_count = len(bio)
        self.char_count_label.config(text=f"Caract√®res : {char_count}")
        # Rafra√Æchir √©galement la section source si besoin
        self._update_raw_content()
    
    def _apply_bio(self):
        """Applique la bio et ferme la fen√™tre"""
        self.generated_bio = self.bio_text.get('1.0', tk.END).strip()
        self.destroy()
    
    def get_bio(self) -> str:
        """Retourne la bio g√©n√©r√©e"""
        return getattr(self, 'generated_bio', '')


class MainWindow(tk.Tk):
    """Fen√™tre principale unifi√©e - Fusion Phase 1 et Phase 2"""
    
    def __init__(self):
        super().__init__()
        
        self.title("StashMaster V2 - Performer")
        self.geometry("1200x900")
        
        self.tag_rules = TagRulesEngine()
        self.metadata = {}
        self.bio_generator = BioGenerator()
        
        self._create_widgets()
        self._create_menu()
    
    def _create_menu(self):
        """Cr√©e le menu principal"""
        menubar = tk.Menu(self)
        self.config(menu=menubar)
        
        # Menu Fichier
        file_menu = tk.Menu(menubar, tearoff=0)
        menubar.add_cascade(label="Fichier", menu=file_menu)
        file_menu.add_command(label="Nouveau", command=self._new_performer)
        file_menu.add_command(label="Ouvrir...", command=self._open_performer)
        file_menu.add_separator()
        file_menu.add_command(label="Quitter", command=self.quit)
        
        # Menu Actions
        actions_menu = tk.Menu(menubar, tearoff=0)
        menubar.add_cascade(label="Actions", menu=actions_menu)
        actions_menu.add_command(label="Scraper & Lancer le flux Bio IA", 
                                 command=self._start_scraping)
        actions_menu.add_command(label="Trivia & Awards...", 
                                 command=self._open_trivia_awards)
        actions_menu.add_command(label="G√©n√©rer Bio...", 
                                 command=self._open_bio_generator)
    
    def _create_widgets(self):
        """Cr√©e l'interface principale"""
        # Notebook pour les onglets
        notebook = ttk.Notebook(self)
        notebook.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        # Onglet 1: M√©tadonn√©es usuelles
        self.metadata_frame = self._create_metadata_tab()
        notebook.add(self.metadata_frame, text="üìã M√©tadonn√©es")
        
        # Onglet 2: Champs avanc√©s
        self.advanced_frame = self._create_advanced_tab()
        notebook.add(self.advanced_frame, text="‚öôÔ∏è Champs Avanc√©s")
        
        # Onglet 3: Bio
        self.bio_frame = self._create_bio_tab()
        notebook.add(self.bio_frame, text="üìù Bio")
        
        # Barre d'outils en bas
        self._create_toolbar()
    
    def _create_metadata_tab(self) -> ttk.Frame:
        """Cr√©e l'onglet des m√©tadonn√©es de base"""
        frame = ttk.Frame()
        
        # Frame avec scrollbar
        canvas = tk.Canvas(frame)
        scrollbar = ttk.Scrollbar(frame, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # Champs de m√©tadonn√©es
        fields = [
            ("Name:", "name"),
            ("Aliases:", "aliases"),
            ("Birthdate:", "birthdate"),
            ("Deathdate:", "deathdate"),
            ("Country:", "country"),
            ("Ethnicity:", "ethnicity"),
            ("Hair Color:", "hair_color"),
            ("Eye Color:", "eye_color"),
            ("Height:", "height"),
            ("Weight:", "weight"),
            ("Measurements:", "measurements"),
            ("Fake Tits:", "fake_tits"),
            ("Career Length:", "career_length"),
        ]
        
        self.metadata_entries = {}
        
        for i, (label, key) in enumerate(fields):
            ttk.Label(scrollable_frame, text=label).grid(row=i, column=0, sticky=tk.W, 
                                                          padx=5, pady=3)
            entry = ttk.Entry(scrollable_frame, width=50)
            entry.grid(row=i, column=1, sticky=(tk.W, tk.E), padx=5, pady=3)
            self.metadata_entries[key] = entry

        # ajouter champ trivia plus bas
        trivia_label = ttk.Label(scrollable_frame, text="Trivia:")
        trivia_label.grid(row=len(fields), column=0, sticky=tk.NW, padx=5, pady=(10,3))
        self.trivia_text = scrolledtext.ScrolledText(scrollable_frame, height=4, wrap=tk.WORD)
        self.trivia_text.grid(row=len(fields), column=1, sticky=(tk.W, tk.E), padx=5, pady=(10,3))
        
        scrollable_frame.columnconfigure(1, weight=1)
        
        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
        
        return frame
    
    def _create_advanced_tab(self) -> ttk.Frame:
        """Cr√©e l'onglet des champs avanc√©s"""
        frame = ttk.Frame()
        frame.columnconfigure(0, weight=1)
        
        row = 0
        
        # Tags - CHAMP SIMPLE LIGNE (g√©n√©r√©s automatiquement)
        ttk.Label(frame, text="Tags (g√©n√©r√©s automatiquement):", 
                  font=('Segoe UI', 10, 'bold')).grid(row=row, column=0, sticky=tk.W, 
                                                       padx=5, pady=(10, 2))
        row += 1
        
        self.tags_entry = ttk.Entry(frame, state='readonly')
        self.tags_entry.grid(row=row, column=0, sticky=(tk.W, tk.E), padx=5, pady=(0, 10))
        row += 1
        
        # Piercings - CHAMP MULTILIGNE
        ttk.Label(frame, text="Piercings:", 
                  font=('Segoe UI', 10, 'bold')).grid(row=row, column=0, sticky=tk.W, 
                                                       padx=5, pady=(10, 2))
        row += 1
        
        self.piercings_text = scrolledtext.ScrolledText(frame, height=4, wrap=tk.WORD)
        self.piercings_text.grid(row=row, column=0, sticky=(tk.W, tk.E), padx=5, pady=(0, 10))
        row += 1
        
        # Tattoos - CHAMP MULTILIGNE
        ttk.Label(frame, text="Tattoos:", 
                  font=('Segoe UI', 10, 'bold')).grid(row=row, column=0, sticky=tk.W, 
                                                       padx=5, pady=(10, 2))
        row += 1
        
        self.tattoos_text = scrolledtext.ScrolledText(frame, height=4, wrap=tk.WORD)
        self.tattoos_text.grid(row=row, column=0, sticky=(tk.W, tk.E), padx=5, pady=(0, 10))
        row += 1
        
        # URLs - CHAMP MULTILIGNE
        ttk.Label(frame, text="URLs:", 
                  font=('Segoe UI', 10, 'bold')).grid(row=row, column=0, sticky=tk.W, 
                                                       padx=5, pady=(10, 2))
        row += 1
        
        self.urls_text = scrolledtext.ScrolledText(frame, height=6, wrap=tk.WORD)
        self.urls_text.grid(row=row, column=0, sticky=(tk.W, tk.E), padx=5, pady=(0, 10))
        # d√©tecter les modifications pour nettoyer/valider automatiquement
        self.urls_text.bind('<KeyRelease>', lambda e: self._on_urls_modified())
        row += 1
        
        # boutons nettoyage/validation des URLs
        btn_frame2 = ttk.Frame(frame)
        btn_frame2.grid(row=row, column=0, pady=(0,10))
        ttk.Button(btn_frame2, text="üßπ Nettoyer URLs", command=self._clean_urls).pack(side=tk.LEFT, padx=(0,5))
        ttk.Button(btn_frame2, text="üîó Valider URLs", command=self._open_url_validator).pack(side=tk.LEFT)
        row += 1
        
        # Bouton pour g√©n√©rer les tags
        ttk.Button(frame, text="üîÑ G√©n√©rer Tags", 
                   command=self._generate_tags).grid(row=row, column=0, pady=10)
        
        return frame
    
    def _create_bio_tab(self) -> ttk.Frame:
        """Onglet Bio : sous-notebook 4 onglets (Scrapp√© / Google / Ollama / Raffiner)."""
        frame = ttk.Frame()
        frame.columnconfigure(0, weight=1)
        frame.rowconfigure(0, weight=1)

        # ‚îÄ‚îÄ Donn√©es internes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        self._bio_slots = ["", "", "", ""]   # 0=scrapp√© 1=trivia 2=google 3=ollama
        self._bio_merge_content = ""         # r√©sultat fusionner

        # ‚îÄ‚îÄ SOUS-NOTEBOOK ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        bio_nb = ttk.Notebook(frame)
        bio_nb.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), padx=5, pady=5)
        self._bio_notebook = bio_nb

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # TAB 1 : Scrapp√© (lecture seule ‚Äî bio_raw + trivia + compteurs)
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        t_scrape = ttk.Frame(bio_nb)
        t_scrape.columnconfigure(0, weight=1)
        t_scrape.rowconfigure(1, weight=2)
        t_scrape.rowconfigure(4, weight=1)

        ctr_f = ttk.Frame(t_scrape)
        ctr_f.grid(row=0, column=0, sticky=tk.EW, padx=5, pady=(4, 2))
        self._lbl_scrape_chars = ttk.Label(ctr_f, text="Bio source : 0 car.")
        self._lbl_scrape_chars.pack(side=tk.LEFT, padx=(0, 12))
        self._lbl_award_count = ttk.Label(ctr_f, text="Awards : 0")
        self._lbl_award_count.pack(side=tk.LEFT, padx=(0, 12))
        self._lbl_url_count = ttk.Label(ctr_f, text="URLs : 0")
        self._lbl_url_count.pack(side=tk.LEFT)

        self._bio_raw_text = scrolledtext.ScrolledText(
            t_scrape, wrap=tk.WORD, state='disabled', bg='#f5f5f5')
        self._bio_raw_text.grid(row=1, column=0, sticky=(tk.W, tk.E, tk.N, tk.S),
                                padx=5, pady=(0, 3))

        ttk.Label(t_scrape, text="üí° Trivia",
                  font=('Segoe UI', 9, 'bold')).grid(row=2, column=0,
                                                      sticky=tk.W, padx=5)
        self._bio_scrape_chars2 = ttk.Label(t_scrape, text="Trivia : 0 car.")
        self._bio_scrape_chars2.grid(row=2, column=0, sticky=tk.E, padx=5)
        self._bio_trivia_disp = scrolledtext.ScrolledText(
            t_scrape, wrap=tk.WORD, state='disabled', height=7, bg='#f5f5f5')
        self._bio_trivia_disp.grid(row=4, column=0, sticky=(tk.W, tk.E, tk.N, tk.S),
                                   padx=5, pady=(0, 5))

        bio_nb.add(t_scrape, text="üìÑ Scrapp√©")

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # TAB 2 : Google
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        t_google = ttk.Frame(bio_nb)
        t_google.columnconfigure(0, weight=1)
        t_google.rowconfigure(1, weight=1)

        gbar = ttk.Frame(t_google)
        gbar.grid(row=0, column=0, sticky=tk.EW, padx=5, pady=(4, 2))
        ttk.Button(gbar, text="üìù G√©n√©rer Google",
                   command=self._bio_generate_google).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(gbar, text="üóëÔ∏è Effacer",
                   command=lambda: self._bio_clear(2)).pack(side=tk.LEFT)
        self._lbl_google_chars = ttk.Label(gbar, text="Caract√®res : 0")
        self._lbl_google_chars.pack(side=tk.RIGHT, padx=5)

        self._bio_google_text = scrolledtext.ScrolledText(t_google, wrap=tk.WORD)
        self._bio_google_text.grid(row=1, column=0,
                                   sticky=(tk.W, tk.E, tk.N, tk.S), padx=5, pady=(0, 5))
        self._bio_google_text.bind('<KeyRelease>',
            lambda e: self._bio_update_chars(2, self._bio_google_text,
                                              self._lbl_google_chars))

        bio_nb.add(t_google, text="üîç Google")

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # TAB 3 : Ollama
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        t_ollama = ttk.Frame(bio_nb)
        t_ollama.columnconfigure(0, weight=1)
        t_ollama.rowconfigure(1, weight=1)

        obar = ttk.Frame(t_ollama)
        obar.grid(row=0, column=0, sticky=tk.EW, padx=5, pady=(4, 2))
        ttk.Button(obar, text="ü§ñ G√©n√©rer Ollama",
                   command=self._bio_generate_ollama).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(obar, text="üóëÔ∏è Effacer",
                   command=lambda: self._bio_clear(3)).pack(side=tk.LEFT)
        self._lbl_ollama_chars = ttk.Label(obar, text="Caract√®res : 0")
        self._lbl_ollama_chars.pack(side=tk.RIGHT, padx=5)
        self._ollama_status = ttk.Label(obar, text="", foreground='gray')
        self._ollama_status.pack(side=tk.LEFT, padx=(10, 0))

        self._bio_ollama_text = scrolledtext.ScrolledText(t_ollama, wrap=tk.WORD)
        self._bio_ollama_text.grid(row=1, column=0,
                                   sticky=(tk.W, tk.E, tk.N, tk.S), padx=5, pady=(0, 5))
        self._bio_ollama_text.bind('<KeyRelease>',
            lambda e: self._bio_update_chars(3, self._bio_ollama_text,
                                              self._lbl_ollama_chars))

        bio_nb.add(t_ollama, text="ü§ñ Ollama")

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # TAB 4 : Raffiner / Fusionner
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        t_merge = ttk.Frame(bio_nb)
        t_merge.columnconfigure(0, weight=1)
        t_merge.rowconfigure(4, weight=1)

        ttk.Label(t_merge, text="Sources √† inclure dans la fusion :",
                  font=('Segoe UI', 9, 'bold')).grid(row=0, column=0,
                                                      sticky=tk.W, padx=5, pady=(6, 2))
        src_f = ttk.Frame(t_merge)
        src_f.grid(row=1, column=0, sticky=tk.EW, padx=10)
        self._merge_vars = []
        src_labels = ["üìÑ Scrapp√© (bio_raw)", "üí° Trivia", "üîç Google", "ü§ñ Ollama"]
        for i, lbl in enumerate(src_labels):
            v = tk.BooleanVar(value=True)
            ttk.Checkbutton(src_f, text=lbl, variable=v).pack(side=tk.LEFT, padx=(0, 12))
            self._merge_vars.append(v)

        ttk.Label(t_merge, text="Directives IA (ton, style, longueur, instructions) :",
                  font=('Segoe UI', 9, 'bold')).grid(row=2, column=0,
                                                      sticky=tk.W, padx=5, pady=(8, 2))
        self.bio_prompt_text = scrolledtext.ScrolledText(t_merge, height=4, wrap=tk.WORD)
        self.bio_prompt_text.grid(row=3, column=0, sticky=tk.EW, padx=5, pady=(0, 4))
        self.bio_prompt_text.insert('1.0',
            "Ton professionnel, fran√ßais, environ 3000 caract√®res. "
            "Fusionner les bios si pr√©sentes. Prose uniquement, z√©ro liste.")

        mbar = ttk.Frame(t_merge)
        mbar.grid(row=4, column=0, sticky=tk.EW, padx=5, pady=(0, 3))
        ttk.Button(mbar, text="üîÄ Fusionner via Ollama",
                   command=self._bio_do_merge).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(mbar, text="‚úèÔ∏è Raffiner Ollama existant",
                   command=self._bio_do_refine).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(mbar, text="‚úÖ Appliquer vers Ollama",
                   command=self._bio_apply_merge).pack(side=tk.LEFT, padx=(0, 10))
        self._lbl_merge_chars = ttk.Label(mbar, text="Caract√®res : 0")
        self._lbl_merge_chars.pack(side=tk.RIGHT, padx=5)
        self._merge_status = ttk.Label(mbar, text="", foreground='gray')
        self._merge_status.pack(side=tk.LEFT)

        self._bio_merge_text = scrolledtext.ScrolledText(t_merge, wrap=tk.WORD)
        self._bio_merge_text.grid(row=5, column=0,
                                  sticky=(tk.W, tk.E, tk.N, tk.S), padx=5, pady=(0, 5))
        self._bio_merge_text.bind('<KeyRelease>',
            lambda e: self._bio_update_chars(None, self._bio_merge_text,
                                              self._lbl_merge_chars))
        t_merge.rowconfigure(5, weight=1)

        bio_nb.add(t_merge, text="üîÄ Raffiner/Fusionner")

        # legacy alias (pour _save et autres m√©thodes)
        self.bio_text = self._bio_google_text
        self.bio_char_label = self._lbl_google_chars

        # ouvrir sur Google par d√©faut
        bio_nb.select(1)
        return frame
    
    def _create_toolbar(self):
        """Barre d'outils en bas ‚Äî Scraper, URLs, Sauvegarder."""
        toolbar = ttk.Frame(self)
        toolbar.pack(side=tk.BOTTOM, fill=tk.X, padx=5, pady=5)

        ttk.Button(toolbar, text="üîé Scraper Tout",
                   command=self._start_scraping).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(toolbar, text="üßπ Nettoyer URLs",
                   command=self._clean_urls).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(toolbar, text="üîó Valider URLs",
                   command=self._open_url_validator).pack(side=tk.LEFT, padx=(0, 5))
        ttk.Button(toolbar, text="üîç Chercher Sources",
                   command=self._open_trivia_awards).pack(side=tk.LEFT)

        ttk.Button(toolbar, text="üíæ Sauvegarder",
                   command=self._save).pack(side=tk.RIGHT, padx=(5, 0))
    
    def _update_raw_content(self):
        """Synchronise les widgets scrapp√©s depuis self.metadata."""
        bio_raw = self.metadata.get('bio_raw', '')
        trivia  = self.metadata.get('trivia', '')
        awards  = self.metadata.get('awards', '')
        if hasattr(self, '_bio_slots'):
            self._bio_slots[0] = bio_raw
            self._bio_slots[1] = trivia

        def _set_ro(widget, content):
            widget.config(state='normal')
            widget.delete('1.0', tk.END)
            widget.insert('1.0', content)
            widget.config(state='disabled')

        if hasattr(self, '_bio_raw_text'):
            _set_ro(self._bio_raw_text, bio_raw)
        if hasattr(self, '_bio_trivia_disp'):
            _set_ro(self._bio_trivia_disp, trivia)

        # Mise √† jour des compteurs
        if hasattr(self, '_lbl_scrape_chars'):
            self._lbl_scrape_chars.config(text=f"Bio source : {len(bio_raw)} car.")
        if hasattr(self, '_bio_scrape_chars2'):
            self._bio_scrape_chars2.config(text=f"Trivia : {len(trivia)} car.")
        if hasattr(self, '_lbl_award_count'):
            n_awards = len([l for l in awards.splitlines() if l.strip()])
            self._lbl_award_count.config(text=f"Awards : {n_awards}")
        self._refresh_url_count()

    def _refresh_url_count(self):
        """Met √† jour le compteur d'URLs."""
        if not hasattr(self, '_lbl_url_count'):
            return
        try:
            urls = [u for u in self.urls_text.get('1.0', tk.END).splitlines() if u.strip()]
            self._lbl_url_count.config(text=f"URLs : {len(urls)}")
        except Exception:
            pass

    # ‚îÄ‚îÄ Helpers bio ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _bio_update_chars(self, slot_idx, widget, label):
        """Sauvegarde le texte dans le slot et met √† jour le compteur."""
        content = widget.get('1.0', tk.END).strip()
        if slot_idx is not None and hasattr(self, '_bio_slots'):
            self._bio_slots[slot_idx] = content
        label.config(text=f"Caract√®res : {len(content)}")

    def _bio_clear(self, slot_idx):
        """Efface un slot √©ditable."""
        targets = {2: self._bio_google_text, 3: self._bio_ollama_text}
        labels  = {2: self._lbl_google_chars, 3: self._lbl_ollama_chars}
        if slot_idx not in targets:
            return
        if messagebox.askyesno("Effacer", "Effacer ce contenu ?"):
            targets[slot_idx].delete('1.0', tk.END)
            self._bio_slots[slot_idx] = ""
            labels[slot_idx].config(text="Caract√®res : 0")

    def _bio_generate_google(self):
        """G√©n√®re la Bio Google, affiche dans l'onglet Google."""
        performer_name = self.metadata_entries['name'].get()
        if not performer_name:
            messagebox.showwarning("Attention", "Veuillez entrer un nom.")
            return
        metadata = self._collect_metadata()
        bio = self.bio_generator.generate_google_bio(performer_name, metadata)
        self._bio_slots[2] = bio
        self._bio_google_text.delete('1.0', tk.END)
        self._bio_google_text.insert('1.0', bio)
        self._lbl_google_chars.config(text=f"Caract√®res : {len(bio)}")
        self._bio_notebook.select(1)      # bascule sur onglet Google

    def _bio_generate_ollama(self):
        """G√©n√®re la Bio Ollama en thread, affiche dans l'onglet Ollama."""
        performer_name = self.metadata_entries['name'].get()
        if not performer_name:
            messagebox.showwarning("Attention", "Veuillez entrer un nom.")
            return
        metadata = self._collect_metadata()
        custom_prompt = self.bio_prompt_text.get('1.0', tk.END).strip()
        self._bio_notebook.select(2)      # bascule sur onglet Ollama
        self._bio_ollama_text.config(state='normal')
        self._bio_ollama_text.delete('1.0', tk.END)
        self._bio_ollama_text.insert('1.0', "‚è≥ G√©n√©ration Ollama en cours‚Ä¶")
        self._bio_ollama_text.config(state='disabled')
        self._ollama_status.config(text="‚è≥ En cours‚Ä¶", foreground='orange')

        def _run():
            bio = self.bio_generator.generate_ollama_bio(
                performer_name, metadata, custom_prompt)
            if not bio:
                bio = "‚ùå Erreur : Ollama non disponible ou timeout."
            self._bio_slots[3] = bio
            def _show():
                self._bio_ollama_text.config(state='normal')
                self._bio_ollama_text.delete('1.0', tk.END)
                self._bio_ollama_text.insert('1.0', bio)
                self._lbl_ollama_chars.config(text=f"Caract√®res : {len(bio)}")
                ok = not bio.startswith("‚ùå")
                self._ollama_status.config(
                    text="‚úÖ Termin√©" if ok else "‚ùå Erreur",
                    foreground='green' if ok else 'red')
            self.after(0, _show)

        threading.Thread(target=_run, daemon=True).start()

    def _bio_do_merge(self):
        """Fusionne les sources s√©lectionn√©es via Ollama ‚Üí onglet Raffiner."""
        selected = [self._bio_slots[i]
                    for i, v in enumerate(self._merge_vars)
                    if v.get() and self._bio_slots[i].strip()]
        if not selected:
            messagebox.showwarning("Fusionner", "S√©lectionnez au moins une source non vide.")
            return
        combined = "\n\n---\n\n".join(selected)
        performer_name = self.metadata_entries['name'].get()
        extra = self.bio_prompt_text.get('1.0', tk.END).strip()
        merge_prompt = (
            f"{extra}\n\n"
            f"Fusionne les textes suivants en UNE SEULE biographie coh√©rente "
            f"de ~3000 caract√®res pour {performer_name}. "
            f"Structure : Introduction, Origines, Carri√®re, Apparence, Distinctions, Conclusion. "
            f"Prose uniquement ‚Äî Z√âRO liste √† puces ni donn√©es brutes. Fran√ßais professionnel."
        )
        self._bio_merge_text.delete('1.0', tk.END)
        self._bio_merge_text.insert('1.0', "‚è≥ Fusion Ollama en cours‚Ä¶")
        self._merge_status.config(text="‚è≥ En cours‚Ä¶", foreground='orange')

        def _run():
            bio = self.bio_generator.refine_bio(combined, merge_prompt)
            if not bio:
                bio = "‚ùå Erreur lors de la fusion Ollama."
            self._bio_merge_content = bio
            def _show():
                self._bio_merge_text.delete('1.0', tk.END)
                self._bio_merge_text.insert('1.0', bio)
                self._lbl_merge_chars.config(text=f"Caract√®res : {len(bio)}")
                ok = not bio.startswith("‚ùå")
                self._merge_status.config(
                    text="‚úÖ Termin√©" if ok else "‚ùå Erreur",
                    foreground='green' if ok else 'red')
            self.after(0, _show)

        threading.Thread(target=_run, daemon=True).start()

    def _bio_do_refine(self):
        """Raffine la bio Ollama existante selon les directives."""
        current = self._bio_ollama_text.get('1.0', tk.END).strip()
        if not current or current.startswith("‚ùå") or current.startswith("‚è≥"):
            messagebox.showwarning("Raffiner", "G√©n√©rez d'abord une bio Ollama.")
            return
        extra = self.bio_prompt_text.get('1.0', tk.END).strip()
        self._bio_merge_text.delete('1.0', tk.END)
        self._bio_merge_text.insert('1.0', "‚è≥ Raffinement en cours‚Ä¶")
        self._merge_status.config(text="‚è≥ En cours‚Ä¶", foreground='orange')

        def _run():
            bio = self.bio_generator.refine_bio(current, extra)
            if not bio:
                bio = "‚ùå Erreur lors du raffinement."
            self._bio_merge_content = bio
            def _show():
                self._bio_merge_text.delete('1.0', tk.END)
                self._bio_merge_text.insert('1.0', bio)
                self._lbl_merge_chars.config(text=f"Caract√®res : {len(bio)}")
                ok = not bio.startswith("‚ùå")
                self._merge_status.config(
                    text="‚úÖ Termin√©" if ok else "‚ùå Erreur",
                    foreground='green' if ok else 'red')
            self.after(0, _show)

        threading.Thread(target=_run, daemon=True).start()

    def _bio_apply_merge(self):
        """Copie le r√©sultat Raffiner/Fusion vers l'onglet Ollama."""
        content = self._bio_merge_text.get('1.0', tk.END).strip()
        if not content or content.startswith("‚ùå"):
            messagebox.showwarning("Appliquer", "Aucun r√©sultat √† appliquer.")
            return
        self._bio_slots[3] = content
        self._bio_ollama_text.config(state='normal')
        self._bio_ollama_text.delete('1.0', tk.END)
        self._bio_ollama_text.insert('1.0', content)
        self._lbl_ollama_chars.config(text=f"Caract√®res : {len(content)}")
        self._bio_notebook.select(2)
        messagebox.showinfo("Appliquer", "Bio copi√©e vers l'onglet Ollama.")

    # ‚îÄ‚îÄ M√©thodes legacy (conserv√©es pour compatibilit√©) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _bio_nav_prev(self): pass
    def _bio_nav_next(self): pass
    def _bio_show_slot(self): pass
    def _bio_save_current_text(self): pass
    def _bio_on_edit(self, e=None): pass
    def _bio_merge_dialog(self): self._bio_do_merge()
    def _bio_apply(self): self._bio_apply_merge()
    def _bio_clear_slot(self): self._bio_clear(2)

    def _update_bio_char_count(self, event=None):
        content = self._bio_google_text.get('1.0', tk.END)
        self._lbl_google_chars.config(text=f"Caract√®res : {len(content.strip())}")
        """Collecte toutes les m√©tadonn√©es depuis les widgets UI."""
        return {
            'name': self.metadata_entries['name'].get(),
            'aliases': [a.strip() for a in
                        self.metadata_entries['aliases'].get().split(',')
                        if a.strip()],
            'birthdate': self.metadata_entries['birthdate'].get(),
            'birthplace': self.metadata_entries['country'].get(),
            'ethnicity': self.metadata_entries['ethnicity'].get(),
            'hair_color': self.metadata_entries['hair_color'].get(),
            'measurements': self.metadata_entries['measurements'].get(),
            'height': self.metadata_entries['height'].get(),
            'weight': self.metadata_entries['weight'].get(),
            'tattoos': self.tattoos_text.get('1.0', tk.END).strip(),
            'piercings': self.piercings_text.get('1.0', tk.END).strip(),
            'trivia': self.metadata.get('trivia', ''),
            'awards': self.metadata.get('awards', ''),
            'career_length': self.metadata_entries['career_length'].get(),
            'career_start': self.metadata_entries['career_length'].get().split('-')[0].strip()
                            if self.metadata_entries['career_length'].get() else '',
            'bio_raw': self.metadata.get('bio_raw', ''),
            'awards_summary': self.metadata.get('awards', ''),
        }

    def _clear_bio(self):
        """Efface le slot bio courant (compatibilit√© legacy)."""
        self._bio_clear_slot()

    def _clean_urls(self):
        """√âpurateur simple de liste d'URLs : enleve vides et doublons."""
        raw = self.urls_text.get('1.0', tk.END)
        urls = [u for u in raw.splitlines()]
        cleaned = clean_urls_list(urls)
        self.urls_text.delete('1.0', tk.END)
        self.urls_text.insert('1.0', '\n'.join(cleaned))
        messagebox.showinfo("Nettoyage URLs", f"{len(cleaned)} URLs conserv√©es.")
        # after cleaning optionally validate
        self._validate_urls()

    def _generate_tags(self):
        """G√©n√®re les tags automatiquement bas√©s sur les m√©tadonn√©es"""
        # R√©cup√©rer les m√©tadonn√©es actuelles
        metadata = {
            'ethnicity': self.metadata_entries['ethnicity'].get(),
            'hair_color': self.metadata_entries['hair_color'].get(),
            'measurements': self.metadata_entries['measurements'].get(),
            'piercings': self.piercings_text.get('1.0', tk.END).strip(),
            'tattoos': self.tattoos_text.get('1.0', tk.END).strip(),
            'career_length': self.metadata_entries['career_length'].get(),
            'trivia': self.trivia_text.get('1.0', tk.END).strip(),
        }
        
        # G√©n√©rer les tags
        tags = self.tag_rules.generate_tags(metadata)
        
        # Afficher les tags
        self.tags_entry.config(state='normal')
        self.tags_entry.delete(0, tk.END)
        self.tags_entry.insert(0, ', '.join(tags))
        self.tags_entry.config(state='readonly')
        
        messagebox.showinfo("Tags g√©n√©r√©s", 
                           f"{len(tags)} tag(s) g√©n√©r√©(s) automatiquement.")
    
    def _on_urls_modified(self):
        """Nettoyage URL + mise √† jour compteur."""
        self._clean_urls()
        self._refresh_url_count()

    def _open_url_validator(self):
        """Ouvre le validateur d'URL (int√®gre URLValidatorWidget)."""
        # reuse configuration if available
        db_path = getattr(self, 'config', {}).get('database_path', '') if hasattr(self, 'config') else ''
        stash_url = getattr(self, 'config', {}).get('stash_url', 'http://localhost:9999') if hasattr(self, 'config') else 'http://localhost:9999'
        try:
            widget = URLValidatorWidget(self, db_path=db_path, stash_url=stash_url)
            widget.show()
        except Exception as e:
            messagebox.showerror("Validation URLs", f"Impossible d'ouvrir le validateur : {e}")

    def _validate_urls(self):
        """Simple in-place URL validation that colours each line based on
        HTTP status and removes dead/error entries.
        """
        from services.url_validator import URLStatus, URLValidator
        from utils.url_utils import clean_urls_list, filter_live_urls
        text = self.urls_text.get('1.0', tk.END).strip()
        if not text:
            return
        urls = [u for u in text.split('\n') if u.strip()]
        validator = URLValidator(timeout=5)
        entries = [{"url": u, "performer_id": 0, "name": "Import", "position": i} for i, u in enumerate(urls)]
        results = validator.validate_urls(entries)
        live_urls = filter_live_urls(urls, results)
        for i, res in enumerate(results):
            status = res.status
            tag = "url_ok"
            if status in (URLStatus.DEAD, URLStatus.ERROR):
                tag = "url_error"
            elif status == URLStatus.REDIRECT:
                tag = "url_warning"
            line_start = f"{i+1}.0"
            line_end = f"{i+1}.end"
            self.urls_text.tag_add(tag, line_start, line_end)
            self.urls_text.tag_config("url_ok", foreground="green")
            self.urls_text.tag_config("url_error", foreground="red")
            self.urls_text.tag_config("url_warning", foreground="orange")
        if live_urls != urls:
            # rewrite cleaned list
            self.urls_text.delete('1.0', tk.END)
            self.urls_text.insert('1.0', "\n".join(clean_urls_list(live_urls)))
    
    def _start_scraping(self):
        """Lance le flux de scraping complet"""
        messagebox.showinfo("Scraping", 
                           "Fonction de scraping √† impl√©menter...")
    
    def _open_trivia_awards(self):
        """Ouvre la fen√™tre Trivia & Awards"""
        # R√©cup√©rer le nom et les URLs
        performer_name = self.metadata_entries['name'].get()
        if not performer_name:
            messagebox.showwarning("Attention", "Veuillez entrer un nom de performer.")
            return
        
        urls_text = self.urls_text.get('1.0', tk.END).strip()
        urls = [url.strip() for url in urls_text.split('\n') if url.strip()]
        
        if not urls:
            messagebox.showwarning("Attention", "Veuillez entrer au moins une URL.")
            return
        
        # Ouvrir la fen√™tre
        window = TriviaAwardsWindow(self, performer_name, urls)
        self.wait_window(window)
        
        # R√©cup√©rer les donn√©es
        data = window.get_data()
        # Stocker dans les m√©tadonn√©es
        self.metadata['trivia'] = data.get('trivia', '')
        self.metadata['awards'] = data.get('awards', '')
        self.metadata['bio_raw'] = data.get('bio_raw', self.metadata.get('bio_raw', ''))
        # refresh raw content display if already visible
        self._update_raw_content()
        
        messagebox.showinfo("Trivia & Awards", 
                           "Donn√©es collect√©es avec succ√®s.")
    
    def _open_bio_generator(self):
        """Navigue vers l'onglet Bio et synchronise les slots scrapp√©s."""
        self._update_raw_content()
        try:
            nb = [w for w in self.winfo_children() if isinstance(w, ttk.Notebook)][0]
            for i in range(nb.index('end')):
                if 'Bio' in nb.tab(i, 'text') or 'üìù' in nb.tab(i, 'text'):
                    nb.select(i)
                    break
        except Exception:
            pass
    
    def _new_performer(self):
        """Nouveau performer ‚Äî remet tous les champs √† z√©ro."""
        for entry in self.metadata_entries.values():
            entry.delete(0, tk.END)
        self.piercings_text.delete('1.0', tk.END)
        self.tattoos_text.delete('1.0', tk.END)
        self.urls_text.delete('1.0', tk.END)
        self.tags_entry.config(state='normal')
        self.tags_entry.delete(0, tk.END)
        self.tags_entry.config(state='readonly')
        self.metadata = {}
        if hasattr(self, '_bio_slots'):
            self._bio_slots = ["", "", "", ""]
        for widget in [getattr(self, '_bio_google_text', None),
                       getattr(self, '_bio_ollama_text', None),
                       getattr(self, '_bio_merge_text', None)]:
            if widget:
                widget.delete('1.0', tk.END)
        for lbl in [getattr(self, '_lbl_google_chars', None),
                    getattr(self, '_lbl_ollama_chars', None),
                    getattr(self, '_lbl_merge_chars', None)]:
            if lbl:
                lbl.config(text='Caract√®res : 0')
        for w in [getattr(self, '_bio_raw_text', None),
                  getattr(self, '_bio_trivia_disp', None)]:
            if w:
                w.config(state='normal'); w.delete('1.0', tk.END); w.config(state='disabled')
        self._update_raw_content()
    
    def _open_performer(self):
        """Ouvre un performer existant"""
        messagebox.showinfo("Ouvrir", "Fonction √† impl√©menter...")

    def _select_all(self): pass
    def _select_videos(self): pass
    def _process_next(self): pass
    def _go_back(self): pass
    
    def _save(self):
        """Sauvegarde les donn√©es dans la base Stash, en traduisant trivia/tattoo/piercing."""
        # R√©cup√©rer les m√©tadonn√©es visibles
        urls_field = [u.strip() for u in self.urls_text.get('1.0', tk.END).splitlines() if u.strip()]
        urls_field = clean_urls_list(urls_field)
        # Choisir la meilleure bio : merge > ollama > google
        best_bio = (getattr(self, '_bio_merge_content', '').strip() or
                    (self._bio_slots[3].strip() if hasattr(self, '_bio_slots') and self._bio_slots[3].strip() else '') or
                    (self._bio_slots[2].strip() if hasattr(self, '_bio_slots') else '') or
                    self._bio_google_text.get('1.0', tk.END).strip())
        updates = {
            'name': self.metadata_entries['name'].get(),
            'aliases': self.metadata_entries['aliases'].get(),
            'birthdate': self.metadata_entries['birthdate'].get(),
            'birthplace': self.metadata_entries['country'].get(),
            'ethnicity': self.metadata_entries['ethnicity'].get(),
            'hair_color': self.metadata_entries['hair_color'].get(),
            'measurements': self.metadata_entries['measurements'].get(),
            'height': self.metadata_entries['height'].get(),
            'weight': self.metadata_entries['weight'].get(),
            'tattoos': self.tattoos_text.get('1.0', tk.END).strip(),
            'piercings': self.piercings_text.get('1.0', tk.END).strip(),
            'trivia': self.trivia_text.get('1.0', tk.END).strip(),
            'career_start': self.metadata_entries['career_length'].get(),
            'website': self.metadata.get('official_website', ''),
            'details': best_bio,
        }
        if urls_field:
            updates['urls'] = urls_field
        # traductions FR via BioGenerator
        from services.bio_generator import BioGenerator
        bg = BioGenerator()
        updates['trivia_fr'] = bg.translate_qc(updates.get('trivia', ''), 'Trivia')
        updates['tattoos_fr'] = bg.translate_qc(updates.get('tattoos', ''), 'Tattoos')
        updates['piercings_fr'] = bg.translate_qc(updates.get('piercings', ''), 'Piercings')

        # demander ID Stash pour injection
        pid = simpledialog.askstring("ID Stash", "Entrez l'ID du performer dans Stash :")
        if not pid:
            messagebox.showwarning("Annul√©", "Aucun ID fourni, sauvegarde annul√©e.")
            return

        # effectue l'injection
        from services.database import StashDatabase
        db = StashDatabase("H:/Stash/stash-go.sqlite")
        try:
            db.save_performer_metadata(pid, updates)
            messagebox.showinfo("Sauvegarde", f"Donn√©es sauvegard√©es pour ID {pid}.")
        except Exception as e:
            messagebox.showerror("Erreur", f"√âchec de la sauvegarde : {e}")


def main():
    app = MainWindow()
    app.mainloop()


if __name__ == "__main__":
    main()


============================================================
[114/124] structure_bdd.md
------------------------------------------------------------
# Documentation de la base de donn√©es

Fichier : stash-go.sqlite

## Table `schema_migrations`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| version | uint64 | False | False | None |
| dirty | bool | False | False | None |


## Table `tags`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| name | varchar(255) | False | False | None |
| created_at | datetime | True | False | None |
| updated_at | datetime | True | False | None |
| ignore_auto_tag | boolean | True | False | '0' |
| description | TEXT | False | False | None |
| image_blob | varchar(255) | False | False | None |
| favorite | boolean | True | False | '0' |
| sort_name | varchar(255) | False | False | None |


## Table `sqlite_sequence`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| name |  | False | False | None |
| seq |  | False | False | None |


## Table `performer_stash_ids`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| performer_id | INTEGER | False | False | None |
| endpoint | varchar(255) | False | False | None |
| stash_id | varchar(36) | False | False | None |
| updated_at | datetime | True | False | '1970-01-01T00:00:00Z' |


## Table `studio_stash_ids`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| studio_id | INTEGER | False | False | None |
| endpoint | varchar(255) | False | False | None |
| stash_id | varchar(36) | False | False | None |
| updated_at | datetime | True | False | '1970-01-01T00:00:00Z' |


## Table `tags_relations`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| parent_id | INTEGER | False | True | None |
| child_id | INTEGER | False | True | None |


## Table `folders`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| path | varchar(255) | True | False | None |
| parent_folder_id | INTEGER | False | False | None |
| mod_time | datetime | True | False | None |
| created_at | datetime | True | False | None |
| updated_at | datetime | True | False | None |
| zip_file_id | INTEGER | False | False | None |


## Table `files`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| basename | varchar(255) | True | False | None |
| zip_file_id | INTEGER | False | False | None |
| parent_folder_id | INTEGER | True | False | None |
| size | INTEGER | True | False | None |
| mod_time | datetime | True | False | None |
| created_at | datetime | True | False | None |
| updated_at | datetime | True | False | None |


## Table `files_fingerprints`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| file_id | INTEGER | True | True | None |
| type | varchar(255) | True | True | None |
| fingerprint | BLOB | True | True | None |


## Table `video_files`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| file_id | INTEGER | True | True | None |
| duration | float | True | False | None |
| video_codec | varchar(255) | True | False | None |
| format | varchar(255) | True | False | None |
| audio_codec | varchar(255) | True | False | None |
| width | tinyint | True | False | None |
| height | tinyint | True | False | None |
| frame_rate | float | True | False | None |
| bit_rate | INTEGER | True | False | None |
| interactive | boolean | True | False | '0' |
| interactive_speed | INT | False | False | None |


## Table `video_captions`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| file_id | INTEGER | True | True | None |
| language_code | varchar(255) | True | True | None |
| filename | varchar(255) | True | False | None |
| caption_type | varchar(255) | True | True | None |


## Table `image_files`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| file_id | INTEGER | True | True | None |
| format | varchar(255) | True | False | None |
| width | tinyint | True | False | None |
| height | tinyint | True | False | None |


## Table `images_files`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| image_id | INTEGER | True | True | None |
| file_id | INTEGER | True | True | None |
| primary | boolean | True | False | None |


## Table `galleries_files`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| gallery_id | INTEGER | True | True | None |
| file_id | INTEGER | True | True | None |
| primary | boolean | True | False | None |


## Table `scenes_files`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| scene_id | INTEGER | True | True | None |
| file_id | INTEGER | True | True | None |
| primary | boolean | True | False | None |


## Table `performers_scenes`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| performer_id | INTEGER | False | True | None |
| scene_id | INTEGER | False | True | None |


## Table `scene_markers_tags`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| scene_marker_id | INTEGER | False | True | None |
| tag_id | INTEGER | False | True | None |


## Table `scenes_tags`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| scene_id | INTEGER | False | True | None |
| tag_id | INTEGER | False | True | None |


## Table `groups_scenes`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| group_id | INTEGER | False | True | None |
| scene_id | INTEGER | False | True | None |
| scene_index | tinyint | False | False | None |


## Table `performers_images`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| performer_id | INTEGER | False | True | None |
| image_id | INTEGER | False | True | None |


## Table `images_tags`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| image_id | INTEGER | False | True | None |
| tag_id | INTEGER | False | True | None |


## Table `scene_stash_ids`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| scene_id | INTEGER | True | True | None |
| endpoint | varchar(255) | True | True | None |
| stash_id | varchar(36) | True | False | None |
| updated_at | datetime | True | False | '1970-01-01T00:00:00Z' |


## Table `scenes_galleries`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| scene_id | INTEGER | True | True | None |
| gallery_id | INTEGER | True | True | None |


## Table `galleries_images`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| gallery_id | INTEGER | True | True | None |
| image_id | INTEGER | True | True | None |
| cover | BOOLEAN | True | False | 0 |


## Table `performers_galleries`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| performer_id | INTEGER | True | True | None |
| gallery_id | INTEGER | True | True | None |


## Table `galleries_tags`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| gallery_id | INTEGER | True | True | None |
| tag_id | INTEGER | True | True | None |


## Table `performers_tags`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| performer_id | INTEGER | True | True | None |
| tag_id | INTEGER | True | True | None |


## Table `tag_aliases`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| tag_id | INTEGER | True | True | None |
| alias | varchar(255) | True | True | None |


## Table `studio_aliases`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| studio_id | INTEGER | True | True | None |
| alias | varchar(255) | True | True | None |


## Table `performer_aliases`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| performer_id | INTEGER | True | True | None |
| alias | varchar(255) | True | True | None |


## Table `galleries_chapters`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| title | varchar(255) | True | False | None |
| image_index | INTEGER | True | False | None |
| gallery_id | INTEGER | True | False | None |
| created_at | datetime | True | False | None |
| updated_at | datetime | True | False | None |


## Table `blobs`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| checksum | varchar(255) | True | True | None |
| blob | BLOB | False | False | None |


## Table `scene_urls`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| scene_id | INTEGER | True | True | None |
| position | INTEGER | True | True | None |
| url | varchar(255) | True | True | None |


## Table `scene_markers`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| title | VARCHAR(255) | True | False | None |
| seconds | FLOAT | True | False | None |
| primary_tag_id | INTEGER | True | False | None |
| scene_id | INTEGER | True | False | None |
| created_at | DATETIME | True | False | None |
| updated_at | DATETIME | True | False | None |
| end_seconds | FLOAT | False | False | None |


## Table `studios`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| name | VARCHAR(255) | True | False | None |
| parent_id | INTEGER | False | False | NULL |
| created_at | DATETIME | True | False | None |
| updated_at | DATETIME | True | False | None |
| details | TEXT | False | False | None |
| rating | TINYINT | False | False | None |
| ignore_auto_tag | BOOLEAN | True | False | FALSE |
| image_blob | VARCHAR(255) | False | False | None |
| favorite | boolean | True | False | '0' |


## Table `saved_filters`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| name | varchar(510) | True | False | None |
| mode | varchar(255) | True | False | None |
| find_filter | BLOB | False | False | None |
| object_filter | BLOB | False | False | None |
| ui_options | BLOB | False | False | None |


## Table `image_urls`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| image_id | INTEGER | True | True | None |
| position | INTEGER | True | True | None |
| url | varchar(255) | True | True | None |


## Table `images`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| title | varchar(255) | False | False | None |
| rating | tinyint | False | False | None |
| studio_id | INTEGER | False | False | None |
| o_counter | tinyint | True | False | 0 |
| organized | boolean | True | False | '0' |
| created_at | datetime | True | False | None |
| updated_at | datetime | True | False | None |
| date | date | False | False | None |
| code | TEXT | False | False | None |
| photographer | TEXT | False | False | None |
| details | TEXT | False | False | None |
| date_precision | TINYINT | False | False | None |


## Table `gallery_urls`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| gallery_id | INTEGER | True | True | None |
| position | INTEGER | True | True | None |
| url | varchar(255) | True | True | None |


## Table `galleries`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| folder_id | INTEGER | False | False | None |
| title | varchar(255) | False | False | None |
| date | date | False | False | None |
| details | TEXT | False | False | None |
| studio_id | INTEGER | False | False | None |
| rating | tinyint | False | False | None |
| organized | boolean | True | False | '0' |
| created_at | datetime | True | False | None |
| updated_at | datetime | True | False | None |
| code | TEXT | False | False | None |
| photographer | TEXT | False | False | None |
| date_precision | TINYINT | False | False | None |


## Table `scenes`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| title | varchar(255) | False | False | None |
| details | TEXT | False | False | None |
| date | date | False | False | None |
| rating | tinyint | False | False | None |
| studio_id | INTEGER | False | False | None |
| organized | boolean | True | False | '0' |
| created_at | datetime | True | False | None |
| updated_at | datetime | True | False | None |
| code | TEXT | False | False | None |
| director | TEXT | False | False | None |
| resume_time | float | True | False | 0 |
| play_duration | float | True | False | 0 |
| cover_blob | varchar(255) | False | False | None |
| date_precision | TINYINT | False | False | None |


## Table `group_urls`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| group_id | INTEGER | True | True | None |
| position | INTEGER | True | True | None |
| url | varchar(255) | True | True | None |


## Table `groups`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| name | varchar(255) | True | False | None |
| aliases | varchar(255) | False | False | None |
| duration | INTEGER | False | False | None |
| date | date | False | False | None |
| rating | tinyint | False | False | None |
| studio_id | INTEGER | False | False | None |
| director | varchar(255) | False | False | None |
| description | TEXT | False | False | None |
| created_at | datetime | True | False | None |
| updated_at | datetime | True | False | None |
| front_image_blob | varchar(255) | False | False | None |
| back_image_blob | varchar(255) | False | False | None |
| date_precision | TINYINT | False | False | None |


## Table `groups_tags`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| group_id | INTEGER | True | True | None |
| tag_id | INTEGER | True | True | None |


## Table `performer_urls`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| performer_id | INTEGER | True | True | None |
| position | INTEGER | True | True | None |
| url | varchar(255) | True | True | None |


## Table `performers`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| id | INTEGER | True | True | None |
| name | varchar(255) | True | False | None |
| disambiguation | varchar(255) | False | False | None |
| gender | varchar(20) | False | False | None |
| birthdate | date | False | False | None |
| ethnicity | varchar(255) | False | False | None |
| country | varchar(255) | False | False | None |
| eye_color | varchar(255) | False | False | None |
| height | INT | False | False | None |
| measurements | varchar(255) | False | False | None |
| fake_tits | varchar(255) | False | False | None |
| career_length | varchar(255) | False | False | None |
| tattoos | varchar(255) | False | False | None |
| piercings | varchar(255) | False | False | None |
| favorite | boolean | True | False | '0' |
| created_at | datetime | True | False | None |
| updated_at | datetime | True | False | None |
| details | TEXT | False | False | None |
| death_date | date | False | False | None |
| hair_color | varchar(255) | False | False | None |
| weight | INTEGER | False | False | None |
| rating | tinyint | False | False | None |
| ignore_auto_tag | boolean | True | False | '0' |
| image_blob | varchar(255) | False | False | None |
| penis_length | float | False | False | None |
| circumcised | varchar[10] | False | False | None |
| birthdate_precision | TINYINT | False | False | None |
| death_date_precision | TINYINT | False | False | None |


## Table `studios_tags`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| studio_id | INTEGER | True | True | None |
| tag_id | INTEGER | True | True | None |


## Table `scenes_view_dates`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| scene_id | INTEGER | True | False | None |
| view_date | datetime | True | False | None |


## Table `scenes_o_dates`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| scene_id | INTEGER | True | False | None |
| o_date | datetime | True | False | None |


## Table `groups_relations`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| containing_id | INTEGER | True | True | None |
| sub_id | INTEGER | True | True | None |
| order_index | INTEGER | True | False | None |
| description | varchar(255) | False | False | None |


## Table `performer_custom_fields`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| performer_id | INTEGER | True | True | None |
| field | varchar(64) | True | True | None |
| value | BLOB | True | False | None |


## Table `studio_urls`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| studio_id | INTEGER | True | True | None |
| position | INTEGER | True | True | None |
| url | varchar(255) | True | True | None |


## Table `tag_stash_ids`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| tag_id | INTEGER | False | False | None |
| endpoint | varchar(255) | False | False | None |
| stash_id | varchar(36) | False | False | None |
| updated_at | datetime | True | False | '1970-01-01T00:00:00Z' |


## Table `sqlite_stat1`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| tbl |  | False | False | None |
| idx |  | False | False | None |
| stat |  | False | False | None |


## Table `sqlite_stat4`

| Champ | Type | Not Null | Cl√© primaire | Valeur d√©faut |
|------|------|---------|--------------|----------------|
| tbl |  | False | False | None |
| idx |  | False | False | None |
| neq |  | False | False | None |
| nlt |  | False | False | None |
| ndlt |  | False | False | None |
| sample |  | False | False | None |



============================================================
[115/124] test_gui_load.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
from gui.performer_frame import PerformerFrame

def test_load_performer():
    root = tk.Tk()
    root.title("Test PerformerFrame Load")
    root.geometry("1024x768")
    
    # Dummy Performer Data
    performer_data = {
        "name": "Riley Reid",
        "birthdate": "1991-07-09",
        "country": "United States",
        "urls": [
            "https://twitter.com/RileyReidx3", # Non-priority
            "https://www.iafd.com/person.rme/perfid=RileyReid/gender=f", # Priority 1 (Valid?)
            # Missing FreeOnes
            # Missing TheNude
            # Missing Babepedia
            # Missing Boobpedia
            # Missing XXXBios
        ]
    }
    
    frame = PerformerFrame(root)
    frame.pack(fill="both", expand=True)
    
    # Button to trigger load
    btn = ttk.Button(root, text="Load Performer", command=lambda: frame.load_performer(performer_data))
    btn.pack(pady=10)
    
    root.mainloop()

if __name__ == "__main__":
    test_load_performer()


============================================================
[116/124] test_id145.py
------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
import sqlite3
import os
import sys

# Add project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from gui.performer_frame import PerformerFrame
from services.config_manager import ConfigManager

def get_performer_data(performer_id):
    # Try to find DB
    paths = [
        r"H:\Stash\stash-go.sqlite",
        "data/database.sqlite",
        "stash-go.sqlite"
    ]
    
    db_path = None
    for p in paths:
        if os.path.exists(p):
            db_path = p
            break
            
    if not db_path:
        print("WARNING: Database not found. Using dummy data.")
        return {
            "name": "Test Performer 145",
            "urls": ["https://www.iafd.com/person.rme/perfid=test145/gender=f"]
        }
        
    print(f"Using DB: {db_path}")
    try:
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        cur = conn.cursor()
        cur.execute("SELECT * FROM performers WHERE id = ?", (performer_id,))
        row = cur.fetchone()
        
        if row:
            data = dict(row)
            
            # Fetch URLs from performer_urls table
            cur.execute("SELECT url FROM performer_urls WHERE performer_id = ?", (performer_id,))
            url_rows = cur.fetchall()
            data['urls'] = [r['url'] for r in url_rows]

            conn.close()
            return data
        else:
            print(f"Performer {performer_id} not found in DB.")
            return None
    except Exception as e:
        print(f"DB Error: {e}")
        return None

def main():
    sys.stdout.reconfigure(line_buffering=True)
    print("Starting test_id145...", flush=True)
    performer_id = 145
    print(f"Fetching data for Performer ID: {performer_id}")
    
    data = get_performer_data(performer_id)
    
    if not data:
        print("No data available. Exiting.")
        return

    print(f"Loaded Data: {data.get('name', 'Unknown')}")
    
    root = tk.Tk()
    root.title(f"Test ID {performer_id}")
    root.geometry("1200x800")
    
    frame = PerformerFrame(root, performer_id=str(performer_id))
    frame.pack(fill="both", expand=True)
    
    # Trigger load after mainloop starts
    root.after(100, lambda: frame.load_performer(data))
    
    root.mainloop()

if __name__ == "__main__":
    main()


============================================================
[117/124] test_import.py
------------------------------------------------------------
print("Start imports...")
try:
    import tkinter as tk
    print("tkinter OK")
    import sqlite3
    print("sqlite3 OK")
    import os
    import sys
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from gui.performer_frame import PerformerFrame
    print("PerformerFrame OK")
    from services.config_manager import ConfigManager
    print("ConfigManager OK")
except Exception as e:
    print(f"Import Error: {e}")
print("End imports")


============================================================
[118/124] test_stashmaster.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Tests unitaires pour StashMaster V2
"""

import unittest
import sys
import os

# Ajouter le r√©pertoire parent au path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from stashmaster_unified import TagRulesEngine, AwardsCleaner
from services.scrapers import DataMerger, ScraperOrchestrator


class TestTagRulesEngine(unittest.TestCase):
    """Tests pour le moteur de g√©n√©ration de tags"""
    
    def setUp(self):
        self.engine = TagRulesEngine()
    
    def test_ethnicity_tags(self):
        """Test des tags d'ethnicit√©"""
        # Test Caucasian
        metadata = {'ethnicity': 'Caucasian'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Caucasian', tags)
        self.assertNotIn('Asian', tags)  # ne doit pas matcher partiellement
        
        # Test Latina
        metadata = {'ethnicity': 'Cuban'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Latina', tags)
        
        # Test Asian
        metadata = {'ethnicity': 'Asian'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Asian', tags)
    
    def test_hair_color_tags(self):
        """Test des tags de couleur de cheveux"""
        # Test Blonde
        metadata = {'hair_color': 'Blonde'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Blonde', tags)
        
        # Test Brunette
        metadata = {'hair_color': 'Brown'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Brunette', tags)
        
        # Test Redhead
        metadata = {'hair_color': 'Red'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Redhead', tags)
    
    def test_measurements_tags(self):
        """Test des tags bas√©s sur les mesures"""
        # Test Big Boobs
        metadata = {'measurements': '38DD-27-34'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Big Boobs', tags)
        
        # Test Small Boobs
        metadata = {'measurements': '32A-24-32'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Small Boobs', tags)
    
    def test_piercings_tags(self):
        """Test des tags de piercings"""
        metadata = {'piercings': 'Navel, Tongue'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Pierced', tags)
        
        # Pas de tag si "none"
        metadata = {'piercings': 'none'}
        tags = self.engine.generate_tags(metadata)
        self.assertNotIn('Pierced', tags)
    
    def test_tattoos_tags(self):
        """Test des tags de tattoos"""
        metadata = {'tattoos': 'Lower back, Right shoulder'}
        tags = self.engine.generate_tags(metadata)
        self.assertIn('Tattooed', tags)
        
        # Pas de tag si "none"
        metadata = {'tattoos': 'none'}
        tags = self.engine.generate_tags(metadata)
        self.assertNotIn('Tattooed', tags)
    
    def test_combined_tags(self):
        """Test de la g√©n√©ration de tags combin√©s"""
        metadata = {
            'ethnicity': 'Latina',
            'hair_color': 'Blonde',
            'measurements': '36DD-25-36',
            'piercings': 'Navel',
            'tattoos': 'Lower back',
            'trivia': 'She has a big butt and is also a bimbo.'
        }
        tags = self.engine.generate_tags(metadata)
        
        self.assertIn('Latina', tags)
        self.assertIn('Blonde', tags)
        self.assertIn('Big Boobs', tags)
        self.assertIn('Pierced', tags)
        self.assertIn('Tattooed', tags)
        self.assertIn('BigButt', tags)
        self.assertIn('Bimbo', tags)
        
        # V√©rifier qu'il n'y a pas de doublons
        self.assertEqual(len(tags), len(set(tags)))


class TestBioGenerator(unittest.TestCase):
    """Tests pour g√©n√©rateur de bio (Google)"""

    def setUp(self):
        from services.bio_generator import BioGenerator
        self.bg = BioGenerator()

    def test_career_start_from_length(self):
        metadata = {'career_length': '2007-2025'}
        bio = self.bg.generate_google_bio('Test', metadata)
        self.assertIn('2007', bio)  # should mention 2007 for d√©but

    def test_template_loaded(self):
        # create a temporary template file with a known pattern
        import os
        tmp_path = os.path.join(os.getcwd(), 'BioGooglemodele_template.txt')
        with open(tmp_path, 'w', encoding='utf8') as f:
            f.write('### {performer_name}\nIntro {birthdate}')
        try:
            bio = self.bg.generate_google_bio('X', {'birthdate': '01/01/2000'})
            self.assertTrue(bio.startswith('### X'))
            self.assertIn('Intro 01/01/2000', bio)
        finally:
            os.remove(tmp_path)

class TestAwardsCleaner(unittest.TestCase):
    """Tests pour le nettoyeur d'awards"""
    
    def setUp(self):
        self.cleaner = AwardsCleaner()
    
    def test_clean_simple_awards(self):
        """Test du nettoyage d'awards simples"""
        raw = """
        2012
        Winner: Unsung Starlet of the Year
        2013
        Nominee: Best Boobs
        """
        
        cleaned = self.cleaner.clean_awards(raw)
        
        # V√©rifier la pr√©sence des ann√©es
        self.assertIn('2012', cleaned)
        self.assertIn('2013', cleaned)
        
        # V√©rifier la pr√©sence des awards
        self.assertIn('Winner', cleaned)
        self.assertIn('Nominee', cleaned)
    
    def test_clean_awards_with_ceremony(self):
        """Test du nettoyage avec type de c√©r√©monie"""
        raw = """
        AVN AWARDS
        2012
        Winner: Unsung Starlet of the Year
        XBIZ AWARDS
        2013
        Nominee: Best Performer
        """
        
        cleaned = self.cleaner.clean_awards(raw)
        
        # V√©rifier la pr√©sence des c√©r√©monies
        self.assertIn('AVN AWARDS', cleaned)
        self.assertIn('XBIZ AWARDS', cleaned)
    
    def test_clean_empty_awards(self):
        """Test avec des awards vides"""
        raw = ""
        cleaned = self.cleaner.clean_awards(raw)
        self.assertEqual(cleaned, "")


class TestURLUtils(unittest.TestCase):
    """Tests pour les utilitaires de gestion des URLs"""

    def test_clean_urls_list(self):
        from utils.url_utils import clean_urls_list
        raw = ["http://a.com", "", "http://a.com", " http://b.com ", "http://c.com", "http://c.com"]
        cleaned = clean_urls_list(raw)
        # blanks removed, duplicates collapsed and whitespace trimmed
        self.assertEqual(cleaned, ["http://a.com", "http://b.com", "http://c.com"])

    def test_merge_urls_by_domain(self):
        from utils.url_utils import merge_urls_by_domain
        base = ["http://iafd.com/foo", "http://example.com/page"]
        new = ["http://freeones.com/bar", "http://iafd.com/other", "http://unique.com"]
        merged = merge_urls_by_domain(base, new)
        # iafd should only appear once, base urls preserved first
        self.assertIn("http://iafd.com/foo", merged)
        self.assertNotIn("http://iafd.com/other", merged)
        # freeones should be present and order should place core domains first
        self.assertTrue(merged.index("http://freeones.com/bar") < merged.index("http://example.com/page"))
        # unique domain appended
        self.assertIn("http://unique.com", merged)


class TestDataMerger(unittest.TestCase):
    """Tests pour le fusionneur de donn√©es"""
    
    def setUp(self):
        self.merger = DataMerger()
    
    def test_merge_identical_data(self):
        """Test de fusion de donn√©es identiques"""
        sources = [
            {
                'source': 'iafd',
                'name': 'Bridgette B',
                'ethnicity': 'Caucasian'
            },
            {
                'source': 'freeones',
                'name': 'Bridgette B',
                'ethnicity': 'Caucasian'
            }
        ]
        
        result = self.merger.merge(sources)
        confirmed = result['confirmed']
        conflicts = result['conflicts']
        # Les donn√©es identiques doivent √™tre confirm√©es
        self.assertIn('name', confirmed)
        self.assertEqual(confirmed['name'], 'Bridgette B')
        # Pas de conflits
        self.assertEqual(len(conflicts), 0)
    
    def test_merge_conflicting_data(self):
        """Test de fusion de donn√©es conflictuelles"""
        sources = [
            {
                'source': 'iafd',
                'hair_color': 'Blonde'
            },
            {
                'source': 'freeones',
                'hair_color': 'Brown'
            }
        ]
        
        result = self.merger.merge(sources)
        conflicts = result['conflicts']
        
        # Doit y avoir un conflit
        self.assertIn('hair_color', conflicts)
        self.assertEqual(len(conflicts['hair_color']), 2)
    
    def test_merge_majority_data(self):
        """Test de fusion avec valeur majoritaire"""
        sources = [
            {'source': 'iafd', 'ethnicity': 'Caucasian'},
            {'source': 'freeones', 'ethnicity': 'Caucasian'},
            {'source': 'thenude', 'ethnicity': 'White'}
        ]
        
        result = self.merger.merge(sources)
        confirmed = result['confirmed']
        # Le champ doit √™tre pr√©sent dans les conflits et la valeur retenue doit √™tre prioritaire
        self.assertIn('ethnicity', result['conflicts'])
        self.assertEqual(result['merged']['ethnicity'], 'Caucasian')
    
    def test_merge_unique_data(self):
        """Test de fusion de donn√©es uniques"""
        sources = [
            {
                'source': 'iafd',
                'name': 'Bridgette B',
                'birthdate': 'October 15, 1983'
            },
            {
                'source': 'freeones',
                'name': 'Bridgette B'
            }
        ]
        
        result = self.merger.merge(sources)
        confirmed = result['confirmed']
        # Les donn√©es uniques doivent appara√Ætre dans le r√©sultat fusionn√©
        self.assertIn('birthdate', result['merged'])
        self.assertEqual(result['merged']['birthdate'], 'October 15, 1983')

    def test_alias_dedup_case_insensitive(self):
        """Aliases avec casse diff√©rente ne doivent pas produire de doublons"""
        sources = [
            {'source': 'iafd', 'aliases': ['Bridgette B', 'bridgette b']},
            {'source': 'freeones', 'aliases': ['BRIDGETTE B']}
        ]
        result = self.merger.merge(sources)
        merged_aliases = result['merged'].get('aliases', [])
        # should keep only one version
        self.assertEqual(len(merged_aliases), 1)
        self.assertIn('Bridgette B', merged_aliases)


class TestBabepediaScraper(unittest.TestCase):
    """Tests sp√©cifiques au scraper Babepedia"""

    def test_tattoos_parsing(self):
        html = '''<div class="info-item"><span class="label">Tattoos:</span> Cage thoracique droite (La Jalousie Est Une Maladie),<br>Derriere le cou (Spanish Doll),<br>Sous le nombril</div>'''
        from services.scrapers import BabepediaScraper
        from bs4 import BeautifulSoup
        scraper = BabepediaScraper()
        soup = BeautifulSoup(html, 'html.parser')
        result = scraper._parse(soup, 'https://babepedia.com')
        self.assertIn('tattoos', result)
        self.assertIn('Cage thoracique droite', result['tattoos'])
        self.assertIn('Derriere le cou', result['tattoos'])
        self.assertIn('Sous le nombril', result['tattoos'])

class TestDatabase(unittest.TestCase):
    """Tests pour le service de base de donn√©es SQLite"""

    def setUp(self):
        import tempfile
        from services.database import StashDatabase
        # utiliser un fichier r√©el pour que os.path.exists() soit vrai
        tmp = tempfile.NamedTemporaryFile(suffix='.sqlite', delete=False)
        tmp.close()
        self.tmpfile = tmp.name
        self.db = StashDatabase(self.tmpfile)
        conn = self.db._get_connection()
        cur = conn.cursor()
        # cr√©er sch√©ma minimal
        cur.execute('CREATE TABLE performers (id TEXT PRIMARY KEY, name TEXT)')
        cur.execute('CREATE TABLE performer_custom_fields (performer_id TEXT, field TEXT, value TEXT)')
        conn.commit()

    def tearDown(self):
        # supprimer fichier temporaire
        try:
            os.remove(self.tmpfile)
        except Exception:
            pass

    def test_save_performer_with_custom_fields(self):
        updates = {
            'name': 'Alice',
            'trivia_fr': 'Je suis une star',
            'tattoos_fr': 'Tatouage 1',
            'piercings_fr': 'Piercing 1',
        }
        # ajouter performer basic
        conn = self.db._get_connection()
        conn.cursor().execute("INSERT INTO performers (id,name) VALUES (?,?)", ('123','Alice'))
        conn.commit()
        # appeler m√©thode
        self.db.save_performer_metadata('123', updates)
        cur = conn.cursor()
        cur.execute("SELECT field,value FROM performer_custom_fields WHERE performer_id=?", ('123',))
        rows = {r['field']: r['value'] for r in cur.fetchall()}
        self.assertEqual(rows.get('Trivia FR'), 'Je suis une star')
        self.assertEqual(rows.get('Tattoos FR'), 'Tatouage 1')
        self.assertEqual(rows.get('Piercings FR'), 'Piercing 1')

    def test_thread_local_connection(self):
        """Chaque thread doit obtenir sa propre connexion, non ferm√©e par les autres"""
        import threading
        conns = []
        def worker():
            c = self.db._get_connection()
            conns.append(c)
        threads = [threading.Thread(target=worker) for _ in range(3)]
        for t in threads: t.start()
        for t in threads: t.join()
        # les objets connexion doivent √™tre distincts
        ids = [id(c) for c in conns]
        self.assertEqual(len(set(ids)), len(ids))

class TestScrapers(unittest.TestCase):
    """Tests pour les scrapers (n√©cessitent une connexion internet)"""
    
    def test_iafd_url_detection(self):
        """Test de d√©tection d'URL IAFD"""
        from services.scrapers import ScraperOrchestrator
        
        orchestrator = ScraperOrchestrator()
        url = "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm"
        
        scraper = orchestrator.detect_source(url)
        self.assertIsNotNone(scraper)
        self.assertEqual(type(scraper).__name__, 'IAFDScraper')
    
    def test_freeones_url_detection(self):
        """Test de d√©tection d'URL Freeones"""
        from services.scrapers import ScraperOrchestrator
        
        orchestrator = ScraperOrchestrator()
        url = "https://www.freeones.xxx/bridgette-b"
        
        scraper = orchestrator.detect_source(url)
        self.assertIsNotNone(scraper)
        self.assertEqual(type(scraper).__name__, 'FreeOnesScraper')


def run_tests():
    """Lance tous les tests"""
    # Cr√©er une suite de tests
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()
    
    # Ajouter tous les tests
    suite.addTests(loader.loadTestsFromTestCase(TestTagRulesEngine))
    suite.addTests(loader.loadTestsFromTestCase(TestAwardsCleaner))
    suite.addTests(loader.loadTestsFromTestCase(TestDataMerger))
    suite.addTests(loader.loadTestsFromTestCase(TestScrapers))
    
    # Lancer les tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # Retourner le statut
    return 0 if result.wasSuccessful() else 1


if __name__ == '__main__':
    sys.exit(run_tests())


============================================================
[119/124] utils\awards_cleaner.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AwardsCleaner - Nettoyage et formatage des r√©compenses
"""

import re

class AwardsCleaner:
    """Nettoie et formate les awards pour avoir 1 par ligne"""
    
    @staticmethod
    def clean_awards(raw_awards: str) -> str:
        """Nettoie le texte brut des awards pour avoir un format lisible"""
        if not raw_awards:
            return ""
        
        # Pattern pour d√©tecter les types d'awards
        award_types = ['AVN AWARDS', 'XBIZ AWARDS', 'NIGHTMOVES', 'XRCO AWARDS', 'TEASE AWARDS', 'VENUS AWARDS']
        
        text = raw_awards
        for award_type in award_types:
            # S'assurer qu'il y a un saut de ligne devant le type d'award
            text = re.sub(f'(?i){re.escape(award_type)}', f'\n{award_type}\n', text)
        
        lines = []
        current_year = None
        
        # Diviser en lignes et nettoyer
        for line in text.split('\n'):
            line = line.strip()
            if not line:
                continue
            
            # D√©tecter si c'est une ann√©e isol√©e
            if re.match(r'^\d{4}$', line):
                current_year = line
                lines.append(f'\n{current_year}')
                continue
            
            # D√©tecter si c'est un type d'award (d√©j√† format√© ci-dessus)
            if any(award_type in line.upper() for award_type in award_types):
                lines.append(f'\n{line}')
                continue
            
            # D√©tecter Winner ou Nominee
            if line.startswith('Winner:') or line.startswith('Nominee:'):
                lines.append(f'  {line}')
            elif current_year and not line.startswith(' '):
                lines.append(f'  {line}')
            else:
                lines.append(line)
        
        # Nettoyage final des doubles sauts de lignes
        result = '\n'.join(lines)
        result = re.sub(r'\n{3,}', '\n\n', result)
        return result.strip()


============================================================
[120/124] utils\normalizer.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Normalizer - Utilitaires de normalisation des donn√©es (Pays, Dates, etc.)
"""

import re
from datetime import datetime
from typing import Dict, Optional

# Mapping des codes pays ISO-2 vers les noms complets (Fran√ßais/Anglais selon besoin)
# Ici on utilise les noms anglais car Stash et IAFD sont majoritairement en anglais
COUNTRY_MAP = {
    'AF': 'Afghanistan', 'AX': '√Öland Islands', 'AL': 'Albania', 'DZ': 'Algeria', 'AS': 'American Samoa',
    'AD': 'Andorra', 'AO': 'Angola', 'AI': 'Anguilla', 'AQ': 'Antarctica', 'AG': 'Antigua and Barbuda',
    'AR': 'Argentina', 'AM': 'Armenia', 'AW': 'Aruba', 'AU': 'Australia', 'AT': 'Austria', 'AZ': 'Azerbaijan',
    'BS': 'Bahamas', 'BH': 'Bahrain', 'BD': 'Bangladesh', 'BB': 'Barbados', 'BY': 'Belarus', 'BE': 'Belgium',
    'BZ': 'Belize', 'BJ': 'Benin', 'BM': 'Bermuda', 'BT': 'Bhutan', 'BO': 'Bolivia', 'BA': 'Bosnia and Herzegovina',
    'BW': 'Botswana', 'BV': 'Bouvet Island', 'BR': 'Brazil', 'IO': 'British Indian Ocean Territory',
    'BN': 'Brunei Darussalam', 'BG': 'Bulgaria', 'BF': 'Burkina Faso', 'BI': 'Burundi', 'KH': 'Cambodia',
    'CM': 'Cameroon', 'CA': 'Canada', 'CV': 'Cape Verde', 'KY': 'Cayman Islands', 'CF': 'Central African Republic',
    'TD': 'Chad', 'CL': 'Chile', 'CN': 'China', 'CX': 'Christmas Island', 'CC': 'Cocos (Keeling) Islands',
    'CO': 'Colombia', 'KM': 'Comoros', 'CG': 'Congo', 'CD': 'Congo, Democratic Republic', 'CK': 'Cook Islands',
    'CR': 'Costa Rica', 'CI': 'C√¥te d\'Ivoire', 'HR': 'Croatia', 'CU': 'Cuba', 'CY': 'Cyprus', 'CZ': 'Czech Republic',
    'DK': 'Denmark', 'DJ': 'Djibouti', 'DM': 'Dominica', 'DO': 'Dominican Republic', 'EC': 'Ecuador', 'EG': 'Egypt',
    'SV': 'El Salvador', 'GQ': 'Equatorial Guinea', 'ER': 'Eritrea', 'EE': 'Estonia', 'ET': 'Ethiopia',
    'FK': 'Falkland Islands (Malvinas)', 'FO': 'Faroe Islands', 'FJ': 'Fiji', 'FI': 'Finland', 'FR': 'France',
    'GF': 'French Guiana', 'PF': 'French Polynesia', 'TF': 'French Southern Territories', 'GA': 'Gabon',
    'GM': 'Gambia', 'GE': 'Georgia', 'DE': 'Germany', 'GH': 'Ghana', 'GI': 'Gibraltar', 'GR': 'Greece',
    'GL': 'Greenland', 'GD': 'Grenada', 'GP': 'Guadeloupe', 'GU': 'Guam', 'GT': 'Guatemala', 'GG': 'Guernsey',
    'GN': 'Guinea', 'GW': 'Guinea-Bissau', 'GY': 'Guyana', 'HT': 'Haiti', 'HM': 'Heard Island and McDonald Islands',
    'VA': 'Holy See (Vatican City State)', 'HN': 'Honduras', 'HK': 'Hong Kong', 'HU': 'Hungary', 'IS': 'Iceland',
    'IN': 'India', 'ID': 'Indonesia', 'IR': 'Iran', 'IQ': 'Iraq', 'IE': 'Ireland', 'IM': 'Isle of Man',
    'IL': 'Israel', 'IT': 'Italy', 'JM': 'Jamaica', 'JP': 'Japan', 'JE': 'Jersey', 'JO': 'Jordan', 'KZ': 'Kazakhstan',
    'KE': 'Kenya', 'KI': 'Kiribati', 'KP': 'Korea, Democratic People\'s Republic', 'KR': 'Korea, Republic',
    'KW': 'Kuwait', 'KG': 'Kyrgyzstan', 'LA': 'Lao People\'s Democratic Republic', 'LV': 'Latvia', 'LB': 'Lebanon',
    'LS': 'Lesotho', 'LR': 'Liberia', 'LY': 'Libyan Arab Jamahiriya', 'LI': 'Liechtenstein', 'LT': 'Lithuania',
    'LU': 'Luxembourg', 'MO': 'Macao', 'MK': 'Macedonia', 'MG': 'Madagascar', 'MW': 'Malawi', 'MY': 'Malaysia',
    'MV': 'Maldives', 'ML': 'Mali', 'MT': 'Malta', 'MH': 'Marshall Islands', 'MQ': 'Martinique', 'MR': 'Mauritania',
    'MU': 'Mauritius', 'YT': 'Mayotte', 'MX': 'Mexico', 'FM': 'Micronesia', 'MD': 'Moldova', 'MC': 'Monaco',
    'MN': 'Mongolia', 'ME': 'Montenegro', 'MS': 'Montserrat', 'MA': 'Morocco', 'MZ': 'Mozambique', 'MM': 'Myanmar',
    'NA': 'Namibia', 'NR': 'Nauru', 'NP': 'Nepal', 'NL': 'Netherlands', 'AN': 'Netherlands Antilles',
    'NC': 'New Caledonia', 'NZ': 'New Zealand', 'NI': 'Nicaragua', 'NE': 'Niger', 'NG': 'Nigeria', 'NU': 'Niue',
    'NF': 'Norfolk Island', 'MP': 'Northern Mariana Islands', 'NO': 'Norway', 'OM': 'Oman', 'PK': 'Pakistan',
    'PW': 'Palau', 'PS': 'Palestinian Territory', 'PA': 'Panama', 'PG': 'Papua New Guinea', 'PY': 'Paraguay',
    'PE': 'Peru', 'PH': 'Philippines', 'PN': 'Pitcairn', 'PL': 'Poland', 'PT': 'Portugal', 'PR': 'Puerto Rico',
    'QA': 'Qatar', 'RE': 'R√©union', 'RO': 'Romania', 'RU': 'Russian Federation', 'RW': 'Rwanda', 'SH': 'Saint Helena',
    'KN': 'Saint Kitts and Nevis', 'LC': 'Saint Lucia', 'PM': 'Saint Pierre and Miquelon',
    'VC': 'Saint Vincent and the Grenadines', 'WS': 'Samoa', 'SM': 'San Marino', 'ST': 'Sao Tome and Principe',
    'SA': 'Saudi Arabia', 'SN': 'Senegal', 'RS': 'Serbia', 'SC': 'Seychelles', 'SL': 'Sierra Leone', 'SG': 'Singapore',
    'SK': 'Slovakia', 'SI': 'Slovenia', 'SB': 'Solomon Islands', 'SO': 'Somalia', 'ZA': 'South Africa',
    'GS': 'South Georgia and the South Sandwich Islands', 'ES': 'Spain', 'LK': 'Sri Lanka', 'SD': 'Sudan',
    'SR': 'Suriname', 'SJ': 'Svalbard and Jan Mayen', 'SZ': 'Swaziland', 'SE': 'Sweden', 'CH': 'Switzerland',
    'SY': 'Syrian Arab Republic', 'TW': 'Taiwan', 'TJ': 'Tajikistan', 'TZ': 'Tanzania', 'TH': 'Thailand',
    'TL': 'Timor-Leste', 'TG': ' Togo', 'TK': 'Tokelay', 'TO': 'Tonga', 'TT': 'Trinidad and Tobago', 'TN': 'Tunisia',
    'TR': 'Turkey', 'TM': 'Turkmenistan', 'TC': 'Turks and Caicos Islands', 'TV': 'Tuvalu', 'UG': 'Uganda',
    'UA': 'Ukraine', 'AE': 'United Arab Emirates', 'GB': 'United Kingdom', 'US': 'USA',
    'UM': 'United States Minor Outlying Islands', 'UY': 'Uruguay', 'UZ': 'Uzbekistan', 'VU': 'Vanuatu', 'VE': 'Venezuela',
    'VN': 'Viet Nam', 'VG': 'Virgin Islands, British', 'VI': 'Virgin Islands, U.S.', 'WF': 'Wallis and Futuna',
    'EH': 'Western Sahara', 'YE': 'Yemen', 'ZM': 'Zambia', 'ZW': 'Zimbabwe'
}

def normalize_country(country: str) -> str:
    """Convertit un code pays ISO-2 en nom complet ou nettoie le nom"""
    if not country: return ""
    c = country.strip().upper()
    if c in COUNTRY_MAP:
        return COUNTRY_MAP[c]
    return country.strip()

def normalize_date(date_str: str) -> str:
    """Convertit divers formats de date en YYYY-MM-DD"""
    if not date_str or date_str == "": return ""
    
    # Formats courants: "October 15, 1983" ou "1983-10-15"
    try:
        # Essayer October 15, 1983
        dt = datetime.strptime(date_str, "%B %d, %Y")
        return dt.strftime("%Y-%m-%d")
    except:
        pass
        
    try:
        # Essayer Oct 15, 1983
        dt = datetime.strptime(date_str, "%b %d, %Y")
        return dt.strftime("%Y-%m-%d")
    except:
        pass
        
    # D√©j√† au bon format ou format inconnu
    if re.match(r'\d{4}-\d{2}-\d{2}', date_str):
        return date_str
        
    return date_str


============================================================
[121/124] utils\tag_engine.py
------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TagRulesEngine - Moteur de r√®gles pour la g√©n√©ration de tags
"""

import re
from datetime import datetime
from typing import Dict, List

class TagRulesEngine:
    """Moteur de r√®gles pour g√©n√©rer des tags bas√©s sur les m√©tadonn√©es"""
    
    # Tags AUTORIS√âS (Whitelist)
    WHITELIST = [
        'Colombian', 'Dominican', 'Thai', 'Venezuelan', 'Mexican', 'Bresilian', 
        'Latina', 'Asian', 'Ebony', 'BigBoobs', 'MILF', 'BigButt', 'Bimbo'
    ]
    # Les Hair Color (all) sont √©galement autoris√©s
    
    # Tags INTERDITS (Blacklist)
    BLACKLIST = ['Small Boobs', 'Pierced', 'Tattooed', 'Caucasian']

    @staticmethod
    def generate_tags(metadata: Dict) -> List[str]:
        """G√©n√®re des tags bas√©s sur les m√©tadonn√©es collect√©es"""
        tags = []
        
        # 1. Ethnicit√© & Nationalit√©
        ethnicity = metadata.get('ethnicity', '').lower()
        country = metadata.get('country', '').lower()
        
        # Nationalit√©s sp√©cifiques
        if 'colomb' in country or 'colomb' in ethnicity: tags.append('Colombian')
        if 'dominic' in country or 'dominic' in ethnicity: tags.append('Dominican')
        if 'thai' in country or 'thai' in ethnicity: tags.append('Thai')
        if 'venezue' in country or 'venezue' in ethnicity: tags.append('Venezuelan')
        if 'mexic' in country or 'mexic' in ethnicity: tags.append('Mexican')
        if 'brazil' in country or 'bresil' in country or 'brazil' in ethnicity or 'bresil' in ethnicity: 
            tags.append('Bresilian')
            
        # Groupes ethniques
        # on utilise des fronti√®res de mots pour √©viter les faux-positifs
        if re.search(r'\b(?:latin(?:a)?|cuban)\b', ethnicity):
            tags.append('Latina')
        if re.search(r'\basian\b', ethnicity):
            tags.append('Asian')
        if re.search(r'\b(?:ebony|african)\b', ethnicity):
            tags.append('Ebony')
        
        # 2. Cheveux (Tous autoris√©s par d√©faut)
        hair_color_raw = metadata.get('hair_color', '').lower()
        if 'blonde' in hair_color_raw or 'blond' in hair_color_raw: tags.append('BlondHair')
        elif 'brown' in hair_color_raw or 'brunette' in hair_color_raw: tags.append('BrownHair')
        elif 'black' in hair_color_raw: tags.append('BlackHair')
        elif 'red' in hair_color_raw: tags.append('RedHead')
        elif 'blue' in hair_color_raw: tags.append('BlueHair')
        elif 'green' in hair_color_raw: tags.append('GreenHair')
        elif 'grey' in hair_color_raw or 'gray' in hair_color_raw: tags.append('GreyHair')
        elif 'pink' in hair_color_raw: tags.append('PinkHair')
        elif 'purple' in hair_color_raw: tags.append('PurpleHair')
        elif 'white' in hair_color_raw: tags.append('WhiteHair')
        
        # 3. Mesures (Seins & Fesses)
        measurements = metadata.get('measurements', '')
        if measurements:
            match = re.search(r'(\d+)', measurements)
            if match:
                size = int(match.group(1))
                if size >= 36:
                    tags.append('BigBoobs')
                # 'Small Boobs' est dans la blacklist, on ne l'ajoute plus
        
        # BigButt & Bimbo (Heuristique bas√©e sur Trivia ou Measurements si possible)
        trivia = str(metadata.get('trivia', '')).lower()
        if 'big butt' in trivia or 'bubble butt' in trivia:
            tags.append('BigButt')
        if 'bimbo' in trivia:
            tags.append('Bimbo')
        
        # 5. √Çge de carri√®re
        career_start = metadata.get('career_start', '')
        if career_start:
            try:
                year_str = career_start.split('-')[0]
                year = int(year_str)
                if datetime.now().year - year > 10:
                    tags.append('MILF')
            except:
                pass
        
        # --- FILTRAGE STRICT ---
        # Garder uniquement ce qui est dans la WHITELIST ou qui se termine par 'Hair'
        final_tags = []
        for t in tags:
            if t in TagRulesEngine.WHITELIST or t.endswith('Hair') or t == 'RedHead':
                if t not in TagRulesEngine.BLACKLIST:
                    final_tags.append(t)
        
        return sorted(list(set(final_tags)))


============================================================
[122/124] utils\url_utils.py
------------------------------------------------------------
"""Utilities for handling and normalizing lists of URLs.

This module provides helper functions used by various GUI components and import
scripts. The goal is to centralize logic for cleaning (removing empty lines and
duplicates) as well as merging URL lists while preferring one URL per domain and
optionally ordering core sources first.
"""
import re
from typing import List

CORE_DOMAINS = ["iafd.com", "freeones.com", "thenude.com", "babepedia.com"]


def clean_urls_list(urls: List[str]) -> List[str]:
    """Return a new list containing the given URLs with empties removed and
    duplicates discarded while preserving the original order.

    Leading/trailing whitespace is stripped from each URL. Comparison for
    duplicates is case‚Äësensitive on the full URL string.
    """
    seen = set()
    cleaned: List[str] = []
    for u in urls:
        u = u.strip()
        if not u:
            continue
        if u not in seen:
            seen.add(u)
            cleaned.append(u)
    return cleaned


def _extract_domain(url: str) -> str:
    """Return the domain part of a URL for deduplication purposes."""
    m = re.search(r'https?://(?:www\.)?([^/]+)', url.lower())
    return m.group(1) if m else url.lower()


def merge_urls_by_domain(base_urls: List[str], new_urls: List[str]) -> List[str]:
    """Merge two lists of URLs, keeping at most one URL per domain.

    The original ``base_urls`` are kept first; additional entries from
    ``new_urls`` are appended only if their domain does not already appear in
    ``base_urls``. Finally the combined list is sorted so that well‚Äëknown
    "core" domains appear first (iafd, FreeOnes, etc.).
    """
    final_urls: List[str] = []
    seen_domains = set()

    # preserve base list order
    for url in base_urls:
        domain = _extract_domain(url)
        seen_domains.add(domain)
        final_urls.append(url)

    for url in new_urls:
        domain = _extract_domain(url)
        if domain not in seen_domains:
            final_urls.append(url)
            seen_domains.add(domain)

    # deduplicate full URLs just in case
    # using dict.fromkeys preserves order
    deduped = list(dict.fromkeys(final_urls))

    # sort by core domains order
    def sort_key(u: str) -> int:
        ul = u.lower()
        for i, dom in enumerate(CORE_DOMAINS):
            if dom in ul:
                return i
        return len(CORE_DOMAINS)

    deduped.sort(key=sort_key)
    return deduped


from typing import TYPE_CHECKING

if TYPE_CHECKING:
    # prevent circular import at runtime
    from services.url_validator import URLCheckResult


def filter_live_urls(urls: List[str], results: list) -> List[str]:
    """Return a sublist of ``urls`` keeping only those whose corresponding
    validation results are not DEAD or ERROR.

    ``results`` should be the list returned by ``URLValidator.validate_urls``.
    """
    from services.url_validator import URLStatus
    """Return a sublist of `urls` keeping only those whose corresponding
    validation results are not DEAD or ERROR.

    The function assumes that ``results`` is in the same order as ``urls``
    (as produced by ``URLValidator.validate_urls``).  It is safe to pass the
    original list of URLs again; the helper will simply ignore any entries
    marked dead so the caller can overwrite the widget content.
    """
    from services.url_validator import URLStatus

    live: List[str] = []
    for url, res in zip(urls, results):
        if res.status not in (URLStatus.DEAD, URLStatus.ERROR):
            live.append(url)
    return live


============================================================
[123/124] V2.md
------------------------------------------------------------
\# üéâ StashMaster V2 \- Application Compl√®te Cr√©√©e \!

\#\# üì¶ Ce qui a √©t√© cr√©√©

Votre application \*\*StashMaster V2\*\*

Au demarrage une fenetre  ou ont doit choisir entre Performer, DVD, Scene et fournir un id pour  le performer.

\#\#\# ‚úÖ Application Performer(stashmaster\_unified.py)

\*\*Interface GUI Unifi√©e\*\* fusionnant Phase 1 et Phase 2 :

\#\#\#\# ü™ü Fen√™tre Principale  
\- \*\*3 Onglets\*\* : M√©tadonn√©es, Champs Avanc√©s, Bio  
\- \*\*Champs de base\*\* : Nom, Aliases, Dates, Pays, Ethnicit√©, Cheveux, Yeux, Taille, Poids, Mesures, etc.  
\- \*\*Champs multilignes\*\* : Piercings, Tattoos, URLs  
\- \*\*Tags auto-g√©n√©r√©s\*\* : Bas√©s sur des r√®gles intelligentes  
\- \*\*Compteurs\*\* : Caract√®res pour la bio

\#\#\#\# üéØ Fonctionnalit√©s Cl√©s

1\. \*\*TagRulesEngine\*\*  
   \- ‚úÖ G√©n√©ration automatique de tags (PAS de scraping)  
   \- ‚úÖ R√®gles bas√©es sur : ethnicit√©, cheveux, mesures, √¢ge de carri√®re  
   \- ‚úÖ Tags :Latina, Asian, Ebony, BigButt BigBoobs, MILF

## NaturalHair

**√âtiquettes affili√©es:**[BlackHair](http://localhost:9999/tags/2) [BlondHair](http://localhost:9999/tags/4) [BrownHair](http://localhost:9999/tags/3) [RedHead](http://localhost:9999/tags/5)

## ColoredHair

**√âtiquettes affili√©es:**[BlueHair](http://localhost:9999/tags/10) [GreenHair](http://localhost:9999/tags/9) [GreyHair](http://localhost:9999/tags/13) [PinkHair](http://localhost:9999/tags/8) [PurpleHair](http://localhost:9999/tags/11) [RedHair](http://localhost:9999/tags/7) [WhiteHair](http://localhost:9999/tags/12)

2\. \*\*TriviaAwardsWindow\*\*  
   \- ‚úÖ Fen√™tre d√©di√©e s√©par√©e  
   \- ‚úÖ Section Trivia avec scraping cibl√©  
   \- ‚úÖ Section Awards avec scraping et nettoyage  
   \- ‚úÖ Format structur√© : 1 award par ligne

3\. \*\*BioGenerationWindow\*\*  
   \- ‚úÖ 3 modes de g√©n√©ration :  
     \- Bio Google automatique (3000 caract√®res)  
     \- Bio Ollama avec IA locale  
     \- Bio Ollama avec prompt personnalis√©  
   \- ‚úÖ Champ pour directives pr√©cises  
   \- ‚úÖ Pr√©visualisation avec compteur de caract√®res  
   \- ‚úÖ Bas√© sur le template BioGooglemodele.txt

4\. \*\*AwardsCleaner\*\*  
   \- ‚úÖ Nettoyage intelligent des awards  
   \- ‚úÖ Format : Ann√©e ‚Üí C√©r√©monie ‚Üí Awards (1 par ligne)  
   \- ‚úÖ Distinction Winner/Nominee

\#\#\# ‚úÖ Module de Scraping (scrapers.py)

\*\*5 Scrapers Complets\*\* :

1\. \*\*IAFDScraper\*\* : Scraping depuis IAFD.com  
2\. \*\*FreeonesScraper\*\* : Scraping depuis Freeones.xxx  
3\. \*\*BabepaediaScraper\*\* : Scraping depuis Babepedia.com  
4\. \*\*TheNudeScraper\*\* : Scraping depuis TheNude.com  
5\. \*\*ScraperOrchestrator\*\* : Orchestration multi-sources

\*\*DataMerger\*\* :  
\- ‚úÖ Fusion intelligente de plusieurs sources  
\- ‚úÖ D√©tection des donn√©es confirm√©es (consensus)  
\- ‚úÖ Identification des nouvelles donn√©es (source unique)  
\- ‚úÖ Signalement des conflits (valeurs diff√©rentes)

\#\#\# ‚úÖ Tests Unitaires (test\_stashmaster.py)

Tests complets pour :  
\- ‚úÖ TagRulesEngine (tous les types de tags)  
\- ‚úÖ AwardsCleaner (nettoyage et formatage)  
\- ‚úÖ DataMerger (fusion et conflits)  
\- ‚úÖ Scrapers (d√©tection d'URLs)

\#\#\# ‚úÖ Documentation Compl√®te

1\. \*\*README.md\*\* (10KB)  
   \- Guide complet d'utilisation  
   \- Instructions d'installation  
   \- Workflow d√©taill√©  
   \- R√®gles de tags document√©es  
   \- Format de bio Google  
   \- FAQ

2\. \*\*CHANGELOG.md\*\* (3KB)  
   \- Historique des versions  
   \- Nouvelles fonctionnalit√©s v2.0  
   \- Roadmap pour v2.1 et v2.2

3\. \*\*CONTRIBUTING.md\*\* (9KB)  
   \- Guide pour contributeurs  
   \- Standards de code (PEP 8\)  
   \- Workflow Git  
   \- Documentation des tests

4\. \*\*EXAMPLES.md\*\* (15KB)  
   \- 8 exemples pratiques  
   \- 3 cas d'usage r√©els  
   \- Int√©grations (Stash API)  
   \- Scripts de batch processing

5\. \*\*data/README.md\*\*  
   \- Structure du dossier data  
   \- Format JSON des performers  
   \- Instructions sauvegarde/restauration

\#\#\# ‚úÖ Fichiers de Configuration

1\. \*\*config.json\*\* : Configuration compl√®te  
   \- Scrapers (timeout, retry, user-agent)  
   \- Bio generation (templates, Ollama)  
   \- Tag rules (seuils, mappings)  
   \- Sources (priorit√©s)  
   \- UI (dimensions, th√®me)

2\. \*\*requirements.txt\*\* : D√©pendances Python  
   \- requests  
   \- beautifulsoup4  
   \- lxml

3\. \*\*.gitignore\*\* : Exclusions Git  
   \- Python cache  
   \- Virtual environments  
   \- Data files  
   \- IDE files

\#\#\# ‚úÖ Scripts Utilitaires

1\. \*\*launch.sh\*\* : Script de lancement  
   \- V√©rification de Python  
   \- Installation des d√©pendances  
   \- Cr√©ation des dossiers  
   \- Lancement de l'application

\---

\#\# üöÄ D√©marrage Rapide

\#\#\# 1\. Installation

\`\`\`bash  
\# Extraire l'archive  
unzip stashmaster-v2.zip  
cd stashmaster-v2

\# Installer les d√©pendances  
pip install \-r requirements.txt

\# Ou utiliser le script de lancement  
chmod \+x launch.sh  
./launch.sh  
\`\`\`

\#\#\# 2\. Premier Lancement

\`\`\`bash  
python3 stashmaster\_unified.py  
\`\`\`

\#\#\# 3\. Workflow Complet

1\. \*\*Saisir les URLs\*\* (onglet Champs Avanc√©s)  
   - boutons üßπ Nettoyer & üîó Valider
   - validation automatique √† l'import / modification
2\. \*\*Scraper\*\* (Menu Actions ‚Üí Scraper & Lancer le flux Bio IA)  
3\. \*\*Valider les m√©tadonn√©es\*\* (onglet M√©tadonn√©es)  
4\. \*\*G√©n√©rer les tags\*\* (onglet Champs Avanc√©s ‚Üí bouton G√©n√©rer Tags)  
5\. \*\*Trivia & Awards\*\* (Menu Actions ‚Üí Trivia & Awards...)  
6\. \*\*G√©n√©rer la bio\*\* (Menu Actions ‚Üí G√©n√©rer Bio...)  
7\. \*\*Sauvegarder\*\* (bouton üíæ Sauvegarder)

\---

\#\# üéØ Fonctionnalit√©s Impl√©ment√©es

\#\#\# ‚úÖ Fusionn√© Phase 1 et Phase 2  
\- Une seule GUI au lieu de deux fen√™tres s√©par√©es  
\- Organisation par onglets claire  
\- Workflow simplifi√© et intuitif

\#\#\# ‚úÖ Syst√®me de Tags Intelligent  
\- \*\*AUCUN scraping de tags\*\* depuis les sources  
\- G√©n√©ration automatique bas√©e sur des r√®gles  
\- 11 types de tags diff√©rents  
\- √âlimination automatique des doublons

\#\#\# ‚úÖ Champs Multilignes  
\- Piercings : Descriptions compl√®tes  
\- Tattoos : Descriptions compl√®tes  
\- URLs : Une par ligne, facile √† g√©rer

\#\#\# ‚úÖ Fen√™tre Trivia & Awards D√©di√©e  
\- Interface s√©par√©e pour ne pas encombrer  
\- Scraping cibl√© et efficace  
\- Nettoyage automatique des awards  
\- Format professionnel (1 par ligne)

\#\#\# ‚úÖ G√©n√©ration de Bio Automatique  
\- \*\*Bio Google\*\* : Template professionnel de 3000 caract√®res  
\- \*\*Bio Ollama\*\* : Optionnelle avec IA locale  
\- \*\*Prompt personnalis√©\*\* : Contr√¥le total  
\- Compteur de caract√®res en temps r√©el

\#\#\# ‚úÖ Scraping Multi-Sources  
\- 4 sources support√©es (IAFD, Freeones, Babepedia, TheNude)  
\- Fusion intelligente des donn√©es  
\- D√©tection automatique des conflits  
\- Gestion des erreurs robuste

\#\#\# ‚úÖ Tests Complets  
\- 15+ tests unitaires  
\- Couverture des composants critiques  
\- Scripts de test automatis√©s

\#\#\# ‚úÖ Documentation Exhaustive  
\- 5 fichiers de documentation  
\- 48KB de docs au total  
\- Exemples pratiques  
\- Guides d'utilisation et contribution

\---

\#\# üìä Statistiques du Projet

\- \*\*Lignes de code Python\*\* : \~1,500 lignes  
\- \*\*Fichiers cr√©√©s\*\* : 14 fichiers  
\- \*\*Documentation\*\* : 48 KB (5 fichiers)  
\- \*\*Tests unitaires\*\* : 15+ tests  
\- \*\*Scrapers\*\* : 4 sources support√©es  
\- \*\*Tags automatiques\*\* : 11 types

\---

\#\# üîß Architecture Technique

\#\#\# Modules Principaux

\`\`\`  
MainWindow  
‚îú‚îÄ‚îÄ MetadataTab          \# Onglet m√©tadonn√©es de base  
‚îú‚îÄ‚îÄ AdvancedTab          \# Onglet champs avanc√©s \+ tags  
‚îú‚îÄ‚îÄ BioTab               \# Onglet biographie  
‚îî‚îÄ‚îÄ Toolbar              \# Barre d'outils

TriviaAwardsWindow       \# Fen√™tre d√©di√©e  
‚îú‚îÄ‚îÄ TriviaSection        \# Section trivia  
‚îî‚îÄ‚îÄ AwardsSection        \# Section awards avec nettoyage

BioGenerationWindow      \# Fen√™tre g√©n√©ration bio  
‚îú‚îÄ‚îÄ GoogleBio            \# Template automatique  
‚îú‚îÄ‚îÄ OllamaBio            \# IA locale  
‚îî‚îÄ‚îÄ CustomPrompt         \# Prompt personnalis√©

ScraperOrchestrator      \# Orchestration multi-sources  
‚îú‚îÄ‚îÄ IAFDScraper  
‚îú‚îÄ‚îÄ FreeonesScraper  
‚îú‚îÄ‚îÄ BabepaediaScraper  
‚îú‚îÄ‚îÄ TheNudeScraper  
‚îî‚îÄ‚îÄ DataMerger           \# Fusion intelligente

TagRulesEngine           \# G√©n√©ration de tags  
AwardsCleaner            \# Nettoyage d'awards  
BioGenerator             \# G√©n√©rateur de bio  
\`\`\`

\#\#\# Technologies Utilis√©es

\- \*\*Python 3.8+\*\* : Langage principal  
\- \*\*Tkinter\*\* : Interface graphique  
\- \*\*BeautifulSoup4\*\* : Parsing HTML  
\- \*\*Requests\*\* : Requ√™tes HTTP  
\- \*\*Ollama\*\* : IA locale (optionnel)  
\- \*\*JSON\*\* : Stockage des donn√©es

\---

\#\# üé® Points Forts de l'Impl√©mentation

\#\#\# 1\. Interface Unifi√©e  
‚úÖ \*\*Plus besoin de jongler entre deux fen√™tres\*\*  
\- Tout est accessible depuis une seule interface  
\- Navigation par onglets intuitive  
\- Workflow lin√©aire et clair

\#\#\# 2\. Tags Intelligents  
‚úÖ \*\*Finies les incoh√©rences de tags scrap√©s\*\*  
\- R√®gles pr√©cises et test√©es  
\- G√©n√©ration instantan√©e  
\- Toujours coh√©rents

\#\#\# 3\. Bio Professionnelle  
‚úÖ \*\*Template bas√© sur votre mod√®le Google\*\*  
\- 3000 caract√®res exactement  
\- Structure professionnelle  
\- Sections bien d√©finies  
\- Option IA pour personnalisation

\#\#\# 4\. Awards Propres  
‚úÖ \*\*Format professionnel automatique\*\*  
\- 1 award par ligne  
\- Hi√©rarchie claire : Ann√©e ‚Üí C√©r√©monie ‚Üí Award  
\- Winner/Nominee distingu√©s

\#\#\# 5\. Code Modulaire  
‚úÖ \*\*Architecture propre et extensible\*\*  
\- Classes bien d√©finies  
\- S√©paration des responsabilit√©s  
\- Facile √† maintenir et √©tendre  
\- Tests unitaires complets

\---

\#\# üöÄ Pr√™t √† l'Emploi \!

Votre application est \*\*100% fonctionnelle\*\* et pr√™te √† √™tre utilis√©e :

1\. ‚úÖ \*\*Interface compl√®te\*\* fusionnant Phase 1 et 2  
2\. ‚úÖ \*\*Tags automatiques\*\* selon vos r√®gles  
3\. ‚úÖ \*\*Champs multilignes\*\* pour Piercings, Tattoos, URLs  
4\. ‚úÖ \*\*Fen√™tre d√©di√©e\*\* pour Trivia & Awards  
5\. ‚úÖ \*\*Bio automatique\*\* Google \+ Ollama optionnel  
6\. ‚úÖ \*\*Scraping multi-sources\*\* avec fusion intelligente  
7\. ‚úÖ \*\*Tests unitaires\*\* complets  
8\. ‚úÖ \*\*Documentation exhaustive\*\* avec exemples

\---

\#\# üìû Support

\- Consultez le \*\*README.md\*\* pour le guide complet  
\- Lisez \*\*EXAMPLES.md\*\* pour des cas d'usage pratiques  
\- V√©rifiez \*\*CHANGELOG.md\*\* pour les nouvelles fonctionnalit√©s  
\- R√©f√©rez-vous √† \*\*CONTRIBUTING.md\*\* pour contribuer

\---

\#\# üéØ Prochaines √âtapes

\#\#\# Version 2.1 (Planifi√©e)  
\- Base de donn√©es SQLite  
\- Export vers Stash  
\- Import JSON  
\- Historique des modifications  
\- Undo/Redo

\#\#\# Version 2.2 (En r√©flexion)  
\- Scraping d'images  
\- D√©tection de doublons  
\- API REST  
\- Plugin system

\---

\#\# üéâ F√©licitations \!

Vous disposez maintenant d'une application professionnelle, compl√®te et document√©e pour g√©rer vos m√©tadonn√©es de performers avec :

\- ‚úÖ Interface unifi√©e et intuitive  
\- ‚úÖ Automatisation intelligente (tags, bio)  
\- ‚úÖ Scraping multi-sources robuste  
\- ‚úÖ Documentation compl√®te  
\- ‚úÖ Tests unitaires  
\- ‚úÖ Code propre et modulaire

\*\*Bon usage de StashMaster V2 \!\*\* üöÄ

\---

\*\*Version\*\* : 2.0.0    
\*\*Date de cr√©ation\*\* : 25 F√©vrier 2026    
\*\*Fichiers\*\* : 14    
\*\*Lignes de code\*\* : \~1,500    
\*\*Documentation\*\* : 48 KB

============================================================
[124/124] verify_stashmaster.py
------------------------------------------------------------
import sys
import os

# Ajouter le dossier courant au path pour les imports
sys.path.append(os.getcwd())

try:
    from utils.tag_engine import TagRulesEngine
    from services.bio_generator import BioGenerator
    from services.database import StashDatabase
    print("‚úÖ Imports r√©ussis.")
except ImportError as e:
    print(f"‚ùå Erreur d'import : {e}")
    sys.exit(1)

def test_tags():
    print("\n--- Test Moteur de Tags ---")
    # Test Cas 1: Performer Colombienne avec gros seins
    meta1 = {
        'ethnicity': 'Colombian Latina',
        'country': 'Colombia',
        'measurements': '36DDD',
        'hair_color': 'Blonde',
        'career_start': '2010-01-01',
        'trivia': 'She has a big butt.'
    }
    tags1 = TagRulesEngine.generate_tags(meta1)
    print(f"Meta 1 (Colombian/36DDD/Blond/2010) -> {tags1}")
    
    # V√©rifications attendues: Colombian, Latina, BigBoobs, BlondHair, MILF, BigButt
    # Interdits: Caucasian, Small Boobs, Pierced, Tattooed
    
    # Test Cas 2: Caucasian (devrait √™tre filtr√©)
    meta2 = {
        'ethnicity': 'Caucasian',
        'piercings': 'Yes',
        'tattoos': 'Yes',
        'measurements': '32A'
    }
    tags2 = TagRulesEngine.generate_tags(meta2)
    print(f"Meta 2 (Caucasian/Pierced/Tattooed/32A) -> {tags2}")
    # Devrait √™tre vide ou ne contenir que des tags autoris√©s s'il y en avait d'autres.

def test_translation():
    print("\n--- Test Traduction Hybride ---")
    bg = BioGenerator()
    
    # Test simple (Google devrait suffire)
    text = "She is a very popular actress from Medellin."
    translated = bg.translate_hybrid(text, "Trivia")
    print(f"Original: {text}")
    print(f"Traduit: {translated}")

def main():
    test_tags()
    test_translation()

if __name__ == "__main__":
    main()
