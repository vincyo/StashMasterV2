# Exemples d'Utilisation

Ce document contient des exemples pratiques pour utiliser StashMaster V2.

## üìã Exemples Basiques

### Exemple 1 : Scraping Simple

```python
# Dans un script Python
from scrapers import ScraperOrchestrator

# Cr√©er l'orchestrateur
orchestrator = ScraperOrchestrator()

# URLs √† scraper
urls = [
    "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
    "https://www.freeones.xxx/bridgette-b"
]

# Scraper et fusionner
confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)

# Afficher les r√©sultats
print(f"Donn√©es de {num_sources} source(s)")
print("\nConfirm√©es:")
for field, info in confirmed.items():
    print(f"  {field}: {info['value']}")

print("\nConflits:")
for field, values in conflicts.items():
    print(f"  {field}:")
    for v in values:
        print(f"    - {v['value']} ({', '.join(v['sources'])})")
```

### Exemple 2 : G√©n√©ration de Tags

```python
from stashmaster_unified import TagRulesEngine

# Cr√©er le moteur de r√®gles
engine = TagRulesEngine()

# M√©tadonn√©es d'exemple
metadata = {
    'ethnicity': 'Latina',
    'hair_color': 'Blonde',
    'measurements': '36DD-25-36',
    'piercings': 'Navel',
    'tattoos': 'Lower back',
    'career_length': '2007-'
}

# G√©n√©rer les tags
tags = engine.generate_tags(metadata)
print(f"Tags g√©n√©r√©s: {', '.join(tags)}")
# Output: Tags g√©n√©r√©s: Latina, Blonde, Big Boobs, Pierced, Tattooed, MILF
```

### Exemple 3 : Nettoyage d'Awards

```python
from stashmaster_unified import AwardsCleaner

cleaner = AwardsCleaner()

# Awards bruts
raw_awards = """
AVN AWARDS2012Winner: Unsung Starlet of the Year2014Nominee: Unsung Starlet of the Year
2015Nominee: Fan Award: Best Boobs
"""

# Nettoyer
cleaned = cleaner.clean_awards(raw_awards)
print(cleaned)
```

Output:
```
AVN AWARDS

2012
  Winner: Unsung Starlet of the Year

2014
  Nominee: Unsung Starlet of the Year

2015
  Nominee: Fan Award: Best Boobs
```

### Exemple 4 : G√©n√©ration de Bio

```python
from stashmaster_unified import BioGenerator

generator = BioGenerator()

# M√©tadonn√©es du performer
metadata = {
    'name': 'Bridgette B',
    'birthdate': 'October 15, 1983',
    'birthplace': 'Barcelona, Spain',
    'ethnicity': 'Caucasian',
    'hair_color': 'Blonde',
    'measurements': '34DD-27-34',
    'height': '173 cm',
    'weight': '129 lbs',
    'career_start': '2007',
    'aliases': ['Bridget B', 'Bridgette', 'Spanish Doll']
}

# G√©n√©rer bio Google
bio = generator.generate_google_bio('Bridgette B', metadata)
print(f"Bio g√©n√©r√©e ({len(bio)} caract√®res):\n{bio}")
```

## üîß Exemples Avanc√©s

### Exemple 5 : Fusion de Donn√©es Complexes

```python
from scrapers import DataMerger

merger = DataMerger()

# Donn√©es de 3 sources diff√©rentes
sources = [
    {
        'source': 'iafd',
        'name': 'Bridgette B',
        'birthdate': 'October 15, 1983',
        'ethnicity': 'Caucasian',
        'hair_color': 'Blonde',
        'measurements': '34DD-27-34'
    },
    {
        'source': 'freeones',
        'name': 'Bridgette B',
        'birthdate': 'October 15, 1983',
        'ethnicity': 'Caucasian',
        'hair_color': 'Blonde',  # Conflit
        'height': '173 cm'
    },
    {
        'source': 'babepedia',
        'name': 'Bridgette B',
        'birthdate': 'October 15, 1983',
        'ethnicity': 'Caucasian',
        'hair_color': 'Brown',  # Conflit
        'weight': '129 lbs'
    }
]

# Fusionner
confirmed, conflicts = merger.merge_data(sources)

print("=== Donn√©es Confirm√©es ===")
for field, info in confirmed.items():
    sources_str = ', '.join(info['sources'])
    print(f"{field}: {info['value']} ({info['count']} sources: {sources_str})")

print("\n=== Conflits ===")
for field, values in conflicts.items():
    print(f"\n{field}:")
    for v in values:
        print(f"  - {v['value']} ({v['count']} sources: {', '.join(v['sources'])})")
```

### Exemple 6 : Scraping avec Gestion d'Erreurs

```python
from scrapers import IAFDScraper
import requests

scraper = IAFDScraper()

urls = [
    "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
    "https://www.iafd.com/person.rme/perfid=invalid/gender=f/invalid.htm"
]

for url in urls:
    print(f"\nScraping: {url}")
    try:
        data = scraper.scrape_performer(url)
        if data:
            print(f"  ‚úÖ Succ√®s: {data.get('name', 'Unknown')}")
            print(f"  Champs: {len(data)}")
        else:
            print("  ‚ùå √âchec: Aucune donn√©e retourn√©e")
    except requests.RequestException as e:
        print(f"  ‚ùå Erreur r√©seau: {e}")
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
```

### Exemple 7 : Cr√©ation d'un Scraper Personnalis√©

```python
from scrapers import ScraperBase
from typing import Dict

class MonSiteScraper(ScraperBase):
    """Scraper pour mon site personnalis√©"""
    
    def scrape_performer(self, url: str) -> Dict:
        """Scrape un performer depuis mon site"""
        soup = self.get_page(url)
        if not soup:
            return {}
        
        data = {
            'source': 'monsite',
            'url': url
        }
        
        try:
            # Extraire le nom
            name_elem = soup.find('h1', class_='performer-name')
            if name_elem:
                data['name'] = name_elem.text.strip()
            
            # Extraire la date de naissance
            birthday_elem = soup.find('span', class_='birthday')
            if birthday_elem:
                data['birthdate'] = birthday_elem.text.strip()
            
            # Ajouter d'autres extractions...
            
        except Exception as e:
            print(f"Erreur: {e}")
        
        return data

# Utilisation
scraper = MonSiteScraper()
data = scraper.scrape_performer("https://monsite.com/performer/123")
print(data)
```

### Exemple 8 : Int√©gration avec l'Interface

```python
# Dans votre propre script
from stashmaster_unified import MainWindow
import tkinter as tk

# Cr√©er et configurer la fen√™tre
app = MainWindow()

# Pr√©-remplir des donn√©es (exemple)
app.metadata_entries['name'].insert(0, "Bridgette B")
app.urls_text.insert('1.0', "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm")

# Lancer l'application
app.mainloop()
```

## üéì Cas d'Usage R√©els

### Cas 1 : Workflow Complet Automatis√©

```python
#!/usr/bin/env python3
"""
Workflow automatis√© complet pour un performer
"""

from scrapers import ScraperOrchestrator
from stashmaster_unified import TagRulesEngine, BioGenerator
import json

def process_performer(name: str, urls: list) -> dict:
    """Traite compl√®tement un performer"""
    print(f"\n{'='*50}")
    print(f"Traitement de: {name}")
    print('='*50)
    
    # 1. Scraping
    print("\n1. Scraping des sources...")
    orchestrator = ScraperOrchestrator()
    confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)
    print(f"   ‚úÖ {num_sources} source(s) scrap√©e(s)")
    print(f"   ‚úÖ {len(confirmed)} champ(s) confirm√©(s)")
    print(f"   ‚ö†Ô∏è  {len(conflicts)} conflit(s)")
    
    # 2. Pr√©parer les m√©tadonn√©es
    print("\n2. Pr√©paration des m√©tadonn√©es...")
    metadata = {key: info['value'] for key, info in confirmed.items()}
    metadata['name'] = name
    
    # 3. G√©n√©rer les tags
    print("\n3. G√©n√©ration des tags...")
    tag_engine = TagRulesEngine()
    tags = tag_engine.generate_tags(metadata)
    metadata['tags'] = tags
    print(f"   ‚úÖ {len(tags)} tag(s) g√©n√©r√©(s): {', '.join(tags)}")
    
    # 4. G√©n√©rer la bio
    print("\n4. G√©n√©ration de la bio...")
    bio_generator = BioGenerator()
    bio = bio_generator.generate_google_bio(name, metadata)
    metadata['bio'] = bio
    print(f"   ‚úÖ Bio g√©n√©r√©e ({len(bio)} caract√®res)")
    
    # 5. Sauvegarder
    print("\n5. Sauvegarde...")
    filename = f"data/performers/{name.lower().replace(' ', '_')}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"   ‚úÖ Sauvegard√©: {filename}")
    
    return metadata

# Exemple d'utilisation
if __name__ == "__main__":
    performer_data = process_performer(
        name="Bridgette B",
        urls=[
            "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
            "https://www.freeones.xxx/bridgette-b"
        ]
    )
    
    print("\n" + "="*50)
    print("‚úÖ Traitement termin√© avec succ√®s!")
    print("="*50)
```

### Cas 2 : Batch Processing de Plusieurs Performers

```python
#!/usr/bin/env python3
"""
Traitement par lots de plusieurs performers
"""

import json
from pathlib import Path
from scrapers import ScraperOrchestrator
from stashmaster_unified import TagRulesEngine, BioGenerator

def batch_process(performers_file: str):
    """Traite plusieurs performers depuis un fichier JSON"""
    
    # Charger la liste
    with open(performers_file, 'r') as f:
        performers = json.load(f)
    
    print(f"Traitement de {len(performers)} performer(s)...\n")
    
    orchestrator = ScraperOrchestrator()
    tag_engine = TagRulesEngine()
    bio_generator = BioGenerator()
    
    results = {
        'success': [],
        'failed': [],
        'partial': []
    }
    
    for i, performer in enumerate(performers, 1):
        name = performer['name']
        urls = performer['urls']
        
        print(f"\n[{i}/{len(performers)}] {name}")
        print("-" * 40)
        
        try:
            # Scraping
            confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)
            
            if num_sources == 0:
                print("  ‚ùå Aucune source valide")
                results['failed'].append(name)
                continue
            
            # M√©tadonn√©es
            metadata = {key: info['value'] for key, info in confirmed.items()}
            metadata['name'] = name
            
            # Tags
            tags = tag_engine.generate_tags(metadata)
            metadata['tags'] = tags
            
            # Bio
            bio = bio_generator.generate_google_bio(name, metadata)
            metadata['bio'] = bio
            
            # Sauvegarder
            filename = f"data/performers/{name.lower().replace(' ', '_')}.json"
            Path("data/performers").mkdir(parents=True, exist_ok=True)
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            if len(conflicts) > 0:
                print(f"  ‚ö†Ô∏è  Succ√®s partiel ({len(conflicts)} conflits)")
                results['partial'].append(name)
            else:
                print("  ‚úÖ Succ√®s complet")
                results['success'].append(name)
        
        except Exception as e:
            print(f"  ‚ùå Erreur: {e}")
            results['failed'].append(name)
    
    # R√©sum√©
    print("\n" + "="*50)
    print("R√âSUM√â")
    print("="*50)
    print(f"‚úÖ Succ√®s complet: {len(results['success'])}")
    print(f"‚ö†Ô∏è  Succ√®s partiel: {len(results['partial'])}")
    print(f"‚ùå √âchecs: {len(results['failed'])}")
    
    return results

# Exemple d'utilisation
if __name__ == "__main__":
    # Cr√©er un fichier performers_list.json avec:
    # [
    #   {
    #     "name": "Performer 1",
    #     "urls": ["url1", "url2"]
    #   },
    #   ...
    # ]
    
    results = batch_process("performers_list.json")
```

### Cas 3 : Validation et Correction Semi-Automatique

```python
#!/usr/bin/env python3
"""
Validation et correction semi-automatique des donn√©es
"""

from scrapers import ScraperOrchestrator
from stashmaster_unified import TagRulesEngine

def validate_and_correct(urls: list) -> dict:
    """Valide et propose des corrections"""
    
    orchestrator = ScraperOrchestrator()
    confirmed, conflicts, num_sources = orchestrator.scrape_urls(urls)
    
    print("="*50)
    print("VALIDATION DES DONN√âES")
    print("="*50)
    
    # Donn√©es confirm√©es
    print("\n‚úÖ Donn√©es confirm√©es:")
    for field, info in confirmed.items():
        print(f"  {field}: {info['value']}")
        print(f"    Sources: {', '.join(info['sources'])}")
    
    # Conflits √† r√©soudre
    if conflicts:
        print("\n‚ö†Ô∏è  CONFLITS √Ä R√âSOUDRE:")
        corrections = {}
        
        for field, values in conflicts.items():
            print(f"\n  {field}:")
            for i, v in enumerate(values, 1):
                print(f"    [{i}] {v['value']} ({', '.join(v['sources'])})")
            
            # Demander √† l'utilisateur de choisir
            while True:
                choice = input(f"  Choisir [1-{len(values)}] ou [s]kip: ")
                if choice.lower() == 's':
                    break
                try:
                    idx = int(choice) - 1
                    if 0 <= idx < len(values):
                        corrections[field] = values[idx]['value']
                        print(f"    ‚úÖ {field} = {values[idx]['value']}")
                        break
                except ValueError:
                    pass
                print("    ‚ùå Choix invalide")
        
        # Appliquer les corrections
        for field, value in corrections.items():
            confirmed[field] = {
                'value': value,
                'note': 'Corrig√© manuellement'
            }
    
    # R√©sultat final
    final_data = {key: info['value'] for key, info in confirmed.items()}
    
    print("\n" + "="*50)
    print("DONN√âES FINALES")
    print("="*50)
    for field, value in final_data.items():
        print(f"  {field}: {value}")
    
    return final_data

# Exemple
if __name__ == "__main__":
    urls = [
        "https://www.iafd.com/person.rme/perfid=bridgetteb/gender=f/bridgette-b.htm",
        "https://www.freeones.xxx/bridgette-b"
    ]
    
    data = validate_and_correct(urls)
```

## üîó Int√©grations

### Int√©gration avec Stash

```python
import requests
import json

class StashAPI:
    """Client pour l'API Stash"""
    
    def __init__(self, url="http://localhost:9999", api_key=None):
        self.url = url
        self.api_key = api_key
    
    def create_performer(self, performer_data: dict) -> dict:
        """Cr√©e un performer dans Stash"""
        # GraphQL mutation
        mutation = """
        mutation PerformerCreate($input: PerformerCreateInput!) {
          performerCreate(input: $input) {
            id
            name
          }
        }
        """
        
        variables = {
            "input": {
                "name": performer_data.get('name'),
                "birthdate": performer_data.get('birthdate'),
                "ethnicity": performer_data.get('ethnicity'),
                "hair_color": performer_data.get('hair_color'),
                "height": performer_data.get('height'),
                "measurements": performer_data.get('measurements'),
                "tags": performer_data.get('tags', [])
            }
        }
        
        response = requests.post(
            f"{self.url}/graphql",
            json={"query": mutation, "variables": variables}
        )
        
        return response.json()

# Utilisation
stash = StashAPI()
result = stash.create_performer(performer_data)
print(f"Performer cr√©√©: {result}")
```

---

Ces exemples couvrent les cas d'usage les plus courants. Pour plus d'informations, consultez le [README.md](README.md) et la documentation des modules.
